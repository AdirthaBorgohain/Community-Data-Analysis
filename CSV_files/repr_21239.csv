ID_Article,communityId,ID_RelatedVenue,title,year,abstract
2619383,21239,21089,The VerbCorner Project: Findings from Phase 1 of crowd-sourcing a semantic decomposition of verbs,2014,"Any given verb can appear in some syntactic frames (Sally broke the vase, The vase broke) but not others (*Sally broke at the vase, *Sally broke the vase to John). There is now considerable evidence that the syntactic behaviors of some verbs can be predicted by their meanings, and many current theories posit that this is true for most if not all verbs. If true, this fact would have striking implications for theories and models of language acquisition, as well as numerous applications in natural language processing. However, empirical investigations to date have focused on a small number of verbs. We report on early results from VerbCorner, a crowd-sourced project extending this work to a large, representative sample of English verbs."
1410820,21239,9078,An interactive method for curve extraction,2010,"We introduce a curve process framework to solve the challenging problem of curve extraction from “non-traceable” curve groups. We propose a comprehensive curve model, which consists of the geometric, photometric and topological sub-models. Two typical categories of the non-traceable curve groups are considered. First, for the interlaced curves with complex structures, we show how to use the proposed curve model especially the topological sub-model to extract curves from the group. Second, for the non-interlaced but over-dense or faint curves we leverage the curve group pattern priors in addition, and extract the whole pattern in a global optimization. Applications and experiments demonstrate the competence of our models and methods."
1095010,21239,9078,A non-parametric statistics based method for generic curve partition and classification,2010,"Generic shape feature extraction is a challenging task for image and video content analysis. We present a non-parametric statistics based method for extracting generic shape tokens based on a Perceptual Curve Partition and Grouping (PCPG) model. In this PCPG model, each curve is made up of Generic Edge Tokens (GET) connected at Curve Partitioning Points (CPP). The types of GET and CPP provide a set of basic shape descriptors for semantic vocabulary. The new implementation of the PCPG is based on: 1) An arctangent space is employed to signify the evidence of CPPs at pixel-level. 2) The pixels' sequential order is taken as heuristic to establish a bin order preserving arctangent histogram for locating CPPs by examining the continuity of generic feature criteria statistically. 3) A new CPP detection scheme is capable of detecting CPPs and classifying GETs on the fly. Experiments are presented for performance demonstration."
567276,21239,374,MPEG PTY-Marks: Cheap Detection of Embedded Copyright Data in DVD-Video,1998,"In this paper we propose a method to watermark digital video content in such a way that detection in consumer electronics equipment is possible with very little hardware (a few thousand gates). The method proposes to modify the MPEG encoding procedure to choose the so-called Picture Type of video-frames not from a regular sequence but according to a message one would like to transmit. Removal of this embedded message, the PTY-Mark, from the resulting MPEG-stream without jeopardizing video quality is only possible after a complete MPEG decoding and re-encoding cycle. We investigate the modifications to current MPEG encoders which are necessary to accommodate these PTY-Marks. Based on tests we comment on their feasibility. Detection of watermarks without secrets is very reminiscent of public-key cryptography. We discuss this relationship by contrasting PTY-marks with pixel-watermarking."
2872071,21239,9078,Automated prediction of crater degradation degree,2016,A learning system that is able to predict the degradation state of impact craters on optical images is presented in this paper. It is based on the extraction of visual features along the crater rim together with the decision with a SVM classifier. The algorithm achieved a sensitivity of 89% and a specificity of 96% (preserved vs non-preserved) in a dataset of annotated craters from Mars.
2697623,21239,8228,PN-sequence masked spread-spectrum data embedding,2015,"Conventional additive spread-spectrum (SS) data embedding has a dangerous security flaw that unauthorized receivers can blindly extract hidden information without the knowledge of carrier(s). In this paper, pseudo-noise (PN) masking technique is adopted as an efficient security measure against illegitimate data extraction. The proposed PN-sequence masked SS embedding can offer efficient security against current SS embedding analysis without inducing any additional distortion to host nor notable recovery performance loss. To further improve recovery performance, optimal carrier design for PN-masked SS embedding is also developed. With any given host distortion budget, we aim at designing a carrier to maximize the output signal-to-interference-plus-noise ratio (SINR) of the corresponding maximum-SINR linear filter. Then, we present jointly optimal carrier and linear processor designs for PN-masked SS embedding in linearly modified transform domain host data. The extensive experimental studies confirm our analytical performance predictions and illustrate the benefits of the designed PN masked optimal SS embedding."
2863605,21239,9078,Adaptive MD-FEC over multilink video distribution network,2016,"Multiple description coding has been shown to provide flexible and distortion-rate optimal video streaming transmission over lossy links. In this paper, we provide an adaptive MD-FEC algorithm for point-to-multipoint video streaming that approximately minimizes the average distortion of the multicast video. Our new algorithm accomplishes this by adapting both the packet lengths and their number to fit heterogeneous link conditions."
359683,21239,8494,Digital watermarking for object-based compressed video,2001,"This paper describes a watermarking algorithm for MPEG4 object-based compressed video. To be compatible to conventional 8/spl times/8 DCT, MPEG-4 proposed several graylevel padding algorithms for transparent background pixels of object boundary blocks. Though conventional block-based watermarking schemes can be also applicable to these padded boundary blocks, we would like to devise a method capable of embedding data in both the interior and boundary blocks with equal robustness. In this paper, we have considered the so-called re-padding and re-quantization attacks for segmented objects. Experiments show that our algorithm obtains good robustness and is promising for integration into the MPEG-4 codec for versatile applications."
2224722,21239,9078,Multiple description coding for Internet video streaming,2003,"We present a system for video streaming well adapted to the unpredictable and varying nature of Internet. The proposed system uses a superposition of several multiple description coding (MDC) schemes, each with N = 2 descriptions, to reach rate scalability and adaptability to varying channel conditions. Each MDC (N = 2 descriptions), that we will call base MDC has a bit rate and a redundancy associated. The superposition of several base MDC results on a MDC scheme for N > 2 descriptions with different bit rates, allowing rate scalability, for different redundancies. The proposed scheme is well adapted to varying channel conditions. In the proposed method multiple descriptions are generated by the coder and downloaded in the server, leaving to the server the only task to choose sending out the right description at the right time depending of channel conditions (bandwidth and loss rate)."
297773,21239,8228,Multiple Description Coding for Video Streaming over Wireless Networks,2003,"We present a system for video streaming over Internet from a server to a wireless client. We show that this system is well adapted to the unpredictable and varying nature of Internet and is effective in presence of channel failures caused by the mobile communication. The proposed system uses a superposition of several Multiple Description Coding (MDC) schemes, each with N = 2 descriptions, to reach rate scalability and adaptability to varying channel conditions. Each MDC (N = 2 descriptions), that we will call base MDC has associated a bit rate and a redundancy. In [1, 2] we show that the base MDC is well suited for transmission over 3G channels and we present results using UMTS simulator. In the proposed method, multiple descriptions (MD) are generated by the coder and downloaded in the server, leaving to the server the only task to choose sending out the right description at the right time depending of channel conditions (bandwidth and loss rate)."
2979663,21239,9078,Concurrent evolution of pixel predictor and context modeling for image coding,2016,"Lossless image coding process predicts the value of current pixel from previously decoded pixel values. Then the prediction error is classified according to the context model. This classification splits the sources with different distributions and hence reduce the total entropy of the prediction error signals. In the literature, the predictor has been intensively studied. Some evolutionary approaches have been applied to generate a predictor to improve compression performance. However, the context modelling method has not relatively been well studied. We propose and investigate a novel method to automatically obtain evolved pair of pixel predictor and context modeling. Simulation results show 1.32–3.90% bit-rate reduction against the pair of predictor and context modeler of one of the best conventional methods (CALIC). It is also demonstrated that the evolved algorithm's size is more compact than former results. We also found that context modeler is evolved in more complex form than the predictor."
1132736,21239,8806,Experimental study on the impact of robust watermarking on iris recognition accuracy,2010,Watermarking has been suggested as a means to improve security of biometric systems or to add additional functionalities to such systems. We experimentally investigate the impact of applying a set of blind robust watermarking schemes on the recognition performance of two iris recognition algorithms. We find that different watermarking schemes result in a very different amount of impact rendering the choice of a particular watermarking scheme an important issue to be considered in the investigated context.
660482,21239,390,Comparative evaluation of inversion algorithms for magnetic resonance elastography,2002,"Magnetic resonance elastography (MRE) is a phase contrast based MRI imaging technique that can directly visualize and quantitatively measure propagating acoustic shear waves in tissue-like materials subjected to harmonic mechanical excitation. Full 3D complex displacement information can be acquired at MR pixel resolution throughout a 3D volume. This data allows the calculation of local quantitative values of shear modulus and the generation of images that depict tissue elasticity or stiffness. Various inversion algorithms to perform such calculations have been proposed. We discuss the assumptions underlying three such algorithms, and compare their noise sensitivity and resolution on synthetic, phantom and in vivo data sets."
1729271,21239,369,A Frame-Level HEVC Rate Control Algorithm for Videos with Complex Scene over Wireless Network,2014,"In this paper, we propose a novel frame-level rate control algorithm for videos with complex scene in High Efficiency Video Coding(HEVC). To achieve a constant bitrate output and reduce the mismatch ratio of bitrate for time varying wireless links, the relationship of content complexity, QP and bitrate is utilized to adjust bit allocation for each GOP and frame via linear extrapolation. In addition, hierarchical frame structure is also considered to optimize QP selection. Experimental results show that compared with the latest rate control algorithm in JCTVC-K0103, our proposed algorithm can significantly improve the bitrate accurate at comparable coding performance in terms of constant quality and average PSNR, especially for videos with complex scene."
2380572,21239,8494,Video enhancement based on saturation adjustment and contrast enhancement,2008,"We propose a new algorithm to improve the visual appearance of compressed videos, which are derived and reproduced from old films. The new technique is especially useful in handling the digitized old films of digital archives. Two main techniques, saturation adjustment and contrast enhancement, are implemented to improve the chrominance and luminance of video frames on HSL color space. For contrast enhancement, we use weighted histogram separation (WHS) to improve the luminance. Subsequently, the saturation ratio transfer function is proposed to adjust the saturation level of every frame. Furthermore, the new calculation method for the error of inter-frame is adopted to avoid the incorrectly enhanced result and the intensive computation consumed in motion estimation. The experimental results demonstrate that the proposed method outperforms the existing approaches in terms of video quality."
194914,21239,8494,Three-loop temporal interpolation for error concealment of MDC,2006,"Multiple description coding (MDC) can be used as an error resilience (ER) technique for video coding. In case of transmission errors, error concealment can be combined with MDC to reconstruct the lost frame, such that the propagated error to the following frames is reduced. In this paper, we propose a new temporal error concealment method named three-loop temporal interpolation (TLTI). TLTI can be well combined with temporal sub-sampling ER methods, such as MDC and alternative motion-compensated prediction. In the simulation, we compare the performance of TLTI with unidirectional motion compensated temporal interpolation (UMCTI). Both visual and quantitive results show that TLTI can achieve a better video quality than UMCTI."
260526,21239,8228,Watermark detection for video bookmarking using mobile phone camera,2010,"In this paper we investigate a watermarking application for bookmarking of video content using a mobile phone's camera. A content identifier and time-stamp information are embedded in individual video frames and decoded from a single frame captured from a display device, allowing to remember ('bookmark') scenes in the video. We propose a simple watermarking scheme and blind image registration to combat the inherent geometric distortion due to digital/analog conversion. The work-in-progress shows promising results over previous approaches."
2391442,21239,8960,A Non-Parametric Multi-Scale Statistical Model for Natural Images,1998,"The observed distribution of natural images is far from uniform. On the contrary, real images have complex and important structure that can be exploited for image processing, recognition and analysis. There have been many proposed approaches to the principled statistical modeling of images, but each has been limited in either the complexity of the models or the complexity of the images. We present a non-parametric multi-scale statistical model for images that can be used for recognition, image de-noising, and in a generative mode to synthesize high quality textures."
223770,21239,11187,Towards Context-Dependence Eye Movements Prediction in Smart Meeting Rooms,2014,�� �� �� ������������ �������� ���������� �������������� �������� ������� �������������� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ���� ����� ����� ����� ����� ����� ����� ����� ����� ����� ���� ����� ����� ����� ����� ����� ����� ����� ����� ����� ���� ����� ����� ����� ����� ��������� � ���� ��� ���� ��� ���� ��� ���� ��� ���� ��������� ����������� ������������ ������� ����������� ����������� ��������� ��������� ����������� ������������ ������� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ���� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ���� ����� ����� ����� ���� ����� ����� ����� � ���� ��� ���� ��� ���� ��� ���� ��� ��������� ���������� ���������� ���������� ���� �� ��������� ���������� ��������� ���������� ���������� ���� ��
837086,21239,9078,Fuzzy morphology for omnidirectional images,2005,This paper describes morphological tools that have been adapted to omnidirectional catadioptric images.
2665387,21239,9078,Effect of camera-IMU displacement calibration error on tracking performance,2015,"Due to copyright restrictions, the access to the full text of this article is only available via subscription."
824052,21239,9078,Vision-based road detection using road models,2009,Vision-based road detection is very challenging since the road is in an outdoor scenario imaged from a mobile platform.
858076,21239,23735,SIFT-Cloud-Model for object detection and pose estimation with GPGPU acceleration,2010,A function to find objects and to estimate their poses is crucial for a home service robot that works in human living environment.
2533877,21239,9078,How to implement N-dimensional image processing optically,1997,A mapping of N-dimensional images to (N-1)-dimensional ones without loss of information is proposed which allows one to reduce N-dimensional image processing to (N-1)-dimensional one.
1738315,21239,9078,Patch confidence k-nearest neighbors denoising,2010,"Recently, patch-based denoising techniques have proved to be very effective. Indeed, they account for the correlations that exist among patches of natural images."
278660,21239,9078,Very low-bit video coding based on gain-shape VQ and matching pursuits,1999,We show that the techniques of gain-shape VQ can be used in optimizing the dictionary of the matching pursuit codec. Various performance of our method were evaluated.
635682,21239,9078,Nonlinear image restoration by radial basis function networks,1994,We investigate a nonlinear technique for image restoration using radial basis function networks. Computer simulation results indicate this new technique can be very useful. >
2533233,21239,9078,On implementation of adaptive local coordinate transformations in optical image processing,1994,Addresses the problem of performing adaptive local coordinate transformations optically in constant time that increase the capabilities of optical computers in image processing. >
559400,21239,9078,Local Feature Extraction—What Receptive Field Size Should Be Used?,2009,"Biologically inspired hierarchical networks for image processing are based on parallel feature extraction across the image using feature detectors that have a limited Receptive Field (RF). It is, h ..."
518401,21239,9078,Optical signal and image processing: from analog systems to digital pipeline smart pixels,1998,Summary form only given. The author discusses developments in optical signal processing and highlights the many ways in which the technology of optical and electronic signal processing is converging.
2151072,21239,9078,JasPer: a software-based JPEG-2000 codec implementation,2000,A software-based implementation of the image codec specified in the emerging JPEG-2000 standard is discussed. The run-time complexity and coding performance of this implementation are also analyzed.
367269,21239,9078,Application of the cluster approximation for the simultaneous restoration and segmentation of tomographic images,1995,We describe a Bayesian restoration and segmentation algorithm based on a pixel-line Markov random field and using an efficient approximation based on locality of interactions. A medical tomography example is given.
2358355,21239,9078,Automatic meteor detection: an application of Hough transforms,1996,The need for automated detection of meteors in astronomical research is given. The outline of such a system and the use of Hough transforms are described. Preliminary results of the use of these transforms are reported.
2122393,21239,9078,Interleaved detection and tracking of faces in video,2005,In this note it is discussed how face detection and tracking in video can be achieved relying on a detection-tracking loop. Such integrated approach is appealing with respect either to robustness and computational efficiency.
1631370,21239,9836,Digital watermarks: Shedding light on the invisible,1998,"Digital media promise rewards-and risk, chiefly in the potential for unauthorized replication and distribution. In the content protection debate, digital watermarks offer one solution."
276018,21239,9078,Feature selection for semisupervised learning applied to image retrieval,2003,This paper proposes a feature selection method for content based image retrieval. The original method is developed in the context of semisupervised learning. It is illustrated using medical image database for diagnoses purposes.
704137,21239,9616,Secure arrays for digital watermarking,1998,"This paper introduces two-dimensional arrays for watermarking images. Binary, greyscale and colour (3D) arrays are examined. New constructions known as the distinct sums arrays are presented."
2179984,21239,9078,Linear filtering and mathematical morphology on an image: A bridge,2009,"In this paper, we propose to show that a particular fuzzy extension of mathematical morphology coincides with a non-additive extension of linear filtering based on convolution kernels thus bridging the two approaches."
1766846,21239,9078,"Scalable, self-organizing 3D camera network for non-intrusive people tracking and counting",2014,"In this proposal we present a scalable, self-organizing people tracking system for the ICIP 2014 Show&Tell. We describe the people tracking system and show why it is a good candidate for an interactive live demo."
376278,21239,9078,Robust block-based EZW image compression with channel noise optimized rate-distortion functions,1999,"We apply the dynamic bit allocation to the block-based EZW algorithm for robust image compression. To optimize the performance of the bit allocation, the effect of the channel noise is concerned. The robustness of our method was evaluated."
612230,21239,9078,Image segmentation by local entropy methods,1995,"This paper will briefly describe local entropy and local relative entropy thresholding methods and compare them to two studied methods from the literature, those of Kittler and Illingworth (1986) and of Otsu (1979)."
2223089,21239,9078,Jointly optimal video coding and rate control for variable-bit-rate transmission,1998,"We consider the problem of optimal rate-distortion coding of video for variable-bit-rate transmission. We present some important properties of the optimal solution, under some conditions. We also outline a computational algorithm."
676170,21239,9078,FM screen design using DBS algorithm,1996,"We describe an algorithm to design a frequency modulated screen using the direct binary search algorithm. Compared with the direct binary search algorithm itself, we show that we can maintain halftone image quality while significantly reducing the required computation."
280778,21239,9078,Uniqueness of blur measure,2004,"After discussing usual approaches to measuring blur, we show theoretically that there is essentially a unique way to quantify blur by a single number and we confirm the usefulness of that measure by some experiment on a natural image."
838523,21239,9078,Multidimensional Generalized Sampling Theorem for wavelet Based Image Superresolution,2006,"The multidimensional generalized sampling theorem (GST) developed here provides a theoretical framework for wavelet based image superresolution, a topic of interest to the signal and image processing community during the last few years."
2524444,21239,9078,Non linear smoothing method based on the just-noticeable contrast,1998,"The present paper provides a new smoothing method, based on psychophysical phenomena, in which the optimum smoothing parameters are automatically chosen. The method is a nonlinear one and it has some similarities with well-known morphological filters."
842539,21239,9078,A new minimal path selection algorithm for automatic crack detection on pavement images,2014,This paper proposes a new algorithm for crack detection based on the selection of minimal paths. It takes account of both photometric and geometric characteristics and requires few information a priori. It is validated on synthetic and real images.
2524679,21239,9078,Perceptual tracking of edge features,1994,"Presents an approach for extracting edge features directly from grey-level images. The approach is based on the principle of perceptual organization, which simulates to human visual process on edge perception in certain aspects. >"
1930595,21239,9078,Sampling and processing of color signals,1994,The paper discusses the sampling of color spectra and its effect on the accuracy of derived properties such as CIE tristimulus values and color rendering indices. The effect of aliasing and common mathematical operations are discussed. >
1230941,21239,9078,A Fourier interpretation of super-resolution techniques,2005,We are concerned with analysis of linear reconstruction-based super-resolution (SR) techniques. We propose a novel Fourier interpretation of an archetype linear SR algorithm and derive practical results on the expected performances of such techniques.
1127504,21239,9078,Rotations in the Mojette space,2013,"In this paper, we develop an exact, reversible and scale change rotation in the Mojette projection space. The whole process is performed using 1D fast operators and has the advantage to be consistent with standard tomographic geometry."
1492546,21239,9078,Improved road crack detection based on one-class Parzen density estimation and entropy reduction,2010,"A novel unsupervised strategy to detect cracks on flexible road pavement images, acquired by laser imaging systems, is proposed. It explores the UINTA entropy reduction filter in an innovative way."
532195,21239,9078,The extraction and linking of brink points in an image,1999,"In this paper, we present a new 2-D approach to extract brink points in an image. These brink points can efficiently represent the image and can be used for various purposes. These extracted brink points are further linked into brink curves and the B-spline approximation is used to compactly represent these linked curves."
2192344,21239,9078,Features for the reconstruction of shredded notebook paper,2005,"In this paper the selection of suitable content-based features for the reconstruction of shredded notebook paper is examined. An algorithm for the detection of squared paper is proposed, based on the Hough transform. Experimental results show a good performance of the algorithm."
2002606,21239,9078,Hidden signatures in images,1996,An image authentication technique by embedding each image with a signature so as to discourage unauthorized copying is proposed. The proposed technique could actually survive several kinds of image processing and the JPEG lossy compression.
2503838,21239,11529,Fast ad-hoc inverse halftoning using adaptive filtering,1999,We propose a novel fast inverse halftoning technique using an adaptive spatial varying filtering. The proposed algorithm is significantly simpler than most existing algorithm while achieving a PSNR close to that of the set theoretic POCS.
1926038,21239,9078,The steerable pyramid: a flexible architecture for multi-scale derivative computation,1995,We describe an architecture for efficient and accurate linear decomposition of an image into scale and orientation subbands. The basis functions of this decomposition are directional derivative operators of any desired order. We describe the construction and implementation of the transform.
1578596,21239,9078,Scratch detection supported by coherency analysis of motion vector fields,2009,"In this paper, a solution for detecting scratches in old films and videos is presented. The focus is on automation and to enable fast, cost-effective processing of mass archives enabling a later reuse and repurposing."
1882169,21239,9078,Edge detection using generalized root signals of 2-D median filtering,1997,"In this paper, we define generalized real and generalized root signals of two-dimensional median filtering. A new edge detection method based on generalized root signals of two-dimensional median filtering is proposed. Simulation results are given and compared with the Sobel and Laplace edge detectors."
1382984,21239,9078,Imaging system having White-RGB color filter array,2010,We have developed an imaging system comprised of a sensor having White-RGB color filter array and back end image processing chain (IPC) that enables seamless integration with existing imaging solutions.
1954346,21239,9078,Interpolation and extrapolation for accurate color measurement,1995,"A comparison is presented between recommended CIE interpolation and extrapolation methods and those derived from signal processing methods. While the DSP based methods are superior in estimating tri-stimulus values, the difference in CIEL*a*b* values is slight."
1000786,21239,422,Sequential latent semantic indexing,2009,This paper presents a new sequential version of the latent semantic indexing method based on the notion of the relative error of approximation of the matrix of observations. The paper gives theoretical and experimental justification of the effectiveness of the proposed method
2480838,21239,9078,Objective evaluation of noise reduction performance in TV-systems,2002,Several state-of-the-art noise reduction algorithms are ranked using both subjective evaluation and 20 objective measures proposed in the literature. It is found that no regression model based on these objective quality measures can replace subjective assessment.
2328460,21239,9078,Coding of interlaced or progressive video sources: a theoretical analysis,1996,This paper presents a theoretical analysis of the coding efficiency of interlaced and progressive formats for intra and prediction error images. It shows that progressive coding is more effective for lower bitrates and larger motion vectors than interlaced coding.
1836171,21239,9078,Generalized max/median filtering,1997,"Generalized max/median filtering is defined as an extension of max/median filtering. Properties for these extensions are given. The output distribution of generalized max/median filtering with independent but not identical inputs is derived and applied to the special case of regular max/median filtering thereby providing a new result. Based on these distribution results, it is shown that max/median and generalized max/median filtering can preserve image details."
1801740,21239,9078,TCQ subband image coding to exploit dependence,1996,"Trellis coded quantization is used with subband tree and raster scanning patterns for low-rate image coding. Subjective improvements in decoded image quality are observed, compared to scalar quantizer based methods."
1555719,21239,9616,An Iterative Method for Superresolution of Optical Flow Derived by Energy Minimisation,2010,Super resolution is a technique to recover a high resolution image from a low resolution image. We develop a variational super resolution method for the subpixel accurate optical flow computation using variational optimisation. We combine variational super resolution and the variational optical flow computation for the super resolution optical flow computation.
2469110,21239,9078,Fractal coding in an object-based system,1994,Object based coding schemes require some kind of texture coding for the objects. We investigate fractal coding techniques for this purpose. The quadtree structure which is used as a basis for the description of object shape is also used as a framework for the fractal coding. >
629240,21239,9078,"Geometric partial differential equations in image analysis: past, present, and future",1995,"The author discusses the main characteristics of the use of partial differential equations and curve/surface evolution theory in computer vision and image processing. The approach and its main advantages are described, together with a number of examples."
2063726,21239,9078,Color image dehazing using the near-infrared,2009,"In landscape photography, distant objects often appear blurred with a blue color cast, a degradation caused by atmospheric haze. To enhance image contrast, pleasantness and information content, dehazing can be performed."
1863080,21239,9078,Image modeling and restoration through contagion urn schemes,1995,"We introduce a novel class of nonlinear stochastic filters based on contagion urn schemes. These filters which rely on biologically inspired sampling processes, offer good restoration results on heavily corrupted binary images."
1077003,21239,9078,Data hiding of intra prediction information in chroma samples for video compression,2010,"New activities have been recently launched in order to challenge the H.264/AVC standard. Several improvements of this standard are already known, however the targeted 50% bitrate saving for equivalent quality is not yet achieved."
605432,21239,9078,A global entropy criterion for focus tuning in exit wavefunction reconstruction in high resolution electron microscopy,1996,Starting from the physical principles of the dynamical scattering of high energetic electrons in high resolution electron microscopy (HREM) a new criterion is proposed for the determination of the absolute focus of reconstructed exit wave functions.
2203862,21239,9078,Very-low-bit rate coding using matching pursuit and codebook adaptation,2000,We propose a codebook adaptation algorithm for very-low-bit-rate real time video compression based on matching pursuit. We start with an initial dictionary and a real-time adaptive dictionary for video coding the MPEG-4 test sequences.
1935794,21239,9078,On the estimation of the probability distribution of a non stationary source for lossless data compression,1997,"In this work, a method for the estimation of the probabilities of a non stationary source is proposed in order to adapt quickly to the source statistics changes. This estimation is based on the time intervals between occurrences of each symbol. The method is applied to lossless compression of images."
2007426,21239,9078,A simple edge-sensitive image interpolation filter,1996,"A novel scheme for edge-preserving image interpolation is introduced, which is based on the use of a simple nonlinear filter which accurately reconstructs sharp edges. Simulation results show the superior performances of the proposed approach with respect to other interpolation techniques."
1955239,21239,9078,The problem of defining the Fourier transform of a colour image,1998,"A discrete two-dimensional Fourier transform based on quaternion (or hypercomplex) numbers allows colour images to be transformed as a whole, rather than as colour-separated components. The transform is reviewed and its basis functions presented with example images."
2456162,21239,9078,Joint segmentation and motion estimation,1998,"This paper presents an algorithm for image sequence segmentation based on clustering and on motion estimation with the Hough transform. The clustering criterion uses the estimated motion information, simultaneously with intensity and position information. This results in a very robust segmentation and motion estimation."
2464959,21239,9078,Interpolation of the DC component of coded images using a rational filter,1997,"In this paper, the problem of the interpolation of the DC components of JPEG or MPEG-coded images is addressed, and the use of nonlinear interpolators is proposed which are able to accurately reconstruct the images yielding both sharp details and low blocking artifacts."
2456525,21239,9078,On Optimal Watermarking Schemes in Uncertain Gaussian Channels,2007,This paper describes the analytical derivation of a new watermarking algorithm satisfying optimality properties when the distortion of the watermarked signal is caused by a Gaussian process. We also extend previous work under the same assumptions and obtain more general solutions.
1771752,21239,9078,Explore multiple clues for urban images matching,2010,"Many well-known existing image matching methods are based on local texture analysis, and consequently have difficulty handling low-textured 3D objects, such as those man-made buildings and road networks in urban scenes."
2037659,21239,9078,Beyond interpolation: optimal reconstruction by quasi-interpolation,2005,"We investigate the use of quasi-interpolating approximation schemes, to construct an estimate of an unknown function from its given discrete samples. We show theoretically and with practical experiments that such methods perform better than classical interpolation, for the same computation cost."
615529,21239,9078,"A tree structured, wavelet-based stochastic process for fast image processing",1997,"We propose a technique for fast image processing based on predicting the scale-space locations of the large wavelet coefficients. A statistical wavelet-based image model is proposed, and bounds are derived for the fast processing method applied to an image obeying this model."
675002,21239,9078,Complexity-regularized image denoising,1997,We introduce a new complexity regularization method for image denoising and explore the use of sophisticated complexity penalties. We have found improvements of the order of 2 dB in reconstructed image mean-squared error over existing complexity-regularized estimators.
225228,21239,9078,Automatic detection of digital zooms,2004,We propose and evaluate a method to determine whether a given digital image is the result of a digital zoom in or more generally of a linear interpolation. This information is important to some post-processing. It also sheds some light on the actual resolution of some digital cameras.
2342655,21239,9078,Advanced imaging systems curricula at EPFL,1996,This paper describes the current status of the image system engineering curriculum at the Swiss Federal Institute of Technology at Lausanne (Ecole Polytechnique Federale de Lausanne-EPFL). The responsibility of this curriculum is with the Signal Processing Laboratory of EPFL.
1889874,21239,9078,Estimation of optical flow for occlusion using extrapolation,2000,"The accuracy of optical flow estimation is much worth on the occluded and appeared objects. In this paper, we describe an extrapolation method for improving the accuracy of optical flow estimation based on the characteristics of constraint lines in the velocity space and the extraction of the occluded/appeared regions using cluster analysis."
556751,21239,9078,Reconstruction of two-dimensional light intensity distribution from the data of photo-count experiments,1996,An analytical method to recover the two-dimensional probability density function (PDF) of a random light field from the PDF of photon counts is presented. Some illustration of the inversion procedure are presented with application to the problem of light intensity interferometry.
1871216,21239,9078,Towards 3-dimensional optical image processing,1995,A novel approach to 3-D optical image processing is proposed which is based on the discovered commutative properties of halftoning. It is shown that set and morphological operations on 3-D images can be performed optically in constant time that paves the way to optical implementation of 3-D mathematical morphology.
2125869,21239,9078,Critical point detection in fluid flow images,1997,"An algorithm for critical point detection in fluid flow images is presented. It is able to locate occluded and degraded flow structures. Applications of this algorithm to flow imagery are included. A performance analysis of the algorithm is included, and it is compared to existing detectors."
2103828,21239,9078,Improving multiscale recurrent pattern image coding with least-squares prediction mode,2009,"The Multidimensional Multiscale Parser-based (MMP) image coding algorithm, when combined with flexible partitioning and predictive coding techniques (MMP-FP), provides state-of-the-art performance. In this paper we investigate the use of adaptive least-squares prediction in MMP."
489016,21239,9078,Three-dimensional surface reconstruction of optical Lambertian objects using cone-beam tomography,1999,"We develop an approach to the determination of 3-D structure and reflectivity of Lambertian objects using a modification of cone-beam tomography. Like other tomographic imaging methods, our approach is entirely numerical and does not rely on heuristics. Furthermore, it requires only ambient illumination."
2320905,21239,8494,Multipurpose image watermarking in DCT domain using subsampling,2005,"In this paper, a DCT-based multipurpose image watermarking algorithm is proposed. Through adopting dither modulation in subimages gained by subsampling, two independent robust watermarks can be embedded in the original image. Experimental results demonstrate the effectiveness of the proposed algorithm."
276223,21239,9078,Morphological segmentation produces a Voronoi tesselation of the markers,2004,"The construction of a minimum spanning forest on a graph, where the trees are rooted in a predefined set of nodes, is shown to be equivalent to constructing the Voronoi tesselation of the nodes for a lexicographic distance function. The domain of application here is morphological segmentation with markers."
2549381,21239,9078,Color imaging: current trends and beyond,2000,"This paper reviews areas of active research in color image processing. Several open problems are mentioned and future directions for research are suggested. Topics discussed include: color perception, image recording, communication, and reproduction."
2426675,21239,9078,Packet video for heterogeneous networks using CU-SeeMe,1996,"Software based desktop videoconferencing tools are developed to demonstrate techniques necessary for video delivery in heterogeneous packet networks. Pyramidal compression, congestion avoidance, end-to-end delivery, and predictive rate control results are presented."
2494117,21239,9078,Morphological operators for very low bit rate video coding,1996,"This paper deals with the use of some morphological tools for video coding at very low bit rates. Rather than describing a complete coding algorithm, the purpose of this paper is to focus on morphological connected operators and segmentation tools that have proved to be attractive for compression."
2106646,21239,9078,Multi-resolution area matching,2000,"We present a general and robust approach to the problem of close-range partial 3D reconstruction of objects from multi-resolution texture matching. The method is based on the progressive refinement of a parametric surface, which is described using an increasing number of radial functions."
464311,21239,9078,Fast alignment of digital images using a lower bound on an entropy metric,2004,"We propose a registration algorithm based on successively refined quantization and an alignment metric derived from a minimal spanning tree entropy estimate. The metric favors edge alignment, is fast to compute, and compares well in experiments with competing approaches."
2173736,21239,9078,Training-based optimization of weighted order statistic filters under breakdown criteria,1999,A novel optimality criterion in the optimization of weighted order statistic filters is studied. The criterion is based on the concepts of finite sample breakdown point and finite sample breakdown probability which are used as constraints with the more traditional mean absolute error and mean square error criteria.
512004,21239,9078,A diffusion process for wavelet-transform-based image denoising and its application to document image binarization,2003,"To distinguish image features from high level noises, we propose to exploit the spatial correlations between wavelet coefficients by replacing the thresholding process with a diffusion process. The proposed smoothing method is more robust to noise when applied to highly corrupted images."
2448190,21239,9078,A Document Page Classification Algorithm in Copy Pipeline,2007,"This paper describes a real-time, strip-based, low-complexity document page classification algorithm, which can be used as a copy mode selector in the copy pipeline. The benefits of such a copy mode selector include improving copy quality, simplifying user interaction, and increasing copy rate."
1933613,21239,9078,Using the fractal code to watermark images,1998,Our paper presents a watermarking scheme based on an insertion of similarities. In the first part different watermarking techniques are presented and classed. In the second part our scheme is described in its spatial and frequential implantations. Finally the different results and perspectives of the work are outlined.
672131,21239,9078,Optimum color spaces for skin detection,2001,"The objective of this paper is to show that for every color space there exists an optimum skin detector scheme such that the performance of all these skin detectors schemes is the same. To that end, a theoretical proof is provided and experiments are presented which show that the separability of the skin and no skin classes is independent of the color space chosen."
2175961,21239,9078,Measurement of laboratory fire spread experiments by stereovision,2010,"This paper presents a stereovision framework developed to monitor and to measure laboratory fire spreads. New algorithms were developed for the estimation of fire characteristics like position, rate of spread, height, depth and the distance between the fire front and metrological instruments."
1886747,21239,9078,Optimal multiresolution polygonal approximation,2004,We propose optimal and near-optimal algorithm for multiresolution polygonal approximation of digital curves. The solution with minimum number of segments is constructed as the shortest path in a weighted graph where the weights are recursively defined as the number of segments of all embedded layers.
2467192,21239,9078,Efficient image-dependent object shape coding,2002,"We present a new shape coding algorithm, which differs from previous algorithms in that both the coding and decoding are dependent on the image in which the object is defined. This way, the correlation between image and shape is removed and shape coding efficiency is improved on average by 3 times over the state-of-the-art algorithms."
2231017,21239,9078,A training environment for ISE courses,1996,"The availability of more and more powerful computers and the widespread diffusion of multimedia technology calls for a work force with expertise in ISE (Image System Engineering). This work details our teaching experience with Pacco, an object-oriented data processing environment."
2146022,21239,9078,Disparity field and depth map coding for multiview image sequence compression,1996,Methods are proposed for coding of the depth map and disparity fields for stereo or multiview image communication applications. Block-based and wireframe modeling techniques are examined for the coding of isolated depth map information and 2-D and 3-D motion compensation techniques for the coding of depth map sequences are evaluated.
1868761,21239,9078,Enhancement of mammograms: experimental results,1996,"For the purpose of enhancing mammographic images, the authors introduce filters based on anisotropic diffusion, and they investigate their relations with classical methods like median, weighted median and tree-structured filters. The performances of the different techniques are quantitatively evaluated and discussed."
2278935,21239,9078,Optimum Laplacian for digital image processing,1997,"Spatially quantized approximations of the Laplacian used in digital image processing are not rotationally invariant. We examine the anisotropy of 3/spl times/3 Laplacian operators for images quantized in square pixels, and derive, from optimizing criteria, the operator which has the minimum overall anisotropy."
1943680,21239,9078,VQ image coding using sub-vector techniques,2000,A fast codeword search algorithm for vector quantization is presented. Three inequalities based on the characteristics of sums and variances of a vector and its two sub-vectors are introduced to reject a larger number of codewords. Simulation results demonstrate the effectiveness of the proposed algorithm.
1927560,21239,9078,Multichannel distance filters,1994,"Two nonlinear digital filters for multichannel signal processing are presented in this paper. A method for the approximate calculation of output mean and variance for one of these filters is proposed. The results of simulations, showing great advantages of the described filters, are also presented. >"
2489432,21239,9078,"Color edge detector using jointly hue, saturation and intensity",1994,"In the hue saturation intensity (HSI) space, a hue difference taking into account the hue relevance is defined. A hue gradient operator is build up with this hue difference. Two color edge detectors are then proposed. These detectors mix in a different way H,S,I gradients. Experimental results are presented. >"
1062613,21239,9078,Increasing Object Recognition Rate using Reinforced Segmentation,2006,In this paper a new approach to object extraction and recognition based on reinforcement learning is presented. We use this novel idea as a method to optimally segment the image and increase the recognition rate. The success rate is compared with a classical approach. Preliminary results demonstrate increase in recognition rate.
2428983,21239,9078,Systolic array for acceleration of template-based ATR,1997,The performance of Hausdorff distance-based clutter/target discrimination is characterized. This allows selection of detection thresholds and pre-processing parameters to achieve the desired detection performance. An efficient systolic array implementation of Hausdorff distance-based target/clutter discrimination is presented.
622055,21239,9078,Sign language recognition using 3-D Hopfield neural network,1995,"This paper presents a sign language recognition system which consists of three modules: model-based hand tracking, feature extraction, and gesture recognition using a 3-D Hopfield neural network. In the experiments, we illustrate that this system can recognize 15 different gestures accurately."
2210684,21239,9078,Image segmentation via multiresolution diffused expectation-maximisation,2005,"Multiresolution diffused expectation maximisation performs segmentation on vector (e.g. color) images within a multiscale framework; segmentation is carried out via the expectation maximisation algorithm, coupled with anisotropic diffusion on classes, in order to account for the spatial dependencies among pixels."
2065056,21239,9078,Multiscale shape analysis using the continuous wavelet transform,1996,"We present a new approach to shape representation and analysis, which consists in treating the contour of a 2D shape as an 1D signal and analyzing it with the 1D continuous wavelet transform. Application is made to the detection of corners, natural scales and fractal behavior in 2D shapes."
2338026,21239,9078,Complexity-regularized image restoration,1998,We propose the use of complexity regularization in image restoration. This is a flexible estimation method which borrows from previous developments in nonparametric estimation theory. The regularized estimation problem is formulated in the wavelet domain and solved using a computationally efficient multiscale relaxation algorithm.
2362788,21239,9078,Progressive coding of stereo images using wavelets and overlapping blocks,2002,"In this paper, we introduce a stereo image coder, which is based on the discrete wavelet transform. To estimate the disparity, overlapping bilinear windows are used. This combination proves to be a very efficient. Our coder outperforms the best stereo image coders known so far. In addition, it supports the progressive mode."
629142,21239,9078,Inference of directional spatial relationship between points: a probabilistic approach,2001,"This paper develops an evaluation of the position probability of a point C which is known to be in a direction /spl beta/ with respect to a point B, itself in the direction /spl alpha/ with respect to another point A. The obtained results can be used in the problem of inference of directional relationships in the case of spatial reasoning."
2859798,21239,9078,PCA-based adaptive color decorrelation algorithm for HEVC,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2819666,21239,9078,Scale-constrained unsupervised evaluation method for multi-scale image segmentation,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2894114,21239,9078,Alignment of optic nerve head optical coherence tomography B-scans in right and left eyes,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2905496,21239,9078,Active contours driven by time-varying fitting energy,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2899413,21239,9078,Target retrieval in large-scale and high-resolution synthetic aperture radar imagery based on deep learning and multi-scale saliency,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2863339,21239,9078,Feature selection and patch-based segmentation in MRI for prostate radiotherapy,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2828619,21239,9078,An improved GPGPU-Accelerated parallelization for rotation invariant thinning algorithm,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2860311,21239,9078,Hyperspectral material classification under monochromatic and trichromatic sampling rates,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2856825,21239,9078,Low-frequency image noise removal using white noise filter,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2888315,21239,9078,Sea-land segmentation via hierarchical region merging and edge directed graph cut,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2880865,21239,9078,Multicolor removal based on color lines for SFS,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2880731,21239,9078,Tumor segmentation by fusion of MRI images using copula based statistical methods,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2722349,21239,9078,Multi-resolution compressive sensing reconstruction,2016,"Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's Publication Services and Products Board Operations Manual, IEEE has chosen to exclude this article from distribution. We regret any inconvenience."
2123640,21239,9078,Errors in the estimation of gradient direction using IIR and FIR implementations,1995,A comparative study of the performance of several digital image gradient direction estimators is presented. FIR and IIR implementations of gradient direction filters have different experimental gradient direction errors and these are confronted in this paper. Experiments were done under several noise conditions are the results are presented.
300281,21239,9078,Two Markov point processes for simulating line networks,1999,We investigate two Markov object processes to model line networks. The first one is based on region interactions whereas the interactions of the second one are defined w.r.t. the segment intersection points. We use a Metropolis-Hastings-Green scheme to simulate both models.
2445360,21239,9078,Optimal algorithm for convexity measure calculation,2005,"Recently a new convexity measure has been proposed based on approximation of input contour with a convex polygon. In this paper, an optimal algorithm is proposed for the construction of the convex polygon. The introduced algorithm provides exact value of the convexity measure and can therefore be used for evaluation of faster heuristic algorithms."
2389362,21239,9078,Reduction of blocking artifacts in block transformed compressed color images,1998,"We use the information in the chrominance bands to reconstruct color block transformed compressed images. For the luminance and the two chrominance channels, we define a reconstruction problem and show how to estimate the unknown hyperparameters and reconstruct each band automatically. The method is tested on real images."
926315,21239,9078,A new segmentation technique for brain and head from high resolution MR image using unique histogram features,2010,"This paper presents a high resolution MR image acquisition protocol for better visualization of normal gray structures of brain, a unique feature of the histogram of the reconstructed images of this protocol, an automated segmentation algorithm of the head contour and the entire brain by using this feature."
2094063,21239,9078,"JPEG 2000: overview, architecture, and applications",2000,"JPEG 2000 will soon be an international standard for still image compression. This paper describes that standard at a high level, indicates the component pieces which empower the standard, and gives example applications which highlight differences between JPEG 2000 and prior image compression standards."
472135,21239,9078,Optimal recovery approach to image interpolation,2001,"We consider the problem of image interpolation from an adaptive optimal recovery point of view. Many different standard interpolation approaches may be viewed through the prism of optimal recovery. We review some standard image interpolation methods and how they relate to optimal recovery as well as introduce a broader, more general and systematic approach to image interpolation using adaptive optimal recovery."
527401,21239,9078,Switched control grid interpolation for motion compensated video coding,1997,"In this paper, fundamental limitations of the control grid interpolation and block matching schemes for motion compensation are remedied by a novel idea which embodies positive features of both schemes. The enhanced scheme performs both consistently and significantly better than either method does individually."
298880,21239,9078,Image processing tools for the enhancement of concealed weapon detection,1999,A number of technologies are being developed for Concealed Weapon Detection (CWD). Use of appropriate processing techniques will be very important to the success of such technologies. This article describes digital image processing procedures currently being investigated to enhance the detection of weapons concealed underneath clothing.
2272306,21239,9078,Adaptive de-blocking de-ringing post filter,2005,"In this paper an adaptive filter for reducing blocking and ringing artifacts is presented. The solution is designed with consideration of mobile equipment with limited computational power and memory. Also, the solution is computationally scalable if there is limited CPU resources in different user cases."
624171,21239,9078,Marching cubes method with connectivity,1999,"In this paper, we solve the topological problem of isosurfaces generated by the marching cubes method using the approach of combinatorial topology. For each marching cube, we examine the connectivity of polyhedral configuration in the sense of combinatorial topology. For the cubes where the connectivities are not considered, we modify the polyhedral configurations with the connectivity and construct polyhedral isosurfaces with the correct topologies."
277022,21239,9078,Robust extraction of low contrast edges using clustering-based segmentation and refinement,1999,This paper further explores the concept of line-support regions for extracting edges from low contrast images. The current method obtains the support regions by using clustering-based segmentation and refines the support regions by using split and merge techniques. Alternative methods for fitting line to cluster are presented.
2372745,21239,9078,Lossless/lossy progressive coding based on reversible wavelet and lossless multi-channel prediction,1999,"In this report, a digital image data compression method which has two modes-lossless mode and lossy mode-and also progressive functionality is proposed. The coding system can be optimized for an arbitrary input image signal under the lossless/lossy unified coding gain introduced here."
278407,21239,9078,Rubberband: an improved graph search algorithm for interactive object segmentation,2002,We present a novel interactive object segmentation algorithm: Rubberband. Rubberband is motivated by graph search based boundary detection algorithms but has greatly improved robustness and controllability performance with (1) a novel user interface; (2) an improved graph search algorithm; (3) an automatic scale selection algorithm.
2046589,21239,9078,Rate-distortion optimal contour prediction for a region-based coder,1997,In this paper a contour prediction scheme is presented. Its application in a region-based motion compensation algorithm is shown. To achieve optimal performance in a rate-distortion sense the predicted regions are optimized by using a Lagrangian cost function. By using contour prediction the bit rate for motion compensation can be decreased.
2504633,21239,9078,"A multi-fractal formalism for stabilization, object detection and tracking in FLIR sequences",2000,"In this paper, we investigate the problem of stabilization, detection and tracking of moving or stationary objects in a forward-looking infrared (FLIR) sequence. A multi-fractal formalism is proposed for stabilization and activity detection and the method is compared to three other classical techniques in image processing."
2387975,21239,9078,Total variation image restoration: numerical methods and extensions,1997,"We describe some numerical techniques for the total variation image restoration method, namely a primal-dual linearization for the Euler-Lagrange equations and some preconditioning issues. We also highlight extension of this technique to color images, blind deconvolution and the staircasing effect."
1490415,21239,9099,"Robust digital image watermarking using DWT, DFT and quality based average",2001,"This paper presents a digital image watermarking system that complements a wavelet based insertion module, with a resynchronization module, and a method for selecting the watermark using an estimated-quality-based average. The proposed system has been tested with attacks performed by Stirmark, obtaining results of robustness over 90%."
2395208,21239,9078,Adaptive multichannel distance filter,1995,Multichannel distance filters are a class of nonlinear filters suitable for multivariate signal processing. The enhancement of one of those filters is proposed. It is based on local signal variance estimation and appropriate modification of the filter parameter. The experimental results on color images filtering are also presented.
2050780,21239,9078,Morphologically constrained Gibbs random fields,1997,We propose a class of Gibbs random fields which incorporate geometric information into stochastic image modeling by means of morphological constraints. This class is shown to be capable of modeling shapes with given morphological size density. Simulation examples illustrate some model properties.
1869142,21239,9078,Zero and infinity images in multi-scale image fusion,2009,Multiscale image fusion fuses information from two source images at different levels of detail. This paper discusses the limits of multiscale image fusion from an algebraic perspective and discusses the constraints that guide the identification of zero and infinity images and also their impact on fusion algorithms and metrics.
2067593,21239,9078,Image coding with fuzzy region-growing segmentation,1996,"A fuzzy rule-based system is presented that can be used to segment an image completely into disjoint regions. Coding the contours of these regions by an advanced chain code leads to an efficient compression scheme that can be utilized, where high compression rates and an acceptable reconstruction quality are needed."
2282773,21239,9078,Identifying Common Source Digital Camera from Image Pairs,2007,"In this paper, we propose a method for verifying whether two digital images were obtained using the same digital camera. The method uses test statistics derived from a two-channel detector taking as input the noise residuals from both images. It is not assumed that the camera that took the images is available."
2441329,21239,9078,Region-based video coder using the MDL formalism,1998,"In this paper, we present an efficient region-based video coding scheme. The region partitioning is obtained using a minimum description length (MDL) formalism in order to optimize the compression. We compare this approach with results obtained by MPEG and H.261 coders and show the gain obtained by our approach."
1916147,21239,8494,Directional watermarks in images,2005,"A new watermark scheme based on directional filter banks and wavelet subband decomposition techniques is proposed. In order to embed the mark into the directional subband images, a projection classification algorithm is presented. These subband images with a particular directional property can prevent unauthorized use attacks."
2019907,21239,9078,Detection of non-uniform motion in image sequences using a reduced order likelihood ratio test,2000,We present a binary hypothesis test to detect moving pixels in an image sequence that requires less computation than previous methods while maintaining a comparable level of performance. We provide a detailed description of the assumptions required to simplify the test and an example that demonstrates its effectiveness.
2031628,21239,9078,On the design of the bandpass filters in harmonic phase MRI,2000,"In this paper we improve the extraction of harmonic images from tagged MR images of the heart. Using simulations, we study the shapes of the spectra of harmonic images and their relation to the heart motion. The filters used to extract the harmonic images are designed based on the shape of the spectra and tuned using simulations. The design also considers additive noise and interference from other harmonic images."
647190,21239,9078,Face detection for pseudo-semantic labeling in video databases,1999,Pseudo-semantic labeling represents a novel approach for automatic content description of video. This information can be used in the context of a video database to improve browsing and searching. In this paper we describe our work on using face detection techniques for pseudo-semantic labeling. We present our results using a database of MPEG sequences.
2645237,21239,9078,Fluorescence blind structured illumination microscopy: A new reconstruction strategy,2016,"In this communication, a fast reconstruction algorithm is proposed for fluorescence blind structured illumination microscopy (SIM) under the sample positivity constraint. This new algorithm is by far simpler and faster than existing solutions, paving the way to 3D and real-time 2D reconstruction."
2010714,21239,9078,Shape matching using a curvature based polygonal approximation in scale-space,2000,"The emerging MPEG-7 standard demands shape description and shape retrieval techniques. Polygonal approximations of the shape contours give attractive solutions in this domain, because of the description simplicity. This paper introduces a shape matching technique based on the turning function comparison of the shape contour polygonal approximations."
448764,21239,9078,On the partition of binary edge maps as a first step for quantitative quality evaluation,1996,"We present a method to obtain a partition of binary edge maps. The edge maps are partitioned into three subsets of active elements, according to the following classification: matched edge elements, smeared edge elements, and false edge elements. Based on this partition, figures of merit for the quantitative evaluation of edge maps can be derived."
2303126,21239,9078,Cluster-based texture matching for image retrieval,1998,Texture is an important attribute for medical image matching and retrieval. We present a texture matching based on clustering texture features which characterize the texture information. We have implemented this technique and tested it for a medical image database of CT images. The test results show that this method have a high efficiency of retrieval.
1482759,21239,9078,A Measure of Color Sensitivity for Imaging Devices,2006,"We define color sensitivity or effective color depth based on the number of reliably distinguished colors, using ideas from information theory. This figure of merit allows the comparison of different sensors or cameras and we indicate how it can be used both for the design of imaging devices and to optimize their adaptation to the scene."
2262359,21239,9078,Education in image sciences and engineering at the Technion,1996,"The departments of electrical engineering and computer science at Technion offer a broad spectrum of courses in digital and optical signals and imaging systems, and a research environment most suitable for specialization in image system engineering. We describe a few of the courses and discuss some issues related to their teaching."
1806921,21239,9078,A 100 MHz 1920×1080 HD-Photo 20 frames/sec JPEG XR encoder design,2008,"A JPEG XR chip for HD-Photo is implemented with 25 mm 2  area in TSMC 0.18 um CMOS 1P6M technology at 100 MHz. According to the simulation results, the 4:4:4 1920x1080 HD-Photo 20 frames/sec can be encoded smoothly."
2113628,21239,9078,Mixed synchronous-asynchronous approach for real-time image processing: a MPEG-like coder,1996,The basic features of our work in progress that aims at combining the respective advantages of the synchronous and synchronous approach for signal and real-time image processing are illustrated by the way of an example. This example is based on the implementation of the high-level code of a distributed MPEG-like real-time image coder.
2379263,21239,9078,A robust system for lineament analysis of aero-magnetic imagery using orientation analysis and edge linking,1994,"This paper presents a robust system for the detection, enhancement, and symbolic description of linear features in imagery. The system consists of two separate sub-systems: an orientation analysis engine based on the steerable filters, and an edge linking scheme which utilizes local orientation information. >"
2169410,21239,9078,Modified absolute moment block truncation coding,1996,"By merging absolute moment block truncation coding (AMBTC) and block truncation coding (BTC), a new three-moment AMBTC (TAMBTC) is presented. A generalized error analysis is presented to explain why TAMBTC is outperformed by AMBTC. More insights on the aspects of computational issues and preserved moments are also discussed."
586778,21239,9078,A method of linear star-sections applied for object separation in ERCP images,1996,"The aim of the present study is to analyze ERCP images (endoscopic retrograde cholangiopancreatography). This means images of a contrasted pancreatic duct obtained through an X-ray method. More precisely, this involves creating an algorithm performing the first stage of the analysis/separation of the object of interest."
2129032,21239,9078,Spatial entropy: a tool for controlling contextual classification convergence,1994,"A new kind of entropy is proposed, which associates spatial and radiometric properties of images. The possible use of this entropy is shown firstly to measure the effect of picture processing algorithms, then to control the evolution of iterative contextual classification algorithms like Markov random fields. >"
2149467,21239,9078,Fast computation of morphological area pattern spectra,2001,"An area based counterpart of the binary structural opening spectra is developed. It is shown that these area opening and closing spectra can be computed using an adaptation of Tarjan's (1975) union-find algorithm. These spectra provide rotation, translation, and scale invariant pattern vectors for texture analysis."
2346773,21239,9078,Concatenated multiple description coding of frame-rate scalable video,2002,In this paper we present a method for concatenated multiple description (MD) coding of frame-rate scalable video. The proposed method combines domain-based MD coding and FEC-based MD coding. We find that the combined system benefits from both of its components and is significantly better than either of them at higher packet loss rates.
2154702,21239,9078,"A fast, accurate method to segment and retrieve object contours in real images",1996,"We have developed a fast method to segment 2-D grey-level images, obtaining closed-line, continuous, smooth and analytical representations of object contours. The results are very good, even in presence of noisy or low contrast images, and the shape of objects are retrieved with very good accuracy."
1096073,21239,9078,Validation of Mojette reconstruction from Radon acquisitions,2013,Two new methods to perform interpolation mapping from Radon sinogram to Mojette domain are presented. Reconstructions are made from both spaces using FBP and SART algorithms. Assessment of the methods is made both from Shepp-Logan phantom and actual data and demonstrate the efficiency of the proposed algorithms.
2420943,21239,9078,Image information retrieval using rough set theory,2002,We describe a method for automatically generating an image retrieval algorithm corresponding to keywords. The retrieval algorithm that we proposed consists of a scalar quantizer and discriminator and can be generated quickly and automatically by utilizing the idea of rough set theory. We demonstrated through experiments that the proposed method is effective to generate image retrieval algorithms.
2396642,21239,9078,Two channel non-separable 2D subband coding and its optimization,1997,A new lossy/lossless coding of digital image data with two channel non-separable filter bank is proposed. The method has freedom of selecting taps and coefficients of the filters and this freedom is used for maximizing the coding gain. Simulation results are shown to confirm its effectiveness.
425059,21239,9078,Robust motion detector for video surveillance applications,2003,This paper presents a robust motion-detector video sensor. It is intended to operate in surveillance applications for long periods of time with time-varying noise level. It makes use of the fact that whenever there is no motion a similarity measure between frames tends to have similar values.
606560,21239,9078,An optimal single pass SNR scalable video coder,1999,"In this paper, we introduce a new methodology for SNR video scalability which is based on the partitioning of the DCT coefficients. The partitioning is done in a way that is optimal in the rate-distortion sense. The optimization is performed using Lagrangian relaxation and dynamic programming (DP). Experimental results are presented and conclusions are drawn."
932211,21239,9078,Denoising the high dynamic range imaging process: A comparative study using SNR and detail loss criteria,2010,"High dynamic range (HDR) imaging techniques have been applied more and more in recent years. They are mostly based on the basic principle of acquiring differently exposed images of the scene, linearizing each image, and finally combining the resulting exposure set into one HDR image using some weighting scheme."
2067736,21239,9078,Efficient coding of segmentation maps based on MPEG-4 shape coding,1998,"Two approaches to the coding of segmentation maps are proposed. Both use the MPEG-4 binary shape coding algorithm. The first approach does so in a straightforward way, whereas the second is more subtle and relies on the 4-colorability of planar graphs. The two approaches are compared for an automatically segmented video sequence."
2780168,21239,9078,Daala: A perceptually-driven still picture codec,2016,"Daala is a new royalty-free video codec based on perceptually-driven coding techniques. We explore using its keyframe format for still picture coding and show how it has improved over the past year. We believe the technology used in Daala could be the basis of an excellent, royalty-free image format."
684766,21239,9078,Local frequency estimation based on the Wigner distribution,2001,"The asymptotic performance of the local frequency (LF) estimator, based on the Wigner distribution (WD) of multidimensional signals, is considered. The optimal estimation window size, producing minimal mean squared error (MSE), is derived. The results are illustrated and confirmed by numerical examples."
846403,21239,9078,Noise reduction in 3D images using morphological amoebas,2005,This article presents the use of morphological amoebas for the enhancement of 3D medical images. Morphological amoebas are kernels adapting their shape in such a way that they do not cross the contours of the image. They can be used in morphological operations in quite a similar way as classical kernel and are well-fitted for noise-reduction in 3D medical images.
1868417,21239,9078,Design of multistage weighted order statistic filters by a neural network,1995,A genetic back-propagation (BP) algorithm is used to solve the optimal design problem of multistage weighted order statistic filter under the mean absolute error criterion. It is shown experimentally that this algorithm can find the optimal WOS filter in image restoration application.
523233,21239,9078,Segmentation of 3D head MR images using morphological reconstruction under constraints and automatic selection of markers,2001,We propose a morphological approach to segment several structures of 3D head magnetic resonance images dedicated to the construction of individual models of the head for applications where topology is one of the main constraints. The originality of the approach lies in the satisfaction of such constraints and in an effort towards robustness.
2272649,21239,9078,A wavelet-domain algorithm for denoising in the presence of noise outliers,1997,"A wavelet domain robust denoising algorithm is presented, which efficiently removes both Gaussian as well as Gaussian mixed with impulse noise. Several wavelet domain operators are developed which help in the denoising process. The superiority of the new algorithm is firmly established by simulation over a variety of images."
1908221,21239,9616,A new multichannel blind deconvolution method and its application to solar images,1998,"A new method of multichannel blind deconvolution is introduced. The algorithm is simple, computationally efficient and does not need any statistical presumptions about the image. Any a priori knowledge about the PSF and the original can be incorporated very easily. Numerical simulations and experiments give promising results comparing to other restoration methods."
1981024,21239,9078,The European COST211ter activities-research towards advanced algorithms for coding of video signals at very low bit rates,1996,A significant effort of the COST211ter group activities is dedicated towards the contribution for the MPEG-4 video group activities. This paper provides an overview of the COST telecommunication framework and discusses the technical simulation model approach taken by the COST211ter group. Video coding with application to multimedia services is discussed.
1746298,21239,9078,Image Classification using Chaotic Particle Swarm Optimization,2006,Particle swarm optimization is one of several meta-heuristic algorithms inspired by biological systems. The chaotic modeling of particle swarm optimization is presented in this paper with application to image classification. The performance of this modified particle swarm optimization algorithm is compared with standard particle swarm optimization. Numerical results of this comparative study are performed on binary classes of images from the Corel dataset.
2419997,21239,9078,Game-theoretic analysis of watermark detection,2001,This paper describes a game-theoretic methodology to design and embed watermarks in images. The optimality criterion is the decoder error probability. Analytical solutions are presented for problems involving Gaussian host images and illustrated with examples. Significant improvements over previous designs are obtained.
1787025,21239,9078,"High quality, low complexity halftoning with good compressibility",2002,Proposes a novel method of hybrid LUT/screen halftoning which achieves both high quality and compressibility using only point processes. Experimental results show that the halftone images from our hybrid LUT/screen algorithm have quality comparable to those from the direct binary search (DBS) algorithm with excellent performance of lossless halftone compression.
1974509,21239,9078,Resolution reduction by growth of zones for visual prosthesis,1996,"We introduce an algorithm for visual prosthesis devices, where the data bandwidth must be dramatically reduced in order to correspond to the biotechnological constraints. The resolution reduction is based on an image segmentation by growth of zones which gives good results and allows an implementation in a low-power VLSI device."
671271,21239,9078,Approximation of the Hunt94 color appearance model by means of feed-forward neural networks,1996,"The authors define here an approximation of the combination of the forward and reverse Hunt94 color appearance models by means of feed-forward neural networks trained with a back-propagation algorithm on a rather small, standard training set. Experimental results confirm the feasibility of the approach."
651404,21239,9078,A risk-based frequency adaptive image restoration,1999,A frequency adaptive choice of the regularization parameter based on an unbiased estimate of the risk is investigated. The method is intended to be used in the restoration of images degraded by a shift invariant blur and additive white Gaussian noise. The results are compared with the more widely studied non adaptive case.
2117739,21239,9078,Applications of the region growing Euclidean distance transform: anisotropy and skeletons,1997,A new region growing algorithm has been proposed for computing Euclidean distance maps in a time comparable to widely used chamfer distance transform. We show how this algorithm can be extended to more complex tasks such as the computation of distance maps on anisotropic grids and the generation of a new type of Euclidean skeletons.
2300087,21239,9078,Non-additive noise and optimal correlation,1996,"The optimal detection and location of a target in a scene in presence of non-additive noise is studied. The optimal processors in the maximum likelihood sense are determined for different white or correlated noise statistics. It is shown that the required computations are mainly correlations, and that there is no need of iterative methods."
480625,21239,9078,Parameter estimation for spatial random trees using the EM algorithm,2003,"A new class of multiscale multidimensional stochastic processes called spatial random trees was recently introduced in J. Pollak et al. (2003). The model is based on multiscale stochastic trees with stochastic structure as well as stochastic states. In this work, we describe a method for estimating the parameters of the process."
734735,21239,9078,Well-composed images and rigid transformations,2013,"We study the conditions under which the topological properties of a 2D well-composed binary image are preserved under arbitrary rigid transformations. This work initiates a more global study of digital image topological properties under such transformations, which is a crucial but under-considered problem in the context of image processing, e.g., for image registration and warping."
441764,21239,11470,Rotation invariant texture classification using Bamberger pyramids,2005,We study the application of the Bamberger directional filter bank to the problem of rotation invariant texture classification. We explore the use of purely directional decompositions and the use of polar-separable Bamberger pyramids. We obtain comparable classification performance to Gabor-based methods using a smaller feature set.
2490281,21239,9078,Invariant representation in image processing,2001,"The paper discusses the role of invariance in image processing, specifically the desire to discriminate against unwanted variations in the scene while maintaining the power to tell the difference between object-intrinsic characteristics and scene-accidental conditions. It provides an analysis and references of what are directly observables in a general scene."
1830283,21239,9078,A parallel implementation of a Magnetic Induction Tomography: Image reconstruction algorithm on the ClearSpeed Advance accelerator board,2009,Magnetic Induction Tomography (MIT) is a relatively new contactless imaging modality which aims at reconstructing conductivity and permittivity distributions within objects. One of MIT's main challenges is the computational intensity required for image reconstruction in potential industrial and medical applications.
1818461,21239,9078,Motion-compensated deblocking and upscaling for viewing low-res videos on high-res displays,2008,"A motion-compensated (MC) video deblocking method has been proposed whose advantage is that it retains as much sharpness as possible after removing the blocking artifacts. When followed by an appropriate upscaling technique, this method is suitable for displaying user and Internet videos on high-definition displays."
2118229,21239,9078,Localization of colored objects,2000,"We present the comparison of different histogram distance measures and color spaces applied to object localization. In particular we examine the Earth Mover's Distance. The evaluation is based on more than 50,000 experiments. We propose an extension to normalized RG color space which enhances histogram comparison results."
2433422,21239,9078,Enhanced fractal image coding by combining IFS and VQ,1997,A novel paradigm for fractal coding selectively corrects the fractal code for selected domain blocks with an image-adaptive VQ codebook. The codebook is generated from the initial uncorrected fractal code and is therefore available at the decoder. An efficient trade-off results between incremental performance and bit rate.
1893697,21239,9078,Distributional clustering for efficient content-based retrieval of images and video,2000,"We present an approach to clustering images for efficient retrieval using relative entropy. We start with the assumption that visual features are represented by probability densities and develop clustering algorithms for probability densities (for example, normalized histograms are crude approximations of probability densities). These clustering algorithms are then used for efficient retrieval of images and video."
1671190,21239,9078,Segmentation by Smoothing B-Spline Active Surface,2006,"In this paper, a new active surface based segmentation algorithm is proposed. We detail the regularization process that is conducted through a smoothing B-spline filtering of the mesh point displacements. The corresponding segmentation algorithm is presented and applied on medical data. The proposed segmentation method is fast and suitable to quadrangular meshes."
1985393,21239,9078,Model-based Tikhonov-Miller image restoration,1997,"We propose a new Miller-Tikhonov restoration method where an a priori model of the solution is included. In sharp contrast with the classical method, this approach incorporates local informations. We show that the optimal model can be directly calculated from the data or a priori given and adjusted by minimizing the reconstruction error."
2275920,21239,9078,Progressive optimality in hierarchical filter banks,1994,The concept of progressive optimization is proposed for the design of hierarchical subband transforms. The time and frequency properties of the product filters in subband trees are discussed and evaluated. It is shown that the performance improvements in image coding are possible by using different PR-QMF banks at different nodes of a subband tree. >
428019,21239,9078,Inpainting surface holes,2003,"An algorithm for filling-in surface holes is introduced in this paper. The basic idea is to represent the surface of interest in implicit form, and fill-in the holes with a system of geometric partial differential equations derived from image inpainting algorithms. The framework and examples with synthetic and real data are presented."
618757,21239,9078,Image systems engineering at Stanford,1996,"A new Image Systems Engineering Program (ISEP) has recently been launched at Stanford University. The program includes more than a dozen faculty participants drawn from four departments. The planned stages of growth of the Program are described, with emphasis on the anticipated respective roles of the university and industry."
261632,21239,9078,A new decoding algorithm based on range block mean and contrast scaling,2003,"A new fractal decoding algorithm based on range block mean and contrast scaling is presented. As a result, the decoding image is represented as the sum of the DC and AC components. We prove that new decoding algorithm converges faster than existing ones. Experiments show that the new decoding algorithm converges at least 2-3 times faster than existing methods."
1892928,21239,9078,Approximations of posterior distributions in blind deconvolution using variational methods,2005,"In this paper the blind deconvolution problem is formulated using the variational framework. With its use approximations of the involved probability distributions are developed resulting in two algorithms for the estimation of the posterior distributions of the hyperparameters, the blur, and the original image. The performance of the two proposed restoration algorithms is demonstrated experimentally."
2367104,21239,9078,The mathematics of color calibration,1998,The mathematical formulation of calibrating color image reproduction and recording devices is presented. This formulation provides a foundation for future research in areas of characterization of devices and display of color images. The procedure outlined in this paper should become standard for displaying color images for the image processing community.
2382327,21239,9078,Application of Non-Linear Image Processing: Digital Video Archive Restoration,1997,"In any field which makes use of images, corruption is a common problem. For broadcasters, it is desirable to have processes capable of removing unwanted corruption in archive video material prior to broad-cast. In this paper a method of video restoration is described which specifically addresses the problem of video scratching."
1055632,21239,9078,Coupled distributed arithmetic coding,2011,"In this paper, we propose a novel scheme of coupled distributed arithmetic coding to overcome the de-synchronization problem caused by causal decoding in existing distributed arithmetic coding system. Simulation results show that decoding performance is significantly improved and longer sequences outperform shorter sequences using this approach."
1133260,21239,9078,Estimation of the Ising field parameter thanks to the exact partition function,2010,"We propose new estimation methods for the Ising field parameter based on an explicit expression for the partition function. This expression is known for a long time [1], but, to the best of our knowledge, it has never been used for parameter estimation. The paper develops standard Bayesian estimates and proposes a numerical comparative evaluation."
2291105,21239,9078,A genetic approach to edge detection,1994,We propose to use optimization techniques known as genetic algorithms in the searching of the optimal edge configuration. An objective function is supplied which assigns a fitness value to each edge configuration. Fitness represents the ability of an individual to survive and reproduce; in our case it is evaluated on the basis of the local nature of the edges. >
2165118,21239,9078,Unsupervised deconvolution of satellite images,1998,"This paper focuses on hyperparameter estimation of a variational model for image deconvolution. Using the generalized maximum likelihood (GML) estimator, the estimation problem is reduced to the ML estimation in the case of perfectly observed data. A method based on stochastic gradient is then developed for the estimation of both linear and nonlinear hyperparameters."
2181607,21239,9078,The polynomial phase difference operator for modeling of nonhomogeneous images,1995,We consider non-homogeneous 2-D signals which can be represented by a constant modulus polynomial-phase model. We develop a computationally efficient estimation algorithm for the parameters of this model. The algorithm is based on a phase differencing operator which is introduced in this paper. The basic properties of the operator are derived and used to develop the estimation algorithm.
2063857,21239,9078,Interactive DVD programming using next generation content-based encoded multimedia data,2000,"In this paper we propose a method of expanding the existing DVD-Video standard by incorporating MPEG-4 encoded content and its associated object-based interactivity into DVD. The proposed integration of MPEG-4 into DVD will enrich the already proven and successful DVD technology with the advanced interactive capabilities of MPEG-4, while maintaining backward capability with the existing DVD standard."
608747,21239,9078,Long-correlation image models for textures with circular and elliptical correlation structures,2001,"A class of random field model having long-correlation characteristic is introduced. Unlike earlier approaches in long-correlation models, the correlation structure is isotropic or elliptical in this new class of random field model. A comprehensive three-step algorithm for parameter estimation is developed, and the validity of the model is demonstrated with real textures."
2399155,21239,9078,Detection of data hiding in binary text images,2005,"We present in this paper a technique for the steganalysis of electronic binary text images. The proposed method utilizes the similarity between same characters or symbols. The proposed method can detect the existence of a secret message hidden by the embedding algorithms, which hide information by flipping centers of L-shape patterns (COL)."
574355,21239,9078,Joint singular value decomposition - a new tool for separable representation of images,2001,"We propose a separable decomposition approximating the Karhunen-Loeve transform for random fields. We show that this problem is related to a joint singular value decomposition of a set of matrices and we provide an efficient algorithm to compute it. Finally, we illustrate the interest of this new tool for image representation and approximation."
2297736,21239,9078,Formation of step images during anisotropic diffusion,1997,This paper analyzes the behavior of the one-dimensional discretized anisotropic diffusion in order to explain the manner in which false step edges are created. The creation of these edges are related to the characteristics of the local convexity of the image intensity function. An approach is proposed to prevent the appearance of false edges.
2048138,21239,9078,An overview of inverse problem regularization using sparsity,2009,"Sparsity constraints are now very popular to regularize inverse problems. We review several approaches which have been proposed in the last ten years to solve inverse problems such as inpainting, deconvolution or blind source separation. We will focus especially on optimization methods based on iterative thresholding methods to derive the solution."
2042652,21239,9078,Constrained regularization methods for superresolution,1998,We apply constrained Tikhonov-Miller regularization to superresolution. The effective solution of this problem necessitates the imposition of a positivity constraint on the conjugate gradient method. This results in a fast projection based quadratic programming method for image superresolution. Difficulties with existing algorithms for this problem are also examined.
2103376,21239,9078,Streaming of photo-realistic texture mapped on 3D surface,1997,We present a novel technique for efficient coding of texture to be mapped on 3D landscapes. The technique enables to stream the data across the network using a back-channel. The use of wavelet and discrete cosine transforms is investigated and compared. This technology has been proposed has a tool for the emerging MPEG-4 standard.
2044295,21239,9078,Symbolic fusion of hue-chroma-intensity features for region segmentation,1996,"A method for color image segmentation processing is presented. This method is based upon several linguistic rules which built a symbolic cooperation between symbolic representation of hue, chroma and intensity features. This fuzzy segmentation is compared with a technique using a fuzzy C-means algorithm, and the influence of the choice of the rules is analyzed."
2162119,21239,9078,A partial differential equation approach to image zoom,2004,"We propose a new model for zooming digital images. This model, driven by a partial differential equation, performs a balance between a linear zoom on homogenous zones and an anisotropic diffusion based. zoom near edges. This allows the advantages of linear zoom models and of some nonlinear ones to be combined while leaving out their drawbacks."
2288780,21239,9078,A model for detecting phasimetric SAR effects coming from rainy events,1998,"Important SAR phasimetric effects can be observed on the evolution of vegetation subjected to rainy periods. We present a segmentation model to detect these artefacts. It is based on a statistical model, expressed in a Markovian random field framework. The novelty of the approach resides in the use of both interferometric phase and coherence."
321755,21239,9078,New discrepancy measures for segmentation evaluation,2003,"In this paper, we propose new evaluation measures for scene segmentation results, which are based on computing the difference between a region extracted from a segmentation map and the corresponding one on an ideal segmentation. The proposed measures take into account separately both under and over detected pixels. It also associates in its computation the compactness of the region under investigation."
2146055,21239,9078,Multichannel filtering for color image processing,1996,"This paper addresses the problem of noise attenuation for multichannel data, such as color images. The proposed filter utilizes adaptive data dependent non-parametric techniques. Simulation results indicate that the new filter suppresses impulsive as well as Gaussian noise and preserves edges and details."
2383888,21239,9078,An efficient algorithm for realizing matching pursuits and its applications in MPEG-4 coding system,2000,"Matching pursuits is proved to be one of the most efficient techniques used in video coding. However, its computation effort is so tremendous that it may not be affordable in some real-time applications. This paper presents an efficient algorithm for the implementation of matching pursuits. Simulation results show that the computation effort can be successfully reduced with the proposed method."
2353495,21239,9078,Error concealment for motion JPEG2000,2005,"In this paper, we present methods that can be used to conceal errors in corrupt motion JPEG2000 codestreams. Motion JPEG2000 is an intra-frame compression technique and the proposed concealment methods utilize motion compensation to conceal errors. The simulation results indicate that the proposed methods can yield over 10 dB improvement in PSNR."
2248331,21239,9078,Radar image denoising by recursive thresholding,1996,"Radar image denoising with wavelet packets has been studied in this paper. We propose a recursive thresholding and reconstruction algorithm on wavelet packet coefficients to obtain a clear radar scan. Utilizing the same algorithm, we can estimate the noise variance as well. We illustrate the performance of the proposed approach with both synthesized and real radar images."
1069186,21239,9078,A general probability framework for improving similarity based approaches for face verification,2013,"This paper introduces a probability model for face verification, aiming at improve various similarity comparison approaches transplanted directly from face identification algorithms. Experiences demonstrate that, when embedded with a few well known subspace based similarity comparison approaches, our probability model can efficiently reduce the error rates in face verification tasks."
2646349,21239,9078,Depth-Weighted Group-Wise Principal Component Analysis for Video Foreground/Background Separation,2015,"A method separates foreground from background in a sequence of images, by first acquiring the sequence of images and a depth map of a scene by a camera. Groups of pixels are determined based on the depth map. Then, the sequence of images is decomposed into a sparse foreground component, and a low rank background component, according to apparent motion in the sequence of images, and the groups."
668821,21239,9078,A fuzzy approach to mouth corner detection,1999,"This paper presents a robust and flexible method for the extraction of mouth corners from videophone sequences. Robustness is referred to illumination conditions, speaker variability, pose and image quality, while flexibility is given by the fuzzy environment which allows to efficiently perform the fusion of several sources of information."
854368,21239,9078,Cardiac Cavity Labeling In Echocardiograms Using Deformable Model-Guided Splitting,2006,Cardiac cavity labeling is crucial for the detection of heart abnormalities and estimation of numerous heart indexes. A deformable model-based splitting technique is presented that improves on our segmentation algorithm and provides labeling of cardiac chambers. Experiments show that the presented work provides accuracy as well as versatility.
57417,21239,9099,Color images watermarking based on minimization of color differences,2006,"In this paper we propose a scheme of watermarking which embeds into a color image a color watermark from the L*a*b* color space. The scheme resists geometric attacks (e.g., rotation, scaling, etc.,) and, within some limits, JPEG compression. The scheme uses a secret binary pattern to modify the chromatic distribution of the image."
2298155,21239,9078,Modelling and restoration of multilook correlated speckled images,1995,"A model for the speckle process is proposed based on the underlying phenomena, which can take into account correlation present in radar images. The restoration problem is then formulated using the Bayesian framework. The new model is found to give significantly better results for correlated speckled images as compared to models which disregard correlation."
633608,21239,9078,Generalized interpolation: Higher quality at no additional cost,1999,"We extend the classical interpolation method to generalized interpolation. This extension is done by replacing the interpolating function by a non-interpolating function that is applied to prefiltered data, in order to preserve the interpolation condition. We show, both theoretically and practically, that this approach performs much better than classical methods, for the same computational cost."
1922945,21239,9078,Design strategy for three-dimensional subband filter banks,1996,"Since three-dimensional (3-D) subband coding has been introduced, most researches on 3-D subband coding perform temporal filtering first. In this paper, however, we investigate the best permutation strategy for temporal, vertical, and horizontal filtering to minimize the requirement of delay elements and find that the results are opposite to our expectation."
2329491,21239,9078,Bilateral interpolation filters for image size conversion,2005,"This paper presents an image size conversion method based on bilateral filtering. We formulate interpolation with bilateral filters by an optimization problem, with which we can enforce the interpolation constraint as well as smoothness constraint. The designed bilateral filter can be used in image size conversion, providing better pass-band characteristics and less artifacts from aliasing."
2362036,21239,9078,Image denoising through a level set approach,1998,The minimization of the total variation has become a very popular tool for image denoising. In this paper we present an alternative approach of the total variation minimization problem. In the BV functional background we introduce a formulation using level sets and the Coarea formula. From this we deduce a practical algorithm for total variation minimization. Finally we present some experimental results.
433064,21239,9078,Recursive method to extract rectangular objects from scans,2003,"In scanning photographs, receipts or other small objects users will often scan many at a time. It would be convenient to automatically detect that the scanned image consists of many small objects rather than a single large one, and segment appropriately. We present a simple, efficient and robust way of doing this."
2527812,21239,9078,Orthonormal bases of non-separable wavelets with sharp directions,2005,"This paper introduces a family of non-adaptive geometrical wavelets that have high anisotropy and directionality. Unlike curvelets and contourlets, they have no redundancy and form orthonormal bases of L/sup 2/ (R/sup 2/). Their implementation derives from a single non-separable filter bank structure that is designed from 1-D and quincunx filters."
1836873,21239,9078,Receiver operating characteristic curves and optimal Bayesian operating points,1995,"The receiver operating characteristic curve is a standard method for reporting the performance of a system. In this paper we show how do choose the optimal operating point when we are given a receiver operating curve, the prior probabilities, and the economic gain matrix. Unlike earlier methods, we make no assumptions regarding underlying distributions."
473918,21239,9078,A survey on watermarking application scenarios and related attacks,2001,"A thorough investigation on all possible scenarios where digital imperceptible watermarking is applicable is presented. All previously proposed watermarking schemes fall to at least one of the referenced application categories. Possible attacks are divided into categories and application scenarios are presented, always referring to the watermarking parameters involved."
567977,21239,9078,Formulae for polytope volume and surface moments,2001,Object moments and moment invariants are widely used in image analysis and recognition. The paper describes an approach and presents explicit formulae for calculation of polyhedron volume and surface moments via coordinates of polyhedron vertices. General formulae for calculation of arbitrary order volume and surface moments for polytopes in R/sup n/ are also obtained.
2120564,21239,9078,Conditions and design examples for instantaneous and simultaneous switching of filter banks,1996,"We give a necessary and sufficient condition for instantaneous and simultaneous switching of analysis and synthesis filter banks in subband-coding systems. By means of a design example we show that under this condition sets of filter banks can be found that-within a set-can be interchanged randomly, and hence can be used for simultaneous and instantaneous switching."
279903,21239,9078,Error propagation analysis for edge postprocessing,1996,"A post-processing scheme for edge detectors is described, that is based on the method of probabilistic relaxation where the binary relations between the possible edgels are explicitly modelled. The probability density functions of the errors in the measurements of these quantities have been rigorously derived from the assumed Gaussian distribution of errors in the image gray values."
1883065,21239,9078,A new image similarity measure based on ordinal correlation,2000,We propose an ordinal-based measure of image similarity. This measure is based on a previously developed general framework for image correspondence and incorporates region-based spatial information. The measure is capable of taking into account differences between images at various scales. Several examples are presented and the measure is evaluated on a set of test images.
348003,21239,9078,Robust watermarking by histogram specification,1999,The paper investigates the use of exact histogram specification for robust watermarking. A class of watermarks are selected such that the presence of certain groups of consecutive gray levels is considerably reduced with no visual degradation of images. The proposed technique is proven to be very resistant to many intentional and unintentional attacks.
2189699,21239,9078,Edge compensated transform coding,1994,Transform based image compression has difficulty with image regions containing edges. Edge compensated transform coding (ECTC) addresses this problem by preprocessing to remove edges. This preprocessing is adapted to transform coding. The edge information is sent in a side channel and the edges are replaced at the receiver. Subjective improvement is demonstrated. >
2172123,21239,9078,Motion detection: Fast and robust algorithms for embedded systems,2009,"This article introduces a new hierarchical version of a set of motion detection algorithms called ΣΔ. These new algorithms are designed to preserve as much as possible the computational efficiency of the basic ΣΔ estimation, in order to target real-time implementation for low power consumption processors and embedded systems."
643378,21239,9078,Multispectral image retrieval using vector quantization,2001,"A novel method for multispectral image retrieval is presented. This method uses a representation of image features based on vector quantization. Feature representation is important for image retrieval, but there are difficulties in applying conventional histogram-based representations to multispectral images. We developed an efficient feature representation and a novel method for the retrieval of images by quantizing each image adaptively."
2052539,21239,9078,An axiomatic approach to multiresolution signal decomposition,1998,"We have been developing general multiresolution signal decomposition schemes that unify traditional (linear) approaches and allow use of nonlinear filtering techniques in the decomposition. This paper summarizes our approach and provides several simple examples. Some of these examples are known in the literature (e.g., Haar pyramid and wavelet) and some of them are new (e.g., morphological Haar pyramid and wavelet)."
1925194,21239,9078,High frame rate video capture by multiple cameras with coded exposure,2010,We propose novel techniques for multiple conventional video cameras to capture high frame rate (HFR) video without sacrificing spatial resolution. The coded exposure technology is used to make random measurements of the HFR video in temporal domain. The recovery of the HFR video from the random measurements is based on sparsities of the 3D video signal.
1872809,21239,9616,Generalized pattern spectra sensitive to spatial information,2002,"Morphological pattern spectra computed from granulometries are frequently used to classify the size classes of details in textures and images. An extension of this technique, which retains information on the spatial distribution of the details in each size class is developed. Algorithms for computation of these spatial pattern spectra for a large number of granulometries on binary images are presented."
2502315,21239,8806,Optical flow using color information: preliminary results,2008,"Optical flow cannot be completely determined only from brightness information of images, without introducing some assumptions about the nature of movements in the scene. Color is an additional natural source of information that facilitates the solution of this problem. This work aims to illustrate the improvement in the optical flow estimation by using color information through experimental results."
651606,21239,9078,A new edge-directed image expansion scheme,2001,"A new interpolation-based scheme for image expansion is introduced. In the proposed method, the fidelity and sharpness of edges in the expanded image is emphasized. This is achieved by applying edge-directed interpolation and edge sharpening operations. Compared with other methods, our method provides natural and sharp expanded images with light computational cost."
1512705,21239,9078,Energetic lattice for optimizing over hierarchies of partitions,2014,"This theoretical paper introduces a novel continuous representation of hierarchy of partitions, and generalizes the conditions of h-increasingness and scale increasingness [1] to obtain a global-local optimum on the hierarchy. It studies in particular the Lagrange optimization problem and gives the condition on the energy to achieve constrained optimization, in the lattice of cuts in the hierarchy."
358953,21239,9078,Robust influence functionals for image filtering,2003,"Based on nonparametric statistics, we propose robust variational filters for image denoising. The approach is a result of optimizing smooth statistical influence functionals subject to some noise constraints. Substantiating numerical examples are provided to demonstrate the potential and the good performance of the proposed algorithms in environmental image filtering."
1998464,21239,9078,Time-frequency based radar image formation,2004,"This paper briefly describes the background of synthetic aperture radar imaging and the time-varying behavior of Doppler frequency shifts in radar signals, discusses the conventional Fourier-based image formation and its problems, and introduces the time-frequency based image formation algorithm and its mathematical model, implementation and applications."
612343,21239,9078,Virtualized stomach wall and its deformation model,1999,"In this paper we try to construct the model of stomach wall from stomach X-ray CT images for making the virtualized stomach. We extract the set of control points from CT image and define their deforming factors. These factors consist of the elasticity of stomach wall, the air pressure inside stomach and the pressure given from the organs around the stomach. In results, generation of abstract form of deformed stomach is achieved."
1983117,21239,9078,Highly efficient coding schemes for contour line drawings,1995,"In this paper, adaptive coding schemes for contour line drawings based on chain code representation is presented. In this scheme, the chain code or the chain-difference code of a contour is modeled as an n-order Markov sequence and then coded with an arithmetic coding scheme adaptively. Experimental result shows that the proposed approach is better than some other conventional approaches."
2170727,21239,9078,Exponential Radon transform inversion based on harmonic analysis of the Euclidean motion group,2005,"This paper presents a new method for the exponential Radon transform inversion based on harmonic analysis of the Euclidean motion group (M(2)). The exponential Radon transform is modified to be formulated as a convolution over M(2). The convolution representation leads to a block diagonalization of the modified exponential Radon transform in the Euclidean motion group Fourier domain, which provides a deconvolution type inversion for the exponential Radon transform. Numerical examples are presented to show the viability of the proposed method."
2497998,21239,9078,A local orientation coherency weighted color gradient for edge detection,2005,We present a novel color gradient operator based on a weighted max distance (a weighted Minkowski distance of L/sub /spl infin// norm) with the weights determined by the coherency of local gradient orientations. We demonstrate its performance advantages by quantitatively comparing it with several existing gradient operators using synthetic images.
1979450,21239,9078,Methods of reduced-complexity overlapped block motion compensation,1994,"Overlapped block motion compensation (OBMC) can significantly improve upon the prediction performance of conventional block motion compensation, though at the cost of increased decoder computational complexity. This paper analyzes the complexity costs of OBMC, and offers several modifications to OBMC that significantly reduce its decoder complexity while retaining most of the OBMC performance gain. >"
2548288,21239,9078,Image fusion algorithms and metrics duality index,2009,"This paper discusses the duality between image fusion algorithms and quality metrics. It discusses odd cases where some quality metrics fail to estimate the added information and proposes a duality index that measures how suitable the metrics are to fusion algorithms. The proposed duality index serves as an objective function against which combinations of fusion algorithms, metrics, and their parameters are tested."
2237859,21239,11470,"Image steganalysis based on moments of characteristic functions using wavelet decomposition, prediction-error image, and neural network",2005,"In this paper, a general blind image steganalysis system is proposed, in which the statistical moments of characteristic functions of the prediction-error image, the test image, and their wavelet subbands are selected as features. Artificial neural network is utilized as the classifier. The performance of the proposed steganalysis system is significantly superior to the prior arts."
2576310,21239,9078,Automatic collimation in peripheral X-ray imaging,1998,A method has been developed using which the foreground (body) in a fluoroscopy image of the peripherals can be segmented from the background (existing collimation and direct exposure). The extracted information about where the body is in an image is used to suggest settings for the collimator hardware during image acquisition. Segmentation is achieved through global feature extraction and supervised classification.
1514719,21239,9078,Accuracy improvement of histogram-based image filtering,2013,"We propose a method for improving the accuracy of histogram-based image filtering. With this method, we define a histogram called intensity-stacked histogram. An image histogram generally consists of a frequency (number of pixels) for each bin. On the other hand, intensity-stacked histogram stores the sum of intensity values for each bin. The intensity-stacked histogram can be calculated in constant time similar to a standard histogram. We apply the intensity-stacked histogram to histogram-based image algorithms for median and bilateral filters. The histogram-based image filter with the intensity-stacked histogram works effectively when using a few bins. We confirmed that the accuracy of histogram-based image filters using intensity-stacked histogram is higher than that using standard histogram."
601881,21239,8494,Hybrid inverse halftoning using adaptive filtering,1999,"We propose a novel fast inverse halftoning technique using a combination of spatial varying filtering and spatial invariant filtering. The proposed algorithm is significantly simpler than most existing algorithms. Without explicit edge region classification, the proposed spatial varying filter is shown to be capable of preserving edges effectively."
913456,21239,9078,A scattering similarity based classification scheme for land applications of polarimetric SAR image,2010,"In this paper, a new classification scheme, which extracts the main scattering mechanism with target scattering similarities, is proposed. This approach not only leads to improved understanding of scattering mechanisms, but also has good performance in discriminating different scattering type of land cover. The NASA/JPL AIRSAR data is used for validating its effectiveness."
2389957,21239,9078,On the optimal design of rational rank selection filters for image restoration,1996,"In this paper, a powerful class of nonlinear filters, called rank conditioned rational rank selection (RCRRS) filters, is proposed to improve the filtering capability of rank conditioned rank selection (RCRS) filters. The approach of rational rank selection is presented in this paper such that they can output appropriate data from a larger set of samples instead of the data from the observation samples."
2066145,21239,9078,Code excited pel-recursive motion compensated video coding,2000,We present a novel pel-recursive motion compensation scheme. The new scheme incorporates vector quantization into pel-recursive motion compensation by using a code excited analysis-by-synthesis coding structure. Our experiments demonstrate that the proposed scheme achieves 4-7 dB PSNR gains or equivalently 40-50% bit savings over the conventional scalar quantizer based pel-recursive motion compensation scheme.
1872238,21239,9078,Video perceptual distortion measure: two-dimensional versus three-dimensional approaches,1997,In this paper the two-dimensional and three-dimensional video perceptual distortion measure systems based on the human vision model are presented. These systems are extensions of the still perceptual distortion measure system. One advantage of the two-dimensional video distortion measure is that the distortion can be weighted for frames in the vicinity of scene cuts. The two-dimensional video distortion measure also requires less computation compared to the three-dimensional video distortion measure. Simulation results are presented.
1765399,21239,9078,Directional image interpolation with ANOVA methodology,2010,"This paper proposes a directional image interpolation technique based on analysis of variance (ANOVA). ANOVA, a hypothesis testing methodology, is used to decide on the interpolation direction. Experimental results show the proposed ANOVA-based image interpolation technique preserves edges and fine structures of an image and suppresses common interpolation artifacts (e.g. ringing, blurring, and jaggies)."
2113194,21239,9078,Texture segmentation using multiscale Hurst features,1997,"We evaluate the effectiveness of multiscale Hurst parameters as features for texture segmentation. These extended Hurst features quantize texture roughness at various scales. The performance of these new features are compared against standard Hurst features using images of texture mosaics. For the experiments, the performance was evaluated with and without supplemental contrast and average grayscale features."
1946507,21239,9078,Intra and inter-band information evaluation in still image coding by means of the wavelet transform,2000,A new method for still image coding that tries to exploit intra and inter-band relations in the wavelet-transformed matrix is presented. An intuitive study of the relation between coefficients is done. The conclusion is that it is more important to exploit intra-band relations than inter-band relations. Results achieved are even better than those obtained with the best embedded published algorithms.
459059,21239,9078,Three-dimensional skeleton extraction by point set contraction,1999,"In this paper, a skeleton extraction method for unstructured three-dimensional data is presented. The algorithm is based on a point set contraction procedure and is proved to be robust to noise and coarse resolution of original data. Preliminary experiments performed on both synthetic and real data are provided, showing the goodness of the proposed method."
2148335,21239,9078,Symmetry and locality: uncertainty revisited,1996,The purpose of this paper is to explore the relationship between symmetry and locality-properties which are regarded as desirable in any image representation and which are frequently mentioned in connection with wavelet transforms. It is shown how the two conspire to cause uncertainty; a new form of the uncertainty principle is introduced and a novel window design technique presented.
1858590,21239,9078,Error concealment for arbitrarily shaped video objects,1998,"Trends in video coding technology have included the coding of arbitrarily shaped objects and the improvement of the robustness of video bitstreams when subjected to errors during transmission. In this paper, we examine the interaction of these two technologies, using a coder and decoder based on MPEG 4-video. Effective techniques are proposed for concealing the effects of transmission errors."
2043922,21239,9078,Morphological decomposition of 2-D binary shapes into conditionally maximal convex polygons,1994,Presents a morphological shape segmentation algorithm that decomposes a 2D binary shape into a collection of restricted convex polygons. The algorithm is simple and the decomposition is always unique. The components produced have well-defined mathematical characterizations and they seem to be in good agreement with the nature structures of the given shape. >
1122965,21239,9078,Scale-space method of image ringing estimation,2009,Suppression of ringing effect is a challenging problem. It is mainly caused by absence of effective methods of ringing artifact detection. In this paper we introduce a ringing estimation method based on scale-space analysis. The estimation shows good results for low-pass filtered test images and in adaptive image deringing.
2077265,21239,9078,Scalable discrepancy measures for segmentation evaluation,2002,We propose a set of scalable discrepancy measures that may be applied for segmentation evaluation when a reference is known. The proposed measures take into account under and over detected points within an adjustable area. They give the intensity of the discrepancy and its relative position. Furthermore a scale parameter allows the accuracy of the measures to be adjusted.
2144221,21239,9078,Texture element extraction via cepstral filtering of projections,1997,"This paper presents a method for the extraction of the texton from an image with ordered or quasi-periodic texture, without the assumption of homogeneity of the element. The method includes application of homomorphic or cepstral filtering in the Radon or projection domain. Good approximations to the texton were obtained even when variations are presented in the textured image."
2114333,21239,9078,An Attack Against Image-Based Selective Bitplane Encryption,2007,"We analyze a recently published lightweight encryption scheme for fingerprint images and discuss several shortcomings. A low-cost attack on this scheme is proposed, which allows access to the full plaintext for most given ciphertexts. We give some recommendations for improvements of the encryption scheme, but conclude that the analyzed scheme remains insecure."
490724,21239,9078,Transcoding of MPEG-II for enhanced resilience to transmission errors,1996,"This paper is concerned with the performance of MPEG-II compressed video when transmitted over noisy channels, such as for digital terrestrial television. We present the results of bit sensitivity and resynchronisation sensitivity measurements, and propose techniques for substantially improving the resilience of MPEG-II to transmission errors without increasing the bit rate."
1954353,21239,9078,Extracting curvilinear features from remotely sensed images using minimum cost path techniques,1994,The extraction of curvilinear features from images is an important task in many image analysis applications. We describe an approach to such tasks based on the combination of an efficient optimal path finding algorithm and the use of window constrained search spaces. We conclude that the combination provides an effective vehicle for curvilinear feature extraction from a variety of images. >
1790337,21239,9078,Image coding with optimal reconstruction,1997,"The optimal reconstruction is an advanced decoding technique for the transform-quantization image coding schemes such as JPEG. From the same encoded data, the image decoded by the optimal reconstruction has better quality than the conventional one. In particular, the optimal reconstruction does not suffer from the Gibbs artifact. The optimal reconstruction is formulated as a constrained optimization problem and solved by an evolution-projection algorithm."
301452,21239,9078,Value of image information categories and its importance for development and application of digital image processing systems,1996,"The values of the information categories density, texture and contour correlates with the information loss caused by the reduction of the image scale. For large scales the information loss of the category density is greater than the information loss of the categories texture and contour. This is reverse for small scales. For different amounts of density ranges an estimation of scale numbers is done for which this reverse case is valid."
507374,21239,9078,Edge detection using Holladay's principle,1996,"We propose herein a new contour detection algorithm based on the human visual system. Using the proposed method, one can automatically select the thresholds that define the significant edges, such as perceived by the human eye. The threshold value is adapted to the background and surround intensities, according to criteria involved by Holladay's principle."
283208,21239,9078,Design of M channel IIR causal stable perfect reconstruction filter banks using similarity transformation,2003,"In this work, the problem of design of M channel IIR causal stable perfect reconstruction filter banks is addressed, and design approach based on similarity transformation of the state transition matrix is presented. The analysis and synthesis polyphase matrices are constrained to be minimal. The state transition matrix of the synthesis polyphase is obtained by similarity transformation of the analysis state transition matrix. Simulation results are provided."
2173443,21239,9078,An estimation-theoretic technique for motion-compensated synthetic-aperture array imaging,1998,"We present an estimation-theoretic approach for reducing motion effects in SAR imaging. Our approach may be viewed as a multi-dimensional matched-filter, whose parameters are determined by the relative motion between the SAR antenna and the target. In contrast to similar multi-dimensional matched filter methods, we propose a fast and easily implementable solution."
1999683,21239,9078,Smoothing of SAR image speckle noise by multiplicative point estimation filters,1996,"This paper investigates the smoothing of speckle noise in synthetic aperture radar (SAR) images using the multiplicative point estimator, a locally adaptive form of the homomorphic filter. The filter adaptation is based on an assumed probability model for speckle noise. Speckle smoothing results for mixed terrain airborne SAR images are presented."
2252673,21239,9078,Modeling the impact of frame rate on perceptual quality of video,2008,"This study aims to understand how perceived quality of a video varies as the frame rate changes. We find that an inverted falling exponential function can accurately reflect the trend observed from subjective testing. We further explore the relationship between the falling rate and the video content features, such as frame difference, motion, and contrast."
94649,21239,21106,Application of backward stochastic differential equations to reconstruction of vector-valued images,2012,In this paper we explore the problem of reconstruction of vector-valued images with additive Gaussian noise. In order to solve this problem we use backward stochastic differential equations. Our numerical experiments show that the new approach gives very good results and compares favourably with deterministic partial differential equation methods.
447139,21239,9078,A new approach to thinning based on time-reversed heat conduction model [image processing],2004,"In this article, a novel thinning algorithm, based on a time-reversed heat conduction model, is proposed. The image is viewed as a thermal conductor and the thinning task is then considered as an inverse process of heat conduction. Given an image, the direction map of heat conduction is first computed, and then time-reversed heat conduction is simulated. The result turns out to be a thinned pattern. The algorithm can be applied to gray-scale or binary images."
2400814,21239,9078,A triage method of determining the extent of JPEG compression artifacts,2002,A triage metric is proposed for determining the extent of blocking and contouring artifacts in an image due to JPEG compression. This metric has a linear correlation with subjective judgements of artifacts in prints. This triage metric can be used to determine the amount of needed image enhancement or to achieve a good trade-off between image quality and system throughput.
1997014,21239,9078,Smart displays-a means of information domination,1998,"While up-to-date technical means of communications and data-processing are absolutely vital to the conduct of modern warfare in all its forms, they will not in themselves suffice for the creation of a functioning command and control system, and they may, if understanding and proper usage by the battle commanders are not achieved constitute part of the problem they are supposed to solve."
371051,21239,9078,Design and optimization of a parallel architecture dedicated to image matching,1999,The design and optimised (in time and space) implementation of a systolic circuit dedicated to aerial image matching is proposed. The final run time data adaptive architecture evaluation with Xilinx XC 4010 XL offers the equivalent processing speed-up of 2000 (compared to a sequential solution).
1997753,21239,9078,The Euclidean k-distance transformation in arbitrary dimensions: a separable implementation,2005,The signed k-distance transformation (k-DT) computes the k nearest prototypes from each location on a discrete regular grid within a given D dimensional volume. We propose a new k-DT algorithm that divides the problem into D 1-dimensional problems and compare its accuracy and computational complexity to the existing raster-scanning and propagation approaches.
2477423,21239,9078,A linear shift invariant multiscale transform,1998,"This paper presents a multiscale decomposition algorithm. Unlike standard wavelet transforms, the proposed operator is both linear and shift invariant. The central idea is to obtain shift invariance by averaging the aligned wavelet transform projections over all circular shifts of the signal. It is shown how the same transform can be obtained by a linear filter bank."
592329,21239,9078,On global and local convergence of half-quadratic algorithms,2002,"This study gives original results on the global and local convergence properties of half-quadratic (HQ) algorithms resulting from the Geman and Yang (GY) and Geman and Reynolds (GR) primal-dual constructions. In particular, we show that the: convergence domain of the GY algorithm can be extended with the benefit of an improved convergence rate."
1911070,21239,9078,A benchmarking protocol for watermarking methods,2001,"A benchmarking system for watermarking algorithms is described. The proposed benchmarking system can be used to evaluate the performance of watermarking methods used for copyright protection, authentication, fingerprinting, etc. Although the system described is used for image watermarking, the general framework can be used, by introducing a different set of attacks, for benchmarking of video and audio data."
2361614,21239,9078,A practical print-scan resilient watermarking scheme,2005,"A blind print-scan (PS) resilient watermarking scheme is proposed in this paper. By employing a series of novel solutions in block classification and different block-based embedding strategies, we achieved a good performance in terms of watermark capacity, robustness and image quality. The experimental results further demonstrate the validity of our proposed scheme."
2328364,21239,9078,Lossless Data Hiding for Medical Images with Patient Information,2007,"This paper presents a lossless data hiding algorithm for medical images, where we embed the patient information into the segmented liver region of the CT image. This algorithm utilizes the characteristics of difference images and modifies pixel values slightly to embed a large amount of data while keeping high visual quality."
2045167,21239,9078,A scalable wavelet image coding scheme using multi-stage pruned tree-structured vector quantization,1995,A hierarchical pruned tree-structured vector quantizer (PTSVQ) employing multi-stage PTSVQ's is introduced to encode image wavelet coefficients. The result is a low-complexity and scalable image coding system. The effects of non-square selection of the block sizes and normalization on the performance of the system are investigated.
2453767,21239,11529,Circularly symmetric watermark embedding in 2-D DFT domain,1999,"This paper presents an algorithm for rotation and scale invariant watermarking of digital images. An invisible mark is embedded in magnitude of the DFT domain. It is robust to compression, filtering, cropping, translation and rotation. The watermark introduces image changes that are invisible to the human eye. The detection algorithm does not require the original image."
2365105,21239,9078,Error resilient arithmetic coding of still images,1996,"This paper examines the use of arithmetic coding in conjunction with the error resilient entropy code (EREC). The constraints on the coding model are discussed and simulation results are presented and compared to those obtained using Huffman coding. These results show that without the EREC, arithmetic coding is less resilient than Huffman coding, while with the EREC both systems yield comparable results."
1980905,21239,9078,Automatic registration of SPOT images and digitized maps,1996,"This paper presents a new method for the automatic registration of SPOT images and digitised maps. Direct matching of high level features (urban areas and crossroads) is performed using an hypothesis generation and propagation scheme. Results are presented for the registration of SPOT images and maps covering the same scene, as well as for the retrieval of partial maps into a large SPOT image."
2376542,21239,9078,Area-based interpolation for scaling of images from a CCD,1997,"We describe a method for scaling images which have been acquired with a charge coupled device (CCD) sensor. This method produces enlarged images which are sharper than those produced by bilinear interpolation, for comparable complexity. It is based on the observation that the sampling process of a CCD is not a point sampling process, but instead can be modelled as an averaging filter followed by sampling."
1892894,21239,9078,The Mumford-Shah digital filter pair (MS-DFP) and applications,2002,"We present a novel coupled filter pair for denoising, segmentation, and inpainting, based on the well known Mumford-Shah image model. Our self-contained digital extension to a general graph domain makes it possible to cheaply and universally benefit from this conventionally expensive yet very powerful image model."
1943848,21239,9078,Efficient two-dimensional ARX modeling,1994,"In this paper a fast spatial adaptive algorithm is presented for the efficient least squares (LS), autoregressive exogenous (ARX), two-dimensional (2-D) modeling. Filter masks of general boundaries are allowed. Efficient space updating recursions are developed by exploiting the spatial shift invariance property of the 2-D data set. >"
1865881,21239,9078,Hypercomplex Color Affine Filters,2007,"In linear gray-scale image convolution filters, pixel values can only be scaled. But, in color image convolution filters, which treat color image pixels as vectors, pixel values can be submitted to a host of affine transforms: scaling, reflection, rotation and shearing. Hyper-complex affine transforms are presented which extend linear color image convolution filtering techniques."
1908430,21239,9078,Error resilient JPEG2000 decoding for wireless applications,2008,"To improve the JPEG2000 compression standard error resiliency in the wireless environment, the use of ternary MQ arithmetic coders/decoders that are based on the concept of forbidden symbol has been proposed. This paper presents two ternary MQ based techniques to reduce both the computational complexity and the memory requirement during the decoding process, with no or little degradation in the PSNR."
2263675,21239,9078,Fast mode decision for inter prediction in H.264,2004,"This paper proposes a fast inter prediction mode decision method for H.264. By pre-encoding a downsampled small image, the candidate inter block modes can be reduced to a small subset. The simulation result shows that our algorithm can achieve up to 50% complexity reduction with less than 0.2dB PSNR decrease."
1787409,21239,9078,An adaptive video stabilization method for reducing visually induced motion sickness,2005,"Visually induced motion sickness (VIMS) is sometimes brought by watching video sequences acquired by handheld video cameras. We present a video stabilization method for the purpose of reducing VIMS. In our method, the scenes including oscillatory motion which can cause VIMS are selected to be stabilized. In the subjective evaluation results, the severity of VIMS is reduced."
2002045,21239,9078,Axial stereovision: modelization and comparison between two calibration methods,1994,"In passive binocular stereovision, there are generally two approaches: the lateral stereovision and the axial stereovision. The authors present a study relating more particularly to axial stereovision. Most of the works on axial stereovision have been theoretical. These studies have not been followed by real experiments because their authors could not produce a practical description of an axial stereovision system. The present authors propose a method to get a real axial stereovision system by calibration then by rectification. They also compare their axial stereovision system calibration method to the only one serious calibration process they found in the existing papers. >"
739558,21239,9078,Morphological floodings and optimal cuts in hierarchies,2014,"The non-horizontal cuts of a hierarchy and the floodings of an image are well-established tools for image segmenting and filtering respectively. We present definitions of non-horizontal cuts and of floodings in the same framework of hierarchies of partitions. We show that, given a hierarchy, there is a one-to-one correspondence between the non-horizontal cuts and the floodings. This opens the door to optimal image filtering based on non-horizontal cuts and, conversely, to nonhorizontal cuts obtained by morphological floodings, or more generally by connected filterings."
733917,21239,9078,A split Bregman method for non-negative sparsity penalized least squares with applications to hyperspectral demixing,2010,"We will describe an alternating direction (aka split Bregman) method for solving problems of the form min u  ∥Au − f∥ 2  + η∥u∥ 1  such that u ≥ 0, where A is an m×n matrix, and η is a nonnegative parameter. The algorithm works especially well for solving large numbers of small to medium overdetermined problems (i.e. m > n) with a fixed A. We will demonstrate applications in the analysis of hyperspectral images."
643132,21239,9078,A multiresolution wavelet scheme for irregularly subdivided 3D triangular mesh,1999,"We propose a new subdivision scheme derived from the regular 1:4 face split, allowing analysis of irregularly subdivided triangular meshes by the wavelet transforms. Some experimental results on real medical meshes prove the efficiency of this approach in multiresolution schemes. In addition we show the effectiveness of the proposed algorithm for lossless compression."
1841768,21239,9078,Color Images Enhancement using Weighted Histogram Separation,2006,"This paper presents a modified approach to the successive mean quantization transform, which is called as the weighted histogram separation (WHS) for enhancement of color images. Property of WHS situates between the successive mean quantization transform and the histogram equalization. In addition, this approach is further applied to the local enhancement, which is similar to the adaptive histogram equalization, and it is termed as the adaptive weighted histogram separation (AWHS). A comparison with successive mean quantization transform and histogram equalization has been performed in the experiments."
1943947,21239,9078,Hypercomplex color-sensitive smoothing filters,2000,In this paper we present a new color-sensitive low pass filter that may he tuned to smooth the color image component in the direction of the color C/sub 1/. A new method for detecting the boundaries of C/sub 1/-colored objects using the color-sensitive smoothing filter is also presented. We demonstrate the color-sensitive smoothing filter and edge detection procedure on both synthetic and natural color images.
2444029,21239,9078,Non-Gaussian mixture image models prediction,2008,In this paper we analyze the problem of prediction using generalized Dirichlet mixtures which have been shown to be effective for approximating a wide varieties of probability distributions. The generalized Dirichlet mixture-based predictor is nonlinear and takes into account the fact that images clutter and texture are generally non-Gaussian. Experimental results involve objects detection in images and image restoration.
2533351,21239,9078,Prediction based on Boolean filters for multiresolution lossless image compression,1997,"In this paper Boolean filters and a variation of these, FIR-Boolean hybrid filters are proposed for realizing the prediction stages of a multiresolution lossless image compression structure. Optimal and adaptive Boolean filters are used for prediction and the proposed predictors performance is compared to the performance of other lossless multiresolution methods: the hierarchical interpolation scheme (HINT) and the S+P transform."
2460631,21239,9078,Sparse edge coding using overcomplete Gabor wavelets,2005,We present here a sparse coding algorithm dedicated exclusively to overcomplete Gabor wavelets and based on perceptual contour extraction methods. The algorithm has reduced computational cost and achieves compression rates similar to the standard image compression algorithms (JPEG and JPEG-2000) with the additional advantage of limiting the appearance of artifacts.
580288,21239,9078,A contour-preserving image interpolation method,2003,"We introduce a novel image interpolation method, which focuses on providing artifact-free contours. In our method, the image contours are divided into edges and ridges, and we estimate the orientation of them differently and apply directional interpolations on them. Our method gives visually pleasing and natural-looking images. Experiment results are shown and compared with other interpolations methods."
2152222,21239,9078,Recognition of partly occluded person actions in meeting scenarios,2004,The proposal describes a novel approach for handling partly occluded gestures in the feature domain with gesture specific Kalman-filters. An estimation of the Kalman-filter parameters using artificial neural networks is introduced. The approach is demonstrated and evaluated on data from a meeting scenario and can be generalized to all gesture based problems concerning partial occlusions.
1297798,21239,9078,Idempotence and automatic linear contrast enhancements,2005,"We investigate the natural requirement that an automated contrast enhancement algorithm be idempotent. We show that in a restricted but natural setting, it leads to a binding of the contrast changes with the decision procedure. We obtain: a natural decision procedure for the standard gamma-transformations; a class of contrast changes generalizing these; a procedure to learn an algorithm from a set of good images."
303432,21239,9078,Color image-based angular map-driven snakes,2001,"We propose a method for shape description of objects in color images. Our method employs angular maps to identify significant changes of color within the image, which are then used to drive snake models. Experimental results show that our angular map-driven snake method not only yields an accurate description of an object shape, but also it is computationally efficient."
2382888,21239,9078,A Gaussian mixture model for edge-enhanced images with application to sequential edge detection and linking,1998,"We present a new stochastic model for pixels in an edge-enhanced image. The model is robust because it allows for the possibilities of false and multiple edges, and may be efficiently estimated using a expectation-maximization technique with a minimum description length metric. The direct applicability of the model for the sequential edge linking algorithm is investigated."
482936,21239,9078,Level set methods in image science,2003,"In this article, we discuss the question what level set methods can do for image science. We examine the presence of these and related methods in image science and introduce some relevant level set techniques that are potentially useful for this class of applications. We will show that image science demands multidisciplinary knowledge and flexible but still robust methods, and that is why the level set method has become a thriving technique in this field."
2009027,21239,9078,Highly Accurate Orientation Estimation using Steerable Filters,2007,"This paper presents a generalized theoretical framework for designing accurate steerable filters for orientation estimation. We derive the necessary properties of orientation estimation filters in their most general form. Based on our framework, we implemented a highly angular-specific filter. Numerical experiments show the enhanced accuracy of our proposed filter as compared with existing filters."
1926813,21239,9078,A context adaptation model for the compression of images with a reduced number of colors,2005,"Recently, Chen et al. proposed a method for compressing color-quantized images that is based on a binary tree representation of colors and on context-based arithmetic coding with variable size templates. In this paper, we address the problem of context adaptation and we propose a model that provides improvements in the lossless compression capabilities of Chen's method."
2121941,21239,9078,Holographic image representations: the subsampling method,1997,"We discuss holographic image representations. Arbitrary portions of a holographic representation enable reconstruction of the whole image, with distortions that decrease gradually with the increase of the size of the portions available. Holographic representations enable progressive refinement in image communication or retrieval tasks, with no restrictions on the order in which the data fragments (sections of the representation) are accessed or become available."
2469812,21239,9078,Incremental motion segmentation in low texture,1996,"The paper studies segmentation of moving objects with low texture in a low textured background. We describe an algorithm that resolves the difficulties associated with other approaches by integrating over time the information in the video sequence. We motivate and demonstrate our approach by building the background and moving object world images, important constructs in generative video."
1909054,21239,9078,Spherical winged B-snakes,1996,"We introduce spherical triangular B-splines for closed shape representation, and discuss the shape reconstruction using our new model winged B-snakes, which are deformable surfaces coupled with active edges and junctions. We show the results of using the spherical winged B-snakes for simultaneous surface reconstruction and feature detection from range images or scattered 3D data."
2319221,21239,9078,Subspace Extension to Phase Correlation Approach for Fast Image Registration,2007,"A novel extension of phase correlation to subspace correlation is proposed, in which 2-D translation is decomposed into two 1-D motions thus only 1-D Fourier transform is used to estimate the corresponding motion. In each subspace, the first two highest peaks from 1-D correlation are linearly interpolated for subpixel accuracy. Experimental results have shown both the robustness and accuracy of our method."
708443,21239,9078,Scalable compressive video,2011,The paper presents a scalable compressive sampling (CS) scheme for video acquisition. The proposed solution enables progressive reconstruction of video frames with novel measurement matrices. Simulation results show significant performance improvements over the traditional CS technique for the base layer and slightly better performance for the final enhancement layer.
847383,21239,9078,End-to-end distortion estimation for H.264 with unconstrained intra prediction,2012,This paper presents a macroblock-level algorithm to estimate the end-to-end distortion of H.264/AVC-based video transmission that allows unrestricted intra prediction. Some fast approximations are developed to reduce its complexity so that it can be used in real-time applications. Experimental results demonstrate the performance of the algorithm and the impact of the unconstrained intra prediction.
635465,21239,9078,Iterative methods for phase diversity-based blind deconvolution in atmospheric optics,1999,"In this paper, we deal with the restoration of images degraded by the atmospheric turbulence. Atmospheric turbulence imposes a strong limit for observation on long propagation paths. We present a phase diversity blind deconvolution algorithm based on the minimization method. An alternating minimization iterative scheme is devised to recover the image and simultaneously identify the phase."
1365547,21239,11317,Deconstructing and restyling D3 visualizations,2014,"The D3 JavaScript library has become a ubiquitous tool for developing visualizations on the Web. Yet, once a D3 visualization is published online its visual style is difficult to change. We present a pair of tools for deconstructing and restyling existing D3 visualizations. Our deconstruction tool analyzes a D3 visualization to extract the data, the marks and the mappings between them. Our restyling tool lets users modify the visual attributes of the marks as well as the mappings from the data to these attributes. Together our tools allow users to easily modify D3 visualizations without examining the underlying code and we show how they can be used to deconstruct and restyle a variety of D3 visualizations."
2390673,21239,21106,Can modern technologies defeat nazi censorship,2012,"Censorship of parts of written text was and is a common practice in totalitarian regimes. It is used to destroy information not approved by the political power. Recovering the censored text is of interest for historical studies of the text. This paper raises the question, whether a censored postcard from 1942 can be made legible by applying multispectral imaging in combination with laser cleaning. In the fields of art conservation (e.g. color measurements), investigation (e.g. analysis of underdrawings in paintings), and historical document analysis, multispectral imaging techniques have been applied successfully to give visibility to information hidden to the human eye.#R##N##R##N#The basic principle of laser cleaning is to transfer laser pulse energy to a contamination layer by an absorption process that leads to heating and evaporation of the layer. Partial laser cleaning of postcards is possible; dirt on the surface can be removed and the obscured pictures and writings made visible again. We applied both techniques to the postcard. The text could not be restored since the original ink seems to have suffered severe chemical damage."
408646,21239,20332,SQUARE: A Benchmark for Research on Computing Crowd Consensus,2013,"While many statistical consensus methods now exist, relatively little comparative benchmarking and integration of techniques has made it increasingly difficult to determine the current state-of-the-art, to evaluate the relative benefit of new methods, to understand where specific problems merit greater attention, and to measure field progress over time. To make such comparative evaluation easier for everyone, we present SQUARE, an open source shared task framework including benchmark datasets, defined tasks, standard metrics, and reference implementations with empirical results for several popular methods. In addition to measuring performance on a variety of public, real crowd datasets, the benchmark also varies supervision and noise by manipulating training size and labeling error. We envision SQUARE as dynamic and continually evolving, with new datasets and reference implementations being added according to community needs and interest. We invite community contributions and participation."
1622858,21239,9078,Automated colorimetric analysis in paper based sensors,2014,"Recent times have seen a wide ranging and large scale use of paper as a potential material in sensors for determining the concentration of an analyte upon appropriate end-point development. Today's clinical, food and environmental sectors require low-cost practical analytical devices which are portable and offer on-site real time detection. We present a novel technique for estimating any analyte's concentration using a mobile app that analyses the image of the paper subsequent to a chromogenic assay. Making use of snakuscules for capturing the region of interest in the image, followed by basic strategies for removing illumination artifacts using the Von-Kries Coefficient Law, we correlated the luminosity of the colour developed on the paper strip against the analyte's concentration. We evaluate our algorithm by determining the glucose concentration levels in blood using commercially available glucometer strips using image processing and comparing them with actual glucose levels as estimated by auto analyzers. The results obtained correlated well with the conventional assay and were almost indistinguishable from the actual values."
2316972,21239,8494,Sharing multiple secrets using visual cryptography,2009,"Visual cryptography provides a very powerful technique by which one secret can be distributed into two or more pieces known as shares. When the shares on transparencies are superimposed exactly together the original secret can be discovered without computer participation. In this paper, we take multiple secrets into consideration, and generate a master key for all the secrets; correspondingly, we share each secret using the master key and obtain multiple shares. We merge these shares into a combined share, we adjust the master key and generate a new key. The secrets are revealed when the key is superimposed on the combined share in different locations using the proposed scheme. We provide the corresponding results in this paper."
2895595,21239,9078,Towards biometric identification using 3D epidermal and dermal fingerprints,2016,"We propose a novel, non-invasive method for identification of human fingerprints obtained from 3D images of the dermis and epidermis. Using images obtained with optical coherence tomography, we compute the Gaussian (K) and mean (H) curvature values of the dermatoglyphics of dermal and epidermal images from volunteers' fingers, which are then converted into curvature-type and KH maps. Next, the regions of maps located around the minutiae are matched based on their normalized cross correlation. To the best of our knowledge, this is the first work to explore the use of KH maps of the finger dermatoglyphics and 3D images of the dermis to support fingerprint identification. The reliability of 3D dermal fingerprints is further explored by comparing unrolled 2D fingerprints extracted from 3D dermal point clouds against a 2D fingerprint test database obtained using a 2D commercial scanner and software. Finally, an experiment is performed to illustrate the robustness of the dermal fingerprint to mild alterations of the epidermis."
2867341,21239,9078,SIFT flow based genetic fisher vector feature for kinship verification,2016,"Anthropology studies show that genetic features are inherited by children from their parents resulting in visual resemblance between them. This paper presents a novel SIFT flow based genetic Fisher vector feature (SF-GFVF) which enhances the facial genetic features for kinship verification. The proposed SF-GFVF feature is derived by applying a novel similarity enhancement method based on SIFT flow and learning an inheritable transformation on the Fisher vector feature so as to enhance and encode the genetic features of parent and child image in kinship relations. In particular, the similarity enhancement method is first presented by applying the SIFT flow algorithm to the densely sampled SIFT features in order to intensify the genetic features. Further analysis shows the relation of the extracted genetic features to anthropological results and discovers interesting patterns in different kinship relations. Finally, an inheritable transformation is applied to the enhanced Fisher vector feature which is learned with the criterion of minimizing the distance between kinship samples and maximizing the distance between non-kinship samples. Experimental results on the two representative kinship databases, namely the KinFace W-I and the Kinship W-II data sets show that the proposed method is able to outperform other popular methods."
2989360,21239,9078,Detection and spatial analysis of hepatic steatosis in histopathology images using sparse linear models,2016,"Hepatic steatosis is a defining feature of nonalcoholic fatty liver disease, emerging with the increasing incidence of obesity and metabolic syndrome. The research in image-based analysis of hepatic steatosis mostly focuses on the quantification of fat in biopsy images. This work furthers the image-based analysis of hepatic steatosis by exploring the spatial characteristics of fat globules in whole slide biopsy images after performing fat detection. An algorithm based on morphological filtering and sparse linear models is presented for fat detection. Then the spatial properties of detected fat globules in relation to the hepatic anatomical structures of central veins and portal tracts are explored. The test dataset consists of 38 high resolution images from 21 patients. The experimental results provide an insight into the size distributions of fat globules and their location with respect to the anatomical structures."
637742,21239,8228,A new approach to commutative watermarking-encryption,2012,"We propose a new approach to commutative watermarking-encryption (CWE). A permutation cipher is used to encrypt the multimedia data, which leaves the global statistics of the multimedia data intact. Therefore, any non-localized watermarking scheme that depends only on global statistics of the multimedia data can be combined with the permutation cipher to form a CWE scheme. We demonstrate this approach by giving a concrete implementation, which manipulates the global histogram to achieve watermark embedding/detection."
2880330,21239,9078,Doorway detection for autonomous indoor navigation of unmanned vehicles,2016,"Fully autonomous navigation of unmanned vehicles, without relying on pre-installed tags or markers, still remains a challenge for GPS-denied areas and complex indoor environments. Doors are important for navigation as the entry/exit points. A novel approach is proposed to autonomously detect™ doorways by using the Project Tango platform. We first detect the candidate door openings from the 3D point cloud, and then use a pre-trained detector on corresponding RGB image regions to verify if these openings are indeed doors. We employ Aggregate Channel Features for detection, which are computationally efficient for real-time applications. Since detection is only performed on candidate regions, the system is more robust against false positives. The approach can be generalized to recognize windows, some architectural structures and obstacles. Experiments show that the proposed method can detect open doors in a robust and efficient manner."
836519,21239,8494,Resolution variant visual cryptography for street view of Google Maps,2010,"Resolution variant visual cryptography takes the idea of using a single share of visual cryptography (VC) to recover a secret from an image at multiple resolutions. That means, viewing the image on a one-to-one basis and superimposing the share will recover the secret. However, if the image is zoomed, using that same share we can recover other secrets at different levels. The same share is used at these varying resolutions in order to recover a large amount of hidden secrets. This process is quite similar to watermarking an image, whereby nothing can be seen while fully zoomed out, but as the zoom level is increased the watermark becomes visible. This would also be associated with a recursive style of secret sharing. This type of secret sharing scheme would be appropriate for recovering specific types of censored information, such as vehicle registration numbers within certain types of images. This adds an additional dimension to our scheme: content based visual cryptography."
997808,21239,8806,Raster image representation of fingerprint minutiae,2011,"This paper provides a solution to converting a varied-length and unordered fingerprint minutiae set into a fixed-length and ordered vector. The proposed method transforms the minutiae in form of discrete coordinates into a fixed-size gray-level raster image by linear combination of pixels looked up against a randomly created 2-dimensional texture image. Then it becomes a trivial task to extract a fixed-length feature vector with ordered components from the fixed-size raster image. Experiments on the public fingerprint database FVC2002DB2_A demonstrate the effectiveness of the proposed method. Via a keyed generation of the random raster texture image, the proposed method enhances the privacy of a fingerprint minutiae template against information leakage. The same idea applies to any other cases requiring a transformation of discrete points based features to fixed-length and ordered features, e.g., media fingerprinting/hashing applications."
2880579,21239,9078,Complexity-based consistent-quality encoding in the cloud,2016,"A cloud-based encoding pipeline which generates streams for video-on-demand distribution typically processes a wide diversity of content that exhibit varying signal characteristics. To produce the best quality video streams, the system needs to adapt the encoding to each piece of content, in an automated and scalable way. In this paper, we describe two algorithm optimizations for a distributed cloud-based encoding pipeline: (i) per-title complexity analysis for bitrate-resolution selection; and (ii) per-chunk bitrate control for consistent-quality encoding. These improvements result in a number of advantages over a simple “one-size-fits-all” encoding system, including more efficient bandwidth usage and more consistent video quality."
400256,21239,369,LAMOR: Lifetime-Aware Multipath Optimized Routing Algorithm for Video Transmission over Ad Hoc Networks,2006,"Multipath routing is a key technique to support video transmission over wireless ad hoc networks (WANETs). In WANETs, the lifetime of a node is related to its residual energy, current traffic conditions and the required energy consumed for sending a packet to its next hop in the path. In this paper, we propose a new adaptive routing scheme termed Lifetime-Aware Multipath Optimal Routing (LAMOR) for supporting high-speed real time video transmission in WANETs, which is optimized in terms of lifetime and analyze its characteristics. Both theoretical analysis and simulation results demonstrate that LAMOR indeed extends network lifetime and improves the transmission quality of video streams."
2872126,21239,9078,A cost minimization with light field in scene depth MAP estimation,2016,"This paper proposes a scene depth map generation method based on lens-based light field cameras. In particular, it achieves the functions of the incident rays and the corresponding directional features, which can favor determining the coordinates of candidate space points. The light rays behind the aperture in the 4D light field would be converted into the rays before the aperture with their directions known. The light rays through the aperture center are denoted as reference light rays and keep their directions. With the probability (cost) of each reference light ray in each depth value, we obtain an initial depth map by selecting the depth value with minimum cost. It would be refined via multi-label optimization and weighted median filtering. Experimental results demonstrate the accuracy of the depth map estimated by the proposed method."
2018003,21239,8960,Scale Mixtures of Gaussians and the Statistics of Natural Images,2000,"The statistics of photographic images, when represented using multiscale (wavelet) bases, exhibit two striking types of non-Gaussian behavior. First, the marginal densities of the coefficients have extended heavy tails. Second, the joint densities exhibit variance dependencies not captured by second-order models. We examine properties of the class of Gaussian scale mixtures, and show that these densities can accurately characterize both the marginal and joint distributions of natural image wavelet coefficients. This class of model suggests a Markov structure, in which wavelet coefficients are linked by hidden scaling variables corresponding to local image structure. We derive an estimator for these hidden variables, and show that a nonlinear normalization procedure can be used to Gaussianize the coefficients."
1907606,21239,8494,Advances in superresolution using L-curve,2001,"Subsequent to the work of Kim, Bose, and Valenzuela in 1990 on the simultaneous filtering and interpolation of a registered sequence of undersampled noisy and shift-invariant blur degraded images, Bose and Boo tackled in 1998 the problem of reconstructing a high-resolution image from multiple undersampled, shifted, degraded frames with subpixel displacement errors. This led to a formulation involving a periodically shift-variant system model. Lertrattanapanich and Bose advanced in 1999 a procedure for construction of a high-resolution video mosaic following the estimation of motion parameters between successive frames in a video sequence generated from a video camera. Current research is focused on simultaneous blur identification and robust superresolution. The blur is not restricted to be linear shift-invariant and could not only be of the linear shift-variant type but also some nonlinear blurs could be accommodated. The optimal tuning parameter may, if desired, be calculated analytically and not by trial-and-error."
1883271,21239,390,A contrast-enhanced trilateral filter for MR image denoising,2011,"It is well known that the noise in magnetic resonance (MR) magnitude images obeys a Rician distribution. Denoising of MR images is of importance for clinical diagnosis and computerized analysis, such as tissue classification, segmentation, and registration. We propose a post-acquisition denoising algorithm in an attempt to automatically remove the random fluctuations and bias introduced by Rician noise. It replaces the intensity value on each pixel with an average value weighted by the geometric, radiometric, and median-metric components between neighboring pixels associated with an entropy function. Moreover, fuzzy functions are introduced to adaptively compute the parameters in terms of local intensity variations. The contrast-enhanced results indicate that this new filter outperformed several existing methods in providing greater noise reduction and clearer structure boundaries in MR images."
1804824,21239,8494,L-shaped segmentations in motion-compensated prediction of H.264,2008,"Playing the role of removing temporal correlation in video signals, motion-compensated prediction aims at the ideal prediction and eliminates the compensation. However, the block-based video coding infrastructure at present was not competent in exactly present the various shapes of moving objects in actual video. Contrast to that, an exact segmentation of coding area according to the actual moving objects in video is also not easy to achieve and will introduce significant computational complexity into the video coding scheme. Concerned with this conflict, this paper proposes a new set of segmentations of macroblocks, in which one macroblock splits into one L-shaped and one square segment. Experiments of this named L-shaped segmentations technique show that it can outperform the H.264 P-picture coding by 0.11 dB."
2235003,21239,390,Diffusion tensor images edge-directed interpolation,2010,"It has been demonstrated that, for scalar images, edge-directed interpolation techniques are able to produce better results, both visually and quantitatively, than non-adaptive traditional interpolation methods. We have extended the edge-directed concept to the interpolation of multi-valued diffusion tensor images. The interpolation framework is based on the improved edge-directed scalar image interpolation, which does not require estimation of local edges, and which interpolates the tensors in their mathematically native Riemannian space in order to respect tensor geometry of positive semi-definiteness. The framework has been tested on phantom scalar images and real diffusion tensor images, and is shown to be able to achieve better results than the Euclidean non-adaptive methods."
1963089,21239,8502,Personalized video summarization with human in the loop,2011,"In automatic video summarization, visual summary is constructed typically based on the analysis of low-level features with little consideration of video semantics. However, the contextual and semantic information of a video is marginally related to low-level features in practice although they are useful to compute visual similarity between frames. Therefore, we propose a novel video summarization technique, where the semantically important information is extracted from a set of keyframes given by human and the summary of a video is constructed based on the automatic temporal segmentation using the analysis of inter-frame similarity to the keyframes. Toward this goal, we model a video sequence with a dissimilarity matrix based on bidirectional similarity measure between every pair of frames, and subsequently characterize the structure of the video by a nonlinear manifold embedding. Then, we formulate video summarization as a variant of the 0–1 knapsack problem, which is solved by dynamic programming efficiently. The effectiveness of our algorithm is illustrated quantitatively and qualitatively using realistic videos collected from YouTube."
978640,21239,390,Parallel computation of a SPECT projection operator for a content adaptative mesh model,2012,In this paper we explore a parallel implementation for fast calculation of a tomographic projection operator for content-adaptive mesh model (CAMM) image reconstruction. Previously we introduced 2D and 3D tomographic image reconstruction using a CAMM for single positron emission computed tomography (SPECT). The proposed parallel method is fast and allows incorporation of a non-uniform attenuation and distance-dependent spatial resolution of the imaging system. This implementation establishes a necessary step for the future development and practical use of the CAMM tomo-graphic reconstruction.
1826875,21239,8494,On the distributions of the relative phase of complex wavelet coefficients,2009,"In this paper, the probability distributions of relative phase are studied. We proposed von Mises and wrapped Cauchy for the probability density function (pdf) of the relative phase in complex wavelet domain. The maximum-likelihood method is used to estimate the two parameters of von Mises and wrapped Cauchy. We demonstrate that the von Mises and wrapped Cauchy fit well with real data obtained from various real images including texture images as well as natural images. The von Mises and wrapped Cauchy models are compared, and the simulation results show that the wrapped Cauchy fits well with the peaky and heavy-tailed pdf of the relative phase and the von Mises fits well with the pdf which is in Gaussian shape. For most of the test images, the wrapped Cauchy model is more accurate than the von Mises, when images are decomposed by different complex wavelet transforms including dual-tree complex wavelet (DTCWT), pyramidal dual-tree directional filter bank (PDTDFB) and a modified version of curvelet."
530067,21239,8494,Image compression with structure-aware inpainting,2006,"This paper carves out a way to image compression that is motivated by the recent advancement in image inpainting. An image coding approach is proposed in which a number of regions of the input image are skipped at the encoder and are recovered through the inpainting process at the decoder. Furthermore, a structure-aware inpainting (SAI) method is developed to restore the skipped structural regions by taking advantage of the available portion of the decoded image. A binary structure map is extracted and compressed into the generated bit-stream to indicate the skipped regions with salient structures. By making use of the decoded texture information together with the structure map, the SAI method can recover the skipped structural regions as well as the non-structural ones effectively at the decoder. Compared with JPEG, our proposed image compression scheme allows smaller file, with the potential of up to 50% bit-saving capability, at similar visual quality levels."
2778949,21239,390,Blind restoration of images degraded with mixed poisson-Gaussian noise with application in transmission electron microscopy,2016,"Noise and blur, present in images after acquisition, negatively affect their further analysis. For image enhancement when the Point Spread Function (PSF) is unknown, blind deblurring is suitable, where both the PSF and the original image are simultaneously reconstructed. In many realistic imaging conditions, noise is modelled as a mixture of Poisson (signal-dependent) and Gaussian (signal independent) noise. In this paper we propose a blind deconvolution method for images degraded by such mixed noise. The method is based on regularized energy minimization. We evaluate its performance on synthetic images, for different blur kernels and different levels of noise, and compare with non-blind restoration. We illustrate the performance of the method on Transmission Electron Microscopy images of cilia, used in clinical practice for diagnosis of a particular type of genetic disorders."
616565,21239,8494,Fine tuning the GALE edge detection method,1999,"Most vision systems require the use of image processing applications that are highly dependent on the efficiency of edge detection techniques. These techniques are commonly implemented by applying an edge enhancement method followed by a thresholding point process. Most of these techniques are based on convolution algorithms that have a time complexity of O(n/sup 2/) when the picture has size n/spl times/n. In order to reduce this time complexity, an improved solution depends on the reduction of the problem space. Such a reduction was recently achieved by a new method, named GALE, which combines the random search mechanisms of genetic algorithms with linear time methods. In this paper, a refinement of the GALE method is accomplished by introducing an iterative process that selectively eliminates from the population of the genetic algorithm those pixels that were previously identified as part of an edge. Experimental results show the improved performance of this method."
1871372,21239,8494,Embedded image coding using quincunx directional filter bank,2006,"An image coding system based on combination of a multiresolution directional decomposition and a morphological dilation algorithm is proposed in this paper. The multiscale filter bank combines the recently introduced non-uniform quincunx directional filter bank at higher scales and the traditional wavelet filter bank at lower scales, to provide a sparse image representation. The coding algorithm then efficiently clusters the significant coefficients using progressive morphological dilation. The remaining scattered significant coefficients are coded with the aid of the Tarp filter. The proposed image coding algorithm achieves better results than the current state-of-the-art wavelet based coders, such as SPIHT and JPEG2000."
2426838,21239,8228,Energy efficient wireless transmission of MPEG-4 fine granular scalable video,2004,"Fine granular scalability is a coding tool, recently introduced in the emerging MPEG-4 standard, which enables the creation of very flexible scalable video bitstreams. This paper investigates the transmission of fine granular scalable (FGS) video over wireless links, using power management for unequal error protection of the bitstream. In wireless systems, energy may be a limited resource, and a wise use of it is important for system efficiency. An algorithm is proposed which is able to optimally distribute the total available power for the transmission of the enhancement layer, given a distortion or energy constraint. Experimental results demonstrate the performance advantage of the proposed algorithm over fixed power schemes and heuristic approaches."
692964,21239,9078,Video and image systems engineering education for the 21st century,1996,"We are developing a new graduate program at Purdue in Video and Image Systems Engineering (VISE). The project is comprised of three parts: a new curriculum centered around a degree option in VISE to be earned as part of the Masters or Ph.D. degrees; a state-of-the-art lecture/laboratory facility for instruction, laboratory experiments, and project and homework activities in VISE courses; and enhancement of existing courses and development of new courses in the VISE area."
675743,21239,9078,Perceiving three-dimensional objects during egomotion,2001,"Classes of powerful stimuli that reliably elicit wrong depth percepts are examined. The depth percepts in turn cause the stimuli to appear to move vividly as observers move in front of them. These illusory percepts highlight the issue of how we obtain a stable percept of the environment as we move about, despite considerable changes in the retinal images. Experimental results on the role of pictorial cues, and possible explanations for the illusory motion are presented. The interaction of data-driven mechanisms and schema-driven cognitive processes is examined."
1466819,21239,9078,Multidataset independent subspace analysis extends independent vector analysis,2014,"Despite its multivariate nature, independent component analysis (ICA) is generally limited to univariate latents in the sense that each latent component is a scalar process. Independent subspace analysis (ISA), or multidimensional ICA (MICA), is a generalization of ICA which identifies latent independent vector components instead. While ISA/MICA considers multidimensional latent components within a single dataset, our work specifically considers the case of multiple datasets. Independent vector analysis (IVA) is a related technique that also considers multiple datasets explicitly but with a fixed and constrained model. Here, we first show that 1) ISA/MICA naturally extends to the case of multiple datasets (which we call MISA), and that 2) IVA is a special case of this extension. Then we develop an algorithm for MISA and demonstrate its performance on both IVA- and MISA-type problems. The benefit of these extensions is that the vector sources (or subspaces) capture higher order statistical dependence across datasets while retaining independence between subspaces. This is a promising model that can explore complex latent relations across multiple datasets and help identify novel biological traits for intricate mental illnesses such as schizophrenia."
593017,21239,9078,Visual computing education at UCSD,1996,"Our goal at the University of California, San Diego, is to emphasize the increased importance of visual computing and image engineering by expanding and redesigning our curriculum. The planned curriculum development includes a 1-year undergraduate sequence in image processing and machine perception, and a redesign of the current core graduate sequence to put increased emphasis on synthesis of ideas from separate areas into a core sequence of visual computing. We are also introducing many advanced courses at both the undergraduate and graduate levels."
2880645,21239,9078,What's on TV: A large scale quantitative characterisation of modern broadcast video content,2016,"Video databases, used for benchmarking and evaluating the performance of new video technologies, should represent the full breadth of consumer video content. The parameterisation of video databases using low-level features has proven to be an effective way of quantifying the diversity within a database. However, without a comprehensive understanding of the importance and relative frequency and of these features in the content people actually consume, the utility of such information is limited. Here, we present a large-scale analysis of programming on BBC One and CBeebies, the most popular television channels in the United Kingdom for adults and children, respectively. Twenty video features are extracted from almost three thousand television programmes shown throughout 2015 before principal components analysis is used to identify just five factors representing the most variation. The meaning and relative significance of these five factors together with the shape of their frequency distributions represent highly valuable information for researchers wanting to model the diversity of modern consumer content in representative video databases."
2429890,21239,8960,Hierarchical Modeling of Local Image Features through L_p-Nested Symmetric Distributions,2009,"We introduce a new family of distributions, called Lp-nested symmetric distributions, whose densities are expressed in terms of a hierarchical cascade of Lp-norms. This class generalizes the family of spherically and Lp-spherically symmetric distributions which have recently been successfully used for natural image modeling. Similar to those distributions it allows for a nonlinear mechanism to reduce the dependencies between its variables. With suitable choices of the parameters and norms, this family includes the Independent Subspace Analysis (ISA) model as a special case, which has been proposed as a means of deriving filters that mimic complex cells found in mammalian primary visual cortex. Lp -nested distributions are relatively easy to estimate and allow us to explore the variety of models between ISA and the Lp-spherically symmetric models. By fitting the generalized Lp-nested model to 8 x 8 image patches, we show that the subspaces obtained from ISA are in fact more dependent than the individual filter coefficients within a subspace. When first applying contrast gain control as preprocessing, however, there are no dependencies left that could be exploited by ISA. This suggests that complex cell modeling can only be useful for redundancy reduction in larger image patches."
1636707,21239,9078,Electro-photographic model based stochastic clustered-dot halftoning with direct binary search,2011,"Most electrophotographic printers use periodic, clustered-dot screening for rendering smooth and stable prints. However, when used for color printing, this approach suffers from the problem of periodic moire´ resulting from interference between the periodic halftones of individual color planes. There has been proposed an approach, called CLU-DBS for stochastic, clustered-dot halftoning and screen design based on direct binary search. We propose a methodology to embed a printer model within this halftoning algorithm to account for dot-gain and dot-loss effects. Without accounting for these effects, the printed image will not have the appearance predicted by the halftoning algorithm. We incorporate a measurement-based stochastic model for dot interactions of an electro-photographic printer within the iterative CLU-DBS binary halftoning algorithm. The stochastic model developed is based on microscopic absorptance and variance measurements. The experimental results show that electrophotography-model based stochastic clustered dot halftoning improves the homogeneity and reduces the graini-ness of printed halftone images."
2943669,21239,8960,Achieving budget-optimality with adaptive schemes in crowdsourcing,2016,"Adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget. However, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal. To bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing data sets. Under this generalized Dawid-Skene model, we characterize the fundamental trade-off between budget and accuracy, and introduce a novel adaptive scheme that matches this fundamental limit. We further quantify the gain of adaptivity, by comparing the trade-off with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand."
2232388,21239,8494,On the probability density function of the derotated phase of complex wavelet coefficients,2008,"The derotated phase of a complex wavelet coefficient is defined as the phase of that coefficient minus twice of the phase of its parent coefficient. It was recently introduced and found to be useful in terms of providing more correlation among coefficients near singularities in scale and space. In this paper, we investigate the derotated phase of a complex wavelet coefficient by deriving its probability density function (pdf). We show that the derotated phase can be well approximated by a Gaussian random variable for moderate signal to noise ratio. The simulation results are shown to confirm the consistency between the derived pdf and the derotated phase of the actual coefficients."
2034561,21239,8502,Intelligent frame selection for anatomic reconstruction from endoscopic video,2009,"Using endoscopic video, it is possible to perform 3D reconstruction of the anatomy using the well known epipolar constraint between matched feature points. Through this constraint, it is possible to recover the translation and rotation between camera positions and thus reconstruct the 3D anatomy by triangulation. However, these motion estimates are not stable for small camera motions. In this work, we propose a covariance estimation scheme to select pairs of frames which give rise to stable motion estimates, i.e. minimal variance with respect to pixel match error. We parameterize the essential matrix using a minimal 5 parameter representation and estimate motion covariance based upon the estimated feature match variance. The proposed algorithm is applied to endoscopic video sequences recorded in porcine sinus passages in order to extract stable motion estimates."
906921,21239,9078,A More Efficient and Video Friendly Spatial Resizing Algorithm,2006,"Arbitrary spatial resizing of compressed video is a useful transcoding operation that can be used to match the video stream to a low bandwidth channel or to end user devices with a different resolution from that of the original source material. By performing this transcoding in the compressed domain the computational cost of the operation can be minimized. Our proposed algorithm resizes a video stream by modifying functional blocks in an existing transcoding architecture to permit spatial resizing. In order for the algorithm to function, a new motion compensation operation suitable for resized video was derived. The algorithm retains the advantages of permitting tradeoffs between computational cost and final quality while re sizing by any rational fraction. Additionally, the algorithm operates on an arbitrary supporting area and complements motion compensation."
2256278,21239,8494,Flexible resizing algorithms for video transcoding,2005,"Arbitrary spatial resizing of compressed video is a useful transcoding operation that can be used to match the video stream to a low bandwidth channel or to end user devices with a different resolution from that of the original source material. By performing this transcoding in the discrete cosine transform (DCT) domain the computational cost of the operation can be minimized. Our method modifies a recent approach to make it more general (the previous approach could only downsize by integer multiples of 1/8, while our approach removes this restriction and allows resizing by any rational fraction), efficient, and flexible (i.e. tradeoffs between computational cost and final quality can be made) while retaining the ability to downsize from an arbitrary supporting area (thus making the method useful for both intercoded and intracoded frames)."
1244890,21239,369,A Novel Image Authentication Approach Using an Overlap-Based Shared Secret for Collaborative Wireless Sensors,2010,"Abstract-In this paper we present a novel approach that leverages overlap regions of images from different wireless sensors for mutual authentication of the transmitted images. We present our theoretical analysis for this approach and derive the authenticity ratio for a binary symmetric channel. Our results show that by selecting appropriate authenticity constraints and appropriate numbers of overlap region packets, a significant improvement of the authentication success probability can be achieved compared to traditional image authentication approaches."
2348849,21239,8228,Optimal Resource Allocation in Wireless Multiaccess Video Transmissions,2007,"We study the problem of optimal resource allocation for multi-user wireless video transmissions from an information- theoretic point of view. We show that the previously known optimal rate allocation solution in wireless multiaccess which maximizes the weighted sum rate is suboptimal in wireless video communications. We further derive the optimal video resource allocation by jointly considering the Application-MAC-PHY layers. This optimal scheme maximizes the weighted sum video quality of all video users for any feasible power control policy. We refer to this policy as Largest Quality Improvement Highest Possible Rate (LQIHPR). We propose a simple greedy algorithm for implementation. With the help of the inherent prioritization mechanism of video coders, we show that LQIHPR is universally optimal for all video coding schemes. Simulation results demonstrate the significant improvement LQIHPR leads to as opposed to the conventional one."
747579,21239,8502,Pseudo-Polar Based Estimation of Large Translations Rotations and Scalings in Images,2005,"One of the major challenges related to image registration is the estimation of large motions without prior knowledge. This paper presents a Fourier based approach that estimates large translation, scale and rotation motions. The algorithm uses the pseudo-polar transform to achieve substantial improved approximations of the polor and log-polar Fourier transforms of an image. Thus, rotation and scale changes are reduced to translations which are estimated using phase correlation. By utilizing the pseudo-polar grid we increase the performance (accuracy, speed, robustness) of the registration algorithms. Scales up to 4 and arbitrary rotation angles can be robustly recovered, compared to a maximum scaling of 2 recovered by the current state-of-the-art algorithms. The algorithm utilizes only 1D FFT calculations whose overall complexity is significantly lower than prior works. Experimental results demosntrate the applicability of these algorithms."
2302018,21239,8494,Content-based scalable sports video retrieval system,2005,"This paper presents a content-oriented video retrieval system which is capable of handling high volumes of content as well as various functionality requirements. It allows the audience to access the video contents based on their different interests in the selected video program. The retrieval system consists of a content-based scalable access platform for supporting content-based scalable multifunctional video retrieval (MFVR). The retrieval methodology is based on different content semantic layers. In the experiments, we demonstrate the effectiveness of our method in generating scalable access of video content."
1916625,21239,8494,Efficient VLSI architecture for buffer used in EBCOT of JPEG2000 encoder,2005,"The EBCOT (embedded block coding with optimised truncation) block coder is one of the main resource intensive components of JPEG2000. Its throughput plays a key role in deciding the overall throughput of a JPEG2000 encoder system. Concurrent symbol processing (CSP) is a promising technique to increase the throughput of the block coder at significantly less increase in the hardware cost. We present an efficient VLSI architecture for the buffer required to realize a CSP capable block coder for the JPEG2000 encoder. Our contributions include the study of the contexts-generation pattern of the natural images for an optimal selection of buffer parameters, viz., buffer length and context-accepting capacity, and the design of a low cost VLSI architecture for the buffer. The architecture is implemented using Altera APEX20KE FPGA and experimental results show that the optimal selection of the buffer parameters results in savings of 76% in the hardware cost with a minimal reduction of 2% in the overall block coder throughput."
500402,21239,8494,Video denoising using vector estimation of wavelet coefficients,2006,"Wavelet-based image denoising can be extended to a video by applying it to each video frame independently. The denoising performance can be improved by exploiting inter-frame correlations, for example, using appropriate temporal filtering. However, fixed temporal filters might not perform sufficiently well due to their inability to cope with the variability of inter-frame correlations across the video. While many adaptive temporal filtering approaches for denoising in spatial domain have been proposed, they do not straightforwardly extend to wavelet-based denoising. We propose a vector extension of popular hidden Markov tree modeling that flexibly exploits the color and frame dependency of wavelet coefficients. Experimental results confirm that the vector estimator of wavelet coefficients yields denoising performance superior to that of existing solutions, both in CPSNR and visual quality sense."
1991149,21239,8494,S frame design for multiple description video coding,2005,"Multiple description (MD) video coding generates several descriptions so that any subset of descriptions can reconstruct video, which provides much error resilience. But most of the current MD video coding schemes are for two descriptions and for on-off channels, which is not suitable for packet-loss networks. This paper proposes a scheme to enhance the error resilience of traditional MD video coding in such environments, by periodically inserting S frames, a kind of switching frame, in the video stream to make the good description recover the 'bad' description, with very small redundancy. This proves to perform well in packet lossy networks especially at lower packet loss rate."
693760,21239,8494,A fast motion estimation algorithm for MPEG2 video using ripple-shaped search,1999,"Although many fast motion estimation algorithms for video coding such as H.261/3 and MPEG1 have been proposed, coding algorithms such as MPEG2 involve much larger search windows due to larger picture size and different coding structures. In this paper, we propose a novel fast motion estimation algorithm using ripple-shaped search. In the proposed algorithm, an initial, search point is determined using motion vector information of neighboring macroblocks, then a ripple-shaped search and four-step search (4SS) are used for macro and micro search, respectively. In addition, a break line method is employed to reduce the number of the mean absolute difference (MAD) calculations within a macroblock. Although 4SS can achieve almost the same speed up factors as the proposed algorithm, the PSNR performance of video coding at 4 and 10 Mbit/s by our algorithm is much better than that of a 4SS and is very close to that of full search."
2919218,21239,9078,Print quality assessment for stochastic clustered-dot halftones using compactness measures,2016,"Most electro-photographic printers prefer clustered-dot halftone textures for rendering smooth and stable prints. Clustered-dot halftone patterns can be periodic or aperiodic. As periodic clustered-dot halftone can lead to undesirable moire patterns, stochastic clustered-dot halftone textures are more preferred. There are available different screening methods to generate stochastic clustered-dot halftone textures but there are no standard print quality assessment measures that can be easily used for quantitatively evaluating and comparing different stochastic clustered-dot halftoning methods. We explore the use of compactness measures for this purpose, and also propose a new compactness measure that seems good metric to quantitatively compare and assess the print quality of different stochastic clustered-dot halftoning methods. Using the proposed metric, we compare three different stochastic clustered-dot halftoning methods, and our results are almost in agreement with psychophysical experiments results reported earlier."
2415368,21239,8494,Bayesian algorithm for video noise reduction in the wavelet domain,2005,"The paper proposes a Bayesian algorithm for the reduction of additive video noise in the wavelet domain. Spatial and temporal redundancies that exist in a video sequence in the time domain also persist in the wavelet domain. This allows video motion to be captured in the wavelet domain. Based on this fact, a new statistical model is proposed for video sequences. We not only model the subband coefficients in individual frames, but also the wavelet coefficient difference occurring between two consecutive frames using the generalized Laplacian distribution. Following this model, a Bayesian processor is developed that estimates the noise-free wavelet coefficients in the current frame, conditioned on the noisy coefficients in the current frame and the filtered coefficients in the past frame. Rigorous experimental results show that the proposed scheme outperforms several state-of-the-art spatio-temporal filters in time and wavelet domains in terms of quantitative performance as well as visual quality."
1936288,21239,8494,A Real Time and Low Cost Hardware Architecture for Video Abstraction System,2007,"In this paper, we propose a real time hardware architecture with low cost for histogram difference calculator to process and low-level extraction in our video abstraction system. The highlights are extracted from video data by directly combining simple visual and audio features of video data without specific domain knowledge and hence its low complexity makes it suitable to be implemented in embedded systems. Experimental results show that the condensed skimming clips comprise the interesting and informative parts and most meaningful semantic contents are extracted efficiently"
1500562,21239,9078,Temporally-Adaptive MAP Estimation for Video Denoising in the Wavelet Domain,2006,"In this paper, we propose a novel, temporally-adaptive maximum a posteriori (MAP) estimation algorithm for the reduction of additive video noise in the wavelet domain. We have exploited the fact that the spatial and temporal redundancies, which exist in a video sequence in the time domain, also persist in the wavelet domain. This allows the video motion to be captured in the wavelet domain. A new statistical model for video sequences is proposed, where the subband coefficients in individual frames as well as the wavelet co-efficient difference occurring between two consecutive frames are modeled using the generalized Laplacian distribution. Based on this model, a MAP estimator is developed that estimates the noise-free wavelet coefficients in the current frame, conditioned on the noisy coefficients in the current frame and the filtered coefficients in the past frame. The proposed algorithm has been tested using several different test sequences and corrupting noise powers and the experimental results show that the proposed scheme outperforms several state-of-the-art spatio-temporal filters in time and wavelet domains in terms of quantitative performance as well as visual quality."
1442947,21239,8494,Basis picking for matching pursuits audio compression,2006,"The 'basis picking' algorithm is an effective method for selecting dictionaries of basis functions for coding of data by matching pursuits. For illustration, bases are picked for a hybrid wavelet/matching pursuits audio codec. Bases are added to a codebook successively from a set of 1289 candidates according to their ranked signal to residual ratio (SRR) performance. Two training sets are used, and the codebooks are evaluated on the training sets and two test sequences. Picked bases make a robust codebook, the best of which outperforms traditional selection by ranked frequency of usage by a significant margin over a range of compressions of the audio clips used in this study."
527954,21239,8494,Video noise reduction in the wavelet domain using temporal decorrelation and adaptive thresholding,2006,"This paper proposes a novel wavelet domain based spatio-temporal filter for video denoising that exploits the temporal as well as spatial correlations which exist in the subband representation of the video sequence. The temporal redundancy or correlation among the corresponding wavelet coefficients in neighboring frames is minimized by the use of discrete cosine transform. These decorrelated noisy wavelet coefficients are then denoised spatially via a low-complexity wavelet shrinkage method, which utilizes the correlation that exists between subsequent resolution levels. The proposed scheme shows promising results and outperforms state-of-the-art spatio-temporal filters in time as well as wavelet domains, both in terms of PSNR and visual quality."
2316531,21239,23735,Guidance of magnetic intraocular microrobots by active defocused tracking,2004,Current laparoscopic techniques for intraocular surgery require that the vitreous humor is removed and at least three cannulas are inserted through the sidewalls of the eye. This paper investigates an alternate intraocular surgical technique based on the use of wireless microrobots guided by external magnetic fields. Issues investigated include the effects of magnetic and viscous drag forces faced by magnetic microrobots in the vitreous humor and the 3D visual servoing of these microrobots using a single microscope view. A new active defocused tracking method is proposed for visually servoing the microrobot along the optical axis of the microscope. This method uses a purposely defocused view of the microrobot to unambiguously resolve depth while servoing. Experimental results demonstrating the method with a microrobot visually servoed in 3D at 60 Hz using a single microscope view are presented.
2171473,21239,20332,The effects of performance-contingent financial incentives in online labor markets,2013,"Online labor markets such as Amazon Mechanical Turk (MTurk) have emerged as platforms that facilitate the allocation of productive effort across global economies. Many of these markets compensate workers with monetary payments. We study the effects of performance-contingent financial rewards on work quality and worker effort in MTurk via two experiments. We find that the magnitude of performance-contingent financial rewards alone affects neither quality nor effort. However, when workers working on two tasks of the same type in a sequence, the change in the magnitude of the reward over the two tasks affects both. In particular, both work quality and worker effort increase (alternatively decrease) as the reward increases (alternatively decreases) for the second task. This suggests the existence of the anchoring effect on workers' perception of incentives in MTurk and that this effect can be leveraged in workflow design to increase the effectiveness of financial incentives."
2561857,21239,20358,Incentivizing High Quality Crowdwork,2015,"We study the causal effects of financial incentives on the quality of crowdwork. We focus on performance-based payments (PBPs), bonus payments awarded to workers for producing high quality work. We design and run randomized behavioral experiments on the popular crowdsourcing platform Amazon Mechanical Turk with the goal of understanding when, where, and why PBPs help, identifying properties of the payment, payment structure, and the task itself that make them most effective. We provide examples of tasks for which PBPs do improve quality. For such tasks, the effectiveness of PBPs is not too sensitive to the threshold for quality required to receive the bonus, while the magnitude of the bonus must be large enough to make the reward salient. We also present examples of tasks for which PBPs do not improve quality. Our results suggest that for PBPs to improve quality, the task must be effort-responsive: the task must allow workers to produce higher quality work by exerting more effort. We also give a simple method to determine if a task is effort-responsive a priori. Furthermore, our experiments suggest that all payments on Mechanical Turk are, to some degree, implicitly performance-based in that workers believe their work may be rejected if their performance is sufficiently poor. Finally, we propose a new model of worker behavior that extends the standard principal-agent model from economics to include a worker's subjective beliefs about his likelihood of being paid, and show that the predictions of this model are in line with our experimental findings. This model may be useful as a foundation for theoretical studies of incentives in crowdsourcing markets."
2534262,21239,20332,Acquiring Reliable Ratings from the Crowd,2015,We address the problem of acquiring reliable ratings of items such as restaurants or movies from the crowd. We propose a crowdsourcing platform that takes into consideration the workers’ skills with respect to the items being rated and assigns workers the best items to rate. Our platform focuses on acquiring ratings from skilled workers and for items that only have a few ratings. We evaluate the effectiveness of our system using a real-world dataset about restaurants.
584614,21239,8494,PLT versus KLT [transforms],1999,"In this paper a new class of transforms named Prediction-based Lower triangular Transform (PLT) is introduced for signal compression. The PLT is a nonunitary transform that yields the same coding gain as the Kahunen-Loeve transform (KLT). Unlike KLT, the derivation of PLT does not involve any eigen problem. An M-dimensional PLT can be solved by the Levinson-Durbin recursion formula in O(M/sup 2/). Moreover the complexity of PLT is less than one half of KLT. For AR(1) inputs, the PLT has a closed form expression and needs only (M-1) multiplications."
549158,21239,20332,Instance-Privacy Preserving Crowdsourcing,2014,"Crowdsourcing is a technique to outsource tasks to a number of workers. Although crowdsourcing has many advantages, it gives rise to the risk that sensitive information may be leaked, which has limited the spread of its popularity. Task instances (data workers receive to process tasks) often contain sensitive information, which can be extracted by workers. For example, in an audio transcription task, an audio file corresponds to an instance, and the content of the audio (e.g., the abstract of a meeting) can be sensitive information. In this paper, we propose a quantitative analysis framework for the instance privacy problem. The proposed framework supplies us performance measures of instance privacy preserving protocols. As a case study, we apply the proposed framework to an instance clipping protocol and analyze the properties of the protocol. The protocol preserves privacy by clipping instances to limit the amount of information workers obtain. The results show that the protocol can balance task performance and instance privacy preservation. They also show that the proposed measure is consistent with standard measures, which validates the proposed measure."
2148896,21239,23922,Optimum Statistical Estimation with Strategic Data Sources,2014,"We propose an optimum mechanism for providing monetary incentives to the data sources of a statistical estimator such as linear regression, so that high q uality data is provided at low cost, in the sense that the weighted sum of payments and estimation error is minimized. The mechanism applies to a broad range of estimators, including linear and polynomial regression, kernel regression, and, under some additional assumptions, ridge regression. It also generaliz es to several objectives, including minimizing estimation error subject to budget constraints. Besides ou r concrete results for regression problems, we contribute a mechanism design framework through which to design and analyze statistical estimators whose examples are supplied by workers with cost for labeling said examples."
300296,21239,11321,Adaptive Task Assignment for Crowdsourced Classification,2013,"Crowdsourcing markets have gained popularity as a tool for inexpensively collecting data from diverse populations of workers. Classification tasks, in which workers provide labels (such as offensive or not offensive) for instances (such as websites), are among the most common tasks posted, but due to human error and the prevalence of spam, the labels collected are often noisy. This problem is typically addressed by collecting labels for each instance from multiple workers and combining them in a clever way, but the question of how to choose which tasks to assign to each worker is often overlooked. We investigate the problem of task assignment and label inference for heterogeneous classification tasks. By applying online primal-dual techniques, we derive a provably near-optimal adaptive assignment algorithm. We show that adaptively assigning workers to tasks can lead to more accurate predictions at a lower cost when the available workers are diverse."
338575,21239,20332,Parallel Task Routing for Crowdsourcing,2014,"An ideal crowdsourcing or citizen-science system would route tasks to the most appropriate workers, but the best assignment is unclear because workers have varying skill, tasks have varying difficulty, and assigning several workers to a single task may significantly improve output quality. This paper defines a space of task routing problems, proves that even the simplest is NP-hard, and develops several approximation algorithms for parallel routing problems. We show that an intuitive class of requesters' utility functions is submodular, which lets us provide iterative methods for dynamically allocating batches of tasks that make near-optimal use of available workers in each round. Experiments with live oDesk workers show that our task routing algorithm uses only 48% of the human labor compared to the commonly used round-robin strategy. Further, we provide versions of our task routing algorithm which enable it to scale to large numbers of workers and questions and to handle workers with variable response times while still providing significant benefit over common baselines."
609247,21239,20358,Getting More for Less: Optimized Crowdsourcing with Dynamic Tasks and Goals,2015,"In crowdsourcing systems, the interests of contributing participants and system stakeholders are often not fully aligned. Participants seek to learn, be entertained, and perform easy tasks, which offer them instant gratification; system stakeholders want users to complete more difficult tasks, which bring higher value to the crowdsourced application. We directly address this problem by presenting techniques that optimize the crowdsourcing process by jointly maximizing the user longevity in the system and the true value that the system derives from user participation.   We first present models that predict the survival probability of a user at any given moment, that is, the probability that a user will proceed to the next task offered by the system. We then leverage this survival model to dynamically decide what task to assign and what motivating goals to present to the user. This allows us to jointly optimize for the short term (getting difficult tasks done) and for the long term (keeping users engaged for longer periods of time).   We show that dynamically assigning tasks significantly increases the value of a crowdsourcing system. In an extensive empirical evaluation, we observed that our task allocation strategy increases the amount of information collected by up to 117.8%. We also explore the utility of motivating users with goals. We demonstrate that setting specific, static goals can be highly detrimental to the long-term user participation, as the completion of a goal (e.g., earning a badge) is also a common drop-off point for many users. We show that setting the goals dynamically, in conjunction with judicious allocation of tasks, increases the amount of information collected by the crowdsourcing system by up to 249%, compared to the existing baselines that use fixed objectives."
2880443,21239,9078,Kinect-based gait analysis for automatic frailty syndrome assessment,2016,"Smart living and well aging represent key challenges for our society. The precursor state of adverse outcomes that characterize aging has been recognized from scientific community with the frailty syndrome, determined by the loss of physical and psychological capacities. In this paper we define gait and posture indexes that can be effectively and unobtrusively measured using computer vision and RGBD sensors, e.g. the popular MS Kinect. In this study we present preliminary results showing evidence that the proposed approach can pave the way to the design of an automatic and objective tool for detection and early prevention of frailty."
2649157,21239,20332,Leveraging crowdsourcing to detect improper tasks in crowdsourcing marketplaces,2013,"Controlling the quality of tasks is a major challenge in crowdsourcing marketplaces. Most of the existing crowdsourcing services prohibit requesters from posting illegal or objectionable tasks. Operators in the marketplaces have to monitor the tasks continuously to find such improper tasks; however, it is too expensive to manually investigate each task. In this paper, we present the reports of our trial study on automatic detection of improper tasks to support the monitoring of activities by marketplace operators. We perform experiments using real task data from a commercial crowdsourcing marketplace and show that the classifier trained by the operator judgments achieves high accuracy in detecting improper tasks. In addition, to reduce the annotation costs of the operator and improve the classification accuracy, we consider the use of crowdsourcing for task annotation. We hire a group of crowdsourcing (non-expert) workers to monitor posted tasks, and incorporate their judgments into the training data of the classifier. By applying quality control techniques to handle the variability in worker reliability, our results show that the use of non-expert judgments by crowdsourcing workers in combination with expert judgments improves the accuracy of detecting improper crowdsourcing tasks."
2294433,21239,10237,Using Crowdsourcing to Compare Document Recommendation Strategies for Conversations,2012,"This paper explores a crowdsourcing approach to the evaluation of a document recommender system intended for use in meetings. The system uses words from the conversation to perform just-in-time document retrieval. We compare several versions of the system, including the use of keywords, retrieval using semantic similarity, and the possibility for user initiative. The system’s results are submitted for comparative evaluations to workers recruited via a crowdsourcing platform, Amazon’s Mechanical Turk. We introduce a new method, Pearson Correlation Coecient-Informa tion Entropy (PCC-H), to abstract over the quality of the workers’ judgments and produce system-level scores. We measure the workers’ reliability by the inter-rater agreement of each of them against the others, and use entropy to weight the diculty of each comparison task. The proposed evaluation method is shown to be reliable, and the results show that adding user initiative improves the relevance of recommendations."
2301276,21239,11321,Optimistic Knowledge Gradient Policy for Optimal Budget Allocation in Crowdsourcing,2013,"In real crowdsourcing applications, each label from a crowd usually comes with a certain cost. Given a pre-xed amount of budget, since dierent tasks have dierent ambiguities and dierent workers have dierent expertises, we want to nd an optimal way to allocate the budget among instance-worker pairs such that the overall label quality can be maximized. To address this issue, we start from the simplest setting in which all workers are assumed to be perfect. We formulate the problem as a Bayesian Markov Decision Process (MDP). Using the dynamic programming (DP) algorithm, one can obtain the optimal allocation policy for a given budget. However, DP is computationally intractable. To solve the computational challenge, we propose a novel approximate policy which is called optimistic knowledge gradient. It is practically ecient while theoretically its consistency can be guaranteed. We then extend the MDP framework to deal with inhomogeneous workers and tasks with contextual information available. The experiments on both simulated and real data demonstrate the superiority of our method."
559001,21239,8960,Double or nothing: multiplicative incentive mechanisms for Crowdsourcing,2015,"Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural no-free-lunch requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers. Interestingly, this unique mechanism takes a multiplicative form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over several hundred workers, we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure."
193606,21239,9078,Emotional representation and animation of 3D facial models: the INTERFACE approach,2001,"The activity of the IST European Research Project INTERFACE is oriented to the development of advanced interfaces based on an interaction engine capable of interpreting natural voice or gestures and answering through artificial voice and animation feed-backs. Classic technologies of voice recognition and synthesis have been integrated with innovative algorithms for emotional speech modeling. Video analysis techniques have been developed for interpreting human gestures capable of being reproduced artificially through three-dimensional virtual animated faces and bodies. The INTERFACE objective is that of integrating all these analysis/synthesis techniques into a single piece of technology capable of accepting queries through talk and facial expressions, as we are used to do in everyday life with relatives, friends and colleagues, and of providing answers to the user through virtual actors capable of artificially reproducing human voice and gestures. The INTERFACE achievements have been recently demonstrated with live demos during international exhibitions and conferences."
2083912,21239,11321,Aggregating Ordinal Labels from Crowds by Minimax Conditional Entropy,2014,We propose a method to aggregate noisy ordinal labels collected from a crowd of workers or annotators. Eliciting ordinal labels is important in tasks such as judging web search quality and rating products. Our method is motivated by the observation that workers usually have difficulty distinguishing between two adjacent ordinal classes whereas distinguishing between two classes which are far away from each other is much easier. We formulate our method as minimax conditional entropy subject to constraints which encode this observation. Empirical evaluations on real datasets demonstrate significant improvements over existing methods.
2659353,21239,8927,Quality Management in Crowdsourcing using Gold Judges Behavior,2016,"Crowdsourcing relevance labels has become an accepted practice for the evaluation of IR systems, where the task of constructing a test collection is distributed over large populations of unknown users with widely varied skills and motivations. Typical methods to check and ensure the quality of the crowd's output is to inject work tasks with known answers (gold tasks) on which workers' performance can be measured. However, gold tasks are expensive to create and have limited application. A more recent trend is to monitor the workers' interactions during a task and estimate their work quality based on their behavior. In this paper, we show that without gold behavior signals that reflect trusted interaction patterns, classifiers can perform poorly, especially for complex tasks, which can lead to high quality crowd workers getting blocked while poorly performing workers remain undetected. Through a series of crowdsourcing experiments, we compare the behaviors of trained professional judges and crowd workers and then use the trained judges' behavior signals as gold behavior to train a classifier to detect poorly performing crowd workers. Our experiments show that classification accuracy almost doubles in some tasks with the use of gold behavior data."
2613699,21239,374,On Security of Content-Based Video Stream Authentication,2015,"Content-based authentication CBA schemes are used to authenticate multimedia streams while allowing content-preserving manipulations such as bit-rate transcoding. In this paper, we survey and classify existing transform-domain CBA schemes for videos into two categories, and point out that in contrary to CBA for images, there exists a common design flaw in these schemes. We present the principles based on video coding concept on how the flaw can be exploited to mount semantic-changing attacks in the transform domain that cannot be detected by existing CBA schemes. We show attack examples including content removal, modification and insertion attacks. Noting that these CBA schemes are designed at the macroblock level, we discuss, from the attacker's point of view, the conditions in attacking content-based authenticated macroblocks."
1249850,21239,20411,BATC: a benchmark for aggregation techniques in crowdsourcing,2013,"As the volumes of AI problems involving human knowledge are likely to soar, crowdsourcing has become essential in a wide range of world-wide-web applications. One of the biggest challenges of crowdsourcing is aggregating the answers collected from crowd workers; and thus, many aggregate techniques have been proposed. However, given a new application, it is difficult for users to choose the best-suited technique as well as appropriate parameter values since each of these techniques has distinct performance characteristics depending on various factors (e.g. worker expertise, question difficulty). In this paper, we develop a benchmarking tool that allows to (i) simulate the crowd and (ii) evaluate aggregate techniques in different aspects (accuracy, sensitivity to spammers, etc.). We believe that this tool will be able to serve as a practical guideline for both researchers and software developers. While researchers can use our tool to assess existing or new techniques, developers can reuse its components to reduce the development complexity."
2467606,21239,422,Learning from crowds in the presence of schools of thought,2012,"Crowdsourcing has recently become popular among machine learning researchers and social scientists as an effective way to collect large-scale experimental data from distributed workers. To extract useful information from the cheap but potentially unreliable answers to tasks, a key problem is to identify reliable workers as well as unambiguous tasks. Although for objective tasks that have one correct answer per task, previous works can estimate worker reliability and task clarity based on the single gold standard assumption, for tasks that are subjective and accept multiple reasonable answers that workers may be grouped into, a phenomenon called  schools of thought , existing models cannot be trivially applied. In this work, we present a statistical model to estimate worker reliability and task clarity without resorting to the single gold standard assumption. This is instantiated by explicitly characterizing the grouping behavior to form schools of thought with a rank-1 factorization of a worker-task groupsize matrix. Instead of performing an intermediate inference step, which can be expensive and unstable, we present an algorithm to analytically compute the sizes of different groups. We perform extensive empirical studies on real data collected from Amazon Mechanical Turk. Our method discovers the schools of thought, shows reasonable estimation of worker reliability and task clarity, and is robust to hyperparameter changes. Furthermore, our estimated worker reliability can be used to improve the gold standard prediction for objective tasks."
653804,21239,22113,Bayesian modelling of community-based multidimensional trust in participatory sensing under data sparsity,2015,"We propose a new Bayesian model for reliable aggregation of crowdsourced estimates of real-valued quantities in participatory sensing applications. Existing approaches focus on probabilistic modelling of user's reliability as the key to accurate aggregation. However, these are either limited to estimating discrete quantities, or require a significant number of reports from each user to accurately model their reliability. To mitigate these issues, we adopt a community-based approach, which reduces the data required to reliably aggregate real-valued estimates, by leveraging correlations between the reporting behaviour of users belonging to different communities. As a result, our method is up to 16:6% more accurate than existing state-of-the-art methods and is up to 49% more effective under data sparsity when used to estimate Wi-Fi hotspot locations in a real-world crowdsourcing application."
2958484,21239,9078,Segmentation of retinal vessels in adaptive optics images for assessment of vasculitis,2016,"In this paper we propose a new method for segmenting retinal vessels in adaptive optics images. This method is particularly dedicated for segmenting vessels with significant morphological alterations due to vasculitis, but it is also accurate for vessels with moderate or without alteration. It relies on a pre-segmentation step which is crucial for the robustness and accuracy of the results. This step is based on a specific morphological processing of isolines of the original image: they constitute of good basis for the segmentation because they are disposed along the wall borders of the vessels. Regularization is then performed using active contour model embedding a parallelism constraint. This novel model allows precise segmenting inner and outer walls of the vessel. In particular it is more accurate in the case of vasculitis than the existing methods. This is the only method that allows quantification. The results and the runtime make it suitable for clinical use."
2534252,21239,20332,Identifying and Accounting for Task-Dependent Bias in Crowdsourcing.,2015,"Models for aggregating contributions by crowd workers have been shown to be challenged by the rise of task-specific biases or errors. Task-dependent errors in assessment may shift the majority opinion of even large numbers of workers to an incorrect answer. We introduce and evaluate probabilistic models that can detect and correct task-dependent bias automatically. First, we show how to build and use probabilistic graphical models for jointly modeling task features, workers' biases, worker contributions and ground truth answers of tasks so that task-dependent bias can be corrected. Second, we show how the approach can perform a type of transfer learning among workers to address the issue of annotation sparsity. We evaluate the models with varying complexity on a large data set collected from a citizen science project and show that the models are effective at correcting the task-dependent worker bias. Finally, we investigate the use of active learning to guide the acquisition of expert assessments to enable automatic detection and correction of worker bias."
643273,21239,11321,Multiview Triplet Embedding: Learning Attributes in Multiple Maps,2015,"For humans, it is usually easier to make statements about the similarity of objects in relative, rather than absolute terms. Moreover, subjective comparisons of objects can be based on a number of different and independent attributes. For example, objects can be compared based on their shape, color, etc. In this paper, we consider the problem of uncovering these hidden attributes given a set of relative distance judgments in the form of triplets. The attribute that was used to generate a particular triplet in this set is unknown. Such data occurs, e.g., in crowdsourcing applications where the triplets are collected from a large group of workers.#R##N##R##N#We propose the Multiview Triplet Embedding (MVTE) algorithm that produces a number of low-dimensional maps, each corresponding to one of the hidden attributes. The method can be used to assess how many different attributes were used to create the triplets, as well as to assess the difficulty of a distance comparison task, and find objects that have multiple interpretations in relation to the other objects."
2228962,21239,20358,Community-based bayesian aggregation models for crowdsourcing,2014,"This paper addresses the problem of extracting accurate labels from crowdsourced datasets, a key challenge in crowdsourcing. Prior work has focused on modeling the reliability of individual workers, for instance, by way of confusion matrices, and using these latent traits to estimate the true labels more accurately. However, this strategy becomes ineffective when there are too few labels per worker to reliably estimate their quality. To mitigate this issue, we propose a novel community-based Bayesian label aggregation model, CommunityBCC, which assumes that crowd workers conform to a few different types, where each type represents a group of workers with similar confusion matrices. We assume that each worker belongs to a certain community, where the worker's confusion matrix is similar to (a perturbation of) the community's confusion matrix. Our model can then learn a set of key latent features: (i) the confusion matrix of each community, (ii) the community membership of each user, and (iii) the aggregated label of each item. We compare the performance of our model against established aggregation methods on a number of large-scale, real-world crowdsourcing datasets. Our experimental results show that our CommunityBCC model consistently outperforms state-of-the-art label aggregation methods, requiring, on average, 50% less data to pass the 90% accuracy mark."
539987,21239,20332,EM-Based Inference of True Labels Using Confidence Judgments,2013,"We have developed a method for accurately inferring true labels from labels provided by crowdsourcing workers, with the aid of self-reported confidence judgments in their labels. Although confidence judgments can be useful information for estimating the quality of the provided labels, some workers are overconfident about the quality of their labels while others are underconfident. To address this problem, we extended the Dawid-Skene model and created a probabilistic model that considers the differences among workers in their accuracy of confidence judgments. Results of experiments using actual crowdsourced data showed that incorporating workers' confidence judgments can improve the accuracy of inferred labels."
1436578,21239,20358,The wisdom of minority: discovering and targeting the right group of workers for crowdsourcing,2014,"Worker reliability is a longstanding issue in crowdsourcing, and the automatic discovery of high quality workers is an important practical problem. Most previous work on this problem mainly focuses on estimating the quality of each individual worker jointly with the true answer of each task. However, in practice, for some tasks, worker quality could be associated with some explicit characteristics of the worker, such as education level, major and age. So the following question arises: how do we automatically discover related worker attributes for a given task, and further utilize the findings to improve data quality? In this paper, we propose a general crowd targeting framework that can automatically discover, for a given task, if any group of workers based on their attributes have higher quality on average; and target such groups, if they exist, for future work on the same task. Our crowd targeting framework is complementary to traditional worker quality estimation approaches. Furthermore, an advantage of our framework is that it is more budget efficient because we are able to target potentially good workers before they actually do the task. Experiments on real datasets show that the accuracy of final prediction can be improved significantly for the same budget (or even less budget in some cases). Our framework can be applied to many real word tasks and can be easily integrated in current crowdsourcing platforms."
2555957,21239,20332,On quality control and machine learning in crowdsourcing,2011,"The advent of crowdsourcing has created a variety of new opportunities for improving upon traditional methods of data collection and annotation. This in turn has created intriguing new opportunities for data-driven machine learning (ML). Convenient access to crowd workers for simple data collection has further generalized to leveraging more arbitrary crowd-based human computation (von Ahn 2005) to supplement automated ML. While new potential applications of crowdsourcing continue to emerge, a variety of practical and sometimes unexpected obstacles have already limited the degree to which its promised potential can be actually realized in practice. This paper considers two particular aspects of crowdsourcing and their interplay, data quality control (QC) and ML, reflecting on where we have been, where we are, and where we might go from here."
2728693,21239,11321,Exact Exponent in Optimal Rates for Crowdsourcing,2016,"In many machine learning applications, crowdsourcing has become the primary means for label collection. In this paper, we study the optimal error rate for aggregating labels provided by a set of non-expert workers. Under the classic Dawid-Skene model, we establish matching upper and lower bounds with an exact exponent $mI(\pi)$ in which $m$ is the number of workers and $I(\pi)$ the average Chernoff information that characterizes the workers' collective ability. Such an exact characterization of the error exponent allows us to state a precise sample size requirement $m>\frac{1}{I(\pi)}\log\frac{1}{\epsilon}$ in order to achieve an $\epsilon$ misclassification error. In addition, our results imply the optimality of various EM algorithms for crowdsourcing initialized by consistent estimators."
2015929,21239,8960,Reputation-based Worker Filtering in Crowdsourcing,2014,"In this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks. Unlike most prior work which has examined this problem under the random worker paradigm, we consider a much broader class of adversarial workers with no specific assumptions on their labeling strategy. Our key contribution is the design of a computationally efficient reputation algorithm to identify and filter out these adversarial workers in crowd-sourcing systems. Our algorithm uses the concept of optimal semi-matchings in conjunction with worker penalties based on label disagreements, to assign a reputation score for every worker. We provide strong theoretical guarantees for deterministic adversarial strategies as well as the extreme case of sophisticated adversaries where we analyze the worst-case behavior of our algorithm. Finally, we show that our reputation algorithm can significantly improve the accuracy of existing label aggregation algorithms in real-world crowdsourcing datasets."
1411478,21239,422,Cross-task crowdsourcing,2013,"Crowdsourcing is an effective method for collecting labeled data for various data mining tasks. It is critical to ensure the veracity of the produced data because responses collected from different users may be noisy and unreliable. Previous works solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in each task individually. In this case, each single task needs large amounts of data to provide accurate estimations. However, in practice, budgets provided by customers for a given target task may be limited, and hence each question can be presented to only a few users where each user can answer only a few questions. This data sparsity problem can cause previous approaches to perform poorly due to the overfitting problem on rare data and eventually damage the data veracity. Fortunately, in real-world applications, users can answer questions from multiple historical tasks. For example, one can annotate images as well as label the sentiment of a given title. In this paper, we employ transfer learning, which borrows knowledge from auxiliary historical tasks to improve the data veracity in a given target task. The motivation is that users have stable characteristics across different crowdsourcing tasks and thus data from different tasks can be exploited collectively to estimate users' abilities in the target task. We propose a hierarchical Bayesian model, TLC (Transfer Learning for Crowdsourcing), to implement this idea by considering the overlapping users as a bridge. In addition, to avoid possible negative impact, TLC introduces task-specific factors to model task differences. The experimental results show that TLC significantly improves the accuracy over several state-of-the-art non-transfer-learning approaches under very limited budget in various labeling tasks."
2864988,21239,507,Towards Globally Optimal Crowdsourcing Quality Management: The Uniform Worker Setting,2016,"We study crowdsourcing quality management, that is, given worker responses to a set of tasks, our goal is to jointly estimate the true answers for the tasks, as well as the quality of the workers. Prior work on this problem relies primarily on applying Expectation-Maximization (EM) on the underlying maximum likelihood problem to estimate true answers as well as worker quality. Unfortunately, EM only provides a locally optimal solution rather than a globally optimal one. Other solutions to the problem (that do not leverage EM) fail to provide global optimality guarantees as well. In this paper, we focus on filtering, where tasks require the evaluation of a yes/no predicate, and rating, where tasks elicit integer scores from a finite domain. We design algorithms for finding the global optimal estimates of correct task answers and worker quality for the underlying maximum likelihood problem, and characterize the complexity of these algorithms. Our algorithms conceptually consider all mappings from tasks to true answers (typically a very large number), leveraging two key ideas to reduce, by several orders of magnitude, the number of mappings under consideration, while preserving optimality. We also demonstrate that these algorithms often find more accurate estimates than EM-based algorithms. This paper makes an important contribution towards understanding the inherent complexity of globally optimal crowdsourcing quality management."
3027991,21239,8502,Crack Segmentation by Leveraging Multiple Frames of Varying Illumination,2017,"In this work, we present an automated inspection approach to assist remote visual examinations of nuclear power plant components. An automated approach would require detecting often low contrast cracks that could be surrounded by or even within textures with similar appearances such as welding, scratches, and grind marks. We propose a crack segmentation method for remote visual examination videos by aggregating the pixel-level classification confidence from multiple frames consisting of different illumination conditions. A dataset of 685 pixel-level ground truth images consisting of 37 cracks from remote visual examination videos is used for evaluation. The results show that the proposed method provides a significant improvement over hand-crafted feature based segmentation and 9% over convolutional neural network based method."
2359074,21239,30,CBIT - context-based image transmission,2001,"Few networks offer sufficient bandwidth for the transmission of high resolution two and three-dimensional medical image sets without incurring significant latency. Traditional compression methods achieve bit-rate reduction based on pixel statistics and ignore visual cues that are important in identifying visually informative regions. The paper describes an approach to managing image transmission in which spatial regions are selected and prioritized for transmission so that visually informative data is received in a timely manner. This context-based image transmission (CBIT) scheme is a lossless form of progressive image transmission (PIT) in which gross structure, represented by an approximate iconic image, is transmitted first. Each part of this iconic image is progressively updated, using a simple set of rules that take into account viewing requirements. CBIT is realized using knowledge about image composition to segment, label, prioritize, and fit geometric models to regions of an image. Tests, using neurological images, show that, with CBIT, a valuable transmitted image is received with a latency that is about one-tenth that of traditional PIT schemes. Frequently, the necessary regions of the image are transmitted in about half the time taken to transmit the full image."
2843546,21239,9078,Gaussian process transforms,2016,"We introduce the Gaussian Process Transform (GPT), an orthogonal transform for signals defined on a finite but otherwise arbitrary set of points in a Euclidean domain. The GPT is obtained as the Karhunen-Loeve Transform (KLT) of the marginalization of a Gaussian Process defined on the domain. Compared to the Graph Transform (GT), which is the KLT of a Gauss Markov Random Field over the same set of points whose neighborhood structure is inherited from the Euclidean domain, the GPT has up to 6 dB higher coding gain."
1938666,21239,8494,A learning-based video compression on low-quality data by unscented kalman filters with Gaussian process regression,2011,"With the ever increasing concern of vision-based video analysis and coding over resource-limited systems, this paper proposes a novel video coding scheme that exploits low-quality video data and formulates as an inverse learning based video reconstruction from online training by diverse stochastic processes. Given a sparsely sampled incomplete data, the intrinsic nonlocal and spatio-temporal geometric regularity related to online training examples in the key frames are considered as a state-dependent uncertainty estimation problem using Gaussian Process (GP) regression. Unlike non-parametric or exemplar-based sampling methods, we consider non-parametric system models for sequential state estimation by using the Unscented Kalman Filter (UKF) as the state estimator. It inherits the unscented transform for linearization to the transition function and the observation function. Once an approximate motion and observation model is available, it can naturally be incorporated to make a further performance improvement."
267438,21239,8494,Generalized lossless data hiding by multiple predictors,2006,"Digital watermarking is to prove the ownership and the authenticity of the media. However, as some applications, such as medical and military, are sensitive to distortion, this highlights the needs of lossless watermarking. In this paper, we propose a new lossless data hiding algorithm by using multiple predictors, which extends and generalizes our previous watermarking idea (Yip, 2005). By using different predictors with different characteristics, we can choose the embedding location to be low variance region or high variance region. The PSNR and the payload capacity are high and there are no salt-and-peppers artifacts."
2936091,21239,9078,Multi-view semantic temporal video segmentation,2016,"In this work, we propose a multi-view temporal video segmentation approach that employs a Gaussian scoring process for determining the best segmentation positions. By exploiting the semantic action information that the dense trajectories video description offers, this method can detect intra-shot actions as well, unlike shot boundary detection approaches. We compare the temporal segmentation results of the proposed method to both single-view and multi-view methods, and also compare the action recognition results obtained on ground truth video segments to the ones obtained on the proposed multi-view segments, on the IMPART multi-view action data set."
2284256,21239,8228,Motion energy estimation of group of pictures and its application for Unequal Error Protection of H.264/AVC video bitstreams,2014,"An Unequal Error Protection (UEP) technique for H.264/AVC video bitstreams constituted by GOP structure is proposed. It is formed by motion energy concept, which is defined as the energy necessary for displacement of macroblocks between two consecutive frames. The importance of GOPs is determined by their motion energies estimated by Lagrange polynomial. This estimation method expresses a function, which considers previous GOPs having a high correlation with each other. Then, video frames are unequally protected based on their motion activities and the importance of GOPs. Since previous GOPs are used for predicting the current one, no delay for transmission of video bitstreams is required. Simulation results show that with similar Forward Error Correcting (FEC) code rates applied for different protection levels, the proposed technique outperforms other GOP-based UEP techniques."
452768,21239,8228,Optimized packet scheduling for live streaming on peer-to-peer network with network coding,2015,"This paper proposed an optimized packet scheduling algorithm for live peer-to-peer streaming system, where network coding technique is extended to improve the efficiency in bandwidth utilization. We identify a problem of undesirable non-innovative packet transmission due to the latency of buffer-map update among peers, which, with many previous proposed techniques, yields to bandwidth inefficiencies. In the proposed system, we present an optimized packet scheduling algorithm for forecasting the number of required packets at parent nodes in advance, a selection mechanism for selecting and encoding packets for forwarding, and an adaptive push algorithm for smart frame skipping. The proposed optimized packet scheduling algorithm calculates the minimum number of required packets for full rate transmission. Transmitting as the scheduled results can reduce non-innovative packet transmission, save limited bandwidth for innovative transmission, thereby improving the streaming continuity. The simulation results show that the proposed scheme provides significantly better video quality, delivery ratio, lower redundancy rate, and higher innovative video packet rate compared with previous packet scheduling algorithms."
385826,21239,8494,Parameter-free view synthesis distortion model with application to depth video coding,2015,"Depth coding in 3D video is unique in that its quality is measured by the view synthesis distortion (VSD) rather than its own depth distortion, which further complicates the coding optimization as VSD is related to both the depth and texture quality. We propose a parameter-free VSD model to directly estimate the impact of depth errors on VSD on the small block basis, given its depth distortion. The proposed model is incorporated into rate-distortion optimization of depth coding, by adapting the Lagrange multiplier and optimizing the selection of quantization parameters. Simulation results show that our proposed scheme improves the BDPSNR and BDBR by 0.6dB PSNR and 20% bits saving on average compared with H.264 coding standard in depth coding, while keeping the implementation easy."
2909227,21239,11104,GPS derived PWV for rainfall monitoring,2016,"Precipitable Water Vapor (PWV) is a good source to monitor precipitation. It is defined by the amount of water vapor present in atmosphere. Traditionally, radiosondes and microwave radiometers were used to derive PWV. However, these devices have poor temporal resolutions and high operational costs. Therefore, GPS signal delay is now widely used for such purposes. The main aim of this paper is to study relationship between GPS derived PWV and precipitation. We present an analysis which shows that PWV increases before any rainfall event, while it decreases after the rainfall event. We also derive a threshold PWV that detects the occurrence of rainfall, once PWV exceeds the threshold value. PWV and rainfall data of June 2010 and 2011 are used for validation."
895844,21239,11104,"Design of low-cost, compact and weather-proof whole sky imagers for high-dynamic-range captures",2015,"Ground-based whole sky imagers are popular for monitoring cloud formations, which is necessary for various applications. We present two new Wide Angle High-Resolution Sky Imaging System (WAHRSIS) models, which were designed especially to withstand the hot and humid climate of Singapore. The first uses a fully sealed casing, whose interior temperature is regulated using a Peltier cooler. The second features a double roof design with ventilation grids on the sides, allowing the outside air to flow through the device. Measurements of temperature inside these two devices show their ability to operate in Singapore weather conditions. Unlike our original WAHRSIS model, neither uses a mechanical sun blocker to prevent the direct sunlight from reaching the camera; instead they rely on high-dynamic-range imaging (HDRI) techniques to reduce the glare from the sun."
2701302,21239,9463,Early Gains Matter: A Case for Preferring Generative over Discriminative Crowdsourcing Models,2015,"In modern practice, labeling a dataset often involves aggregating annotator judgments obtained from crowdsourcing. State-of-theart aggregation is performed via inference on probabilistic models, some of which are dataaware, meaning that they leverage features of the data (e.g., words in a document) in addition to annotator judgments. Previous work largely prefers discriminatively trained conditional models. This paper demonstrates that a data-aware crowdsourcing model incorporating a generative multinomial data model enjoys a strong competitive advantage over its discriminative log-linear counterpart in the typical crowdsourcing setting. That is, the generative approach is better except when the annotators are highly accurate in which case simple majority vote is often sufficient. Additionally, we present a novel mean-field variational inference algorithm for the generative model that significantly improves on the previously reported state-of-the-art for that model. We validate our conclusions on six text classification datasets with both human-generated and synthetic annotations."
1762028,21239,369,A vision system for intelligent mission profiles of micro air vehicles,2004,"Recently, much progress has been made toward the development of small-scale aircraft, known broadly as Micro Air Vehicles (MAVs). Until recently, these platforms were exclusively remotely piloted, with no autonomous or intelligent capabilities, due at least in part to stringent payload restrictions that limit onboard sensors. However, the one sensor that is critical to most conceivable MAV missions, such as remote surveillance, is an onboard video camera and transmitter that streams flight video to a nearby ground station. Exploitation of this key sensor is, therefore, desirable, since no additional onboard hardware (and weight) is required. As such, in this paper we develop a general and unified computer vision framework for MAVs that not only addresses basic flight stability and control, but enables more intelligent missions as well. This paper is organized as follows. We first develop a real-time feature extraction method called multiscale linear discriminant analysis (MLDA), which explicitly incorporates color into its feature representation, while implicitly encoding texture through a dynamic multiscale representation of image details. We demonstrate key advantages of MLDA over other possible multiscale approaches (e.g., wavelets), especially in dealing with transient video noise. Next, we show that MLDA provides a natural framework for performing real-time horizon detection. We report horizon-detection results for a range of images differing in lighting and scenery and quantify performance as a function of image noise. Furthermore, we show how horizon detection naturally leads to closed-loop flight stabilization. Then, we motivate the use of tree-structured belief networks (TSBNs) with MLDA features for sky/ground segmentation. This type of segmentation augments basic horizon detection and enables certain MAV missions where prior assumptions about the flight vehicle's orientation are not possible. Again, we report segmentation results for a range of images and quantify robustness to image noise. Finally, we demonstrate the seamless extension of this framework, through the idea of visual contexts, for the detection of artificial objects and/or structures and illustrate several examples of such additional segmentation. This extension thus enables mission profiles that require, for example, following a specific road or the tracking of moving ground objects. Throughout, our approach and algorithms are heavily influenced by real-time constraints and robustness to transient video noise."
2782988,21239,8502,Detection of cracks in nuclear power plant using spatial-temporal grouping of local patches,2016,"Robust inspection is important to ensure the safety of nuclear power plant components. An automated approach would require detecting often low contrast cracks that could be surrounded by or even within textures with similar appearances such as welding, scratches and grind marks. We propose a crack detection method for nuclear power plant inspection videos by fine tuning a deep neural network for detecting local patches containing cracks which are then grouped in spatial-temporal space for group-level classification. We evaluate the proposed method on a data set consisting of 17 videos consisting of nearly 150,000 frames of inspection video and provide comparison to prior methods."
2888616,21239,9078,3-D point cloud normal estimation based on fitting algebraic spheres,2016,"In this paper, we proposed a novel method to estimate the normal information of the unorganized point cloud, which plays an essential part in 3D reconstruction. The original point cloud is firstly divided into cubes with different sizes by the octree method. Then, we fit algebraic sphere in each cube instead of planar surface to improve the accuracy of normal estimation. Finally, the raw normals are refined by a weighting function which increases along with the depth of octree. For evaluation, we compute the intersection angles between the estimated normals and the corresponding groundtruth. Besides, the estimated normals are also plugged into the Poisson surface reconstruction algorithm for intuitive comparison. Experimental results demonstrate the effectiveness of our normal estimating methods. Moreover, the strategy that normal estimation after division saves much more computing time, which promises the efficiency of our method."
1975494,21239,8960,Ranking annotators for crowdsourced labeling tasks,2011,"With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a dataset labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Often we have low quality annotators or spammers–annotators who assign labels randomly (e.g., without actually looking at the instance). Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the consensus labels. In this paper we formalize the notion of a spammer and define a score which can be used to rank the annotators—with the spammers having a score close to zero and the good annotators having a high score close to one."
2999473,21239,9078,RealSense = real heart rate: Illumination invariant heart rate estimation from videos,2016,"Recent studies validated the feasibility of estimating heart rate from human faces in RGB video. However, test subjects are often recorded under controlled conditions, as illumination variations significantly affect the RGB-based heart rate estimation accuracy. Intel newly-announced low-cost RealSense 3D (RGBD) camera is becoming ubiquitous in laptops and mobile devices starting this year, opening the door to new and more robust computer vision. RealSense cameras produce RGB images with extra depth information inferred from a latent near-infrared (NIR) channel. In this paper, we experimentally demonstrate, for the first time, that heart rate can be reliably estimated from RealSense near-infrared images. This enables illumination invariant heart rate estimation, extending the heart rate from video feasibility to low-light applications, such as night driving. With the (coming) ubiquitous presence of RealSense devices, the proposed method not only utilizes its near-infrared channel, designed originally to be hidden from consumers; but also exploits the associated depth information for improved robustness to head pose."
2294727,21239,8228,Floating signal constellations for multimedia steganography,2000,"Conventional spread spectrum communication methods essentially map a discrete symbol from an alphabet to a length-N real valued sequence, which represents a point in the N-dimensional signal constellation. The real valued sequence is in turn transmitted over a communications channel. However, communication schemes for the purpose of multimedia steganography (or data hiding) have to transmit the real valued sequence corresponding to a point in the signal constellation superimposed on the original or cover content (without affecting the fidelity of the original content noticeably). The detector therefore, has to estimate the origin of the signal constellation, and thereafter proceed to decode the transmitted symbol. In this paper, we explore some efficient solutions for signaling methods employing floating signal constellations."
2287003,21239,8494,Automatic video diagnosing method using embedded crypto-watermarks,2008,"This paper proposes a novel automatic video diagnosing method. The purpose of this study is to detect the attack added on the authorized video and identify the attack category. The video is added with the crypto-watermarks using dual-domain quaternary watermarking algorithm in advance. The crypto-watermarks, which are generated using visual cryptography, have different capabilities against the various attacks. We extract the watermarks from the suspected video, measure the bit-error-rate between the extracted and specified crypto-watermarks, and then analyze bit-error-rates to determine what kind of attack added on the video. The experimental results demonstrate that the proposed method not only can identify the single attack, but it can identify the composite attack and detect the corrupted frames. Even if the video is not embedded with crypto-watermarks, we can differentiate it from the authorized videos."
2330833,21239,8960,Learning to be Bayesian without Supervision,2007,"Bayesian estimators are defined in terms of the posterior distribution. Typically, this is written as the product of the likelihood function and a prior probability density, both of which are assumed to be known. But in many situations, the prior density is not known, and is difficult to learn from data since one does not have access to uncorrupted samples of the variable being estimated. We show that for a wide variety of observation models, the Bayes least squares (BLS) estimator may be formulated without explicit reference to the prior. Specifically, we derive a direct expression for the estimator, and a related expression for the mean squared estimation error, both in terms of the density of the observed measurements. Each of these prior-free formulations allows us to approximate the estimator given a sufficient amount of observed data. We use the first form to develop practical nonparametric approximations of BLS estimators for several different observation processes, and the second form to develop a parametric family of estimators for use in the additive Gaussian noise case. We examine the empirical performance of these estimators as a function of the amount of observed data."
972394,21239,339,Tamper proofing and attack identification of corrupted image by using semi-fragile multiple-watermarking algorithm,2006,"We propose a novel semi-fragile multiple-watermarking algorithm based on quantization index modulation. This algorithm utilizes two quantization steps to yield the non-uniform intervals in the real-number axis. Each interval corresponds to one binary symbol, includes stable-zero (S 0 ), unstable-zero (U 0 ), stable-one (S 1 ), and unstable-one (U 1 ). In addition, visual cryptography is integrated with the watermarking algorithm to increase the watermark capacity. Therefore, the host image is embedded the multiple watermarks, and then we extract the watermarks from the corrupted image. According to the extracted watermarks, the algorithm achieves the tamper proofing and attack identification. From the experimental result, it shows single and multiple tampered areas are detected and demonstrates that the amount of test images will not influence the accuracy of attack identification."
1719866,21239,8502,Image quality quantification for fingerprints using quality-impairment assessment,2013,"A quality impairment assessment along with a quality score would enable Automatic Fingerprint Identification Systems (AFIS) to make appropriate decisions to a) reject the fingerprint and recapture another sample, b) use other fingers or biometric features for recognition, c) use image enhancement techniques. Our approach provides a quality score in addition to a quality impairment assessment into dry, wet or small contact area fingerprints, using which the fingerprint could be rejected to re-capture another sample after wiping the finger/using additional pressure. A manual labeling of dry, wet and normal fingerprint regions in the FVC2002 DB1 database is used to create classifiers for the quality impairment assessment. A block based quality impairment classification approach is used to compute an overall image quality score. The block classification into dry, wet or normal blocks has 96.07% accuracy. The overall quality score is between -1(poor quality) and 1(excellent quality), which is found to be satisfactory through a visual inspection."
615718,21239,8494,CDMA-based watermarking resisting to cropping,2004,"In this paper, we propose an image watermarking algorithm in DWT domain with self-synchronization, focusing on resisting to cropping attack. In embedding, the CDMA encoded watermark, concatenated by the synchronization code, is embedded into the selected DWT coefficients of the original image. During the oblivious watermarking detection, the corresponding DWT coefficients of the cropped watermarked image are relocated by self-synchronizing efficiently, and then the correlation between the extracted watermark information and the Gold sequence is analyzed. By exploiting the time-frequency localization characteristics of DWT, we reduce the computation load to search the synchronization code. The experiment results demonstrate that the thus generated watermark is robust to cropping, JPEG, Gaussian noise and other common attacks."
2968092,21239,9078,Laplacian-guided image decolorization,2016,"In this paper we introduce a novel decolorization strategy built on image fusion principles. Decolorization (color-to-grayscale), is an important transformation used in many monochrome image processing applications. We demonstrate that aside from color spatial distribution, local information plays an important role in maintaining the discriminability of the image conversion. Our strategy blends the three color channels R, G, B guided by two weight maps that filter the local transitions and measure the dominant values of the regions using the Laplacian information. In order to minimize artifacts introduced by the weight maps, our fusion approach is designed in a multi-scale fashion, using a Laplacian pyramid decomposition. Additionally, compared with most of the existing techniques our straightforward technique has the advantage to be computationally effective. We demonstrate that our technique is temporal coherent being suitable to decolorize videos. A comprehensive qualitative and also quantitative evaluation based on an objective visual descriptor demonstrates the utility of our decolorization technique."
2393003,21239,8494,Redundant multiscale structure coding for error resilient video completion,2008,"The fundamental problem associating error resilience with vision-related technique is how to generate a plausible substitute for the unknown regions with complex and semantic structure, implicitly or explicitly by adding significant context redundancy to the coded video, subject to local similarity and global consistence in tempo-spatial domain. Following the structure-aware inpainting for image coding, an adaptive error resilience algorithm using redundant structure coding with multiscale B-spline based feature localization is proposed in this paper. In the proposed scheme, the curvature-based curve representation information extracted from original video pictures is coded and encapsulated into redundant slice of the H.264/AVC standard. To meet the required channel bandwidth and conditions, the underlying redundant structure coding allows for different representations with an optimal coding strategy from corresponding texture compressed version in the primary slice. With the proposed curvature and distortion measure, we can observe that acceptable compressed rate is achieved while progressive reconstruction accuracy is ensured."
1431470,21239,8806,Shifting score fusion: on exploiting shifting variation in iris recognition,2011,"Iris recognition applies pattern matching techniques to compare two iris images and retrieve a comparison score that reflects their degree of (dis-)similarity. While numerous approaches to generating iris-codes have been proposed for the relatively young discipline of automated iris recognition, there are only few, usually simple, comparison techniques, e.g. fractional Hamming distance. However, in case of having access to specific iris-codes only or black-boxed feature extraction, there may be situations where improved comparison (even at potentially higher processing cost) is desirable. In this paper we present a new strategy for comparing iris-codes, which utilizes variations within comparison scores at different shift positions. We demonstrate that by taking advantage of this information, which even comes at negligible cost, recognition performance is significantly improved. The soundness of the approach is confirmed by experiments using two different iris-code based feature extraction algorithms."
3007996,21239,9078,A new metric for judder in high frame-rate video,2016,"Recent advances on high frame-rate (HFR) hold-type display systems have resulted in the increased demands for HFR contents. Frame-rate up conversion (FRUC) technique could be used for creating HFR contents from the existing low frame-rate (LFR) contents. However, the judder artifacts caused by FRUC would degrade the quality of HFR. The aim of this work is to analyze the cause of judder in the reconstructed HFR video from FRUC, and to propose a new metric to evaluate judder. A judder measurement method using subjective assessments is presented. As the prior study defines judder as the differences between the natural motion and result from the sampled motion of video, the proposed metric quantifies the judder by measuring the differences in retinal space. Experimental results demonstrate that the proposed judder determination method is consistent with the subjective assessment results."
2870737,21239,9078,Real-time noise-aware tone-mapping and its use in luminance retargeting,2016,"With the aid of tone-mapping operators, high dynamic range images can be mapped for reproduction on standard displays. However, for large restrictions in terms of display dynamic range and peak luminance, limitations of the human visual system have significant impact on the visual appearance. In this paper, we use components from the real-time noise-aware tone-mapping to complement an existing method for perceptual matching of image appearance under different luminance levels. The refined luminance retargeting method improves subjective quality on a display with large limitations in dynamic range, as suggested by our subjective evaluation."
2838460,21239,9078,Rate-distortion optimization of a tone mapping with SDR quality constraint for backward-compatible high dynamic range compression,2016,"This paper addresses the problem of designing a global tone mapping operator for rate-distortion optimized backward compatible compression of HDR images. We consider a two layer coding scheme in which a base SDR layer is coded with HEVC, inverse tone mapped and subtracted from the input HDR signal to yield the enhancement HDR layer. The tone mapping curve design is formulated as the minimization of the distortion on the reconstructed HDR signal under the constraint of a total rate cost on both layers, while preserving a good quality for the SDR signal. We first demonstrate that the optimum tone mapping function only depends on the rate of the base SDR layer and that the minimization problem can be separated in two consecutive minimization steps. Experimental results show that the proposed tone mapping optimization yields the best trade-off between rate-distortion performance and quality preservation of the coded SDR."
846827,21239,8494,Optimized inpainting-based macroblock prediction in video compression,2010,"In this paper, we propose an optimized inpainting-based macroblock prediction mode in the state-of-the-art H.264/AVC video engine. Parallel with the existing intra- and inter- modes, it is regularized by the global tempo-spatial consistency between the current MB and the co-located decoded region. The target is formulated as a local optimization problem by minimizing the energy of Markov Random Field (MRF). An ordered belief propagation (BP) is developed to solve the optimization problem with spatio-temporal consistency regularity and achieve the patch assignment of largest probability. It ensures a stable marginal belief distribution through updating local messages via iterative forward and backward process, to impose a prioritized inference on the structure. Rate-distortion optimization is used to evaluate the mode selection within the inpainting-based prediction mode, intra- and inter modes. It has been implemented into H.264/AVC, and achieves bit-rate saving and higher PSNR performance, especially in low bit-rate."
2906664,21239,9078,Direct inference on compressive measurements using convolutional neural networks,2016,"Compressive imagers, e.g. the single-pixel camera (SPC), acquire measurements in the form of random projections of the scene instead of pixel intensities. Compressive Sensing (CS) theory allows accurate reconstruction of the image even from a small number of such projections. However, in practice, most reconstruction algorithms perform poorly at low measurement rates and are computationally very expensive. But perfect reconstruction is not the goal of high-level computer vision applications. Instead, we are interested in only determining certain properties of the image. Recent work has shown that effective inference is possible directly from the compressive measurements, without reconstruction, using correlational features. In this paper, we show that convolutional neural networks (CNNs) can be employed to extract discriminative non-linear features directly from CS measurements. Using these features, we demonstrate that effective high-level inference can be performed. Experimentally, using hand written digit recognition (MNIST dataset) and image recognition (ImageNet) as examples, we show that recognition is possible even at low measurement rates of about 0.1."
1903983,21239,8494,Parallel algorithm for hardware implementation of inverse halftoning,2005,"A parallel algorithm and its hardware implementation are proposed for an inverse halftone operation. The algorithm is based on lookup tables from which the inverse halftone value of a pixel is directly determined using a pattern of pixels. A method has been developed that allows accessing more than one value from the lookup table at any time. The lookup table is divided into smaller lookup tables, such that each pattern selected at any time goes to a separate smaller lookup table. The 15-pixel parallel version of the algorithm was tested on sample images and a simple and effective method has been used to overcome quality degradation due to pixel loss in the proposed algorithm. It can provide at least 4 times decrease in lookup table size when compared with a serial lookup table method implemented multiple times for the same number of pixels."
2312436,21239,8960,Learning Sparse Multiscale Image Representations,2003,"We describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coefficients. The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coefficients to have exact zero values. Coefficients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler. The learned basis is similar to the Steerable Pyramid basis, and yields slightly higher SNR for the same number of active coefficients. De-noising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods."
1708482,21239,8494,Fast block-size partitioning using empirical rate-distortion models for MPEG-2 to H.264/AVC transcoding,2010,"We present an efficient H.264/AVC block-size partitioning prediction method, which is based on our proposed empirical rate and distortion models. Compared to other state-of-the-art transcoding methods, and for the same rate-distortion performance, our proposed algorithm requires the least computational complexity, reaching a 73 % reduction in variable block-size motion estimation for SDTV sequences, and 71% reduction for CIF sequences."
1940980,21239,8494,Peak Transform for Efficient Image Representation and Coding,2007,"In this work, we introduce a nonlinear geometric transform, called peak transform (PT), for efficient image representation and coding. The proposed PT is able to convert high-frequency signals into low-frequency ones, making them much easier to be compressed. Coupled with wavelet transform and subband decomposition, the PT is able to significantly reduce signal energy in high-frequency subbands and achieve a significant transform coding gain. This has important applications in efficient data representation and compression. To maximize the transform coding gain, we develop a dynamic programming solution for optimum PT design. Based on PT, we design an image encoder, called the PT encoder, for efficient image compression. Our extensive experimental results demonstrate that, in wavelet-based subband decomposition, the signal energy in high-frequency subbands can be reduced by up to 60% if a PT is applied. The PT image encoder outperforms state-of-the-art JPEG2000 and H.264 (INTRA) encoders by up to 2-3 dB in peak signal-to-noise ratio (PSNR), especially for images with a significant amount of high-frequency components. Our experimental results also show that the proposed PT is able to efficiently capture and preserve high-frequency image features (e.g., edges) and yields significantly improved visual quality. We believe that the concept explored in this work, designing a nonlinear transform to convert hard-to-compress signals into easy ones, is very useful. We hope this work would motivate more research work along this direction."
1272363,21239,8335,Algorithms and DSP implementation of H.264/AVC,2006,"This survey paper intends to provide a comprehensive coverage of the techniques that are pertinent to the processor-based implementation of H.264/AVC video codec, particularly on DSP. Most of this paper is devoted to the computationally efficient algorithms, or the  fast algorithms.  Fast algorithms for motion estimation, intra-prediction and mode decision are described to reduce the computational complexity. In addition, in order to port the H.264/AVC codec to DSP, we also outline the basic principles of DSP code optimization."
483782,21239,9078,Image denoising using learned overcomplete representations,2003,"We describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coefficients. The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coefficients to have exact zero values. Coefficients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler. Denoising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods."
2114132,21239,9078,Peak Transform - A Nonlinear Transform for Efficient Image Representation and Coding,2007,"In this work, we introduce a nonlinear geometric transform, called peak transform, for efficient image representation and coding. Coupled with wavelet transform and subband decomposition, the peak transform is able to significantly reduce signal energy in high-frequency subbands and achieve a significant transform coding gain. This has important applications in efficient data representation and compression. Based on peak transform (PT), we design an image encoder, called PT encoder, for efficient image compression. Our extensive experimental results demonstrate that, in wavelet-based subband decomposition, the signal energy in high-frequency subbands can be reduced by up to 60% if a peak transform is applied. The PT image encoder outperforms state-of-the-art JPEG2000 and H.264 (INTRA) encoders by up to 2-3 dB in PSNR (peak signal-to-noise ratio), especially for images with a significant amount of high-frequency components."
1836905,21239,8228,Constrained Wavelet Tree Quantization for Image Watermarking,2007,"This paper investigates the operations of the wavelet tree based quantization and proposes a constrained wavelet tree quantization for image watermarking. The wavelet coefficients of the cover image are grouped into super trees for watermark embedding where quantization is performed. The watermark bits are extracted based on a modulus approach and the minimum mean comparison of the super tree coefficients efficiently distinguishes which super tree is quantized. Without the needs of the requantization index at the decoder, the constrained quantization of the super trees reduces the uncertainty of the maximum likelihood detection. Therefore, the robustness of the proposed scheme can be effectively improved. This study has performed intensive comparison for the proposed scheme with the non-constrained tree quantization method under various geometric and nongeometric attacks. The experiment results demonstrate that the proposed technique yields better performance with higher degree of robustness."
1675388,21239,22288,Towards Quality Aware Collaborative Video Analytic Cloud,2012,"As cloud diversifies into different application fields, understanding and characterizing the specific workloadsand application requirements play important roles in thedesign of efficient cloud infrastructure and system softwaresupport. Video analytic is a rapidly advancing field and it iswidely used in many application domains (i.e., health, medicalcare, surveillance, and defense). To support video analyticapplications efficiently in cloud, one has to overcome manychallenges such as lack of understanding of the relationship andtradeoff between analytic performance metrics and resourcerequirements. Furthermore, cloud computing has grown fromthe early model of resource sharing to data sharing andworkflow sharing. To address the challenges and to leverageemerging trends, we propose and experiment with a domainspecific cloud environment for video analytic applications. Wedesign a cloud infrastructure framework for sharing videodata, analytic software, and workflow. In addition, we create avideo analytic quality aware resource plan model to guaranteeusers QoS and optimize usage of resources based on predictiveknowledge of video analytic softwares performance metrics anda resource planning model that optimizes the overall analyticservice quality under users constraints (i.e., time and cost).The predictive knowledge is represented as input and analyticsoftware specific predictors. The experimental results show thatthe video analytic quality aware resource planning model canbalance the tradeoff between analytic quality and resourcerequirements, and achieve optimal or near-optimal planning forvideo analytic workloads with constraints in a resource sharedenvironment. Simulation studies show that resource planningresults using ground truth and video analytic performancepredictions are very similar, which indicates that our analytic quality/resource predictors are very accurate."
479707,21239,20592,Spamscatter: characterizing internet scam hosting infrastructure,2007,"Unsolicited bulk e-mail, or SPAM, is a means to an end. For virtually all such messages, the intent is to attract the recipient into entering a commercial transaction -- typically via a linked Web site. While the prodigious infrastructure used to pump out billions of such solicitations is essential, the engine driving this process is ultimately the point-of-sale -- the various money-making scams that extract value from Internet users. In the hopes of better understanding the business pressures exerted on spammers, this paper focuses squarely on the Internet infrastructure used to host and support such scams. We describe an opportunistic measurement technique called spamscatter that mines emails in real-time, follows the embedded link structure, and automatically clusters the destination Web sites using image shingling to capture graphical similarity between rendered sites. We have implemented this approach on a large real-time spam feed (over 1M messages per week) and have identified and analyzed over 2,000 distinct scams on 7,000 distinct servers."
2819791,21239,8960,Can Peripheral Representations Improve Clutter Metrics on Complex Scenes,2016,"Previous studies have proposed image-based clutter measures that correlate with human search times and/or eye movements. However, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception. Here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz et al.) as our non foveated clutter model, and we stack a peripheral architecture on top of Feature Congestion for our foveated model. We introduce the Peripheral Integration Feature Congestion (PIFC) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity. We finally show that Foveated Feature Congestion (FFC) clutter scores (r(44) = −0.82 ± 0.04, p < 0.0001) correlate better with target detection (hit rate) than regular Feature Congestion (r(44) = −0.19 ± 0.13, p = 0.0774) in forced fixation search; and we extend foveation to other clutter models showing stronger correlations in all cases. Thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps. Code for building peripheral representations is available."
745745,21239,9078,Analysis of Subframe Generation for Superimposed Images,2006,"Displays and projectors can increase their addressable resolution without changing the expensive spatial light modulators by using a technique called wobulation, which consists of using mechanical actuators for rapidly shifting projected images (subframes) by fractions of pixel lengths. In this work we provide an analysis of the properties of the resulting super-imposed images, and discuss theoretical issues related to the stability of subframe generation and existence of solutions."
1875796,21239,30,A Sorting System for Hierarchical Grading of Diabetic Fundus Images: A Preliminary Study,2008,"Diabetic retinopathy is a leading cause of blindness in developed countries. Diabetic patients can prevent severe visual loss by attending regular eye examinations and receiving timely treatments. In the United States, standard protocols have been developed and refined for years to provide better screening and evaluation procedures of the fundus images. Due to the emerging number of diabetic retinopathy cases, accurate and efficient evaluations of the fundus images have become a serious burden for the ophthalmologists or care providers. While diabetic retinopathy remains too complicated to call for an automatic diagnosis system, an efficient tool to facilitate the grading process with a limited number of personnel is in great demand. The current study is to develop a sorting system with a user-friendly interface, based upon the standardized early treatment diabetic retinopathy study (ETDRS) protocol, to assist the professional graders. The raw fundus images will be screened and assigned to different graders according to their skill levels and experiences. The developed hierarchical sorting process will greatly support the graders and enhance their efficiency and throughput. The proposed hybrid intelligent system with multilevel knowledge representation is used to construct this sorting system. A preliminary case study is conducted using only the features of the spot lesion group coupled with the ETDRS standard to demonstrate its feasibility and performance. The results obtained from the case study show a promising future."
1917843,21239,30,Compression guidelines for diagnostic telepathology,1997,"As the healthcare community has begun to rely increasingly upon digital technologies for acquisition, storage, and transmission of pictorial data, image compression has become an indispensable tool. The authors have investigated the feasibility of lossy compression in a well-defined task domain, the clinical assessment of digitized images of chromatic microscopic pathology specimens. The effect of compression was measured under two distinct perceptual criteria, just noticeable difference (JND) and largest tolerable distortion (LTD), differing in the involvement required from subjects, who were experts in pathology. For standard JPEG compressed images it was found that when the experiment is performed under the LTD criterion, a significantly larger compression ratio is reported as satisfactory. It is concluded that lossy compression holds promise for diagnostic telepathology."
2999294,21239,9078,Classification of mammographic microcalcification clusters using a combination of topological and location modelling,2016,"We have investigated the classification of micro-calcification clusters in mammograms by combining two existing approaches. One of the approaches involves extracting and using topological information (connectivity) about micro-calcification clusters as feature vectors to classify them as being benign or malignant. The other approach involves extracting and using location details of micro-calcification clusters (where they appear in a breast and/or mammogram) as feature vectors to classify them as being benign or malignant. We have investigated various aspects of both methods and their combination. Our initial results, based on MIAS and DDSM indicate no significant improvement over the topological approach on its own."
2569676,21239,30,Optical flow with structure information for epithelial image mosaicing,2015,"Mosaicing of biological tissue surfaces is challenging due to the weak image textures. This contribution presents a mosaicing algorithm based on a robust and accurate variational optical flow scheme. A Riesz pyramid based multi-scale approach aims at overcoming the “flattening-out” problem at coarser levels. Moreover, the structure information present in images of epithelial surfaces is incorporated into the data-term to improve the algorithm robustness. The algorithm accuracy is first assessed with simulated sequences and then used for mosaicing standard clinical endoscopic data."
2447972,21239,8960,Products of ``Edge-perts,2006,"Images represent an important and abundant source of data. Understanding their statistical structure has important applications such as image compression and restoration. In this paper we propose a particular kind of probabilistic model, dubbed the products of edge-perts model to describe the structure of wavelet transformed images. We develop a practical denoising algorithm based on a single edge-pert and show state-of-the-art denoising performance on benchmark images."
1370623,21239,8228,Objective quality prediction model for lost frames in 3D video over TS,2013,"This paper proposes an objective model to predict the quality of lost frames in 3D video streams. The model is based only on header information from three different packet-layer levels: Network Abstraction Layer (NAL), Packetised Elementary Streams (PES) and Transport Stream (TS). Transmission errors leading to undecodable TS packets are assumed to result in frame loss. The proposed method estimates the size of the lost frames, which is used as a model parameter to predict their objective quality measured as the Structural Similarity Index Metric (SSIM). The results show that SSIM of missing stereoscopic frames in 3D coded video can be predicted with Root Mean Square Error (RMSE) accuracy of about 0.1 and Pearson correlation coefficient of 0.8, taking the SSIM of uncorrupted frames as reference. It is concluded that the proposed model is capable of estimating the SSIM quite accurately using only the lost frames estimated sizes."
2312752,21239,8494,A scalable encryption method allowing backward compatibility with JPEG2000 images,2005,"A new method for encryption of JPEG2000 images, which is referred to as 'scalable encryption', is proposed in this paper. The scalable encryption method makes the encrypted images have multi-level encryption and reduces the computational complexity of encryption, since different encryption algorithms can be simultaneously used in its procedure. Moreover, the encrypted images produced by the proposed method have complete compliance with JPEG2000, so that a standard JPEG2000 decoder can decode the encrypted images and the useful functionalities of the JPEG2000 codestream are preserved after the encryption. For example, the proposed method enables that content holders have no need of preparing two or more encrypted images for various users who are provided different access rights. In addition to this, the time for the encryption can be controlled by selection of adequate encryption algorithms for faster processing."
1432260,21239,9078,Skeleton-based human segmentation in still images,2012,"In this paper we propose a skeleton-based model for human segmentation in static images. Our approach explores edge information, orientation coherence and anthropometric-estimated parameters to generate a graph, and the desired contour is a path with maximal cost. Experimental results show that the proposed technique works well in non trivial images."
1787204,21239,9078,The analytic image,1997,"We introduce a novel directional multidimensional Hilbert transform and use it to define the complex-valued analytic image associated with a real-valued image. The analytic image associates a unique pair of instantaneous amplitude and frequency functions with an image, and also admits many of the other important properties of the one-dimensional analytic signal."
492211,21239,9078,Partial-scrambling of images encoded using JPEG2000 without generating marker codes,2003,"A method is described for efficient partial-scrambling of JPEG2000 images that avoids generating marker codes and improves the ability to control the degree, strength, and computational complexity of scrambling. This higher control ability is due to the use of a parameter. This parameter also controls the scrambling time, an important consideration for real-time processing."
488991,21239,9078,Steerable filter cascades,1999,"In this paper, we present the notion of cascading steerable filters to improve their angular resolution. Additionally, we illustrate that the results of such cascades can be steered themselves. An advantage of this approach is that only a single, relatively small set of steerable filters can be employed to achieve various angular resolutions. Improving angular resolution has previously required an entirely different, larger set of filters."
2164663,21239,9078,Observer-dependent sharpening,2002,"Image quality evaluation by human observers is heavily subjective in nature. Individual observers judge the image quality differently. In previous works, an observer-dependent system for subjective image enhancement, which is based on fusion of different algorithms, was introduced. In this paper, the system configuration for sharpness/smoothness is discussed and experimental results are provided."
2084765,21239,9078,Localized compression of video conferencing,1997,"A new localized approach to video teleconferencing is introduced using three-dimensional vector quantization (VQ). According to the proposed model, recent localized history of the sequences is used to encode present frames based on a hierarchical approach. The results indicate a high compression ratio of over 100:1 with a high quality of the perceived sequence."
2246440,21239,9078,Sensitivity Analysis Attacks Against Randomized Detectors,2007,"Sensitivity analysis attacks present a serious threat to the security of popular spread spectrum watermarking schemes. Randomization of the detector is thought to increase the immunity of such schemes against cryptanalysis. In this paper, we introduce a new attack against randomized detectors. This attack is successful, which implies that spread spectrum schemes still lack security."
862018,21239,20796,A web service for long tail book publishing,2008,"More than 32M unique book titles are available in US libraries, but Amazon, the biggest retailer, had only 1.2M unique titles available for sale in 2004. Currently there is an effort underway by public libraries, universities, the Open Content Alliance, Google and others, to non-destructively scan these 32M unique books and make them available for on-line viewing and search. Twenty percent (6.4M) of the 32M titles are out of copyright and out of print. A publisher estimates that an average of 40 copies of each title can be sold per year if they could be made available for sale. This long tail opportunity represents a several billion dollar market with the right cost structure. To address this long-tail book market we need to take the cost out of several parts of the value chain: automatic book preparation to minimize publishing setup costs, print-on-demand to remove warehouse and waste costs, and web 2.0 techniques to minimize marketing costs. We have created this system with several partners based on HP technology, and available as an incubation business."
2027169,21239,23735,Achieving three-dimensional automated micromanipulation at the scale of several micrometers with a nanotip gripper,2009,"Three-dimensional (3-D) automated micromanipulation at scale of several micrometers using a nanotip gripper is presented. The gripper is constructed from protrudent tips of two independently actuated atomic force microscope (AFM) cantilevers and each cantilever. A protocol allows these two cantilevers to form a gripper for grasping and releasing the microspheres to target positions without obstacle of adhesive forces in air. For grasping, amplitude feedback from the dithering cantilevers is employed to locate the grasping points by laterally scanning the side of the microspheres. Real time force sensing is used to monitor the whole process of the pick-and-place with steps of pickup, transport and release. For trajectory planning, an algorithm based on the shortest path solution is used to obtained 3-D micropatterns with high efficiencies. In experiments, microspheres with diameters from 3 µm to 4 µm were manipulated and 3-D micropyramids with two layers were achieved. 3-D micromanipulation and 3-D microassembly at the scale of several microns to submicron could become feasible through the newly developed nanotip gripper."
1893283,21239,23735,Judging distance by motion-based visually mediated odometry,2003,"Inspired by the abilities of both the praying mantis and the pigeon to judge distance by use of motion-based visually mediated odometry, we create miniature models for depth estimation that are similar to the head movements of these animals. We develop mathematical models of the praying mantis and pigeon visual behavior and describe our implementation and experimental environment. We investigate structure from motion problems when images are taken from a camera whose focal point is translating the first case is reminiscent of a praying mantis peering its head left and right, apparently to obtain depth perception, hence the moniker mantis head camera. In the second case this motion is reminiscent of a pigeon bobbing its head back and forth, also apparently to obtain depth perception, hence the moniker  pigeon head camera. We present the performance of the mantis head camera and pigeon head camera models and provide experimental results of the algorithms. We provide the comparison of the definitiveness of the results obtained by both models. The precision of our mathematical model and its implementation is consistent with the experimental facts obtained from various biological experiments."
867646,21239,23735,Semi-autonomous visual inspection of vessels assisted by an unmanned Micro Aerial Vehicle,2012,"Vessel maintenance entails periodic visual inspections of internal and external parts of the hull in order to detect the typical defective situations affecting metallic structures, such as cracks, coating breakdown, corrosion, etc. The main goal of the EU-FP7 project MINOAS is the automation of the inspection process, currently undertaken by human surveyors, by means of a fleet of robotic agents. This paper overviews a semi-autonomous approach to the inspection problem consisting of an autonomous Micro Aerial Vehicle (MAV) to be used as part of this fleet and which is in charge of regularly supplying images that can effectively teleport the surveyor from a base station to the areas of the hull to be inspected. Specific image processing software to analyze those images and assist the surveyor during the repair/no repair decision making process is also contributed. The control software approach adopted for the MAV, including self-localization and obstacle avoidance, is described and discussed, and experimental results in this regard are as well reported."
1123769,21239,9078,Multispectral venous images analysis for optimum illumination selection,2013,"Intravenous (IV) catheterization is the most important phase in medical practices of daily life. It is hard to localize veins in patients who have deep veins, minor age or dark skin; hence multiple attempts become indispensable for proper catheterization in such cases. Near Infrared (NIR) Imaging allow to visualize the veins underneath the skin of persons having non-visibility of veins problem. This paper reports the pre-selection of illuminants that ensure best veins/tissues contrast for patients having different skin tone. The sample subjects have been divided in four different classes based on the Luminance value of their skin tone in order to extract the best illuminant wavelengths range for each class. A multispectral approach has been used which provides the flexibility of wavelength range from visible to NIR (380 to 1040nm). The veins/tissue reflectance contrast obtained helps in determining the best wavelengths range where the contrast is maximum for each of the four classes. Using these results, we are planning to build a prototype system which can automatically select the illuminants based on different physiological characteristics of a subject."
2970133,21239,507,CrowdDQS: Dynamic Question Selection in Crowdsourcing Systems.,2017,"In this paper, we present CrowdDQS, a system that uses the most recent set of crowdsourced voting evidence to dynamically issue questions to workers on Amazon Mechanical Turk (AMT). CrowdDQS posts all questions to AMT in a single batch, but delays the decision of the exact question to issue a worker until the last moment, concentrating votes on uncertain questions to maximize accuracy. Unlike previous works, CrowdDQS also (1) optionally can also decide when it is more beneficial to issue gold standard questions with known answers than to solicit new votes (both can help us estimate worker accuracy, but gold standard questions provide a less noisy estimate of worker accuracy at the expense of not obtaining new votes), (2) estimates worker accuracies in real-time even with limited evidence (with or without gold standard questions), and (3) infers the distribution of worker skill levels to actively block poor workers. We deploy our system live on AMT to over 1000 crowdworkers, and find that using our techniques, CrowdDQS can accurately answer questions using up to 6x fewer votes than standard approaches. We also find there are many non-obvious practical challenges involved in deploying such a system seamlessly to crowdworkers, and discuss techniques to overcome these challenges."
2831560,21239,8228,A flexible monitor for assessing 3D video QoE in real-time,2016,"With the evolution of 3D technology, 3D IPTV services may prove to be a common service widely distributed by operators. So it is important that they have the necessary means to easily and inexpensively monitor the Quality of Experience (QoE) of this new service. Deployment of 3D video QoE monitors anywhere in the network will enable operators to adapt their service and network infrastructure in order to guarantee a desired QoE level, e.g., in scenarios where 3D IPTV streaming is offered to users with multi-homed equipment and simultaneous access to the network by means of heterogeneous smartcells in the customer premises."
2434677,21239,8960,Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform,2010,"Divisive normalization (DN) has been advocated as an effective nonlinear efficient coding transform for natural sensory signals with applications in biology and engineering. In this work, we aim to establish a connection between the DN transform and the statistical properties of natural sensory signals. Our analysis is based on the use of multivariate t model to capture some important statistical properties of natural sensory signals. The multivariate t model justifies DN as an approximation to the transform that completely eliminates its statistical dependency. Furthermore, using the multivariate t model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the DN transform. We compare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations confirm DN as an effective efficient coding transform for natural sensory signals. On the other hand, we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small."
2988750,21239,9078,Pornographic image recognition by strongly-supervised deep multiple instance learning,2016,"In this paper, we propose a principled framework for pornographic image recognition. Specifically, we present our definition of pornographic images, which characterizes the pornographic contents in images as the exposure of private body parts. As the private body parts often lie in local image regions, we model each image as a bag of local image patches (instances), and assume that for each pornographic image at least one instance accounts for the pornographic content within it. This treatment allows us to cast the model training as a Multiple Instance Learning (MIL) problem. Furthermore, we propose a strongly-supervised setting for MIL by identifying the most likely pornographic instances in positive bags, which effectively prevents the algorithm from getting trapped in a bad local optima. Last but not least, we formulate our strongly-supervised MIL under the deep CNN framework to learn deep representations; hence we call it Strongly-supervised Deep MIL (SD-MIL). We demonstrate that our SD-MIL based system produces remarkable accuracy with 97.01% TPR at 1% FPR, testing on 117K pornographic images and 117K normal images from our newly-collected large scale dataset."
2977202,21239,9078,Gaussian noise approximation for disparity-based autofocus,2016,"An analytical characterization of the accuracy of disparity-based autofocus requires an effective error model of disparity estimation. In this paper, we investigate the approximation of photon shot noise of images by a Gaussian model for disparity error analysis that takes defocus blur and image noise into account. We show that, counterintuitively, defocus blur alone does not affect the disparity estimation. However, its presence makes disparity estimation more vulnerable to image noise. In addition, we show that the Gaussian model is a fair approximation of the photon shot noise for disparity-based autofocus. The Gaussian model allows defocus blur to be mathematically separable from noise and facilitates further analysis of disparity error for disparity-based autofocus."
2884760,21239,9078,Eye center localization and detection using radial mapping,2016,"We propose a geometrical method, applied over eye-specific features, to improve the accuracy of the art of eye-center localization. Our solution is built upon: (a) checking radially constrained gradient vectors, (b) adding weightage to iris specific features and (c) considering bi-directional image gradients to eliminate errors due to reflection on pupil. Our system outperforms the state of the art methods, when compared collectively across multiple benchmark databases, such as BioID and FERET. Our process is lightweight, robust and significantly fast: achieving 50–60 fps for eye center localization, using a single threaded approach on a 2.4 GHz CPU with no GPU. This makes it practicable for real-life applications."
2843413,21239,9078,The visibility of motion artifacts and their effect on motion quality,2016,"The visibility of motion artifacts in a video sequence e.g. motion blur and temporal aliasing, affects perceived motion quality. The frame rate required to render these motion artifacts imperceptible is far higher than is currently feasible or specified in current video formats. This paper investigates the perception of temporal aliasing and its associated artifacts below this frame rate, along with their influence on motion quality, with the aim of making suitable frame rate recommendations for future formats. Results show impairment in motion quality due to temporal aliasing can be tolerated to a degree, and that it may be acceptable to sample at frame rates 50% lower than those needed to eliminate perceptible temporal aliasing."
2051276,21239,8494,Near-perfect cover image recovery anti-multiple watermark embedding approaches,2005,"Robustness is a critical requirement for a watermarking scheme to be practical. Especially, in order to resist geometric distortions, a common way is to insert multiple-redundant watermarks locally in the hope that partial watermarks could still be detected. However, there exist watermark-estimation attacks (WEA), such as the collusion attack, that can remove watermarks while making the attacked data further transparent to its original. Another kind of attack is the copy attack, which can cause protocol ambiguity within a watermarking system. The paper proposes an efficient cover data recovery attack, which is more powerful than the conventional collusion attack. We begin by gaining insight into the WEA, leading to formal definitions of optimal watermark estimation and near-perfect cover data recovery. Subject to these definitions, an exquisite collusion attack is derived. Experimental results verify the effectiveness of the proposed watermark estimation and recovery algorithm."
2865356,21239,390,Joint desmoking and denoising of laparoscopy images,2016,"Laparoscopic images in minimally invasive surgery get corrupted by surgical smoke and noise. This degrades the quality of the surgery and the results of subsequent processing for, say, segmentation and tracking. Algorithms for desmoking and denoising laparoscopic images seem to be missing in the medical vision literature. This paper formulates the problem of joint desmoking and denoising of laparoscopic images as a Bayesian inference problem. It relies on a novel probabilistic graphical model of the images, which includes novel prior models on the uncorrupted color image as well as the transmission-map image that indicates color attenuation due to smoke. The results on simulated and real-world laparoscopic images, including clinical expert evaluation, shows the advantages of the proposed method over the state of the art."
2906540,21239,9078,Vascular network formation in silico using the extended cellular potts model,2016,"Cardiovascular diseases belong to the most widespread illnesses in the developed countries. Therefore, the regenerative medicine and tissue modeling applications are highly interested in studying the ability of endothelial cells, derived from human stem cells, to form vascular networks. Several characteristics can be measured on images of these networks and hence describe the quality of the endothelial cells. With advances in the image processing, automatic analysis of these complex images becomes increasingly common. In this study, we introduce a new graph structure and additional constraints to the cellular Potts model, a framework commonly utilized in computational biology. Our extension allows to generate visually plausible synthetic image sequences of evolving fluorescently labeled vascular networks with ground truth data. Such generated datasets can be subsequently used for testing and validating methods employed for the analysis and measurement of the images of real vascular networks."
3053890,21239,9078,Direct N-point DCT computation from three adjacent N/3-point DCT coefficients,2004,"An efficient method for computing a length-N DCT given three consecutive length-N/3 DCTs is proposed. This method differs from previous ones in that it reduces considerable arithmetic operations and uses only length-N/3 DCTs instead of length-N DCTs. We also find its great applications in fractional scaling of a DCT-based image by the factor of N/2/sup /spl alpha//3/sup /spl beta//. This would be very useful in HDTV standard, whose display size is usually 16:9. The comparison with conventional methods is provided in this paper."
2340325,21239,8494,Digital watermarking using Walsh code sequences with error spreading technique and intra-pixel prediction,2005,"Digital watermarking is one solution to protect intellectual properties and copyright by hiding information, such as a random sequence or a logo, into digital media. In this paper, a new watermarking scheme is proposed. A real logo is embedded into the media with a set of Walsh code sequences (WCSs) and some error spreading techniques, such as interleaving. The logo can be extracted from a possibly corrupted image, without the help of the original uncorrupted image. The extracted logo is then corrected by intra-pixel prediction (IPP) and becomes more readable and noticeable. The major advantage of hiding a real logo is that the logo may still be readable by human eyes even though the watermarked image is corrupted. Also, the extracted logo can be corrected by using the characteristics of images."
2888537,21239,9078,In-loop radial distortion compensation for long-term mosaicing of aerial videos,2016,"For the generation of overview panoramic images from aerial surveillance videos, registered video frames are stitched together. Assuming a planar landscape, feature points can be detected and used to estimate a homography. However, if the features are affected by radial distortion, their mapping depends on their position within the frame and the resulting homography becomes inaccurate. As a result, the length of aerial panorama images is typically restricted to several hundred frames. To overcome this issue, we derive a model for the joint estimation of several homographies and one constant radial distortion. Due to the computational complexity of the solution, we propose a fast, iterative algorithm. Based on geometrical constraints, we regularize the projection of a jointly estimated picture group. We present panorama images from uncalibrated aerial videos with more than 1500 frames."
2906454,21239,9078,Softcast with per-carrier power-constrained channels,2016,"This paper considers the Softcast joint source-channel video coding scheme for data transmission over parallel channels with different power constraints and noise characteristics, typical in DSL or PLT channels. To minimize the mean square error at receiver, an optimal precoding matrix design problem has to be solved, which requires the solution of an inverse eigenvalue problem. Such solution is taken from the MIMO channel precoder design literature. Alternative suboptimal precoding matrices are also proposed and analyzed, showing the efficiency of the optimal precoding matrix within Softcast, which provides gains increasing with the encoded video quality."
2984888,21239,9078,Temporal distortion costs for sample adaptive offset in H.265/HEVC,2016,"This paper proposes a novel temporal distortion cost (TDC) for sample adaptive offset (SAO) filter in High Efficiency Video Coding. The TDC addresses the images with temporal distortion by evaluating the errors between pre/post-SAO samples and raw samples in continuous frames. Due to high computational complexity of TDC, we further propose a low-complexity TDC (L-TDC) to estimate the value of TDC by a difference classification scheme and a rate-distortion decision scheme. We also contribute to deriving a spatio-temporal distortion cost (STDC) and a low-complexity STDC (L-STDC) by incorporating a spatial distortion cost into the proposed TDCs. Simulation results show that, compared to the classical SAO filter, the proposed SAO filters can save up to around 10% ∼ 15% bitrates under the same flicker measure, and achieve better flicker measure with over 10% improvement in average under the same bitrate. Some valuable threshold analyses are also provided."
2880361,21239,9078,Accurate 3D face modeling and recognition from RGB-D stream in the presence of large pose changes,2016,"We propose a 3D face modeling and recognition system using an RGB-D stream in the presence of large pose changes. In the previous work, all facial data points are registered with a reference to improve the accuracy of 3D face model from a low-resolution depth sequence. This registration often fails when applied to non-frontal faces. It causes inaccurate 3D face models and poor performance of matching. We address this problem by pre-aligning each input face (‘frontalization’) before the registration, which avoids registration failures. For each frame, our method estimates the 3D face pose, assesses the quality of data, segments the facial region, frontalizes it, and performs an accurate registration with the previous 3D model. The 3D-3D recognition system using accurate 3D models from our method outperforms other face recognition systems and shows 100% rank 1 recognition accuracy on a dataset with 30 subjects."
1766983,21239,8502,Saliency retargeting: An approach to enhance image aesthetics,2011,"A photograph that has visually dominant subjects in general induces stronger aesthetic interest. Inspired by this, we have developed a new approach to enhance image aesthetics through saliency retargeting. Our method alters low-level image features of the objects in the photograph such that their computed saliency measurements in the modified image become consistent with the intended order of their visual importance. The goal of our approach is to produce an image that can redirect the viewers' attention to the most important objects in the image, and thus making these objects the main subjects. Since many modified images can satisfy the same specified order of visual importance, we trained an aesthetics score prediction model to pick the one with the best aesthetics. Results from our user experiments support the effectiveness of our approach."
1959188,21239,8960,An Homotopy Algorithm for the Lasso with Online Observations,2009,"It has been shown that the problem of l1-penalized least-square regression commonly referred to as the Lasso or Basis Pursuit DeNoising leads to solutions that are sparse and therefore achieves model selection. We propose in this paper RecLasso, an algorithm to solve the Lasso with online (sequential) observations. We introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point. We compare our method to Lars and Coordinate Descent, and present an application to compressive sensing with sequential observations. Our approach can easily be extended to compute an homotopy from the current solution to the solution that corresponds to removing a data point, which leads to an efficient algorithm for leave-one-out cross-validation. We also propose an algorithm to automatically update the regularization parameter after observing a new data point."
2378347,21239,8494,Joint transcoding of multiple MPEG video bitstreams,1999,"This paper addresses the problem of bit-rate conversion of a previously compressed video. We provide an MPEG joint transcoder for transcoding several video bitstreams simultaneously. We show that joint transcoding reduces the quality variation between multiple video sequences, as compared to independently transcoding each sequence at a fixed bit rate. Hence, joint transcoding results in a better utilization of the channel capacity. The joint transcoder can be used in a congested communication network as an alternative to data/packet dropping, and in applications which require multiplexing video signals onto a fixed communication channel such as video servers providing video on demand (VOD) service."
723326,21239,8494,Multiple-description video coding based on JPEG 2000 MQ-coder registers,2010,"Wireless channels are more prone to transmission errors than wired communications counterparts, leading to excessive packet retransmission that is very inconvenient on media streaming. The multiple-description-coding (MDC) has proven to be very effective against transmission errors, thus providing a solution for that problem. What is presented is a method for MDC using the highly optimized and scalable JPEG 2000. Descriptions are encoded by our modified version of JPEG 2000 which produces compatible codestreams provided with key-error detection registers. The multiple-description JPEG 2000 decoder is then capable of precisely detecting transmission errors and to efficiently choose between available description-information, achieved by a clever exploitation of the EBCOT system. To test the potential of the proposed method, it is integrated as a spatial MD-Coder in a state-of-the-art joint-source channel video coder framework capable of an efficient bit-allocation between descriptions. A comprehensive set of experimental results is presented."
537248,21239,8494,JPEG2000. Part 10. Volumetric data encoding,2006,"The joint photographic experts group (JPEG) committee (ISO/IEC JTC1/SC29/WG1) is currently pursuing the standardization of a three-dimensional extension of the JPEG-2000 standard (Parts 1 and 2) to support the encoding of volumetric data sets. This extension, Part 10 - extensions for three-dimensional data (JP3D), will support functionalities like resolution scalability, quality scalability and region-of-interest coding, while exploiting the entropy in the additional third dimension to improve the rate-distortion performance. In this paper, we give an overview of the markets and application areas targeted by JP3D, the imposed requirements and the algorithm under study."
2954471,21239,9078,Nighttime image dehazing with local atmospheric light and weighted entropy,2016,"In this paper, we propose a novel framework for nighttime image dehazing based on a nighttime haze model which accounts for varying light sources and their glow. First, glow effects are decomposed using relative smoothness. Atmospheric light is then estimated by combining global and local atmospheric lights using a local atmospheric selection map. The transmission is estimated by maximizing an objective function designed with weighted entropy. Finally, haze is removed using two estimated parameters which are atmospheric light and transmission. Experimental results validate the proposed method can achieve haze-free results while alleviating the glow effect."
1136058,21239,8502,Iris Extraction Based on Intensity Gradient and Texture Difference,2008,"Biometrics has become more and more important in security applications. In comparison with many other bio- metric features, iris recognition has very high recognition accuracy. Successful iris recognition depends largely on correct iris localization, however, the performance of current techniques for iris localization still leaves room for improvement. To improve the iris localization performance, we propose a novel method that optimally utilizes both the intensity gradient and texture difference. Experimental results demonstrate that our new approach gives much better results than previous approaches. In order to make the iris boundary more accurate, we present a new issue called model selection and propose a method to choose between ellipse/circle and circle/circle models. Furthermore, we propose a dome model to compute mask images and remove eyelid occlusions in the unwrapped images rather than in the original eye images with a least commitment strategy."
2894328,21239,9078,Spectral slopes for automated classification of land cover in landsat images,2016,"In the literature, various techniques for supervised/ semi-supervised classification of satellite imageries require manual selection of samples for each class. In this paper, we propose a spectral-slope based classification technique, which automates the process of initial labeling of a set of sample points. These are subsequently used in a supervised classifier as training samples and it performs the task of classification over all the pixels in the image. We demonstrate the effectiveness of our proposed classification technique in summarizing the changes in temporal image sets. For selecting the training samples from the satellite imageries, a set of rules is proposed by using the spectral-slope properties. We classify the land-cover into three classes, namely, water, vegetation, and vegetation-void, and validate the classification results using very high resolution satellite imagery. The approach has also been used in the analysis of images acquired by different sensors operating under similar wavelength ranges."
2824613,21239,9078,Spatio-temporal saliency detection using abstracted fully-connected graphical models,2016,"A novel approach to spatio-temporal saliency detection in video is proposed. Saliency computation is considered as an optimization problem that maximizes the energy of a fully-connected graphical model based on spatio-temporal feature distinctiveness. Each pixel in a video is modeled by a node, and the spatio-temporal feature distinctiveness between pixels by edges connecting the nodes in the graph. The computational complexity is addressed by compressing the fully-connected graph into an abstracted, fully-connected graph with far fewer nodes, where each node in the new graph characterizes nodal groups. The saliency value of each pixel is then computed based on spatio-temporal feature distinctiveness and the energy representation of its nodal group given the constructed graphical model. Experimental results show that our approach outperforms existing approaches to spatio-temporal salient region detection."
2845582,21239,9078,Sparse signal recovery based on nonconvex entropy minimization,2016,"We propose a new sparsity-promoting objective function to be used in sparse signal recovery. Specifically, the objective is an entropy function &#x1D459; 1  defined on the sparse signal x. Compared to the conventional &#x1D459; 1 , it is a nonconvex function and the optimization problem can be solved based on the fast iterative shrinkage thresholding algorithm (FISTA). Experiments on 1-dimensional sparse signal recovery and 2-dimensional real image recovery show that minimizing &#x1D459; p  favors sparse solutions, and that it could recover sparse signals better than the convex &#x1D459; 1  norm minimization and the nonconvex l p -norm minimization."
686750,21239,8494,Perceptual coding of digital colour images based on a vision model,2004,"This work presents a perceptual order for Y-Cr-Cb (YUV) colour spaces. The approach extends from the single colour channel model of D.M. Tan et al. (2004) and D.M. Tan et al. (2001) and the RGB colour domain model of D.M. Tan et al. (2001) that mimics the human visual system (HVS) in exploiting its intra-band and inter-orientational masking properties. Although the proposed perceptual coder retains most of the embedded block coding with optimized truncation (EBCOT) coding features in D. Taubman (2000) and is fully bit-stream compliant with JPEG2000 standard of M. Boliek et al. (2000), the simulation results have revealed comparable or better visual masking of JPEG2000 verification model 8.0 coder."
2194926,21239,8502,Restoration for weakly blurred and strongly noisy images,2011,"In this paper we present an adaptive sharpening algorithm for restoration of an image which has been corrupted by mild blur, and strong noise. Most existing adaptive sharpening algorithms can not handle strong noise well due to the intrinsic contradiction between sharpening and de-noising. To solve this problem we propose an algorithm that is capable of capturing local image structure and sharpness, and adjusting sharpening accordingly so that it effectively combines denoising and sharpening together without either noise magnification or over-sharpening artifacts. It also uses structure information from the luminance channel to remove artifacts in the chrominance channels. Experiments illustrate that compared with other sharpening approaches, our method can produce state of the art results under practical imaging conditions."
1597230,21239,390,Non-parametric regression for patch-based fluorescence microscopy image sequence denoising,2008,"We present a non-parametric regression method for denoising fluorescence video-microscopy volume sequences. The designed method aims at using the 3D+t information in order to restore acquired data contaminated by Poisson and Gaussian noise. We propose to use a variance stabilization transform to deal with the combination of Poisson and Gaussian noise. Consequently, we further propose an adaptive patch-based framework able to preserve space-time discontinuities and reduce significantly noise level using the 3D+t space-time context. This approach lead to an algorithm whose parameters are calibrated and then ready for intensive use. The performance of the proposed method are then demonstrated on both synthetic and real image sequences using quantitative as well as qualitative criteria."
563134,21239,8494,Canny edge based image expansion,2002,"In this paper, a Canny edge-based image expansion method is introduced. Our proposed expansion method outperforms the pixel replication, the bilinear interpolation and the bicubic interpolation methods. It gives crisp and less zigzag pictures. Our method is applied on the image after it has been expanded using bilinear or bicubic interpolation. The edges of such an expanded image are obtained using the Canny edge detector. The values of pixels around the edges are modified to yield a crisper and less zigzagged picture."
2888628,21239,9078,Underwater image restoration based on minimum information loss principle and optical properties of underwater imaging,2016,"Restoring underwater image from a single image is known to be an ill-posed problem. Some assumptions made in previous methods are not suitable in many situations. In this paper, an effective method is proposed to restore underwater images. Using the quad-tree subdivision and graph-based segmentation, the global background light can be robustly estimated. The medium transmission map is estimated based on minimum information loss principle and optical properties of underwater imaging. Qualitative experiments show that our results are characterized by relatively genuine color, natural appearance, and improved contrast and visibility. Quantitative comparisons demonstrate that the proposed method can achieve better quality of underwater images when compared with several other methods."
1876007,21239,8494,Lossless/lossy coding gain to evaluate coding performance of the lossless/lossy wavelet,2002,"In this report, we propose lossless/lossy coding gain as a new objective criterion to theoretically evaluate lossless and lossy coding performance of the lossless/lossy wavelet (LLW). The proposed lossless/lossy coding gain consists of three parameters: lossless coding gain, quantization-lossy coding gain and rounding errors. Performances of 15 kinds of the LLW are measured with two-dimensional (2D) octave-decomposition. Some standard images are applied as input signals to evaluate coding performance of the LLW."
1524374,21239,8502,Motion Layer Based Object Removal in Videos,2005,"This paper proposes a novel method to generate plausible video sequences after removing relatively large objects from the original videos. In order to maintain temporal coherence among the frames, a motion layer segmentation method is applied. Then, a set of synthesized layers are generated by applying motion compensation and region completion algorithm. Finally, a new video, in which the selected object is removed, is plausibly rendered given the synthesized layers and the motion parameters. A number of example videos are shown in the results to demonstrate the effectiveness of our method"
801964,21239,8494,Graph cut video object segmentation using histogram of oriented gradients,2008,"This paper introduces a novel way to implement graph cut for video object segmentation with shape information. Graph Cut is a very efficient algorithm for image segmentation and histogram of oriented gradients (HOG) is useful in detecting humans. We combine the HOG feature to incorporate a shape prior into graph cut algorithm as a new way to enhance video object segmentation accuracy. In previous work, we used a fully connected 3-D that is slow and is subject to weak edges, inconsistent luminance. The new method is compared with old methods to show that it helps by introducing a shape prior for segmentation of pre-trained objects such as humans."
2824464,21239,9078,Locality preserving partial least squares for neighbor embedding-based face hallucination,2016,"Neighbor embedding-based face hallucination is structured on the assumption that the manifolds formed by low resolution (LR) and high resolution (HR) image patches in two distinct feature spaces have similar local geometry. However, that is not always true. By introducing local information, a novel partial least squares (PLS) method is proposed, called locality preserving PLS (LPPLS), to find a unified feature space where the correlation between LR and HR image patches on that space is maximized. Applying the proposed LPPLS, we learn the joint mapping of LR and HR image patches simultaneously and then map these image patches onto the unified feature space. The k-nearest neighbor searching and the optimal reconstruction weights computing are performed in this unified feature space as well. Experiments show the effectiveness of proposed method."
2077182,21239,9078,Content adaptive watermarking using a 2-stage predictor,2005,"Digital watermarking is one of the solutions to protect intellectual properties and copyright by hiding information, such as a random sequence or a logo, into digital media. In this paper, a new watermarking scheme is proposed. The embedding process takes place in the spatial domain. A bi-level logo is embedded into digital media by comparing the absolute difference between the original pixel value and the predicted pixel value, which is computed and predicted by using the concept of activity measurement, followed by spatial varying filter (SVF). The logo will be extracted from a possibly corrupted image, without the help of original uncorrupted image. The new proposed algorithm can withstand the geometric attacks as well as common signal processing, and has a high payload capacity."
574797,21239,9078,"Just enough reality, microstereopsis, and the prospect of zoneless autostereoscopic displays",2000,"Just as color can be shouted in primary tones or whispered in soft pastel hues, so stereo can be shoved in your face or raised ever so gently off the screen plane. This paper reviews the problems with in your face stereo and demonstrates that just enough reality is both gentle and effective in achieving stereoscopy's fundamental goal: resolving the front-back ambiguity inherent in 2D projections. Something unexpected and perhaps fortuitous falls out: soft stereo means small on-screen disparities, and when on-screen disparities are small, crosstalk is perceived as blur rather than as ghosting. Blur, particularly if it occurs only in the background and foreground (to which regions it can be relegated by center-of-interest compensation), is, in contrast to ghosting, quite unobjectionable. Gracefully accepting crosstalk may enable zoneless autostereoscopic displays."
2640044,21239,21089,Linguistically debatable or just plain wrong,2014,"In linguistic annotation projects, we typically develop annotation guidelines to minimize disagreement. However, in this position paper we question whether we should actually limit the disagreements between annotators, rather than embracing them. We present an empirical analysis of part-of-speech annotated data sets that suggests that disagreements are systematic across domains and to a certain extend also across languages. This points to an underlying ambiguity rather than random errors. Moreover, a quantitative analysis of tag confusions reveals that the majority of disagreements are due to linguistically debatable cases rather than annotation errors. Specifically, we show that even in the absence of annotation guidelines only 2% of annotator choices are linguistically unmotivated."
559853,21239,9078,Visual adaptation and the relative nature of perception,2001,"Our perception is constantly shaped by processes of adaptation that adjust visual sensitivity in response to the images currently before us. Many classic after-effects demonstrate the striking changes in perceived color, form and motion that can result from only brief exposures to simple adapting patterns. But what are the adapting patterns we encounter in everyday viewing, and how do they influence everyday visual judgments? We have examined the states of adaptation induced by the properties of the natural visual environment, and show that they can profoundly affect many natural visual tasks, from color perception to face recognition to the perception of image blur. These sensitivity adjustments are an intrinsic part of the visual response to any stimulus, and presumably functions to match visual coding to the current characteristics of the world. Consequently, how the world looks may-to a surprisingly large extent-depend on what you've recently been looking at."
1233641,21239,9078,Selective variational image segmentation combined with registration: Models and algorithms,2012,"In this paper, I present some new and joint work on local and selective segmentation models and algorithms which have potential applications in medical imaging. First I review a familiar segmentation model of global energy minimization framework in two dimensions (three dimensions may be presented similarly). Then I discuss selective segmentation models and several refined models where pre-defined geometric constraints guide local segmentation. Such 2D models can be generalized to 3D and some brief experiments are given to demonstrate the ideas of the paper. Finally I discuss the use of image registration methods to obtain geometric constraints or equivalent initial contours towards an automatic segmentation framework. As mentioned, the work discussed here represents a small portion of results obtained in the Liverpool's Centre for Mathematical Imaging Techniques (CMIT) and is jointly carried out with collaborators; for this paper, these include Noor Badshah (Peshawar, Pakistan), Jian-ping Zhang and Bo Yu (Dalian, China), Lavdie Rada (Liverpool), Noppadol Chumchob (Silpakorn, Thailand), Carlos Brito (Yucatan, Mexico), and Derek A. Gould (Royal Liverpool University Hospital, Liverpool)."
1767979,21239,9078,Image perception of vision impaired by epi-retinal electrode stimulation,2014,"This Retinal prostheses, which helps the vision impaired patients with outer retinal degeneration, uses an outside camera to detect image, convert light energy of the image into a pattern electrical stimulation signal which is transmitted to an array of electrodes placed on the retinal surface, stimulate the remaining retinal ganglion cells to elicit electric activity which is pass down to optic nerve for final processing in the brain and synthesis of a visual image. The visual pathway functions as a complex image processor as well as an information conduit. At higher levels, the visual signals arrive with significant processing completed. In reality due to its easier access, simpler processing and the retinotopic organization, the retina has been the primary focus for artificial stimulation. Studies suggest that retinal implants may provide the patients with an acceptable level of visual mobility via a typical implanted electrode array containing tens of electrodes. In this study, a wide field implantable epi-retinal microelectrode array was designed and fabricated with parylene as flexible substrate material and Pt as electrode and route material, feature test was carried out on the array, and electric characteristics of the array was tested. The feature analysis showed that morphological and electrical properties of the array well met the requirements of implantation and electrical stimulation of retina. The pattern stimulation protocols and projected visual field were discussed."
265835,21239,9078,Detection of blood perfusion,1999,"Ultrasound detection of capillary blood flow is difficult as capillary blood flow provides a weak backscattered signal with a small Doppler shift. To aid in the detection of such signals, a new algorithm is introduced exploiting two powerful ideas. First, the receiver is developed by creating subspaces corresponding to expected flow and clutter signals. It then exploits the fact that some of the basis vectors in the flow and clutter subspaces are very nearly perpendicular. By projecting the input data onto the perpendicular components of the flow subspace a very sensitive detector can be realized. Secondly, the detector coherently sums inputs from several angles. In vitro ultrasound data is collected and applied to the detector. The algorithm utilizing multiple angles is shown to be sensitive detecting flow in the experimental data down to 5 mm/sec in a 6 mm diameter channel."
519578,21239,8228,Extending an Open MPEG-4 Video Streaming Platform to Exploit a Differentiated Services Network,2003,"This paper describes extensions implemented on the MPEG4IP streaming platform, to exploit Differentiated Services. These extensions are based on concepts of proposed QoS frameworks and are implemented by exploiting platform communication capabilities. Within this context, a Packet-marking layer component is introduced performing packet Type of Service (ToS) marking. In the case of live streams, semantics are captured in real-time during encoding, and propagated to the transmission layer. In the case of preencoded streams, information about the semantics is included in the media file metadata and provided as hints to the streaming server. Furthermore, a Video Quality Study component enables the user to preview loss effects on a video stream before its transmission, by simulating packet losses during encoding. In this way new video quality metrics and packet marking algorithms can be investigated. The applicability of certain QoS policies on the extended platform is experimentally evaluated over a Differentiated Services testbed."
79499,21239,8228,Digital image marking by m-sequences for the aim of secure e-commerce,2007,"In this paper we use the properties of m-sequences resulted from a linear feedback shift register(LFSR) as a good tool for logo encrypting for image watermarking. We actually use the properties of m-sequences as a 2-D random signals in order to manipulate digital image watermarking. Since the security of the method is important for us, We embed the resulted watermark by a simple method in the spatial domain. The resulted marked image can be used in the internet and the owner can reveal the mark by our method as a proof for its authority so ease the E-commerce applications."
2594386,21239,8927,Crowdsourcing High Quality Labels with a Tight Budget,2016,"In the past decade, commercial crowdsourcing platforms have revolutionized the ways of classifying and annotating data, especially for large datasets. Obtaining labels for a single instance can be inexpensive, but for large datasets, it is important to allocate budgets wisely. With limited budgets, requesters must trade-off between the quantity of labeled instances and the quality of the final results. Existing budget allocation methods can achieve good quantity but cannot guarantee high quality of individual instances under a tight budget. However, in some scenarios, requesters may be willing to label fewer instances but of higher quality. Moreover, they may have different requirements on quality for different tasks. To address these challenges, we propose a flexible budget allocation framework called Requallo. Requallo allows requesters to set their specific requirements on the labeling quality and maximizes the number of labeled instances that achieve the quality requirement under a tight budget. The budget allocation problem is modeled as a Markov decision process and a sequential labeling policy is produced. The proposed policy greedily searches for the instance to query next as the one that can provide the maximum reward for the goal. The Requallo framework is further extended to consider worker reliability so that the budget can be better allocated. Experiments on two real-world crowdsourcing tasks as well as a simulated task demonstrate that when the budget is tight, the proposed Requallo framework outperforms existing state-of-the-art budget allocation methods from both quantity and quality aspects."
287395,21239,8228,Trust-aware optimal crowdsourcing with budget constraint,2015,"Crowdsourcing has been extensively used for aggregating data from a large pool of workers. In a real crowdsourcing market, each answer obtained from a worker incurs cost. The cost is associated with both the level of trustworthiness of workers and the difficulty of tasks. Typically, access to expert-level (more trustworthy) workers is more expensive than to average crowd and completion of a challenging task is more costly than a click-away question. In this paper, we address the problem of optimal assignment of heterogeneous tasks to workers of varying trust levels with budget constraint. Specifically, we design a trust-aware task allocation algorithm that takes as inputs the estimated trust of workers and pre-set budget, and outputs the optimal assignment of tasks to workers. We derive the bound of total error probability that relates to budget, trustworthiness of crowds, and costs of obtaining labels from crowds naturally. Higher budget, more trustworthy crowds, and less costly jobs result in lower theoretical bound. Our allocation scheme does not depend on the specific design of the trust evaluation component. Therefore, it can be combined with generic trust evaluation algorithms. Our algorithm outperforms state-of-the-art by up to 30% on real data."
2364554,21239,8960,Reducing statistical dependencies in natural signals using radial Gaussianization,2009,"We consider the problem of transforming a signal to a representation in which the components are statistically independent. When the signal is generated as a linear transformation of independent Gaussian or non-Gaussian sources, the solution may be computed using a linear transformation (PCA or ICA, respectively). Here, we consider a complementary case, in which the source is non-Gaussian but elliptically symmetric. Such a source cannot be decomposed into independent components using a linear transform, but we show that a simple nonlinear transformation, which we call radial Gaussianization (RG), is able to remove all dependencies. We apply this methodology to natural signals, demonstrating that the joint distributions of nearby bandpass filter responses, for both sounds and images, are closer to being elliptically symmetric than linearly transformed factorial sources. Consistent with this, we demonstrate that the reduction in dependency achieved by applying RG to either pairs or blocks of bandpass filter responses is significantly greater than that achieved by PCA or ICA."
2258777,21239,8228,Selective Encryption for Hierarchical MPEG,2006,"Selective encryption of visual data and especially MPEG has attracted a considerable number of researchers in recent years. Scalable visual formats are offering additional functionality, which is of great benefit for streaming and networking applications. The MPEG-2 and MPEG-4 standards provide a scalability profile in which a resolution scalable mode is specified. In this paper we evaluate a selective encryp- tion approach on the basis of our hierarchical MPEG video codec."
693315,21239,8494,Improved halftone image data hiding with intensity selection,2001,"In this paper, we propose a novel algorithm called intensity selection (IS) that can be applied to three existing halftone image data hiding algorithms DHST, DHPT and DHSPT to achieve improved visual quality. The proposed IS algorithm generalizes the hidden data representation and select the best location out of a set of candidate locations for the application of DHST/DHPT/DHSPT. It chooses pixel locations that are either very bright or very dark. The IS requires the inverse-halftoned image which implies potentially high computation requirement. Experiments suggest that significant improvement in visual quality can be achieved."
75163,21239,8228,Watermark Security via Secret Wavelet Packet Subband Structures,2003,Wavelet packet decompositions generalize the classical pyramidal wavelet structure. We use the vast amount of possible wavelet packet decomposition structures to create a secret wavelet domain and discuss how this idea can be used to improve the security of watermarking schemes. Two methods to create random wavelet packet trees are discussed and analyzed. The security against unauthorized detection is investigated. Using JPEG and JPEG2000 compression we assess the normalized correlation and Peak Signal to Noise Ratio (PSNR) behavior of the watermarks. We conclude that the proposed systems show improved robustness against compression and provide around 2 1046  possible keys. The security against unauthorized detection is greatly improved.
1658854,21239,9616,Ranking images based on aesthetic qualities.,2014,"The qualitative assessment of image content and aesthetic impression is affected by various image attributes and relations between the attributes. Modelling of such assessments in the form of objective rankings and learning image representations based on them is not a straightforward problem. The criteria can be varied with different levels of complexity for various applications. A highly-complex problem could involve a large number of interrelated attributes and features alongside varied rules. An example of such an application is fashion-interpretation. In this case one can use attribute recognition to label different parts such as clothing and body shape automatically. Thus, the presence or absence of objects in the image is not ambiguous and a similarity measure can be established between images. It is however not clear how to establish such measure between the aesthetic impressions the images make. #R##N##R##N#As a first contribution an approach for ranking images by pooling from the knowledge and experience of crowdsourced annotators is presented. Specifically, the highly subjective and complex problem of fashion interpretation and assessment of aesthetic qualities of images is addressed. To utilize the visual judgements, a novel dataset complete with labellings of various attributes of clothing and body shapes is introduced. Large scale pairwise comparisons of the order of tens of thousands are performed by annotators who follow fashion. Various consistency measures are then applied to verify agreement and correlation between the annotators to rule out inconsistencies amongst them. Based on the annotations, reliable rankings to automatically compare images according to fashion rules are established. #R##N##R##N#Then, Bag of Visual Words object recognition is used to perform classification of the attributes. By incorporating annotator rankings from the first stage and these classification estimates in a lookup model for automatic assessment of images, pairwise comparisons can be automatically performed. Each visual attribute of clothing and body shape is represented within the rankings. #R##N##R##N#Next, rankings obtained from the crowdsourcing procedure are included within several matching approaches to achieve a matching-based ranking. Nearest neighbour matches can be found for a pair of test images which can be compared with their rankings from the annotators. This can be utilized to establish which configuration is ranked better in an image pair. In particular, two prominent approaches of Bag of Visual Words and Local Descriptor Matching are employed to facilitate an evaluation. Several random splits of the dataset proposed in the first stage are used to form the training and test sets. Matches obtained are incorporated within an approach introduced to generate a global ranking. Evaluation from this stage is used as a comparative basis for the approach proposed next in which a learning procedure based on graphical modelling captures the annotator rankings.#R##N##R##N#Finally, a novel approach for learning image representation based on qualitative assessments of visual aesthetics is proposed. It relies on a multi node multi-state model that represents image attributes and their relations. The model is learnt from pairwise image preferences provided by annotators. To demonstrate the effectiveness the approach is applied to fashion image rating, i.e., comparative assessment of aesthetic qualities. The attributes and their relations are assigned learnt potentials which are used to rate the images. Evaluation of the representation model has demonstrated a high performance rate in ranking fashion images."
2144546,21239,9078,Parallel implementation of octtree generation algorithm,1998,"Octtree data structure represents 3D objects as a disjoint union of cubes of varying sizes. The octtree encoding provides an efficient object representation of 3D objects in medical imaging also for storage of planner objects, both in image space and object space with a definite advantage of space saving. Although 2D display technique from a transactional CT scan is quite adequate for diagnosis, it does not optimally communicate the 3D nature of the involved anatomy or the full extent of pathology; 3D imaging presents complex anatomical findings easily, helps in surgical planning and is also used to display radiation beams and isodose surface for radiation therapy planning. Typically, a single CAT scan results in as many as 277 slices (spaced at 1 mm), each slice having 256/spl times/256 points (spaced at 1.2 mm/spl times/1.2 mm). The image space of (2/sup n//spl times/2/sup n//spl times/2/sup n/); n is an integer, is reasonably homogeneous and suggests itself the use of octtree encoding for storage. The process of constructing octtree from CAT scan slices has to start by looking at intensity distribution per slice and form possible cubes of larger size from voxel cubes generated for each plane. A processor managing P planes (P=2/sup p/; p is an integer) can be farmed to different worker nodes to generate the tree up to the level p or octternary codes for level (0,1,...p) independently and communicate with their parent worker/farmer in the tree topology to generate the tree for higher levels. The i-o access time and communication harness is minimized in this transputer model as against an earlier report, since it communicates with each other via their communication links and memory shared between worker and parent worker/farmer. T9000 virtual channel processor would allow any number of logical channels to be multiplexed into four physical communication links. The paper addresses this issue for an efficient implementation on transputers."
872282,21239,9078,Rate-distortion optimized merge frame using piecewise constant functions,2013,"The ability to efficiently switch from one pre-encoded video stream to another is a valuable attribute for a variety of interactive streaming applications, such as switching among streams of the same video encoded in different bit-rates for real-time bandwidth adaptation, or view-switching among videos capturing the same dynamic 3D scene but from different viewpoints. It is well known that intra-coded I-frames can be used at switch boundaries to facilitate stream-switching. However, the size of an I-frame is large, making frequent insertion impractical. A recent proposal towards a more efficient stream-switching mechanism is distributed source coding (D-SC), which exploits worst-case correlation between a set of potential predictor frames in the decoder buffer (called side information (SI) frames) and a target frame to lower encoding rate. However, the conventional use of bit-plane and channel coding means the encoding and decoding complexity of DSC frames is large. In this paper, we pursue a novel approach to the stream-switching problem based on the concept of “signal merging”, using piecewise constant (p-wc) function as the merge operator. Specifically, we propose a new merge mode for a code block, where for each k-th transform coefficient in the block, we encode appropriate step size and horizontal shift parameters at the encoder, so that the resulting floor function at the decoder can map corresponding coefficients from any SI frame to the same reconstructed value, resulting in an identically merged signal. The selection of shift parameter per coefficient, as well as coding modes between intra and merge per block, are optimized in a rate-distortion (RD) optimal manner. Experiments show encouraging coding gain over a previous implementation of DSC frame at low-to mid-bitrates at reduced computation complexity."
1664723,21239,9078,Graph-based joint denoising and super-resolution of generalized piecewise smooth images,2014,"Images are often decoded with noise at receiver due to capturing errors and/or signal quantization during compression. Further, it is often necessary to display a decoded image at a higher resolution than the captured one, given available high-resolution (HR) display or a need to zoom-in for detailed examination. In this paper, we address the problems of image denoising and super-resolution (SR) jointly in one unified graph-based framework, focusing on a special class of signals called generalized piecewise smooth (GPWS) images. GPWS images are composed mostly of smooth regions connected by transition regions, and represent an important subclass of images, including cartoon, sub-regions of video frames with captions, graphics images in video games, etc. Like our previous work on piecewise smooth (PWS) images, GPWS images also imply simple-enough graph representations in the pixel domain, so that suitable graph-based filtering techniques can be readily applied. Specifically, leveraging on previous work on graph spectral analysis, for a given pixel block in low-resolution (LR) we first use the second eigenvector of a computed graph Laplacian matrix to identify a hard boundary, and then use the third eigenvector to identify two piecewise smooth regions and a transition region that separates them. The LR hard boundary is then super-resolved into HR via a procedure based on local self-similarity, while graph weights of the LR transition region is mapped to those of the HR transition region via polynomial fitting. Using the computed HR boundary and weights in the transition region, we construct a suitable HR graph corresponding to the LR counterpart, and perform joint denoising / SR using a graph smoothness prior. Experimental results show that our proposed algorithm outperforms two representative separable denoising / SR schemes in both subjective and objective quality."
1603940,21239,9078,Geodesic methods for biomedical image segmentation,2014,"Tubular and tree structures appear very commonly in biomedical images like vessels, microtubules or neuron cells. Minimal paths have been used for long as an interactive tool to segment these structures as cost minimizing curves. The user usually provides start and end points on the image and gets the minimal path as output. These minimal paths correspond to minimal geodesics according to some adapted metric. They are a way to find a (set of) curve(s) globally minimizing the geodesic active contours energy. Finding a geodesic distance can be solved by the Eikonal equation using the fast and efficient Fast Marching method. In the past years we have introduced different extensions of these minimal paths that improve either the interactive aspects or the results. For example, the metric can take into account both scale and orientation of the path. This leads to solving an anisotropic minimal path in a 2D or 3D+radius space. On a different level, the user interaction can be minimized by adding iteratively what we called the keypoints, for example to obtain a closed curve from a single initial point. The result is then a set of minimal paths between pairs of keypoints. This can also be applied to branching structures in both 2D and 3D images. We also proposed different criteria to obtain automatically a set of end points of a tree structure by giving only one starting point. More recently, we introduced a new general idea that we called Geodesic Voting or Geodesic Density. The approach consists in computing geodesics between a given source point and a set of points scattered in the image. The geodesic density is defined at each pixel of the image as the number of geodesics that pass over this pixel. The target structure corresponds to image points with a high geodesic density. We will illustrate different possible applications of this approach. The work we will present involved as well F. Benmansour, Y. Rouchdy and J. Mille at CEREMADE."
778775,21239,9078,Saliency-cognizant robust view synthesis in free viewpoint video streaming,2013,"In free viewpoint video, texture and depth maps from two camera-captured viewpoints are transmitted, so that at receiver, a novel virtual view chosen by the client can be synthesized via depth-image-based rendering (DIBR). When irrecoverable packet losses occur during transmission-typically affecting less important spatial regions in the video given unequal error protection (UEP) is deployed-appropriate error concealment strategies must be used at decoder to minimize resulting visual degradation in the synthesized view. Towards this goal, we propose a new optimization framework based on visual saliency to combine two different concealment techniques. First, given a pixel in the virtual view is typically constructed as a convex combination of corresponding pixels in the left and right captured views, weighted pixel blending (WPB) readjusts the weights in the linear sum to reflect the expected error in code blocks that contain the corresponding pixels. Second, exemplar-based patch matching (EPM) finds the most similar patches in the known spatial region to complete missing pixels in the unknown region. To choose between candidates constructed using the two techniques when filling a given pixel patch in the synthesized view, we first compute a weighted sum of expected error and visual saliency for each candidate patch. The candidate with the smaller sum (one with small expected error and visual saliency, so that even if errors do occur, they do not stand out visually) is selected for pixel completion. Experimental results show that our scheme can outperform the use of co-located blocks from a previous frame by up to 0.7dB in PSNR and improve subjective visual quality."
1854144,21239,21106,Towards real-time multi-modality 3-D medical image registration,2001,"Intensity value-based registration is a widely used technique for the spatial alignment of medical images. Generally, the registration transformation is determined by iteratively optimizing a similarity measure calculated from the grey values of both images. However, such algorithms may have high computational costs, especially in the case of multi-modality registration, which makes their integration into systems difficult. At present, registration based on mutual information (MI) still requires computation times of the order of several minutes. In this contribution we focus on a new similarity measure based on local correlation (LC) which is well-suited for numerical optimization. We show that LC can be formulated as a least-squares criterion which allows the use of dedicated methods. Thus, it is possible to register MR neuro perfusion time-series (128/sup 2//spl times/30 voxel, 40 images) on a moderate workstation in real-time: the registration of an image takes about 500 ms and is therefore several times faster than image acquisition time. For the registration of CT-MR images (512/sup 2//spl times/87 CT 256/sup 2//spl times/128 MR) a multiresolution framework is used. On top of the decomposition, which requires 47 s of computation time, the optimization with an algorithm based on Ml previously described in the literature takes 97 s. In contrast, the proposed approach only takes 13 s, corresponding to a speedup about a factor of 7. Furthermore, we demonstrate that the superior computational performance of LC is not gained at the expense of accuracy. In particular experiments with dual contrast MR images providing ground truth for the registration show a comparable sub-voxel accuracy of LC and MI similarity."
825784,21239,9078,Hybrid parametric-nonparametric modeling with application to natural image upsampling,2011,"Linear autoregressive (AR) model is widely used in signal processing. Usually the AR models are solved by classical least square (LS) method. An important issue with the LS solution of the AR model, which has been seemingly overlooked, is its numerical stability. The issue is related to the rank condition of the design matrix. We observed, in case of natural images, that the probability of numerical rank deficiency is rather high, roughly thirty-five per cent, due to discrete nature and structures of the digital images. Without care numerical rank deficiency can adversely affect the parameter estimation of the AR model. In this paper we use the rank revealing QR (RRQR) factorization to select optimal subset from the design matrix so as to effectively lower the condition number of the system. By removing the ill conditioned part of the right orthogonal matrix of the RRQR decomposition, we obtain a robust truncated solution to the linear system. On the other hand, for natural images, the unselected data tend to highly correlate with the pixel being modeled, and their exclusion from the modeling process waste valuable information. To avoid this loss we recycle the data including those discard by the parametric AR estimator into a nonparametrgic model of nonlocal type. Interestingly, the data that cause ill condition to the parametric AR model are of high quality for the non-local nonparametric modeling. Therefore, an approach of hybrid parametric-nonparametric modeling can make the best use of data and improve the model performance. The hybrid modeling approach is applied to image resolution upconversion, and it greatly improves the performance of the state-of-the-art image interpolator, achieving a gain of 3dB or more in PSNR in some cases."
2259788,21239,11470,Hidden Markov Model for eye gaze prediction in networked video streaming,2011,"With the advent of eye gaze tracking technology, eye gaze is increasingly being used as a media interaction trigger in a variety of applications, such as eye typing, video content customization, and network video streaming based on region-of-interest (ROI). The reaction time of a gaze-based networked system, however, is in practice lower-bounded by the round trip time (RTT) of today's networks, which can be large. To improve the efficacy of gaze-based networked systems, in the paper we propose a Hidden Markov Model (HMM)-based gaze prediction strategy to predict future gaze locations to lower end-to-end reaction delay. We first design an HMM with three states corresponding to human's three major types of intrinsic eye movements. HMM parameters are obtained offline on a per-video basis during training phase. During testing phase, a window of noisy gaze observations are collected in real-time as input to a forward algorithm, which computes the most likely HMM state. Given the deduced HMM state, linear prediction is used to predict gaze location RTT seconds into the future. We demonstrate the applicability of our gaze prediction strategy by focusing on ROI-based bit allocation for network video streaming. To reduce transmission rate of a video stream without degrading viewer's perceived visual quality, we allocate more bits to encode the viewer's current spatial ROI, while devoting fewer bits in other spatial regions. The challenge lies in overcoming the delay between the time a viewer's ROI is detected by gaze tracking, to the time the effected video is encoded, delivered and displayed at the viewer's terminal. To this end, we use our proposed gaze-prediction strategy to predict future eye gaze locations, so that optimized bit allocation can be performed for future frames. Our experiments show that bit rate can be reduced by 21% without noticeable visual quality degradation when end-to-end network delay is as high as 200ms."
669088,21239,9078,Adaptive frame and QP selection for temporally super-resolved full-exposure-time video,2011,"In order to allow sufficient amount of light into the image sensor, videos captured in poor lighting conditions typically have low frame rate and frame exposure time equals to inter-frame period — commonly called full exposure time (FET). FET low-frame-rate videos are common in situations where lighting cannot be improved a priori due to practical (e.g., large physical distance between camera and captured objects) or economical (e.g., long duration of nighttime surveillance) reasons. Previous computer vision work has shown that content at a desired higher frame rate can be recovered (to some degree of precision) from the captured FET video using self-similarity-based temporal super-resolution. For a network streaming scenario, where a client receives a FET video stream from a server and plays back in real-time, the following practical question remains, however: what is the most suitable representation of the captured FET video at encoder, given that a video at higher frame rate must be constructed at the decoder at low complexity? In this paper, we present an adaptive frame and quantization parameter (QP) selection strategy, where, for a given targeted rate-distortion (RD) tradeoff, FET video frames at appropriate temporal resolutions and QP are selected for encoding using standard H.264 tools at encoder. At the decoder, temporal super-resolution is performed at low complexity on the decoded frames to synthesize the desired high frame rate video for display in real-time. We formulate the selection of individual FET frames at different temporal resolutions and QP as a shortest path problem to minimize Lagrangian cost of the encoded sequence. Then, we propose a computation-efficient algorithm based on monotonicity in predictor's temporal resolution and QP to find the shortest path. Experiments show that our strategy outperforms alternative na¨ive non-adaptive approaches by up to 1.3dB at the same bitrate."
727867,21239,9078,Joint gaze-correction and beautification of DIBR-synthesized human face via dual sparse coding,2014,"Gaze mismatch is a common problem in video conferencing, where the viewpoint captured by a camera (usually located above or below a display monitor) is not aligned with the gaze direction of the human subject, who typically looks at his counterpart in the center of the screen. This means that the two parties cannot converse eye-to-eye, hampering the quality of visual communication. One conventional approach to the gaze mismatch problem is to synthesize a gaze-corrected face image as viewed from center of the screen via depth-image-based rendering (DIBR), assuming texture and depth maps are available at the camera-captured viewpoint(s). Due to self-occlusion, however, there will be missing pixels in the DIBR-synthesized view image that require satisfactory filling. In this paper, we propose to jointly solve the hole-filling problem and the face beautification problem (subtle modifications of facial features to enhance attractiveness of the rendered face) via a unified dual sparse coding framework. Specifically, we first train two dictionaries separately: one for face images of the intended conference subject, one for images of “beautiful” human faces. During synthesis, we simultaneously seek two code vectors — one is sparse in the first dictionary and explains the available DIBR-synthesized pixels, the other is sparse in the second dictionary and matches well with the first vector up to a restricted linear transform. This ensures a good match with the intended target face, while increasing proximity to “beautiful” facial features to improve attractiveness. Experimental results show naturally rendered human faces with noticeably improved attractiveness."
281544,21239,11052,Image Sequence Restoration: A PDE Based Coupled Method for Image Restoration and Motion Segmentation,1998,"There is a strong need to automatically remove noise and degradations from noisy image sequences. Applications areas include Image surveillance, Forensic Image Processing, Digital video broadcasting, Digital Film Restoration, Virtual Studio, Medical Image Processing, Remote Sensing $\ldots$. Image sequence restoration is tightly coupled to motion segmentation. It requires to extract moving objects in order to separately restore the background and each moving region along its particular motion trajectory. Most of the work done to date mainly involves motion compensated temporal filtering techniques with appropriate 2D or 3D Wiener filter for noise suppression, 2D/3D median filtering or more appropriate morphological operators for removing impulsive noise. Usually, motion segmentation and image restoration are tackled separately in image sequence restoration. In this article, the motion segmentation and the image restoration parts are done in a coupled way, allowing the motion segmentation part to positively influence the restoration part and vice-versa. This is the key of our approach that allows to deal simultaneously with the problem of restoration and motion segmentation. To take into account both requirements, we present an original PDE based method which permits to solve the two problems in a coupled way. This PDE based approach allows to anisotropically restore the image sequence : edges are well preserved and blur is not introduced during the restoration process. To this end, we reformulate the image sequence restoration problem as an energy functional minimization. A suitable numerical scheme based on half-quadratic minimization is proposed and its stability demonstrated. Experimental results obtained on noisy synthetic data and real images will illustrate the capabilities of this original and efficient approach."
2954233,21239,9078,Investigation of adaptive local threshold segmentation in context of 3D-handwriting forensics,2016,"Image segmentation plays an important role in digitized crime scene forensics. Particularly in context of modern high resolution contact-less and non-destructive acquisition and analysis of handwriting impression traces by means of 3D sensors, one main challenge is the separation of writing trace areas and non-traces by image segmentation. In earlier work authors have presented the general, yet qualitative feasibility to do so by an initial processing pipeline based on data acquisition, pre-processing and a global segmentation approach. However, quantitative measurements with regards to the segmentation quality have not been studied yet, as well as the discussion of alternative strategies for 3D image segmentation in this scenario. In this paper, we extent the earlier work by introducing a concept for benchmarking segmentation accuracy for 3D handwriting traces. Further we present results with regards to the initial approach as well as a new, adaptive local threshold segmentation. The benchmarking is based on ground truth data, determined using data of handwriting traces acquired by a high-quality flatbed scanner and segmentation information retrieved from those by means of an Otsu operator. This ground truth allows for calculation of true positive, true negative, false positive and false negative error rates as quality measurement. The practical impact of the suggested benchmarking is shown by comparison of experimental results based on initial segmentation approach and new adaptive approach. Experiments are based on ten handwriting traces each of eleven persons. The comparison of results indicates that the best parameter set of the adaptive thresholding leads to an quality increase of 12.1% in terms of precision for writing trace and decrease of 1.4% in terms of precission for background."
354242,21239,9078,Robust perceptual image hashing via matrix invariants,2004,"In this paper we suggest viewing images (as well as attacks on them) as a sequence of linear operators and propose novel hashing algorithms employing transforms that are based on matrix invariants. To derive this sequence, we simply cover a two dimensional representation of an image by a sequence of (possibly overlapping) rectangles R/sub i/ whose sizes and locations are chosen randomly/sup 1/ from a suitable distribution. The restriction of the image (representation) to each R/sub i/ gives rise to a matrix A/sub i/. The fact that A/sub i/'s will overlap and are random, makes the sequence (respectively) a redundant and non-standard representation of images, but is crucial for our purposes. Our algorithms first construct a secondary image, derived from input image by pseudo-randomly extracting features that approximately capture semi-global geometric characteristics. From the secondary image (which does not perceptually resemble the input), we further extract the final features which can be used as a hash value (and can be further suitably quantized). In this paper, we use spectral matrix invariants as embodied by singular value decomposition. Surprisingly, formation of the secondary image turns out be quite important since it not only introduces further robustness (i.e., resistance against standard signal processing transformations), but also enhances the security properties (i.e. resistance against intentional attacks). Indeed, our experiments reveal that our hashing algorithms extract most of the geometric information from the images and hence are robust to severe perturbations (e.g. up to %50 cropping by area with 20 degree rotations) on images while avoiding misclassification. Our methods are general enough to yield a watermark embedding scheme, which will be studied in another paper."
2369773,21239,9616,Automatic Object Segmentation by Quantum Cuts,2014,"In this study, the link between quantum mechanics and graph-cuts is exploited and a novel saliency map generation and salient object segmentation method is proposed based on the ground state solution of a modified Hamiltonian. First, the graph representation of certain quantum mechanical operators is studied. This reveals strong connections with widely used graph-cut algorithms while quantum mechanical constraints exhibit crucial advantages over the existing graph-cut algorithms. Furhtermore, concepts such as potential field helps solving a particular singularity problem related to laplacian matrices. In the proposed approach, the ground state (wave function) corresponding to a sub-atomic particle of a modified Hamiltonian operator corresponds to a particular optimization problem, the solution of which yields the salient object segmentation in a digital image. This approach provides a parameter-free -hence dataset independent-, unsupervised and fully automatic saliency map generation, which outperforms many existing state-of-the-art algorithms. The results of the proposed salient object extraction method exhibit such a promising accuracy that pushes the frontier in this field to the borders of the input-driven processing only - without the use of object knowledge aided by long-term human memory and intelligence. Furthermore, with the novel technologies for measuring a quantum wave function, the proposed method has a unique potential: Salient object segmentation in an actual physical setup in nano-scale. Such an unprece-dendent property will not only produce segmentation results instantaneously, but may be a unique opportunity to achieve accurate object segmentation in real-time for the massive visual repositories of today's Big Data."
587011,21239,20358,Language Understanding in the Wild: Combining Crowdsourcing and Machine Learning,2015,"Social media has led to the democratisation of opinion sharing. A wealth of information about public opinions, current events, and authors' insights into specific topics can be gained by understanding the text written by users. However, there is a wide variation in the language used by different authors in different contexts on the web. This diversity in language makes interpretation an extremely challenging task. Crowdsourcing presents an opportunity to interpret the sentiment, or topic, of free-text. However, the subjectivity and bias of human interpreters raise challenges in inferring the semantics expressed by the text. To overcome this problem, we present a novel Bayesian approach to language understanding that relies on aggregated crowdsourced judgements. Our model encodes the relationships between labels and text features in documents, such as tweets, web articles, and blog posts, accounting for the varying reliability of human labellers. It allows inference of annotations that scales to arbitrarily large pools of documents. Our evaluation using two challenging crowdsourcing datasets shows that by efficiently exploiting language models learnt from aggregated crowdsourced labels, we can provide up to 25% improved classifications when only a small portion, less than 4% of documents has been labelled. Compared to the six state-of-the-art methods, we reduce by up to 67% the number of crowd responses required to achieve comparable accuracy. Our method was a joint winner of the CrowdFlower - CrowdScale 2013 Shared Task challenge at the conference on Human Computation and Crowdsourcing (HCOMP 2013)."
2296526,21239,11052,An Extended Phase Field Higher-Order Active Contour Model for Networks and Its Application to Road Network Extraction from VHR Satellite Images,2008,"This paper addresses the segmentation from an image of entities that have the form of a `network', i.e. the region in the image corresponding to the entity is composed of branches joining together at junctions, e.g. road or vascular networks. We present a new phase field higher-order active contour (HOAC) prior model for network regions, and apply it to the segmentation of road networks from very high resolution satellite images. This is a hard problem for two reasons. First, the images are complex, with much `noise' in the road region due to cars, road markings, etc., while the background is very varied, containing many features that are locally similar to roads. Second, network regions are complex to model, because they may have arbitrary topology. In particular, we address a severe limitation of a previous model in which network branch width was constrained to be similar to maximum network branch radius of curvature, thereby providing a poor model of networks with straight narrow branches or highly curved, wide branches. To solve this problem, we propose a new HOAC prior energy term, and reformulate it as a nonlocal phase field energy. We analyse the stability of the new model, and find that in addition to solving the above problem by separating the interactions between points on the same and opposite sides of a network branch, the new model permits the modelling of two widths simultaneously. The analysis also fixes some of the model parameters in terms of network width(s). After adding a likelihood energy, we use the model to extract the road network quasi-automatically from pieces of a QuickBird image, and compare the results to other models in the literature. The results demonstrate the superiority of the new model, the importance of strong prior knowledge in general, and of the new term in particular."
2987350,21239,9078,Resource-efficient latent fingerprint age estimation for Adhoc crime scene forensics: Quality assessment of flat bed scans and statistical features,2016,"In the field of biometrics and forensics, age estimation is an important topic, for example when determining the age of human subjects or when assessing the age of a particular taken biometric sample or captured forensic trace from crime scenes. The latter case is investigated in this paper, with the focus on latent fingerprints. Here, the trace age represents the time between deposition and capture of a print, which might assist an investigation in several respects, such as forgery detection (trace freshness inconsistencies) or strengthening a prints evidentiary value in court (linking a suspect to the time of a crime). Non-invasive, high-resolution and very cost-intensive capturing devices have recently been proposed to address this 80 year old challenge of latent print age estimation by studying print time series. This approach is transferred here to a (cheap) off-the-shelf flat bed scanner, enabling a broad variety of researchers to study print aging in the future. Based on 700 time series (including 10% empty substrate series for comparison), a separation performance of 89.2% (eccrine) and 98.7% (sebaceous) was achieved for separating fresh prints and those aged for at least half a day. Results include a comparison to prior used high-resolution Chromatic White Light (CWL) sensors and Confocal Laser Scanning Microscopes (CLSM), also showing the possibility of significant preprocessing simplifications due to decreased capturing distortions as well as the need for further (quantitative) studies."
2535469,21239,9099,Automatic skin enhancement with visible and near-infrared image fusion,2010,"Skin tones, portraits in particular, are of critical importance in photography and video, but a number of factors, such as pigmentation irregularities (e.g., moles, freckles), irritation, roughness, or wrinkles can reduce their appeal. Moreover, such defects are oftentimes enhanced by scene lighting conditions.   Starting with the observations that melanin and hemoglobin, the key components of skin color, have little absorption in the near-infrared (NIR) part of the spectrum, and that the depth of light penetration in the epidermis is proportional to the incident light's wavelength, we show that near-infrared images provide information that can be used to automatically smooth skin tones in a physically realistic manner.   Specifically, we developed a prototype camera system that consists of capturing a pair of visible/near-infrared images and separating both of them into base and detail layers (akin to a low/high frequency decomposition) with the fast bilateral filter. Smooth and realistic output images are obtained by fusing the base layer of the visible image with the near-infrared detail layer. The proposed method delivers consistently good results across various skin types.   The prototype system is currently in use at the Swiss Camera Museum in Vevey, Switzerland, where the visitors can take their pictures and e-mail themselves the results. In the process, we are collecting the users' preference for either the original (visible) image or the enhanced (visible and NIR fused) image. The system has been deployed for three months. Preliminary statistics indicate that a large majority (79%) prefers the enhanced image."
2359815,21239,9078,Likelihood-based texture discrimination with multiscale stochastic models,1994,"A class of multiscale models describing stochastic processes indexed by the nodes of a tree has recently been introduced by Chou et al. (1994). Experimental and theoretical results indicate that this class of models is quite rich, and moreover these models lead to extremely efficient algorithms for optimal estimation based on noisy observations. This motivates further algorithmic development, and in particular, in this paper we present a likelihood calculation algorithm for this class of multiscale models. That is, we consider the problem of computing the log of the conditional probability of a set of data assuming that they correspond to a particular multiscale model. We exploit the structure of the multiscale models to develop an efficient, scale recursive algorithm that allows for multiresolution data and parameters which vary in both space and scale. We illustrate one possible application of the algorithm to a texture classification problem in which one must choose from a given set of models that model which best represents or most likely corresponds to a given set of random field measurements. Texture modeling with Gaussian Markov random field (GMRF) models is well documented. One difficulty in using GMRF models, however, is that the calculation of likelihoods may be prohibitively complex computationally if there is an irregular sampling pattern. It is shown here that GMRF models can be represented within our multiscale model class which allows us to approximately compute likelihoods for GMRF models based on measurements over arbitrarily sampled regions. As we demonstrate in the context of texture discrimination problems, the multiscale approach not only leads to computationally efficient implementations, but also to virtually the same performance as the optimal GMRF-based likelihood ratio test. We discuss further applications in the area of synthetic aperture radar imagery processing. >"
2050720,21239,9078,Error-resilient mobile video by edge adapted multiscale transforms,2001,"Summary form only given, as follows. Mobile video systems face particularly challenging problems due to the hostile nature of the mobile radio channel and the very low bit-rate available. For the fading channel, the Shannon separation principle does not hold, and hence source and channel coding should be considered jointly. This talk reviews some advances in our work on joint optimization of video coding and error control schemes for the mobile radio channel. As a new formal framework for end-to-end optimization, the so-called distortion-distortion-function (DDF) is introduced that shows the distortions introduced by source coding vs. the distortions resulting from channel errors for varying bit allocation between source and channel coding. We show the impact of various error-resiliency techniques on the DDF, such as motion-compensated concealment, increasing the frequency of synchronization words in the bit-stream, or increasing the percentage of intra-coded blocks. We show that for increasing burstiness of channel errors, the importance of an error-resilient decoder increases, while the effectiveness of FEC decreases. Channel-adaptive source coding is shown to be a powerful technique, if channel feedback can be incorporated into the source coder. Finally we address techniques that take advantage of layered scalable representations of the video signal. In order to achieve unequal error protection of the layers, we discuss FEC across IP packets and receiver-driven adaptive pre-etching techniques. Such techniques can be advantageously combined with proxy servers for mobile multimedia access."
1804014,21239,9078,Transform domain sparsification of depth maps using iterative quadratic programming,2011,"Compression of depth maps is important for “texture plus depth” format of multiview images, which enables synthesis of novel intermediate views via depth-image-based rendering (DIBR) at decoder. Previous depth map coding schemes exploit unique depth data characteristics to compactly and faithfully reproduce the original signal. In contrast, since depth map is only a means to the end of view synthesis and not itself viewed, in this paper we explicitly manipulate depth values, without causing severe synthesized view distortion, in order to maximize representation sparsity in the transform domain for compression gain — we call this process transform domain spar-sification (TDS). Specifically, for each pixel in the depth map, we first define a quadratic penalty function, with minimum at ground truth depth value, based on synthesized view's distortion sensitivity to the pixel's depth value during DIBR. We then define an objective for a depth signal in a block as a weighted sum of: i) signal's sparsity in the transform domain, and ii) per-pixel synthesized view distortion penalties for the chosen signal. Given that sparsity (70-norm) is non-convex and difficult to optimize, we replace the Zo-norm in the objective with a computationally inexpensive weighted 12-norm; the optimization is then an unconstrained quadratic program, solvable via a set of linear equations. For the weighted /2-norm to promote sparsity, we solve the optimization iteratively, where at each iteration weights are readjusted to mimic sparsity-promoting Z T -norm, 0 < r < 1. Using JPEG as an example transform codec, we show that our TDS approach gained up to 1.7dB in rate-distortion performance for the interpolated view over compression of unaltered depth maps."
684568,21239,9078,Depth map compression using multi-resolution graph-based transform for depth-image-based rendering,2012,"Depth map compression is important for efficient network transmission of 3D visual data in texture-plus-depth format, where the observer can synthesize an image of a freely chosen viewpoint via depth-image-based rendering (DIBR) using received neighboring texture and depth maps as anchors. Unlike texture maps, depth maps exhibit unique characteristics like smooth interior surfaces and sharp edges that can be exploited for coding gain. In this paper, we propose a multi-resolution approach to depth map compression using previously proposed graph-based transform (GBT). The key idea is to treat smooth surfaces and sharp edges of large code blocks separately and encode them in different resolutions: encode edges in original high resolution (HR) to preserve sharpness, and encode smooth surfaces in low-pass-filtered and down-sampled low resolution (LR) to save coding bits. Because GBT does not filter across edges, it produces small or zero high-frequency components when coding smooth-surface depth maps and leads to a compact representation in the transform domain. By encoding down-sampled surface regions in LR GBT, we achieve representation compactness for a large block without the high computation complexity associated with an adaptive large-block GBT. At the decoder, encoded LR surfaces are up-sampled and interpolated while preserving encoded HR edges. Experimental results show that our proposed multi-resolution approach using GBT reduced bitrate by 68% compared to native H.264 intra with DCT encoding original HR depth maps, and by 55% compared to single-resolution GBT encoding small blocks."
1115116,21239,9748,Acceleration of Bilateral Filtering Algorithm for Manycore and Multicore Architectures,2012,"Bilateral filtering is an ubiquitous tool for several kinds of image processing applications. This work explores multicore and many core accelerations for the embarrassingly parallel yet compute-intensive bilateral filtering kernel. For many core architectures, we have created a novel pair-symmetric algorithm to avoid redundant calculations. For multicore architectures, we improve the algorithm by use of low-level single instruction multiple data (SIMD) parallelism across multiple threads. We propose architecture specific optimizations, such as exploiting the unique capabilities of special registers available in modern multicore architectures and the rearrangement of data access patterns as per the computations to exploit special purpose instructions. We also propose optimizations pertinent to Nvidia's Compute Unified Device Architecture (CUDA), including utilization of CUDA's implicit synchronization capability and the maximization of single-instruction-multiple-thread efficiency. We present empirical data on the performance gains we achieved over a variety of hardware architectures including Nvidia GTX 280, AMD Barcelona, AMD Shanghai, Intel Harper town, AMD Phenom, Intel Core i7 quad core, and Intel Nehalem 32 core machines. The best performance achieved was (i) 169-fold speedup by the CUDA-based implementation of our pair-symmetric algorithm running on Nvidia's GTX 280 GPU compared to the compiler-optimized sequential code on Intel Core i7, and (ii) 38-fold speedup using 16 cores of AMD Barcelona each equipped with a 4-stage vector pipeline compared to the compiler-optimized sequential code running on the same machine."
2036284,21239,9078,Multiscale image texture analysis in wavelet spaces,1994,"The paper describes a new method for texture feature extraction and analysis in images using wavelet transform (WT), KL-expansion and Kohonen maps. For this purpose, the authors first apply a global wavelet transform on the initial image. Due to the localization properties of the WT both in the spatial and in the frequency domain it is possible to describe the local texture features in the surroundings of any pixel by a set of respective wavelet coefficients. This is accomplished by a local traversal of the wavelet pyramid and finally results in the feature vector required. Since the localization is limited by Heisenberg's uncertainty principle one must approximate the single coefficients for each pixel by piecewise linear interpolation. Once the feature vector is derived from the WT, further steps in the analysis pipeline perform decorrelation, normalization and finally clustering and supervised classification. In contrast to many related wavelet-based approaches, that usually apply different WTs on every texture sample and classify based on means derived from the former, the present method especially accounts for many real world applications. In those cases there are not usually large coherent texture regions that allow separated treatment. Moreover the approach employs a global WT and then stresses the local properties of the basis functions to identify local areas of interest from the initial image, as for instance training areas. The authors illustrate the efficiency of the method by classifying different real world textures with LVQ classifiers. >"
376126,21239,9078,Multiple description scalable coding using wavelet-based motion compensated temporal filtering,2003,"Packet delay jitter and loss due to network congestion pose significant challenges for designing and deploying delay sensitive multimedia applications over the best effort packet switched networks such as the Internet. Recent studies indicate that using multiple descriptions coding (MDC) in conjunction with path or server diversity can mitigate these effects. However, the proposed MDC coding and streaming techniques are based on non-scalable coding techniques. A key disadvantages of these techniques is that they can only improve the error resilience of the transmitted video, but are not able to address two other important challenges associated with the robust transmission of video over unreliable networks: adaptation to bandwidth variations and receiving device characteristics. In this paper, we present a new paradigm, referred to as multiple description scalable coding (MDSC), that is able to address all the previously mentioned challenges by combining the advantages of scalable coding and MDC. This framework enables tradeoffs between throughput, redundancy and complexity at transmission time, unlike previous non-scalable MDC schemes. Furthermore, we also propose a novel MDSC scheme based on motion compensated temporal filtering (MCTF), denominated multiple description motion compensated temporal filtering (MD-MCTF). We use the inherent ability of current MCTF schemes, using the lifting implementation of temporal filtering. We show how tradeoffs between throughput, redundancy and complexity can easily be achieved by adaptively partitioning the video into several descriptions after MCTF. Based on our simulations using different network conditions, the proposed MD-MCTF framework outperforms existing MDC schemes over a variety of network conditions."
1770384,21239,11470,"Optimizing Algorithms for Region-of-Interest Video Compression, with Application to Mobile Telehealth",2006,"Wireless communication of video poses constraints on information capacity. Region-of-Interest (ROI) video coding provides higher quality in the ROI, but poorer quality in the background (BKGRND), for a given total bitrate (TBR). Researchers, including the authors, have also proposed more graceful quality management methods, using what is referred to here as an Extended-Region-of-Interest (EROI). We consider three levels of losslessness - mathematical, diagnostic, and perceptual, with the goal of associating them with the above-mentioned regions. We describe work in progress aimed at optimizing an elastic expert system based on the above methodology, with telehealth video as its anchor. The optimizations are threefold - user, perceptual, and network oriented, and are incorporated in the rate control algorithm. We propose a rate control method where, unlike conventional methods, bit allocation is shifted from the frame level to individual regions within the frame. Thereafter, the above-mentioned criteria are used to determine regional bit allocation. Peak-Signal-to-Noise-Ratio (PSNR) results show, as expected, that the proposed scheme achieves higher ROI-EROI quality than the verification model VM8 of MPEG4. This is illustrated with four examples of pediatrics video. The value and design of the proposed methodology is being corroborated by subjective testing involving medical experts. We are independently researching another standing issue in the telehealth application, that of low complexity segmentation and tracking of the ROI-EROI boundaries."
1326909,21239,9078,Optimizing peer grouping for live free viewpoint video streaming,2013,"In free viewpoint video, a user can pull texture and depth videos captured from two nearby reference viewpoints to synthesize his chosen intermediate virtual view for observation via depth-image-based rendering (DIBR). For users who are observing the same video at the same time but not necessarily from the same virtual viewpoint, they have incentive to pull the same reference views so that the streaming cost can be shared. On the other hand, in general distortion of a synthesized virtual view increases with its distance to the reference views, and so a user also has incentive to select reference views that tightly “sandwich” his chosen virtual view, minimizing distortion. In a previous work, reference view sharing strategies-ones that optimally trade off shared streaming costs with synthesized view distortions-were investigated for the case when users are first divided into groups, and each user group independently pulls two reference views and shares the resulting streaming cost. In this paper, we generalize the previous notion of user group, so that a user can simultaneously belong to two groups, and each group shares the streaming cost of a single view. We also aim to find a Nash Equilibrium (NE) solution of reference view selection, which is stable and from which no one has incentive to unilaterally deviate. Specifically, we first derive a lemma based on known properties of synthesized view distortion functions. We then design a search algorithm to find a NE solution, leveraging on the derived lemma to reduce search complexity. Experimental results show that the stable NE solution increases the overall cost only slightly when compared to the unstable optimal reference selection that gives the lowest overall cost. Further, a larger network will give a lower average cost for each user, and thus, users tend to join large networks for cooperation."
2146011,21239,9078,Object based video with progressive foreground,1997,"A novel algorithm is described for coding objects in video compression systems which gives complete control over the bit allocation to the video objects. The method is evaluated by application to a two layer codec, with background and foreground objects. Images in a sequence are divided into macroblocks, which are compared to a stored background model. Those whose mean square difference (MSD) exceeds a noise threshold are considered as foreground, and may be split into successively finer partitions described by a quadtree data structure. This defines more precisely the outline of the foreground object within each macroblock and gives a set of microblocks of variable size. The microblocks are transformed by the 2D DCT, and the DCT coefficients are coded bitplane by bitplane over the selected object using a zigzag masked (ZZM) technique. For robustness, data is then transmitted macroblock by macroblock, and intraframe or interframe coding of blocks can be selected adaptively at frame, macroblock or microblock level. The scheme is designed to stop sending bits when a certain bit allocation or PSNR is reached for any object, leading to fully controllable bit rates. The relationship between the average PSNR of the sequence and the average bits per pixel in a frame shows that the codec gives good performance over a range of peak bit rates from 0.1 bpp to 1 bpp. Over the whole range, the quality of the reconstructed sequence is significantly better than 'motion JPEG', while the codec is much simpler and faster than MPEG."
1884660,21239,23735,"SEGMENTS: a layered, dual-Kalman filter algorithm for indoor feature extraction",2000,"A layered algorithm for extracting features in an indoor environment from planar range data is presented. At the lower (signal processing) level, the SEGMENTS algorithm exploits the speed and accuracy of two extended Kalman filters working in tandem and processes sequentially the distance measurements within each scan. The first Kalman filter is responsible for initiating straight line segments and detecting clutter while the second estimates the parameters of each line segment (such as distance and orientation) and determines when this is interrupted. The dual filter combination is capable of detecting edges and straight line segments within the scene in front of the robot. At the higher (post-processing) level, the identified segments are combined to form more complex features such as extended walls, corners (concave and convex), doors and corridors. During the composition cycle, SEGMENTS makes use of (i) the parametric representation of the straight line segments and (ii) the prespecific topological models of the features this algorithm seeks. A list of the identified features along with their location with respect to the laser sensor is finally available to the user. The cluttered regions in the scene are also marked on a polar representation of the environment. The presented algorithm has been tested on a Pioneer 2 DX mobile robot equipped with a SICK LMS 200 proximity laser scanner performing map-based localization. Low computational requirements, accuracy and robustness to uncertainty and noise characterize the performance of this new method for feature extraction."
101957,21239,11052,Recursive bilateral filtering,2012,"This paper proposes a recursive implementation of the bilateral filter. Unlike previous methods, this implementation yields an bilateral filter whose computational complexity is linear in both input size and dimensionality. The proposed implementation demonstrates that the bilateral filter can be as efficient as the recent edge-preserving filtering methods, especially for high-dimensional images. Let the number of pixels contained in the image be N, and the number of channels be D, the computational complexity of the proposed implementation will be O(ND). It is more efficient than the state-of-the-art bilateral filtering methods that have a computational complexity of O(ND2) [1] (linear in the image size but polynomial in dimensionality) or O(Nlog(N)D) [2] (linear in the dimensionality thus faster than [1] for high-dimensional filtering). Specifically, the proposed implementation takes about 43 ms to process a one megapixel color image (and about 14 ms to process a 1 megapixel grayscale image) which is about 18 × faster than [1] and 86× faster than [2]. The experiments were conducted on a MacBook Air laptop computer with a 1.8 GHz Intel Core i7 CPU and 4 GB memory. The memory complexity of the proposed implementation is also low: as few as the image memory will be required (memory for the images before and after filtering is excluded). This paper also derives a new filter named gradient domain bilateral filter from the proposed recursive implementation. Unlike the bilateral filter, it performs bilateral filtering on the gradient domain. It can be used for edge-preserving filtering but avoids sharp edges that are observed to cause visible artifacts in some computer graphics tasks. The proposed implementations were proved to be effective for a number of computer vision and computer graphics applications, including stylization, tone mapping, detail enhancement and stereo matching."
3052232,21239,9616,Multi-scale underwater descattering,2016,"Underwater images suffer from severe perceptual/visual degradation, due to the dense and non-uniform medium, causing scattering and attenuation of the propagated light that is sensed. Typical restoration methods rely on the popular Dark Channel Prior to estimate the light attenuation factor, and subtract the back-scattered light influence to invert the underwater imaging model. However, as a consequence of using approximate and global estimates of the back-scattered light, most existing single-image underwater descattering techniques perform poorly when restoring non-uniformly illuminated scenes. To mitigate this problem, we introduce a novel approach that estimates the back-scattered light locally, based on the observation of a neighborhood around the pixel of interest. To circumvent issue related to selection of the neighborhood size, we propose to fuse the images obtained over both small and large neighborhoods, each capturing distinct features from the input image. In addition, the Laplacian of the original image is provided as a third input to the fusion process, to enhance texture details in the reconstructed image. These three derived inputs are seamlessly blended via a multi-scale fusion approach, using saliency, contrast, and saturation metrics to weight each input. We perform an extensive qualitative and quantitative evaluation against several specialized techniques. In addition to its simplicity, our method outperforms the previous art on extreme underwater cases of artificial ambient illumination and high water turbidity."
2099413,21239,9078,A neural approach to optical image reconstruction,1995,"The paper presents, with experimental results, a method of applying Fourier optical signal processing as a pre-processor to a digital signal system. The input illumination is a coherent light source from a HeNe laser and computation of the Fourier transform (FT) is carried out via a FT lens. An image is placed in the front focal plane of the FT lens and the Fourier transform appears at the rear focal plane. A low cost charged coupled devices (CCD) camera is employed to capture the optical Fourier signal. Due to avalanche effects and the easily saturated characteristics inherent in all CCD cameras, a very noisy and saturated power spectral density is captured. In order to retrieve the original image back using the input/output approach proposed by Fienup [1978], important criteria like the initial guess object and the imposed Fourier object constraints plays a crucial role in the reconstruction process. The paper also proposes a low-cost and an efficient way of how neural networks can be used as a productive tool in the process of solving the phase-retrieval problem of reconstructing a general object from the modulus of its Fourier-transformed optical image. Problems encountered during the construction phase were studied and solutions were provided both at the optical processing and digital system ends. This helps to understand the accuracy of the transformation, the practical behaviour and characteristics of the optical lens system."
2121246,21239,9078,Stochastic modeling and entropy constrained estimation of motion from image sequences,1998,"We consider the problem of coding video signals using motion compensation and a forward coded dense motion field. First, we develop a motion estimation technique that yields dense estimates suitable for the coding application; next, we develop a prototype of a video coder, which we use to verify that high coding performance is attainable within our framework. To find our sought motion estimates, we assume motion in an observed image sequence to be a stochastic process, modeled as a Markov random field (MRF). The standard maximum a posteriori (MAP) estimation problem with MRF priors is formulated as a constrained optimization problem (where the constraint is on the entropy of the sought estimate), but then transformed into a classical MAP estimation problem, and solved using standard techniques. A key advantage of the constrained formalization is that, in the process of transforming it back to the classical framework, parameters which in the classical framework are left unspecified (and often tweaked in an experimental stage) become now uniquely determined by the introduced entropy constraint. To verify that our motion estimates are indeed useful for coding, we compare the performance of a prototype video coder with that of an equivalent coder based on block-matching motion estimates. Experimental results reveal, for various types of video signals and at various rates, that: (a) in terms of PSNR, our system equals or improves upon the performance of full search block matching; and (b) in terms of visual quality our improvements are significant, since our images are completely free of blocking artifacts."
1850314,21239,9078,Real-Time Automatic Detection of Violent-Acts by Low-Level Colour Visual Cues,2007,"Automatic recognition of human activities is important for the development of next generation video-surveillance systems. In this paper we address the specific problem of automatically detecting violent interpersonal acts in monocular colour video streams. Unlikely previous approaches, only little knowledge is assumed about the acquisition setup and about the content of the acquired scenes. So the proposed approach is suitable in a wide range of practical cases. Reliability and general-purpose applicability is achieved by analysing low-level features (like the spatial-temporal behaviour of coloured stains), and by measuring some warping and motion parameters. In this way it is not necessary to extract accurate target silhouettes, that is a critical task because of occlusions and overcrowding that are typical during interpersonal contacts. A suitable index called maximum warping energy (MWE) has been defined to describe the localized spatial-temporal complexity of colour conformations. Our experiments show that aggressive activities give significantly higher MWE values if compared with safe actions like: walking, running, embracing or handshaking. So it is possible to distinguish violent acts from normal behaviours even in presence of many people and crowded environments. Homography is used to improve robustness by verifying the real targets nearness. False interactions because of perspective-induced occlusions are discarded."
2275342,21239,11529,An adaptive Markov random field based error concealment method for video communication in an error prone environment,1999,"Loss of coded data during its transmission can affect a decoded video sequence to a large extent, making concealment of errors caused by data loss a serious issue. Previous work in spatial error concealment exploiting MRF models used a single pixel wide region around the erroneous area to achieve a reconstruction based on an optimality measure. This practically restricts the amount of available information that is used in a concealment procedure to a small region around the missing area. Incorporating more pixels usually means a higher order model and this is expensive as the complexity grows exponentially with the order of the MRF model. Using previously proposed approaches, the damaged area is reconstructed fairly well in very low frequency portions of the image. However, the reconstruction process yields blurry results with a significant loss of details in high frequency, or edge portions of the image. In our proposed approach, a MRF is used as the image a priori model. More available information is incorporated in the reconstruction procedure not by increasing the order of the model but instead by adaptively adjusting the model parameters. Adaptation is done based on the image characteristics determined in a large region around the damaged area. Thus, the reconstruction procedure can make use of information embedded in not only immediate neighborhood pixels but also in a wider neighborhood without a dramatic increase in computational complexity. The proposed method outperforms the previous methods in the reconstruction of missing edges."
648198,21239,9078,Real-time reconstruction of wavelet encoded meshes for view-dependent transmission and visualization,2002,"Wavelet methods for geometry encoding is a recently emerged superset of multiresolution analysis which has proven to be very efficient in term of compression and adaptive transmission of 3D content. The decorrelating power and space/scale localization of wavelets enable efficient compression of arbitrary meshes as well as progressive and local reconstruction. Recent techniques based on zerotree compression have shown to be among the best lossy mesh compression methods, while remaining compatible with selective transmission of geometric data at various level of detail. While some progressive reconstruction schemes have been proposed in the past, we show in this paper that this representation, recently proposed in the MPEG4 standard, can be efficiently used to perform real-time, view-dependent reconstruction of large meshes. The proposed system combines algorithms for local updates, cache management and server/client dialog. The local details management is an improvement of progressive reconstructions built on top of hierarchical structures. It enables fast, homogeneous accommodation and suppression of wavelet coefficients at any level of subdivision, with time complexity independent of the size of the reconstructed mesh. The cache structure wisely exploits the hierarchical character of the received data, in order to avoid redundant information transmission. The whole system enables the client to have total control on the quality of navigation according to its storage and processing capabilities, whatever the size of the mesh."
603951,21239,9078,Applied partial differential variational techniques,1997,"We review and present examples for two new image processing methods based on scale space evolution under partial differential operators. These operators arise from minimizing variational objective functions and from a novel approach that respects the discrete nature of real images. The first method considered is a variational approach that provides a closed-form expression for the optimal boundary function. This method gives good results with a 2-norm penalty term for approximation error and a 1-norm smoothness term; in this case there is a strong connection with total variation methods. The second method is a peer group averaging approach (developed by Hewer et al.) that smooths the image without blurring edges. This is accomplished by averaging over window pixels with nearly the same intensity; these pixels are the 'peer group' of the window's central pixel. The direct correspondence between the object characteristics and the parameters of peer group size and window diameter make the parameter selection easier for peer group averaging than for standard variational methods. These methods are used in combination with PGA providing the initial approximation for the variational descent procedure. This is applied to real image examples and compared to existing methods including a new level-set histogram modification method (developed by Caselles et al. (see Hewlett Packard Technical Report HPL97-58, 1997)) that treats connected level-sets of pixels as individual units."
1837049,21239,10228,Scene adaptive multiple coding scheme for robust image transmission,2000,"We propose a combined source and channel coding scheme for image transmission over noisy channels. The key component is extracting and preserving the scene information and incorporating it with unequal error protection to combat the channel errors. After hierarchical wavelet decomposition of the image, wavelet coefficients with a parent-child relationship are grouped into wavelet blocks and classified according to corresponding scenes in the original image. For each class, spatial neighborhood coefficients in the high frequency subbands are constrained so that the spatially isolated coefficients are removed and clustered coefficients are retained at the same time. All the wavelet blocks in the same class are grouped together and coded using the set partitioning in hierarchical trees (SPIHT) algorithm. High source coding efficiency is preserved even though multiple source coded bitstreams are generated since the wavelet tree structure is intact. In order to combat the channel errors, an unequal error protection strategy implemented by RCPC/CRC channel coding is designed based on the bit contribution to both PSNR and human visual sensitivity. Finally, a postprocessing method is developed at the receiving end to restore the degradation due to the residual error after channel decoding. Experimental results show that the proposed scheme is indeed able to provide protection for more important bits and more important visual content under a noisy transmission environment. In particular, the reconstructed images illustrate consistently better visual quality."
1430731,21239,9078,Frame structure optimization for interactive multiview video streaming with bounded network delay,2011,"Interactive multiview video streaming (IMVS) is an application that streams to a client one out of N available video views for observation, but client can periodically request switches to neighboring views as the video is played back uninterrupted in time. Previous IMVS works focused on the design of a frame structure at encoding time, trading off expected transmission rate with storage, without knowing the exact view trajectory a client may select at stream time. None of the existing IMVS schemes, however, explicitly addressed the network delay problem, and so a client will suffer a round trip time (RTT) delay for each requested view-switch. In this paper, we optimize frame structure for a bounded RTT, so that a client can switch to neighboring views as the video is played back without view-switching delay. The key idea is to send additional views likely to be requested by a client within one RTT beyond the current requested view. Each required set of contiguous views (corresponding to a given current requested single view) are pre-encoded using frames of previously transmitted set of views as predictors to lower transmission rate. Using I-, P- and distributed source coding (DSC) frames, we first formulate the structure design problem as a Lagrangian minimization for a desired bandwidth/storage tradeoff. We then develop a low-complexity greedy algorithm to automatically generate a good structure. Experimental results show that for the same storage cost, the transmission rate of the proposed structure can be 42% lower than that of I-frame-only structure, and 8% lower than that of the structure without DSC frames."
1995288,21239,9078,Part-based Bayesian recognition using implicit polynomial invariants,1995,"We present an approach to recognition that is based on partitioning and invariant recognition in a Bayesian framework. The intended application domain is that of complex articulated objects in arbitrary position and under considerable occlusion. First, since the performance of traditional model-based recognition strategies degrades with increasing object data-base size, with partial occlusion, and with articulation, we employ a partitioning that does not rely on apriori primitives or models. Rather, this scheme decomposes segmented shapes into parts, where the form of each part is not known apriori, but is derived based on generic geometric assumptions about objects and their projections. Specifically, two types of parts, neck-based and limb-based, give rise to a shape decomposition that remains invariant under occlusion in the visible portion of the object, unaltered under articulation of parts, is stable under slight changes in viewing geometry and finally is robust with changes in resolution and scale. Second, the parts derived from the first stage are described by implicit polynomial curves. These polynomials represent the parts well and are computationally simple to fit to the data. However, the great advantage in using implicit polynomials is the algebraic invariance associated with them. Each part is represented by a vector of invariants that remains essentially independent of viewing geometry, and as such is suitable for matching purposes. The matching process is a Bayesian engine based on asymptotic distributions. In the conclusion section, we briefly indicate how this technology fits into a complete object recognition system."
2892090,21239,9078,Redundant frame structure using M-frame for interactive light field streaming,2016,"A light field (LF) is a 2D array of closely spaced viewpoint images of a static 3D scene. In an interactive LF streaming (ILFS) scenario, a user successively requests desired neighboring viewpoints for observation, and in response the server must transmit pre-encoded data for correct decoding of the requested viewpoint images. Designing frame structures for ILFS is challenging, since at encoding time it is not known what navigation path a user will take, making differential coding very difficult to employ. In this paper, leveraging on a recent work on the merge operator — a new distributed source coding technique that efficiently merges differences among a set of side information (SI) frames into an identical reconstruction — we design redundant frame structures that facilitate ILFS, trading off expected transmission cost with total storage size. Specifically, we first propose a new view interaction model that captures view navigation tendencies of typical users. Assuming a flexible one-frame buffer at the decoder, we then derive a set of recursive equations that compute the expected transmission cost for a navigation lifetime of T views, given the proposed interaction model and a pre-encoded frame structure. Finally, we propose an algorithm that greedily builds a redundant frame structure, minimizing a weighted sum of expected transmission cost and total storage size. Experimental results show that our proposed algorithm generates frame structures with better transmission / storage tradeoffs than competing schemes."
664348,21239,9099,Visual attention detection in video sequences using spatiotemporal cues,2006,"Human vision system actively seeks interesting regions in images to reduce the search effort in tasks, such as object detection and recognition. Similarly, prominent actions in video sequences are more likely to attract our first sight than their surrounding neighbors. In this paper, we propose a spatiotemporal video attention detection technique for detecting the attended regions that correspond to both interesting objects and actions in video sequences. Both spatial and temporal saliency maps are constructed and further fused in a dynamic fashion to produce the overall spatiotemporal attention model. In the temporal attention model, motion contrast is computed based on the planar motions (homography) between images, which is estimated by applying RANSAC on point correspondences in the scene. To compensate the non-uniformity of spatial distribution of interest-points, spanning areas of motion segments are incorporated in the motion contrast computation. In the spatial attention model, a fast method for computing pixel-level saliency maps has been developed using color histograms of images. A hierarchical spatial attention representation is established to reveal the interesting points in images as well as the interesting regions. Finally, a dynamic fusion technique is applied to combine both the temporal and spatial saliency maps, where temporal attention is dominant over the spatial model when large motion contrast exists, and vice versa. The proposed spatiotemporal attention framework has been applied on over 20 testing video sequences, and attended regions are detected to highlight interesting objects and motions present in the sequences with very high user satisfaction rate."
2445094,21239,9078,Design and optimization of a differentially coded variable block size motion compensation system,1996,"While motion fields estimated by maximizing the temporal prediction quality tend to be noisy and demand a large number of bits to encode, finding the motion vectors (MVs) that allow efficient representations requires explicitly considering rate and distortion simultaneously. However, the application of rate-distortion optimization to current variable block size motion compensation (MC) systems is hampered by the fact that MV dependency introduced by the differential coding stage, e.g., the medium differential coding in the advanced prediction mode of the ITU-T standard H.263, makes the rate-distortion (R-D) optimization process extremely difficult. We propose two 1-D differential MV coding frameworks for variable block size MC systems. We show that for one of the proposed differential coding structures the optimal block sizes and the MVs can be jointly obtained by applying dynamic programming (DP) and tree-pruning techniques hierarchically without enumerating all combinations, while a near-optimal solution can be obtained for the other one by adopting a similar optimization procedure with little modification. By comparing the performance of our R-D encoding scheme with that of the H.263 test model TMN5, we find that our approaches achieve 30-60% bit-rate reductions in coding motion vectors, which results in greater than 1 dB gains within a MC hybrid coding environment for most head-and-shoulder videophone sequences under a low-bit-rate constraint."
2125651,21239,9099,Ligne-claire video encoding for power constrained mobile environments,2007,"Digital video playback on mobile devices is fast becoming widespread and popular. Since mobile devices are typically resource constrained in terms of network bandwidth, battery power and available screen resolution, it is often necessary to formulate special encoding techniques in order to optimize power consumption during video streaming and playback. The existing H.264 standard is popular for video encoding on mobile devices, since it results in a low-bitrate video with visual clarity that is adequate for video playback on mobile devices. However, due to the complexity of the H.264 representation, the video decoding procedure is typically computationally intensive. In this paper, we propose a novel lossy video representation termed as Ligne-Claire (LC) video. LC videos are obtained via graphics overlay of outlines or silhouettes of objects in the video over an approximated texture video. Since the playback of LC video is typically meant for mobile devices, the visual quality of video is adequate for most mobile applications wherein the semantic content of the video can be characterized by object shapes and approximate texture information. Experimental results presented in the paper demonstrate that the proposed lossy LC video encoding scheme results in power savings of 50% or more during video playback compared to standard H.264-encoded videos, of similar video file size. In order to evaluate the visual quality of the LC video, we compare the performance of LC videos with H.264-encoded videos in the context of some typical computer vision tasks. Our results indicate that the performance of the computer vision algorithms on these videos is similar. This fact, coupled with subjective evaluation, and the resulting significant power savings, indicates that the proposed LC representation can be used effectively to encode video for power-constrained mobile devices."
1962058,21239,21106,Feature Preserving Image Smoothing Using a Continuous Mixture of Tensors,2007,"Many computer vision and image processing tasks require the preservation of local discontinuities, terminations and bifurcations. Denoising with feature preservation is a challenging task and in this paper, we present a novel technique for preserving complex oriented structures such as junctions and corners present in images. This is achieved in a two stage process namely. All image data are pre- processed to extract local orientation information using a steerable Gabor filter bank. The orientation distribution at each lattice point is then represented by a continuous mixture of Gaussians. The continuous mixture representation can be cast as the Laplace transform of the mixing density over the space of positive definite (covariance) matrices. This mixing density is assumed to be a parameterized distribution, namely, a mixture of Wisharts whose Laplace transform is evaluated in a closed form expression called the Rigaut type function, a scalar-valued function of the parameters of the Wishart distribution. Computation of the weights in the mixture Wisharts is formulated as a sparse deconvolution problem. The feature preserving denoising is then achieved via iterative convolution of the given image data with the Rigaut type function. We present experimental results on noisy data, real 2D images and 3D MRI data acquired from plant roots depicting bifurcating roots. Superior performance of our technique is depicted via comparison to the state-of-the-art anisotropic diffusion filter."
72095,21239,21106,Vitality assessment of boar sperm using an adaptive LBP based on oriented deviation,2012,"A new method to describe sperm vitality using a hybrid combination of local and global texture descriptors is proposed in this paper. In this regard, a new adaptive local binary pattern (ALBP) descriptor is presented in order to carry out the local description. It is built by adding oriented standard deviation information to an ALBP descriptor in order to achieve a more complete representation of the images and hence it has been called ALBPS. Regarding semen vitality assessment, ALBPS outperformed previous literature works with an 81.88% of accuracy and it also yielded higher hit rates than the LBP and ALBP base-line methods. Concerning the global description of sperm heads, several classical texture algorithms were tested and a descriptor based on Wavelet transform and Haralick feature extraction (WCF13) obtained the best results. Both local and global descriptors were combined and the classification was carried out with a Support Vector Machine. Therefore, our proposal is novel in three ways. First, a new local feature extraction method ALBPS is introduced. Second, a hybrid method combining the proposed local ALBPS and a global descriptor is presented outperforming our first approach and all other methods evaluated for this problem. Third, vitality classification accuracy is greatly improved with the two former texture descriptors presented. F-Score and accuracy values were computed in order to measure the performance. The best overall result was yielded by combining ALBPS with WCF13 reaching a F-Score equals to 0.886 and an accuracy of 85.63%."
2412122,21239,9078,A para-pseudo inverse based method for reconstruction of filter bank frame-expanded signals from erasures,2004,"Packet losses due to congestion or buffer overflows is a common problem in packet switched networks. Current network protocols manage this problem by retransmitting the lost packets. However, the delay due to the retransmission of the lost packets may be unacceptable for many real-time applications. Recent focus to resolve this problem is to recover the lost data from the received packets using some error control coding scheme. In this context, signal representation using frames has gained attention and has been studied in J. Kovacevic et al. (2002), P.L. Dragotti et al. (2001), G. Rath and C. Guillemot (2004), and R. Motwani and C. Guillemot (2004). Oversampled transforms and oversampled filter banks have been considered as joint-source channel codes and methods for reconstructing from erasures is studied in these articles. However, for oversampled filter banks, the reconstruction methods based on operating the pseudo-inverse based on the entire signal length are computationally complex and those based on reconstructing the erasures are not optimal as far as reconstruction mean square error is concerned. In this paper, we propose a method for reconstruction from erasures using a synthesis filter bank which functions as a pseudo-inverse. Hence, the scheme minimizes the reconstruction mean square error. Further, the method is computationally efficient, because it does not operate the pseudo-inverse corresponding to the entire signal vector. The synthesis filter bank, which obviously depends on the erasure pattern implements the pseudo-inverse at a practical computational cost. Some typical bursty erasure patterns which permit existence of a FIR synthesis filter banks are studied. The theoretical results are validated for bursty erasure patterns by simulations using image data."
2284137,21239,9078,Fast one-pass motion compensated frame interpolation in high-definition video processing,2009,"In this paper, a fast one-pass processing method with an efficient architecture is proposed for motion compensated frame interpolation (MCFI) in high-definition (HD) videos. Unlike previous works involving high complexity, complicated time-consuming iterations, and less practicability, the proposed method adopts one-pass and low-complexity concept. The proposed method operates a modified fast full-search algorithm, multi-level successive eliminate algorithm (MSEA), in a raster scan order as in the usual block-based processing order in popular codecs, such as H.264/AVC and VC-1. The proposed method analyzes and classifies temporal information to compensate insufficient spatial information based on preprocessed neighboring blocks. According to the analyzed temporal information, our method explores true motion candidates and refines the accuracy of true motions for sub-blocks. When searching motion candidates, the proposed method introduces an adaptive overlapped block matching algorithm called a multi-directional enlarged matching algorithm (MDEMA), and considers different overlapped types based on directions of current sought motion vector in order to enhance the searching accuracy and visual quality. Experimental results show that the proposed algorithm provides better video quality than conventional methods and shows satisfying performance."
604677,21239,9078,Film transfer for HDTV,1998,"Summary form only given. Motion picture film will continue to be a viable capture and release media for theatrical story telling and television programming. High quality images of long term economic value can be preserved on film for decades. With the evolution of video into higher sampling structures referred to as high definition television, the Eastman Kodak Company invested research and development resources into the improvement of the techniques and hardware for transfer of motion picture film record into digital electronic video and image file representations. This effort created value adding engineering strategies in the fields of integrated circuits, optics and illumination systems as well as applying a systems approach that included film through signal processing. The author discusses the architecture and system components of the Kodak designed and manufactured scanner incorporated in the Philips 'Spirit' DataCine. This equipment is used world wide in the post production services environment for film to video transfer as well as image data capture for creative manipulation. A key value adding accomplishment of the development team was the signal to noise performance of the system, as well as a sampling structure consistent with the video applications in the evolving digital distribution standards. The real time motion scanning was another defining and challenging performance boundary."
513129,21239,9078,Data embedding in text for a copier system,1999,"In this paper, we present a scheme for embedding data in copies (color or monochrome) of predominantly text pages that may also contain color images or graphics. Embedding data imperceptibly in documents or images is a key ingredient of watermarking and data hiding schemes. It is comparatively easy to hide a signal in natural images since the human visual system is less sensitive to signals embedded in noisy image regions containing high spatial frequencies. In other instances, e.g. simple graphics or monochrome text documents, additional constraints need to be satisfied to embed signals imperceptibly. Data may be embedded imperceptibly in printed text by altering some measurable property of a font such as position of a character or font size. This scheme however, is not very useful for embedding data in copies of text pages, as that would require accurate text segmentation and possibly optical character recognition, both of which would deteriorate the error rate performance of the data-embedding system considerably. Similarly, other schemes that alter pixels on text boundaries have poor performance due to boundary-detection uncertainties introduced by scanner noise, sampling and blurring. The scheme presented in this paper ameliorates the above problems by using a text-region based embedding approach. Since the bulk of documents reproduced today contain black on white text, this data-embedding scheme can form a print-level layer in applications such as copy tracking and annotation."
1310677,21239,9078,Compressive Distance Classifier Correlation Filter,2013,"Compressed Sensing (CS) is seen as the pathway to increase the efficiency of sensor systems such as MRI, SAR and SAS while avoiding the huge costs and related processing accompanying high-resolution data acquisition. While there has been a surge in the number of sensor systems and related algorithms using CS, target/object recognition in the sensing domain which offers numerous advantages, is a rather nascent field. The state-of-the-art in this field includes the Smashed Filter (SF), which is a reduced dimensionality maximum likelihood classifier. Nevertheless, the accuracy of the filter remains low for practical applications, especially with variations in scale, translation and rotation in the test data. This paper offers a new type of filter - called the Compressive Distance Classifier Correlation Filter (CDCCF), which applies a transformation in the CS domain thereby increasing the distance between intra-class correlation peaks while reducing the distance between inter-class correlation peaks and is based on the Restricted Isometry Property (RIP) of the compressed manifold and the Johnson Lindenstrauss Lemma. Results presented show that the accuracy of the CDCCF filter is about 70% on a 12 class test data set, which is over a two-fold increase in accuracy over the SF. Confusion matrices, measures of ROC, Mean Average Precision and Accuracy demonstrate the robust performance of the algorithm over SF across different compressive sampling resolutions."
2525665,21239,9078,Over-complete representation and fusion for semantic concept detection,2004,"Automatic semantic concept detection in images is a promising tool for alleviating the user effort in annotating and cataloging digital media collections. It enables automatic identification of people, places and objects, for enhanced indexing and searching of home photographs, for example. While constructing robust semantic detectors has been shown feasible for global generic concepts with a sufficient number of good training examples (e.g., indoors, outdoors), many interesting concepts, such as face, people, occur at subpicture granularity, occupy only a portion of the image and therefore frequently have training examples with a reduced signal-to-noise ratio. Such regional concepts are harder to detect due to imperfections in automatic image segmentation algorithms leading to inaccurate object boundaries and low-level feature ambiguities. In this paper we focus on the problem of boosting detection performance of existing regional concept detectors by exploiting detection redundancy. Specifically, we propose to use the same detector multiple times to evaluate and combine multiple detection hypotheses for the same content-but at different content granularities-in order to reduce detection sensitivity to segmentation errors. We validate the approach using support vector machine classifiers for 14 regional semantic concepts from the NISTTRFCVID 2003 common annotation lexicon and show performance improvements of multigranular detection and fusion."
374328,21239,9078,"Nonlinear, noniterative Bayesian tomographic image reconstruction",1999,"In this research, rather than developing a forward model to be inverted, we propose directly modeling the inverse operator. The goal is to develop a non-iterative Bayesian reconstruction method which requires computation comparable to conventional FBP methods, but achieves quality competitive with that of iterative Bayesian methods such as maximum a posteriori probability (MAP). The method we propose, which we call nonlinear back projection (NBP), forms a back projected image cross-section by applying nonlinear filters to the projected data. This method attempts to directly model a type of optimal inverse operator through off-line training of the non-linear filters using example training data of known image cross sections and noisy realizations of projections. The Radon domain filtering is two-dimensional, exploiting redundancy among adjacent angles' measurements. This direct approach to modeling the inverse operator has several potential advantages which make it interesting. First, the elimination of iterative estimation should save computation time relative to common Bayesian techniques. Secondly, some of the inherently nonlinear attributes of the forward process may be implicitly incorporated into the training of the nonlinear backprojection. Finally, training based on sample images and projections may more effectively incorporate greater complexity in the statistical behavior of images than the simple Markov random field models found in most Bayesian formulations."
438282,21239,9078,An adaptive multigrid algorithm for region of interest diffuse optical tomography,2003,"Due to diffuse nature of light photons, diffuse optical tomography (DOT) image reconstruction is a challenging 3D problem with a relatively large number of unknowns and limited measurements. As a result, the computational complexity of the existing DOT image reconstruction algorithms remains prohibitive. In this work, we investigate an adaptive multigrid approach to improve the computational efficiency and the quantitative accuracy of DOT image reconstruction. The key idea is based on locally refined grid structure for region of interest (ROI). The ROI may be defined as diagnostically significant regions, strong background heterogeneities and/or deep optical edges, A 2-level mesh is generated to provide high resolution for ROI and sufficiently high resolution for the rest of the image. A least squares (LS) solution is formulated for the inverse problem. Fast adaptive composite (FAC) 2-grid algorithm is employed to solve the inverse problem. Conjugate gradient (CG) is used at the relaxation stage of FAC 2-grid. Same problem is also solved using direct CG and standard 2-grid method for globally fine grid structure. Our numerical studies demonstrate that the proposed FAC based adaptive 2-grid approach provides up to 90% reduction in computational requirements as compared to the direct iterative and standard 2-grid methods while providing better image quality. The fundamental ideas introduced in this study are directly applicable to other linear and nonlinear inverse problems with Newton type global linearization."
1493877,21239,9078,Arithmetic edge coding for arbitrarily shaped sub-block motion prediction in depth video compression,2012,"Depth map compression is important for compact representation of 3D visual data in “texture-plus-depth” format, where texture and depth maps of multiple closely spaced viewpoints are encoded and transmitted. A decoder can then freely synthesize any chosen inter-mediate view via depth-image-based rendering (DIBR) using neighboring coded texture and depth maps as anchors. In this work, we leverage on the observation that “pixels of similar depth have similar motion” to efficiently encode depth video. Specifically, we divide a depth block containing two zones of distinct values (e.g., foreground and background) into two sub-blocks along the dividing edge before performing separate motion prediction. While doing such arbitrarily shaped sub-block motion prediction can lead to very small prediction residuals (resulting in few bits required to code them), it incurs an overhead to losslessly encode dividing edges for sub-block identification. To minimize this overhead, we first devise an edge prediction scheme based on linear regression to predict the next edge direction in a contiguous contour. From the predicted edge direction, we assign probabilities to each possible edge direction using the von Mises distribution, which are subsequently inputted to a conditional arithmetic codec for entropy coding. Experimental results show an average overall bitrate reduction of up to 30% over classical H.264 implementation."
1089907,21239,9078,Using distributed source coding and depth image based rendering to improve interactive multiview video access,2011,"Multiple-views video is commonly believed to be the next significant achievement in video communications, since it enables new exciting interactive services such as free viewpoint television and immersive teleconferencing. However the interactivity requirement (i.e. allowing the user to change the viewpoint during video streaming) involves a trade-off between storage and bandwidth costs. Several solutions have been proposed in the literature, using redundant predictive frames, Wyner-Ziv frames, or a combination of them. In this paper, we adopt distributed video coding for interactive multiview video plus depth (MVD), taking advantage of depth image based rendering (DIBR) and depth-aided inpainting to fill the occlusion areas. To the authors' best knowledge, very few works in interactive MVD consider the problem of continuity of the playback during the switching among streams. Therefore we survey the existing solutions, we propose a set of techniques for MVD coding and we compare them. As main results, we observe that DIBR can help in rate reduction (up to 13.36% for the texture video and up to 8.67% for the depth map, wrt the case where DIBR is not used), and we also note that the optimal strategy to combine DIBR and distributed video coding depends on the position of the switching time into the group of pictures. Choosing the best technique on a frame-to-frame basis can further reduce the rate from 1% to 6%."
2370302,21239,9078,A group theoretical toolbox for color image operators,2005,"In this paper we describe how to use the direct product of the dihedral group D(4) and the symmetric group 5(3) to automatically derive low-level image processing filter systems for RGB images. For important classes of stochastic processes it can be shown that the resulting operators lead to a block-diagonalization of the correlation matrix. We show that the group theoretical derivation of the operators leads to a very fast implementation consisting mainly of additions and subtractions. They can therefore be implemented in fast graphics computation hardware such as a GPU. We then illustrate the block-diagonalization property in an experiment where we used 20,000 subimage patches collected from 10,000 random images in a large image database. The very short execution times make these operators suitable for applications where many images have to be processed. Typical examples are video processing and content-based image database search. We describe one example where we use the operators to compute content based descriptors of images. These descriptors are currently used in one search mode in an image database browser operating on a database with more than 750,000 images. The group theoretical tools used to derive these filters are very general and can directly be applied for other types of image data. Examples are the following generalizations of the methodology: filter systems for multiband images with more than the ordinary three RGB-channels or images with other grid geometries such as hexagonal sampling."
2390666,21239,9078,A new class of sampling theorems for Fourier imaging of multiple regions,1998,"Traditional Fourier imaging utilizes the Whittaker-Kotel'nikov-Shannon (WKS) sampling theorem. This specifies the spatial frequency components which need to be measured in order to reconstruct an image completely contained within a known field of view (FOV). Here, the authors generalize this result in order to find the optimal k-space sampling for images that vanish except in multiple, possibly non-adjacent regions within the FOV. This provides the basis for multiple region Fourier imaging, a method of producing such images from a fraction of the k-space samples required by the WKS theorem. Sampling is optimal in the sense that it is minimal and does not lead to noise amplification during image reconstruction, just as for WKS sampling. Image reconstruction is computationally cheap because it is performed with small fast Fourier transforms. The new technique can also be used to reconstruct images that have low spatial frequency components throughout the entire FOV and high spatial frequencies (i.e. edges) confined to multiple small regions. The method's greater sampling efficiency can be parlayed into increased temporal or spatial resolution whenever the imaged objects have signal or edge intensity confined to multiple small portions of the FOV. The method may be applicable to various types of magnetic resonance (MR) imaging, as well as to other Fourier imaging modalities and multi-band telecommunications. The technique is demonstrated by using it to reconstruct MR angiographic images of the carotid arteries of a volunteer."
2230255,21239,8960,A Nonparametric Approach to Bottom-Up Visual Saliency,2007,"This paper addresses the bottom-up influence of local image information on human eye movements. Most existing computational models use a set of biologically plausible linear filters, e.g., Gabor or Difference-of-Gaussians filters as a front-end, the outputs of which are nonlinearly combined into a real number that indicates visual saliency. Unfortunately, this requires many design parameters such as the number, type, and size of the front-end filters, as well as the choice of nonlinearities, weighting and normalization schemes etc., for which biological plausibility cannot always be justified. As a result, these parameters have to be chosen in a more or less ad hoc way. Here, we propose to learn a visual saliency model directly from human eye movement data. The model is rather simplistic and essentially parameter-free, and therefore contrasts recent developments in the field that usually aim at higher prediction rates at the cost of additional parameters and increasing model complexity. Experimental results show that—despite the lack of any biological prior knowledge—our model performs comparably to existing approaches, and in fact learns image features that resemble findings from several previous studies. In particular, its maximally excitatory stimuli have center-surround structure, similar to receptive fields in the early human visual system."
1501541,21239,9078,"Multimodal semi-supervised image classification by combining tag refinement, graph-based learning and support vector regression",2013,"We investigate an image classification task where the training images come along with tags, but only a subset being labeled, and the goal is to predict the class label of test images without tags. This task is crucial for image search engine on photo sharing Web sites. In previous work, it is handled by first learning a multiple kernel learning classifier using both image content and tags to score unlabeled training images, and then building up a least-squares regression (LSR) model on visual features to predict the label of test images. However, there exist three important issues in the task: (1) Image tags on photo sharing Web sites tend to be inaccurate and incomplete, and thus refining them is beneficial; (2) Supervised learning with a limited number of labeled samples may be unreliable to some extent, while a graph-based semi-supervised approach can be adopted by also considering similarities of unlabeled data; (3) LSR is established upon centered visual kernel columns and breaks the symmetry of kernel matrix, whereas support vector regression can readily use the original visual kernel and thus leverage its full power. To handle the task more effectively, we propose to combine tag refinement, graph-based learning and support vector regression together. Experimental results on the PASCAL VOC'07 and MIR Flickr datasets show the superior performance of the proposed approach."
1821907,21239,9078,Fast automatic X-ray image processing by means of a new multistage filter for background modelling,1994,"The automatic evaluation of radioscopic images includes the detection of deviations from the regular structure of the object under inspection. For this task two principle problems have to be taken into account: a way for powerful image segmentation must be found which is relatively insensitive to noise because of the nature of X-ray images and secondly the algorithm has to work very quickly in order to satisfy industrial standards. Both problems touch each other because the design of the segmentation algorithm has to be a compromise between efficiency and reliability on the one side and processing time on the other side. The application of image processing methods in the field of nondestructive testing like radioscopic inspection requires the consideration of some special conditions. While visual optical images usually contain sharp edges, less noise and relative high percentage of homogeneous greyvalue areas, X-ray images are highly contaminated by noise, have very smooth edges because of scattered radiation and have only small homogeneous areas and many corner regions because of superposition; the latter condition strongly depends on complexity of the inspected object. This means that deviations or defects like flaws, pores or holes, which can be viewed as kind of impulsive noise, have to be detected both in edge and homogeneous noisy regions. In this paper we present the subsequent application of background modelling and pixel classification for the realization of this inspection task. >"
1424324,21239,9078,Knowledge-Based Image Processing for Classification and Recognition in Surveillance Applications,2006,"This short paper serves as preface for the ICIP special session on knowledge-based image processing for classification in surveillance applications. This special session presents work on integrative research aimed at automatic classification and semantic-based recognition of scenes and events for surveillance applications. The focus is integrative research on low-level multimedia analysis, knowledge extraction and semantic analysis. The eight papers selected for the special session target convergence of these fields by integrating, for a purpose, what can be disparate disciplines. The integration covers image processing, knowledge representation, information retrieval and semantic analysis. The targeted application scenario involves the processing of input data captured by video-cameras where security is of interest. The work presented in this special session has originated in two large international cooperative projects funded by the European commission under the sixth framework programme of the Information Society Technology (1ST). The mandate and scope of these two projects is very generic embracing several applications that rely on technology for bridging the gap between low-level content descriptions that can be computed automatically by a machine and the richness and subjectivity of semantics in high-level human interpretations of audiovisual media (i.e. the semantic gap). However, the section presented in these conference proceedings is restricted to research work undertaken under more limited scenarios for specific video-based surveillance."
2057605,21239,11470,A vector-based approach to digital image scaling,2010,"Image and video capture devices are often designed with different capabilities and end-users in mind. As a result, very often images are captured at one resolution and need to be displayed at another resolution. Resizing thus continues to be an important research area in today's milieu. Well-known pixel-based methods (like bicubic interpolation) have traditionally been used for the purpose. However, these methods are often found lacking in quality from a perceptual standpoint as they tend to soften edges and blur details. Recently, vectorization based approaches have been explored because of their inherent property of artifact free scaling. However, these methods fail to faithfully reproduce textures and fine details in an image. In this work, we present a novel, layered approach of combining image vectorization with a pixel based interpolation technique to achieve high quality scaling of digital images. The proposed method decomposes an image into two layers: a ‘coarse’ layer capturing the visually important structure of the image, and a ‘fine’ layer containing the details. We vectorize only the coarse layer and render at the desired scale, while using classical interpolation on the fine layer. The final scaled image is composed by blending the independently scaled layers. We compare the performance of the proposed method with several state-of-the-art vectorization and scaling methods, and report comparably better performance."
2605948,21239,9078,Improving BM3D on non-stationary Gaussian models for real image noise,2015,"Most of the work related to image denoising is based on artificial noise of stationary Gaussian distribution (synthetically added to the image in arrears). However, this choice is only a rough approximation of the real noise distribution. That is why, various research works were recently focusing on how to perfectly model the real noise inherent in captured images. In this paper, we model this distribution as non-stationary conditional Gaussian, where the standard deviation is depending on the pixel intensity. Experimental results show that this assumption models the real image noise more accurately (specifically when it comes to its adaptive removal). For that, we developed an extended version of BM3D called NBM3D. It is suitable for adaptive non-stationary noise removal in natural captured images where the original BM3D performance is relatively limited. To accurately verify the denoising performance under real case scenarios, we compute a noise-free ground truth image as the average of a sequence of images captured on a static scene by a non-moving monocular camera. Then, we estimate the conditional Gaussian distribution. The resulting model is used for the execution of NBM3D. The comparison between our method and the state-of-the-art BM3D is done by comparing the denoised images using both approaches against the ground truths. Results on three different cameras and 15 sequences with varying lighting conditions show that the proposed NBM3D provides consistent denoising improvements compared to state-of-the-art in terms of real noise removal."
1373608,21239,9078,Alignment of uncalibrated images for multi-view classification,2011,"Efficient solutions for the classification of multi-view images can be built on graph-based algorithms when little information is known about the scene or cameras. Such methods typically require a pair-wise similarity measure between images, where a common choice is the Euclidean distance. However, the accuracy of the Euclidean distance as a similarity measure is restricted to cases where images are captured from nearby viewpoints. In settings with large transformations and viewpoint changes, alignment of images is necessary prior to distance computation. We propose a method for the registration of uncalibrated images that capture the same 3D scene or object. We model the depth map of the scene as an algebraic surface, which yields a warp model in the form of a rational function between image pairs. The warp model is computed by minimizing the registration error, where the registered image is a weighted combination of two images generated with two different warp functions estimated from feature matches and image intensity functions in order to provide robust registration. We demonstrate the flexibility of our alignment method by experimentation on several wide-baseline image pairs with arbitrary scene geometries and texture levels. Moreover, the results on multi-view image classification suggest that the proposed alignment method can be effectively used in graph-based classification algorithms for the computation of pairwise distances where it achieves significant improvements over distance computation without prior alignment."
939593,21239,9078,Compressed Sensing Image Reconstruction Via Recursive Spatially Adaptive Filtering,2007,"We introduce a new approach to image reconstruction from highly incomplete data. The available data are assumed to be a small collection of spectral coefficients of an arbitrary linear transform. This reconstruction problem is the subject of intensive study in the recent field of compressed sensing (also known as compressive sampling). Our approach is based on a quite specific recursive filtering procedure. At every iteration the algorithm is excited by injection of random noise in the unobserved portion of the spectrum and a spatially adaptive image denoising filter, working in the image domain, is exploited to attenuate the noise and reveal new features and details out of the incomplete and degraded observations. This recursive algorithm can be interpreted as a special type of the Robbins-Monro stochastic approximation procedure with regularization enabled by a spatially adaptive filter. Overall, we replace the conventional parametric modeling used in CS by a nonparametric one. We illustrate the effectiveness of the proposed approach for two important inverse problems from computerized tomography: Radon inversion from sparse projections and limited-angle tomography. In particular we show that the algorithm allows to achieve exact reconstruction of synthetic phantom data even from a very small number projections. The accuracy of our reconstruction is in line with the best results in the compressed sensing field."
1637339,21239,9748,A Power-Aware Study of Iris Matching Algorithms on Intel's SCC,2013,"Biometric applications become paramount across private sectors, industry, as well as government agencies. As large amount of data being collected from many different sources, managing such volumes of data and developing efficient and effective large-scale operational solutions are becoming a concern. For example, real-time identification of individuals with the purpose of allowing or denying their access to specific system or resource is challenging from the performance point of view. In addition, processing large amount of data would definitely consume a significant amount of energy. The Single-chip Cloud Computer (SCC) is an experimental processor created by Intel Labs. In this paper we employ SCC, which supports dynamic frequency and voltage scaling (DVFS), to investigate the power-aware computing and performance enhancement of an iris matching algorithm on such many-core architecture. This biometric application contains a large degree of parallelism that we can exploit by porting it onto the SCC. Results in terms of performance, power, energy, energy delay product (EDP), and power per speedup (PPS) metrics of executing the iris matching application under different number of cores, frequency, and voltage settings of the SCC platform are presented. We also analyze how the results for these metrics vary as we change these parameters."
588133,21239,9078,High-throughput hyperspectral imaging with tomographic reconstruction for weak signal sources,2001,"This paper presents a novel technique for wide field hyperspectral imaging using a tomographic approach which yields superior throughput over conventional schemes, an attribute that is highly desirable in spectral imaging of low signal-to-noise ratio sources. This technique is capable of simultaneous multi-object spectroscopy on point sources and imaging spectroscopy of faint diffuse emissions, and consequently has applications in astrophysical and space remote imaging at ultraviolet (UV) and optical wavelengths. Unlike conventional hyperspectral imaging techniques which raster the scene through a slit spectrograph over some time, this method encodes three dimensions of data (two spatial and one spectral) into a two-dimensional signal, using an optical system where photons from the entire scene are collected at all times. Inversion of this data back into the three-dimensional space is accomplished by formulating a tomographic reconstruction problem. Intrinsic presence of noise, however, poses difficulties on conventional inversion methods such as the filtered back-projection (FBP) or the minimum norm least squares methods such that they often produce unacceptable results. This limitation is overcome by the incorporation of regularization schemes, minimizing appropriate l/sub 1/ and l/sub 2/ norm functionals. Through simulations, the performance of this technique is demonstrated quantitatively in comparison with conventional spectral imaging techniques."
2046614,21239,9078,Multi-modal ear and face modeling and recognition,2009,"In this paper we describe a multi-modal ear and face biometric system. The system is comprised of two components: a 3D ear recognition component and a 2D face recognition component. For the 3D ear recognition, a series of frames is extracted from a video clip and the region of interest (i.e., ear) in each frame is independently reconstructed in 3D using Shape From Shading. The resulting 3D models are then registered using the iterative closest point algorithm. We iteratively consider each model in the series as a reference model and calculate the similarity between the reference model and every model in the series using a similarity cost function. Cross validation is performed to assess the relative fidelity of each 3D model. The model that demonstrates the greatest overall similarity is determined to be the most stable 3D model and is subsequently enrolled in the database. For the 2D face recognition, a set of facial landmarks is extracted from frontal facial images using the Active Shape Model. Then, the response of facial images to a series of Gabor filters at the locations of facial landmarks are calculated. The Gabor features (attributes) are stored in the database as the face model for recognition. The similarity between the Gabor features of a probe facial image and the reference models are utilized to determine the best match. The match scores of the ear recognition and face recognition modalities are fused to boost the overall recognition rate of the system. Experiments are conducted using a gallery set of 402 video clips and a probe of 60 video clips (images). As a result, a rank-one identification rate of 100% was achieved using the weighted sum technique for fusion."
2483793,21239,9078,Model-based programming for parallel image processing,1994,"We describe a programming environment which is being developed for the automatic generation of parallel image processing applications. Through the use of model-based software synthesis, we transparently create large grained data parallel applications which can be executed on arbitrary processor networks. The high-level abstractions provided by the modeling paradigm isolates the user from the complexity of the underlying implementation, allowing developers with little or no experience in parallel programming to rapidly create parallel applications. The data parallel modeling facilities perform the same tasks as the data alignment and distribution compiler directives of High Performance Fortran and the aggregate objects of pC++. However, we have found that by introducing the parallelism on the system level, instead of in the algorithm, we can use traditional compilers and leave the application specific code unchanged. This allows us to take advantage of existing well developed image processing code libraries. Here we describe a system which generates data parallel versions of applications created in Khoros, the popular image processing package developed by the University of New Mexico. This system retains the best qualities of Khoros: its interactive and experimental nature, and its visual interface, but adds the capability for automatically generating much higher performance parallel implementations when needed. This system demonstrate the suitability of the model-based approach for developing parallel imaging software. >"
2977483,21239,9078,Block-size adaptive transform domain estimation of end-to-end distortion for error-resilient video coding,2016,"The accuracy of end-to-end distortion (EED) estimation is crucial to achieving effective error resilient video coding. An established solution, the recursive optimal per-pixel estimate (ROPE), does so by tracking the first and second moments of decoder-reconstructed pixels. An alternative estimation approach, the spectral coefficient-wise optimal recursive estimate (SCORE), tracks instead moments of decoder-reconstructed transform coefficients, which enables accounting for transform domain operations. However, the SCORE formulation relies on a fixed transform block size, which is incompatible with recent standards. This paper proposes a non-trivial generalization of the SCORE framework which, in particular, accounts for arbitrary block size combinations involving the current and reference block partitions. This seemingly intractable objective is achieved by a two-step approach: i) Given the fixed block size moments of a reference frame, estimate moments of transform coefficients for the codec-selected current block partition; ii) Convert the current results to transform coefficient moments corresponding to a regular fixed block size grid, to facilitate EED estimation for the next frame. Experimental results first demonstrate the accuracy of the proposed estimate in conjunction with transform domain temporal prediction. Then the estimate is leveraged to optimize the coding mode and yields considerable gains in rate-distortion performance."
1740320,21239,9078,A DSP-based solution to increase the energy efficiency of real-time video encoders,2012,"Implementation of energy-efficient real-time video coders in hardware becomes a new research frontier as the demand for High-Definition(HD) video encoders growing to be a part of popular applications in hand-held smartphones and tablets. In this paper, we tackle an important problem encountered in hardware video encoder design, which wastes hardware cycles (hence loss of battery power) and introduces delays by reducing data throughput. Video conferencing applications in smartphones such as FaceTime™ and Skype™ necessitate energy-efficient real-time video encoders. In these applications, the video encoder should produce independently decodable data units (e.g., H.264/AVC NAL-units, slices) with size smaller than the maximum transmission unit (MTU) size of the network, to prevent fragmentation of packets. In this paper, we propose a method that significantly reduces the requirement to flush hardware pipeline when maximum NAL-unit size is reached by accurate estimation of the bit-rates of macroblocks at very early stages in the encoding pipeline. With this new method, we ensure that our video encoders in communication terminals to produce H.241 compliant maximum NAL-units with minimum waste of hardware cycles. Experiments show that without our method encoder hardware needs more than 16,000 pipeline flushes for one minute of video, which can be reduced by order of magnitudes with the proposed method to hundred or even less flushes."
451316,21239,9078,On adaptive wavelet transform for unified progressive coding,1999,"Various kinds of images, including bi-level documents, continuous-tone images and their compound images, are now present in a wide spectrum of digital image systems. A unified coding scheme is highly desired for such systems that deal with diversified images, and a scalable coding method is necessary for applications that exchange images between devices of different resolution or represent an image in multi-resolution levels, such as quick-look, image archiving and internet transmission. Though wavelet transforms have been proved to be a powerful analysis tool for multi-resolution image representation in scalable lossless coding, their compression performance for bi-level and compound images is not acceptable. However any discrete wavelet transform or two band sub-bandfiltering with finite filters can be decomposed into simple lifting filtering steps so that inverse transform can be easily constructed. In this paper by referencing the already transmitted low-pass band signals in each lifting step, we show how a proper filter is selected in each step to let the wavelet transform adaptive to different kinds of images. Furthermore, in the proposed adaptive wavelet transform, a proper mapping of the coefficients is employed so that bi-level document regions in a compound image can be transmitted prior to multi-level regions. Simulation results also show that the proposed scheme has high compression ratio for both bi-level and multi-level images."
2324875,21239,10192,Robust Video Transmission Over Packet Erasure Wireless Channels Based on Wyner-Ziv Coding of Motion Regions,2008,"This paper presents a new scheme for robust video transmission over packet erasure wireless channels based on Wyner-Ziv coding of motion regions. The multipath fading and shading of the wireless channels usually lead to loss or erroneous video packets which on occasions result in some spontaneous drop in video quality. Existing approaches with forward error correction (FEC) and error concealment have not been able to provide the desired robustness in video transmission. We develop a new scheme with a motion-based Wyner-Ziv coding (MWZC) by leveraging distributed source coding (DSC) ideas for error robustness. This new scheme is based on the fact that motion regions of a given video frame are particularly important in both objective and perceptual video quality and hence should be given preferential Wyner-Ziv coding based embedded protection. To achieve high coding efficiency, we determine the underlining motion regions based on a rate-distortion model. Within the framework of H.264/AVC specification, motion region determination can be efficiently implemented using flexible macroblock ordering (FMO) and data partitioning (DP). The bit stream generated by the proposed scheme consists two parts: the systematic portion generated from conventional H.264/AVC bit stream and the supplementary bit stream for error robust video transmission generated by the Wyner-Ziv coding of motion regions. Experimental results demonstrate that the proposed scheme significantly outperforms both decoder-based error concealment (DBEC) and conventional FEC with DBEC approaches."
1084453,21239,9078,ShellCam: Interactive geometry-aware virtual camera control,2014,"We introduce ShellCam, a geometry-aware virtual camera control model which defines a smooth motion subspace enabling Pan&Zoom navigation on arbitrary 3D objects. The basic idea is to define a scale-dependent offset shell around the visible geometry which provides, at any point, a meaningful tangent direction for panning and helps computing the camera-object distance to rule accurately a logarithmic zoom motion. We define the underlying motion space as a visualization hull and evaluate it on-the-fly using a moving least-squares approach. As a result, ShellCam provides smooth object-aware 3D motions, combining rotations and translations, based on a simple 2D user input such as typically produced by mouse motions. We also provide an efficient GPU implementation which makes use of the standard rasterization pipeline to compute this 3D motion efficiently. Our approach is robust to inconsistent geometry such as point clouds or polygon soups, works on shapes with complex topology, does not require any pre-computation and can be used on dynamic data. ShellCam offers a convenient control for 3D inspection tasks and a transparent swap with other control models for more general 3D navigation. Last, our model is straightforward to integrate in any 3D application."
2443805,21239,9078,Probabilistic DBNN via expectation-maximization with multi-sensor classification applications,1995,"The original learning rule of the decision based neural network (DBNN) is very much decision-boundary driven. When pattern classes are clearly separated, such learning usually provides very fast and yet satisfactory learning performance. Application examples including OCR and (finite) face/object recognition. Different tactics are needed when dealing with overlapping distribution and/or issues on false acceptance/rejection, which arises in applications such as face recognition and verification. For this, a probabilistic DBNN would be more appealing. This paper investigates several training rules augmenting probabilistic DBNN learning, based largely on the expectation maximization (EM) algorithm. The objective is to establish evidence that the probabilistic DBNN offers an effective tool for multi-sensor classification. Two approaches to multi-sensor classification are proposed and the (enhanced) performance studied. The first involves a hierarchical classification, where sensor information are cascaded in sequential processing stages. The second is multi-sensor fusion, where sensor information are laterally combined to yield improved classification. For the experimental studies, a hierarchical DBNN-based face recognition system is described. For a 38-person face database, the hierarchical classification significantly reduces the false acceptance (from 9.35% to 0%) and false rejection (from 7.29% to 2.25%), as compared to non-hierarchical face recognition. Another promising multiple-sensor classifier fusing face and palm biometric features is also proposed."
2013087,21239,9078,A block-based super-resolution for video sequences,2008,"An algorithm for video resolution enhancement is presented. The approach borrows from previous methods for still-image super- resolution, introducing modifications better suited for the characteristics specific to video domain problems. Each high-resolution (HR) frame is determined through a series of MMSE spatial interpolations based on the local features (statistics) of the frame. Cross-frame registration is estimated externally and the reconstruction algorithm does not limit the form of the motion model, unlike previous data-fusion/deconvolution approaches which have required motion models that do not alter the point-spread function (i.e., motion/blur commutability). This feature is made possible using a reverse motion model mapping the locations of desired HR pixels onto their corresponding locations in the observation frames. An ticipating the existence of registration error found in typical video sequences, the algorithm also provides an internal validation of the observation pixels, helping to reduce significant mis-registration artifacts. An arbitrary enhancement factor can be used, allowing an output at any desired resolution. Interpolation and deblurring are incorporated as a single MMSE filtering operation, providing a non-iterative one-step reconstruction process. Experimental results demonstrating the capabilities of the algorithm are made available."
642951,21239,9078,From local to global parameter estimation in panoramic photographic reconstruction,1999,"This paper addresses a key issue in the problem of reconstructing a panoramic view from several pictures taken with a hand-held camera, namely the estimation of some ill-posed parameters using an external constraint. For many practical reasons, a panoramic reconstruction has to be performed in several independent steps, resulting in a set of different measurements of the same reality. For example, the focal length can be estimated with each pair of overlapping images. The idea is to introduce some a priori knowledge about the world by means of a constraint on the parameter set. In the former example, the constraint would impose equality on all the focal length estimates. This paper describes the appropriate correction that needs to be applied to the parameters in order to obtain a coherent result. It also suggests a way to evaluate if a constraint is plausible given a set of initial estimates. The basic idea behind the method is to modify the parameters without significantly changing the overlapping part of the images. The method is evaluated using two different experimental setups. The first aims at improving the quality of a full panoramic image. The second measures independently the positions in space of two planes using two pictures. The latter experiment shows that the two computed motions can be considered as a single one with two different planes in space."
1389993,21239,9078,Sure-optimal two-dimensional Savitzky-Golay filters for image denoising,2013,"Savitzky-Golay (SG) filters are linear, shift-invariant lowpass filters employed for data smoothing. In their pathbreaking paper published in Analytical Chemistry, Savitzky and Golay mathematically established that polynomial regression of data over local intervals and evaluation of their values at the center of the approximation window is equivalent to convolution with a finite impulse response filter. In this paper, we expound SURE (Stein's unbiased risk estimate) based adaptive SG filters for image denoising. Our goal is to optimally choose SG filter parameters, namely, order and window length, the optimality defined in terms of the mean squared error (MSE). In practical scenarios, only a single realization of the noisy image is available and the ground truth is inaccessible. Hence, we propose SURE, which is an unbiased estimate of MSE, to solve the parameter selection problem. It is observed that bandwidth of the minimum MSE (MMSE)-optimum SG filter is small at relatively slowly varying portions of the underlying image, and vice versa at abrupt transitions, thereby enabling us to trade off bias and variance to obtain near-optimal performance. The denoising results obtained exhibit considerable peak signal-to-noise-ratio (PSNR) improvement. At low SNRs, the filter performance is further enhanced by using a regularized cost function."
821192,21239,9078,"Source separation in cosmology, from global to local models",2011,"After a series of successful full-sky CMB (Cosmic Microwave Background) experiments (COBE and WMAP to only name two), the latest spatial mission of the European Space Agency, Planck, has started observing the whole sky in mid-2009. This experiment is of premier importance for the cosmologists to study the birth of our universe via the analysis of the CMB. The latter astrophysical component, among others, is not directly observable in the Planck data but rather mixed up with other components. For the sake of scientific exploitation, accessing such precious physical information requires extracting several different astrophysi-cal components (CMB, Sunyaev-Zel'dovich clusters, galactic dust). Mathematically, this problem amounts to a component separation problem. In the field of CMB studies, a very large range of state-of-the art source separation methods have been applied. Most of these methods assume that the sought after components are mixed up in the data according to the standard global linear mixture model. Nevertheless, this model does not hold and more accurate models require modeling the mixtures locally rather than globally. The purpose of this paper is to introduce an extension of GMCA (Generalized Morphological Component Analysis) to handle a local mixture model. Preliminary results on simulated Planck data are presented."
1161432,21239,9078,Optimized adaptive depth map filtering,2013,"Three-Dimensional (3-D) Television (TV) is believed to be the future TV broadcasting technology which then would replace the traditional 2-D TV systems. Future 3-D TV could enhance the visual home entertainment experience in a way that the user can navigate through the scene or switch the viewpoint. Moreover, new technologies such as autostereoscopic multiview displays provide a 3-D perception to the viewer without the need to wear additional glasses. For these technologies, the need arise to generate additional Virtual Views (ViV) of given scenes with different viewpoints. A new view can be synthesized at the receiver side by utilizing Depth Image-based Rendering (DIBR). Although DIBR has many advantages, one of the key challenges is, how to fill uncovered areas caused by disocclusions and incorrect depth values. To handle such disoccluded areas a new depth preprocessing method is proposed, which is applied prior to 3-D warping. Depth discontinuities in the Depth Map (DM) are smoothed to reduce the number and the size of uncovered areas in the ViV. Considering the fact that filtering the whole image introduces strong distortions, we adaptively weight the filter to reduce filter-induced artifacts. The proposed method shows considerable objective and subjective gains compared to the state-of-the-art."
2462158,21239,9099,An object-based video coding framework for video sequences obtained from static cameras,2005,"This paper presents a novel object-based video coding framework for videos obtained from a static camera. As opposed to most existing methods, the proposed method does not require explicit 2D or 3D models of objects and hence is general enough to cater for varying types of objects in the scene. The proposed system detects and tracks objects in the scene and learns the appearance model of each object online using incremental principal component analysis (IPCA). Each object is then coded using the coefficients of the most significant principal components of its learned appearance space. Due to smooth transitions between limited number of poses of an object, usually a limited number of significant principal components contribute to most of the variance in the object's appearance space and therefore only a small number of coefficients are required to code the object. The rigid component of the object's motion is coded in terms of its affine parameters. The framework is applied to compressing videos in surveillance and video phone domains. The proposed method is evaluated on videos containing a variety of scenarios such as multiple objects undergoing occlusion, splitting, merging, entering and exiting, as well as a changing background. Results on standard MPEG-7 videos are also presented. For all the videos, the proposed method displays higher Peak Signal to Noise Ratio (PSNR) compared to MPEG-2 and MPEG-4 methods, and provides comparable or better compression."
777849,21239,9078,Perceptual hashing of color images using hypercomplex representations,2013,"This paper presents a new perceptual image hashing approach that exploits the image color information using hypercomplex (quaternionic) representations. Unlike grayscale-based techniques, the proposed approach preserves the color interaction between the image components that have a significant contribution in the generated perceptual image hash codes. Having a robust image hash function optimizes a wide range of applications including content-based retrieval, image authentication, and image watermarking. Initially, the input color image is processed in a “holistic” manner using the hypercomplex representation where the red, green and blue (RGB) components are handled as a single entity. Then, non-overlapping 8 × 8 image blocks are processed using the Quaternion Fourier transform (QFT). Binary image hash codes are generated by comparing the block mean frequency energy to the global mean frequency energy. For retrieval purposes, the Hamming distance (HD) is used as the comparison metric to retrieve perceptually similar images. The performance of the proposed perceptual hashing for color image is compared to that based on the conventional complex Fourier transform (CFT). Simulation results clearly indicate the superior retrieval performance of the proposed QFT-based perceptual hashing technique in term of HD values of intra-and inter-class image samples. Moreover, the performance improvement of the QFT-based technique is achieved at a computational complexity similar to the CFT-based scheme."
2463435,21239,9078,Bilateral Breast Volume Asymmetry in Screening Mammograms as a Potential Marker of Breast Cancer: Preliminary Experience,2007,"The biological concept of bilateral symmetry as a marker of developmental stability and good health is well established. Although most individuals deviate slightly from perfect symmetry, humans are essentially considered bilaterally symmetrical. Studies have shown that if an individual is exposed to genetic mutations or environmental stresses, the homeostatic mechanisms that maintain symmetry of paired structures (such as breasts) tend to break down. Consequently, increased fluctuating asymmetry of paired structures could be an indicator of poor health. This preliminary study tested if bilateral morphological breast asymmetry in screening mammograms correlates with the presence of breast cancer. Following the biological definition of breast asymmetry in terms of volume, we applied automated computer algorithms for screening mammograms that segment the breast region and then measure each segmented breast's volume. These parameters were measured separately for each breast in each mammographic view (CC and MLO). Then, the normalized absolute differences of these parameters were investigated as measurements of fluctuating asymmetry (FA). Based on 268 cancer cases and 82 normal cases from the DDSM database, we observed that cancer patients demonstrate statistically significantly higher fluctuating asymmetry in their screening mammograms than patients with normal screening studies. Using an artificial neural network to combine FA measurements from both views along with the patient's age and breast parenchymal density resulted in an ROC area of 0.80plusmn0.03. These results suggest that bilateral breast volume asymmetry estimated in screening mammograms should be studied as a risk factor for breast cancer."
2856623,21239,9078,Hardware-friendly universal demosaick using non-iterative map reconstruction,2016,"Non-Bayer color filter array (CFA) sensors have recently drawn attention due to their superior compression of spectral energy, ability to deliver improved signal-to-noise ratio, or ability to provide high dynamic range (HDR) imaging. Demosaicking methods that perform color interpolation of Bayer CFA data have been widely investigated. However, a bottleneck to the adaption of emerging non-Bayer CFA sensors is the unavailability of efficient color-interpolation algorithms that can demosaick the new patterns. Designing a new demosaick algorithm for every proposed CFA pattern is a challenge. In this paper, we propose a hardware-friendly universal demosaick algorithm based on maximum a-posteriori (MAP) estimation that can be configured to demosaick raw images captured using a variety of CFA sensors. The forward process of mosaicking is modeled as a linear operation. We then use quadratic data-fitting and image prior terms in a MAP framework and pre-compute the inverse matrix for recovering the full RGB image from CFA observations for a given pattern. The pre-computed inverse is later used in real-time application to demosaick the given CFA pattern. The inverse matrix is observed to have a Toeplitz-like structure, allowing for hardware-efficient implementation of the algorithm. We use a set of 24 Kodak color images to evaluate the quality of our demosaick algorithm on three different CFA patterns. The PSNR values of the reconstructed full-channel RGB images from CFA samples are reported in the paper."
912797,21239,9078,Adaptive 3D multi-view video streaming over P2P networks,2014,"Streaming 3D multi-view video to multiple clients simul­taneously remains a highly challenging problem due to the high-volume of data involved and the inherent limitations imposed by the delivery networks. Delivery of multimedia streams over Peer-to-Peer (P2P) networks has gained great interest due to its ability to maximise link utilisation, preventing the transport of multiple copies of the same packet for many users. On the other hand, the quality of experience can still be significantly degraded by dynamic variations caused by congestions, unless content-aware precautionary mechanisms and adaptation methods are deployed. In this paper, a novel, adaptive multi-view video streaming over a P2P system is in­troduced which addresses the next generation high resolution multi-view users' experiences with autostereoscopic dis­plays. The solution comprises the extraction of low-overhead supplementary metadata at the media encoding server that is distributed through the network and used by clients performing network adaptation. In the proposed concept, pre-selected views are discarded at a times of network congestion and reconstructed with high quality using the metadata and the neighbouring views. The experimental results show that the robustness of P2P multi-view streaming using the proposed adaptation scheme is significantly increased under congestion."
555740,21239,9078,Signal acquisition and processing for magnetic resonance imaging,1994,"An understanding of data acquisition and processing in magnetic resonance imaging (MRI) facilitates subsequent manipulation and analysis of the resultant images. Unlike other medical imaging modalities, MRI is not well described in terms of the transmission and reception of a propagating wave since the wavelength of the radiofrequency signals involved is on the order of meters. Rather, MRI exploits the fact that the magnetization vectors associated with certain atoms (notably protons in water) will, when placed in a strong magnetic field, oscillate at a frequency proportional to that field. By spatially varying the magnetic field, position is mapped by the frequency and phase of the field effects caused by the oscillating magnetization vectors at each location. The data acquisition associated with this spatial mapping can be described as temporally scanning the Fourier or spatial frequency domain of the volume of interest. Image reconstruction is achieved by an inverse Fourier transform of the received signal. The Fourier representation of the imaging process is developed from an overview of the physics underlying MRI signal behaviour. Under this formalism, trade-offs in acquisition time, resolution, and field-of-view as well as image degradation associated with noise are addressed. Finally, the ability in MRI to manipulate soft tissue contrast over a wide range of independent parameters is outlined. >"
2349511,21239,9078,Robust brain activation detection in functional MRI,2008,"Functional Magnetic Resonance Imaging (MRI) is today one of the most important non-invasive tools to study the brain from a functional point of view. The blood-oxygenation-level-dependent (BOLD) signal is used to detect the activated regions based on the assumption that in these regions the metabolic activity increases. The normal procedure is the application of known sequences of stimulus and find out the brain regions whose activation sequence is correlated with the applied stimulus. This inference problem is difficult because the BOLD signal is very week and noisy. The underlying information is embedded in a large number of other signal related with the normal brain activity and in the noise introduced by the MRI scanner. Furthermore, the hemodynamic impulse response function (HRF), needed to know the expected BOLD response to a given stimulus, is usually unknown and is not constant across the whole brain. In this paper a robust Bayesian algorithm is proposed to detect regions where the activation patterns are correlated with the applied stimulus. The activation process is modeled by using binary explicative variables and the HRF is estimated at each location according to a physiological model proposed by the authors in [1]. Monte Carlo tests using synthetic data are performed to evaluate the performance of the algorithm and results with real data are compared with the ones obtained by a neurologist with the commercial package BrainVoyager."
1648338,21239,9078,Image denoising using dual tree statistical models for complex wavelet transform coefficient magnitudes,2013,"Wavelet shrinkage is a standard technique for denoising natural images. Originally proposed for univariate shrinkage in the Discrete Wavelet Transform (DWT) domain, it has since been optimised through the exploitation of translationally invariant wavelet decompositions such as the Dual-Tree Complex Wavelet Transform (DT-CWT) alongside bivariate analysis techniques that condition the shrinkage on spatially related coefficients across neighbouring scales. These more recent techniques have denoised the real and imaginary components of the DT-CWT coefficients separately. Processing real and imaginary components separately has been found to lead to an increase in the phase noise of the transform which in turn affects denoising performance. On this basis, the work presented in this paper offers improved denoising performance through modelling the bivariate distribution of the coefficient magnitudes. The results were compared to the current state of the art non-local means denoising technique BM3D, showing clear subjective improvements, through the retention of high frequency structural and textural information. The paper also compares objective measures, using both PSNR and the more perceptually valid structural similarity measure (SSIM). Whereas PSNR results were slightly below those for BM3D, those for SSIM showed closer correlation with subjective assessment, indicating improvements over BM3D for most noise levels on the images tested."
2207376,21239,9078,Coding of deinterlaced image sequences,1994,"This paper investigates a coding method for interlaced image sequences which uses deinterlaced images as an intermediate format. The purpose of this method is threefold: firstly, to achieve a higher coding gain, as the processing of progressive pictures raises much less difficulty than the processing of interlaced ones; secondly, to provide an intermediate step toward the development of a fully progressive production and transmission chain; and finally to open the way to efficient solutions for important image processing issues such as frame rate conversion and compatibility. In previous work by the authors (1994), an interlace-to-progressive converter was proposed. It was based on theoretically correct generalized interpolation formulas, working under the assumption of a uniform translational motion. The scheme proposed here makes use of this converter to produce the progressive images from the interlaced source, which are coded and then re-interlaced. Another solution is considered, with the purpose of improving the coding efficiency alone. It is based on an intermediate progressive format at half the original temporal sampling rate. However, in that case, some additional information needs to be transmitted in order to accurately recover the full rate interlaced signal. The two methods mentioned above are described in some detail and tested on actual sequences. The picture quality is assessed, both objectively and subjectively, and conclusions are drawn. >"
435024,21239,9078,A perceptually optimized and error-resilient video codec based on 3-D SPIHT algorithm,2003,"Video transmission over unreliable networks such as Internet or wireless networks suffers from various adverse conditions such as bandwidth fluctuation, of burst-error contamination, packet loss, and packet delay due to network congestion. By taking advantage of multiple logical channels provided by digital networks, a multiple description video coding scheme is proposed to accommodate varying network environments and to meet quality of service requirements. To avoid the error propagation that usually occurred in predictive video codecs as decoding erroneous or incomplete signals, a 3-D SPIHT based coder that generates embedded bit streams is adopted. To let each description bit stream convey the most visual information with the smallest possible amount of coding bits, the wavelet coder is optimized by a color visual model that can evaluate the perceptual significance of each wavelet coefficient within subband of each color component. The error resilience in spatio-temporal domain is achieved by spatially and temporally adjacent data from other descriptions through interpolation prediction. The simulation results show that the visual quality can be maintained in transmitting video sequences at low bit rate (200 Kbps) over the channel of high data loss rate (50%)."
2943805,21239,23757,Automatic tattoo image registration system,2016,"Surveillance systems are very important for law enforcement and military applications. Capturing a biometric modality at a distance and under difficult conditions is a very challenging process. While face or gait can be used to identify an individual in such application, tattoos can also help in the identification process whenever available. Tattoos are considered a soft biometric and in some scenarios may be the only clue that can be used to verify the identity of a suspect or to rule out a suspect. One of the major challenges in tattoo recognition systems is image registration, i.e. the alignment of one tattoo image to a reference image. Accurate registration can greatly improve recognition accuracy. In this paper, we propose a two-level automatic tattoo registration and correction system based on SIFT descriptors and the RANSAC algorithm with a homography model. By using image quality index techniques and a postprocessing step (where we refine our original registration results by an automated correction process where outliers are first identified and then re-processed), our system is able to demonstrate accurate registration results. We tested our registration system using two tattoo image databases. The first one is the NIST-Tatt-C database with 109 subjects collected under uncontrolled condition, and the second one is the new WVU tattoo database (WVU-Tatt) with 79 subjects, which is collected under controlled conditions. Experimental results show that, first, we obtained 100% registration accuracy in both databases. Then, the effect of our registration process on tattoo recognition performance was assessed when using both the NIST-Tatt-C database where the accuracy improved from 54.13% (no registration) to 100% (with registration) and the WVU-Tatt database where the accuracy improved from 86.08% (no registration) to 98.73% (with registration)."
720013,21239,9078,Quasar — A new heterogeneous programming framework for image and video processing algorithms on CPU and GPU,2014,"In image and video processing research, rapid prototyping and testing of different variations of an algorithm is quite essential (e.g., to find out if a given algorithm can solve a given problem or work in real-time). In the past decade, the computational performance of graphical processing units (GPUs) has improved significantly, where speed-up factors of 10×–50× compared to single-threaded CPU execution are not uncommon. However, GPU programming is challenging, requiring a significant programming expertise and moreover, most existing programming approaches are not well suited for rapid prototyping. In this Show & Tell session, we present a new programming framework, aimed at making the bridge between high-level program specification and low-level implementation and optimization on heterogeneous computation devices. The goal is that the programmer is relieved from (most) implementation issues and can focus on the specification and improvement of the algorithms. We present a new prototype domain-specific programming language (in the first place aimed at image and video processing) that provides a uniform programming approach toward different hardware devices, a run-time environment to manage and communicate with the heterogeneous devices and an integrated development environment (IDE). The IDE provides various useful image processing-related debugging and profiling features."
879884,21239,9078,"Sensing, predicting, and utilizing human visual attention",2014,"In this talk, I will describe our recent works on human visual attention from three aspects: sensing, predicting, and utilizing visual attention. Over the last two decades, the concept of visual saliency and its computational models have attracted a lot of interest, inspired by the seminal work by Koch and Ullman on a computational model of visual saliency. Visual saliency models predict our eye fixations driven by our vision system's bottom-up control triggered by visual stimuli, and it has been shown experimentally that a visual saliency map computed by a visual saliency model is highly correlated with an actual distribution of our fixation points. Based on this observation, we introduce a method for estimating gaze directions using visual saliency maps without explicit personal calibration. The key idea is to use the saliency maps of the video frames that a person is looking at as the probability distributions of the gaze points so that we can avoid cumbersome calibration procedures asking a user to fixate calibration targets. I will explain the details of our method and experimental results. I will also talk about our recent attempt to develop a new computational model of visual saliency for better accuracy in predicting gaze fixation points. Unlike the existing visual saliency models, our model elaborates the fact that the characteristics of human eyes vary significantly within the visual field, e.g., fovea and peripheral vision. Experiments using human fixation data in a wide field of view setting demonstrate that our visual saliency model achieves higher accuracy than a current state-of-the-art model. Lastly, we briefly talk about several applications of eye movements for activity recognition and image analysis."
1454128,21239,9078,Extension of High Efficiency Video Coding (HEVC) for multiview video and depth data,2012,"This paper presents an approach for 3D video coding that uses a format in which a small number of views as well as associated depth maps are coded and transmitted. At the receiver side, additional views required for displaying the 3D video on an autostereoscopic display can be generated based on the corresponding decoded signals by using depth image based rendering (DIBR) techniques. In terms of coding technology, the proposed coding scheme represents an extension of High Efficiency Video Coding (HEVC), similar to the Multiview Coding (MVC) extension of H.264/AVC. Besides the well-known disparity-compensated prediction, advanced techniques for inter-view and inter-component prediction, the representation of depth blocks, and the encoder control for depth signals have been developed and integrated. In comparison to simulcasting the different signals using HEVC, the proposed approach provides about 40% and 50% average bit rate savings for a whole test set when configured to comply with a 2- and 3-view scenario, respectively. The proposed codec was submitted as response to a Call for Proposals on 3D Video Technology issued by the ISO/IEC Moving Picture Experts Group (MPEG) and it was ranked as the overall best performing HEVC-based proposal in the related subjective tests."
1788974,21239,10192,Packet Scheduling for Scalable Video Streaming Over Lossy Packet Access Networks,2007,"Video streaming applications have gained in popularity in recent years. The quality of service offered by such applications is limited by the available transmission rates as well as time-varying conditions, such as, channel fading and network congestion, which lead to packet losses. Scalable video coding techniques that allow for the flexible adaptation of temporal resolution as well as quality of an encoded bitstream can be immensely useful in developing video streaming applications that can adapt to time-varying network and channel conditions. Scalable coding techniques, however, are generally designed to offer progressive refinement, which introduces dependencies between encoded video packets. Therefore, when determining a packet scheduling technique for scalable coded video, the possibility of random packet losses, which might affect the decodability of subsequent packets, must be taken into account. In this paper, we take into account the available transmission rate, possibly time-varying channel conditions, and the possibility of random packet losses, to design a scheduling technique for video packets in a scalable bit-stream. Since the optimal solution to the scheduling problem requires an exhaustive, and therefore, intractable computation, we propose a greedy algorithm that will schedule the optimal packet for transmission at a given transmission opportunity based on the encoded content and the available channel state information. Simulation results show significant gains in performance when the proposed technique is compared to content and channel independent packet scheduling techniques."
354813,21239,9078,Still image compression with multiple overlapping local cosine bases,2003,"This paper investigates several multiple overlapping transform based image coders. For better image representation, decorrelation and energy compaction the coders are based on new and powerful classes of multiple overlapping localized trigonometric bases (LCB-x, x>2). The compression results outperform the wavelet coders SPIHT [A. Said et al., June 1996] and JPEG2000 [S. Trautmann] for various test images in both objective and subjective coding performance. In the last years, much of the research activities in image coding have been focused on the discrete wavelet transform (DWT), which avoids blocking artifacts. Several new classes of multiple overlapping localized cosine bases [K. Bittner, 2002, A. Mali et al., 2003] can also eliminate completely blocking artifacts. Additionally unlike the wavelet transform, textures and oscillating patterns in a lot number of images are well preserved using trigonometric bases [F.G. Meyer, June 2002]. This provides significant improvements in reconstructed image quality over the discrete cosine transform and the discrete wavelet transform. First we recall the main results on multiple overlapping trigonometric bases. To solve the problem of infinite dual trigonometric bases, various sequences of new window functions for finite signals are introduced. Then, with aid of the generalized unfolding operator we propose fast algorithms for signal analysis and synthesis. Finally, we apply the transforms for image compression."
2018581,21239,9078,Adaptive wavelet packet image coding using an estimation-quantization framework,1998,"We extend the statistical model-based estimation-quantization (EQ) wavelet image coding algorithm introduced by LoPresto, Ramchandran and Orchard (see Proceedings of the Data Compression Conference, Snowbird, UT, 1997) to include an adaptive transform component. For this, we resort to the rich, space-frequency diverse, and easy-to-search library of transforms provided by the family of wavelet packet (WP) bases and their adaptive extensions. We use rate-distortion criteria to find the best basis jointly with the statistical model-based best adaptive quantization and entropy coding strategy of LoPresto et al. based on an efficient and fast tree pruning algorithm. A key underlying attribute of our paradigm is that the spatially-varying generalized Gaussian mixture model for wavelet coefficients introduced by LoPresto et al. is also applicable to the more arbitrary framework of (adaptive) wavelet packet transform coefficients as well. Our WP-EQ framework produces excellent results on standard test images. The most attractive property of our paradigm is its universality and robustness: based on an overall performance criterion that considers diverse classes of input test images that have varying space-frequency characteristics, it is more powerful than most of the existing image coding algorithms, using reasonable complexity, and a generic, integrated, non-training based framework."
1901928,21239,9078,"Hierarchical mesh-based global motion estimation, including occlusion areas detection",2000,"We present a method which is able to determine the occlusion areas appearing in a mesh-based motion estimation. The use of the motion estimation scheme based on a non-uniform hierarchical triangular mesh and the optimization of the nodal motion vectors by means of a multi-resolution differential method leads to efficient and fast motion estimation as well as motion field refinement. Although global motion estimation avoids the blocking artifacts due to the independence of the neighboring blocks of conventional block matching, it can only find the optimal motion parameters, which derive from the motion vectors at the node points that are shared by several patches of a continuous field. However real motion in a video sequence is discontinuous. Thus, our goal consists of modeling the artifacts, resulting from the occlusions by using a global mesh. Contrary to the segmented mesh associated with the video object when fast motion has to be estimated, surface mesh may contain overlapping triangles and the estimated image is thus deteriorated. In fact, these overlaps allow the detection of the occlusions. Thus, our method consists of taking into account this information to determine the optimal motion vectors in the continuous field and to generate the content of the uncovered area or to exclude the content of the covered area. To illustrate the new performances of the proposed method, results showing the capabilities of our mesh-based motion estimation to determine discontinuities lines are presented."
756637,21239,9078,Study of compressed sensing application to low dose computed tomography data collection,2014,"Computed Tomography (CT) is one of the most used imaging techniques in medical field which enables us to generate two-dimensional as well as three-dimensional image of the inside the body. Fundamentally, any imaging system contains two main stages the data collection and the image reconstruction. For CT, The data collection stage depends on the resolution of image collected and field of view. The image reconstruction of an object depends on projections by passing a series of rays through an object. The problem in general is that CT is computationally very intensive, due to the large number of projections. The large computational requirements have led to large times for CT image reconstruction, and extra X-ray dose to get high quality images. To accelerate the reconstruction process and decrease the effect of x-ray on patients, we need to decrease the number of projections. In this paper we introduce a compressed sensing (CS) technique for CT to reconstruct images from reduced projection data and compare it with other algorithms, A CS iterative algorithm reconstructed an image of digital Shepp-Logan phantom using small number of projections with high quality resolution compared to traditional iterative technique, we studied the effect of algorithm controlling parameters on the reconstructed image. Finally using the proposed technique will decrease the high risk associated with the high dose x-ray needed in the traditional CT scans."
2446538,21239,9078,A source and channel coding approach to data hiding with application to hiding speech in video,1998,"Digital data hiding is a technology being developed for multimedia services, where non-trivial amounts of signature data is invisibly hidden inside a host data source by the owner before the latter is freely distributed. Only those authorized can recover the hidden data from the host, even after the latter has undergone standard transformations such as compression. We adopt a quantitative source and channel coding approach to hiding large amounts of compressible signature data inside the raw host. The signature data is source coded by vector quantization, and the indices are embedded in the host by perturbing it using orthogonal transform domain vector perturbations. The transform coefficients of the parent data are grouped into vectors, and the vectors are perturbed using noise-resilient channel codes derived from multidimensional lattices. The perturbations are constrained by a maximum allowable mean-squared error that can be introduced in the host. The generic approach is readily adapted to make retrieval possible even for applications where the original host is not available to the retriever. This scheme is applied to hiding speech in video. The host video is wavelet transformed frame by frame, and vectors of coefficients are perturbed using lattice channel codes to represent hidden vector quantized speech. The embedded video is subjected to H.263 compression before retrieving the hidden speech from it. The retrieved speech is intelligible even with large compression ratios of the host video."
2013197,21239,9078,Improved fast search method for vector quantization using discrete Walsh transform,2004,"In a framework of vector quantization (VQ), the fast search method for finding the best-matched codeword (winner) is a key issue because it is the time bottleneck for practical applications. To speed up VQ encoding process, some fast search methods that are based on the concept of projection axes or Walsh transform have already been proposed in previous works (L. Guan and M. Kamel, Oct 1992)-(S. Baek and K. Sung 2001). However, there still exist two serious problems in them because they use both spatial domain and partial Walsh domain simultaneously. First, they need extra memories for storing projected values on selected projection axes or the first several elements in partial Walsh domain, which becomes an overhead of memory. Second, once all rejection tests fail finally, they completely discard the obtained distortion that has already been computed in partial Walsh domain and return to spatial domain to compute real Euclidean distance again from the very beginning, which is certainly a waste and becomes an overhead of computation. In order to solve the overhead problems of both memory and computation as described above, firstly a memory-efficient storing way for a vector is proposed by completely mapping a vector into Walsh domain but NOT using-the original spatial domain any more, which can avoid extra memory requirement Secondly, the discarded distortion in partial Walsh domain is reused so as to avoid any waste to the executed computation. In addition, a more efficient rejection test is suggested to reduce more search space. Experimental results confirmed that the proposed method outperforms the previous works obviously."
1206791,21239,9078,Simpler Alternatives to Information Theoretic Similarity Metrics for Multimodal Image Alignment,2006,"Mutual information (MI) based methods for image registration enjoy great experimental success and are becoming widely used. However, they impose a large computational burden that limits their use; many applications would benefit from a reduction of the computational load. Although the theoretical justification for these methods draws upon the stochastic concept of mutual information, in practice, such methods actually seek the best alignment by maximizing a number that is (deterministically) computed from the two images. These methods thus optimize a fixed function, the similarity metric, over different candidate alignments of the two images. Accordingly, we study the important features of the computationally complex MI similarity metric with the goal of distilling them into simpler surrogate functions that are easier to compute. More precisely, we show that maximizing the MI similarity metric is equivalent to minimizing a certain distance metric between equivalence classes of images, where images f and g are said to be equivalent if there exists a bijection ? such that f(x) = ?(g(x)) for all x. We then show how to design new similarity metrics for image alignment with this property. Although we preserve only this aspect of MI, our new metrics show equal alignment accuracy and similar robustness to noise, while significantly decreasing computation time. We conclude that even the few properties of MI preserved by our method suffice for accurate registration and may in fact be responsible for MI's success."
1958298,21239,9078,Multiscale wedgelet image analysis: fast decompositions and modeling,2002,"The most perceptually important features in images are geometrical, the most prevalent being the smooth contours (edges) that separate different homogeneous regions and delineate distinct objects. Although wavelet based algorithms have enjoyed success in many areas of image processing, they have significant shortcomings in their treatment of edges. Wavelets do not parsimoniously capture even the simplest geometrical structure in images, and as a result wavelet based processing algorithms often produce images with ringing around the edges. The multiscale wedgelet framework is a first step towards explicitly capturing geometrical structure in images. The framework has two components: decomposition and representation. The multiscale wavelet decomposition divides the image into dyadic blocks at different scales and projects these image blocks onto wedgelets - simple piecewise constant functions with linear discontinuities. The multiscale wedgelet representation is an approximation of the image built out of wedgelets from the decomposition. In choosing the wedgelets to form the representation, we can weigh several factors: the error between the representation and the original image, the parsimony of the representation, and whether the wedgelets in the representation form natural geometrical structure. We show that an efficient multiscale wedgelet decomposition is possible if we carefully choose the set of possible wedgelet orientations. We also present a modeling framework that makes it possible to incorporate simple geometrical constraints into the choice of wedgelet representation, resulting in parsimonious image approximations with smooth contours."
2246090,21239,9078,Target-aided fixed-quality-of-service compression of SAR imagery for transmission over noisy wireless channels,2002,"We present a target-aided, fixed-quality-of-service (FQoS) system for coding synthetic aperture radar (SAR) imagery, whereby regions of interest and background information are coded independently of each other. A multiresolution, constant-false-alarm-rate (CFAR) detection scheme is used to discriminate between target regions and natural clutter. Based on the detected target regions, we apply less compression to targets, and more compression to background data. This methodology preserves the relevant features of targets for further analysis, and preserves the background only to the extent of providing contextual information. The background data is coded using channel-optimized trellis-coded quantization (COTCQ), while the target regions are coded using a combination of COTCQ and channel coding, depending upon the channel state. The combination of COTCQ and channel coding provides fixed target detection performance at the decoder, independent of the channel bit error probability. Additionally, all-pass filtering is employed, which significantly increases the detection performance at low encoding bit rates, while increasing the security level of the encoded bit stream. The proposed coder is evaluated in terms of target detection performance on the decoded SAR image for a wide variety of channel conditions and encoding bit rates. The resulting system dramatically reduces the bandwidth requirements of digital SAR imagery, and is shown to provide outstanding performance in adverse channel conditions."
984577,21239,9078,Perceptually optimized JPEG2000 coder based on CIEDE2000 color difference equation,2005,"In this paper, a perceptually optimized JPEG2000 coder for color image compression is proposed. Based on the CIEDE2000 color difference equation, this paper presents a visual model for estimating the perceptual redundancy inherent in each pixel of a color image in terms of a triple of values, one for each color component, as a just noticeable distortion (JND). By considering the varying sensitivities of the human visual perception to luminance and chrominance signals of different spatial frequencies, the full-band JND profile for each color channel is decomposed into component JND profiles for different wavelet subbands. With error visibility thresholds provided by the JND profile of each subband, the perceptually insignificant wavelet coefficients in three color channels are first removed. Without altering the format of the compressed bit stream of the JPEG2000, the encoder is modified in a way that the bit rate is inversely correlated with the perceptible distortion rather than the distortion of mean square errors. In each coding pass, each coding block is monitored to stop the coding process if the perceptible distortion defined by the corresponding JND is minimized for a given bit rate. Simulation results show that the performance of the perceptually tuned coder in terms of the bit rate required for the same subjective visual quality and the amount of perceptual distortion at the same bit rate is superior to that of the un-tuned JPEG2000 coder."
2070990,21239,9078,Intra prediction using template matching with adaptive illumination compensation,2008,"Modern video coding standards such as H.264/AVC use intra prediction for efficient coding of Intra pictures. These usually exploit local directional signal correlations. More recently, intra prediction modes using non-local signal information have been introduced. A very popular approach is the so called template matching prediction (TMP), which uses template based texture synthesis for signal prediction. This, combined with regular directional prediction, significantly improves intra coding efficiency compared to H.264/AVC. However, current TMP techniques have trouble synthesizing picture data with non-uniform illumination characteristics. They assume that similar picture regions resemble at the same time in structure and illumination, which is often not the case. In order to solve this, we propose a template matching technique with locally adaptive illumination compensation. The proposed technique is based on a linear compensation model with a scaling and an offset parameters to compensate for contrast and brightness disparities respectively. The model parameters are calculated by solving an auto-regressive Least Square problem during the template search for TMP. This permits to synthesize signal structures while capturing the local characteristics of illumination without needing extra side information. The total improvement in intra coding efficiency with respect to H.264/AVC can be of up to 18%."
1451317,21239,9078,A sensor aided H.264 encoder tested on aerial imagery for SFM,2014,"Standard video coding systems currently employed in UAV (Unmanned Aerial Vehicle) and aerial drone applications do not rely on some peculiarities in terms of scene 3D model and correlation among successive frames. In particular, the observed scene is static, i.e. the camera movement is dominant, and it can often be well approximated with a plane. Moreover, camera position and orientation can be obtained from the navigation system. Therefore, correspondent points on two video frames are linked by a simple homography. This paper presents novel results obtained by a low-complexity sensor aided H.264 encoder, recently developed at CIRA and yet tested on simulated data. The proposed encoder employs a new motion estimation scheme which make use of the global motion information provided by the onboard navigation system. The homography is used in order to initialize the block matching algorithm allowing a more robust motion estimation and a smaller search window, and hence reducing the complexity. The tests are made coding real aerial imagery, captured to be used for 3D scene reconstruction. The images are acquired by an high resolution camera mounted on a small drone, flying at low altitude."
1026313,21239,22130,Unsupervised Texture Segmentation using Active Contours and Local Distributions of Gaussian Markov Random Field Parameters.,2012,"In this paper, local distributions of low order Gaussian Markov Random Field (GMRF) model parameters are proposed as texture features for unsupervised texture segmentation. Instead of using model parameters as texture features, we exploit the variations in parameter estimates found by model fitting in local region around the given pixel. The spatially localized estimation process is carried out by maximum likelihood method employing a moderately small estimation window which leads to modeling of partial texture characteristics belonging to the local region. Hence significant fluctuations occur in the estimates which can be related to texture pattern complexity. The variations occurred in estimates are quantified by normalized local histograms. Selection of an accurate window size for histogram calculation is crucial and is achieved by a technique based on the entropy of textures. These texture features expand the possibility of using relatively low order GMRF model parameters for segmenting fine to very large texture patterns and offer lower computational cost. Small estimation windows result in better boundary localization. Unsupervised segmentation is performed by integrated active contours, combining the region and boundary information. Experimental results on statistical and structural component textures show improved discriminative ability of the features compared to some recent algorithms in the literature."
400011,21239,9078,Scene-dependent frequency weighting for subjective quality improvement of MPEG-4 fine-granularity-scalability,2002,"Fine-granularity-scalability (FGS) has recently been standardized in MPEG-4 due to its flexibility in adapting in real-time to Internet bandwidth variations and its resilience to packet-losses. However, the flexibility and robustness come at the expense of degrading video quality when compared with non-scalable MPEG-4 video coding at a given bit-rate. To reduce this visual quality penalty at low and medium bit-rates, the Frequency Weighting (FW) method has been standardized that allows the prioritized transmission of low frequency DCT coefficients. In this paper, we propose a novel scene-characteristic-dependent adaptive FW method aimed at improving the visual quality of FGS. After a thorough analysis of the FGS (i.e., SNR) residual signal at various bit-rates, we conclude that for an improved subjective quality, different FW matrices should be used to improve the FGS visual quality depending on the video sequence characteristics. Subsequently, a simple classification mechanism is developed that categorizes the video sequences based on their brightness, motion and texture activity in four distinct classes, each using a different FW matrix. For each class, the appropriate FW matrix was determined a priori based on the differences of the residual signals for two representative single-layer bit-rates. This adaptive FW (AFW) method has been subjectively evaluated and shows a clear improvement in visual quality compared with non-frequency weighted or non-adaptive frequency weighted sequences."
1978625,21239,9078,Randomness-in-Structured Ensembles for compressed sensing of images,2009,"Leading compressed sensing (CS) methods require m = O (k log(n)) compressive samples to perfectly reconstruct a k-sparse signal x of size n using random projection matrices (e.g., Gaussian or random Fourier matrices). For a given m, perfect reconstruction usually requires high complexity methods, such as Basis Pursuit (BP), which has complexity O(n 3 ). Meanwhile, low-complexity greedy algorithms do not achieve the same level of performance (as BP) in terms of the quality of the reconstructed signal for the same m. In this paper, we introduce a new CS framework, which we refer to as Randomness-in-Structured Ensemble (RISE) projection. RISE projection matrices enable compressive sampling of image coefficients from random locations within the k-sparse image vector while imposing small structured overlaps. We prove that RISE-based compressed sensing requires only m = ck samples (where c is not a function of n) to perfectly recover a k-sparse image signal. For the case of n ≤ O(k 2 ), the complexity of our solver is O(nk) which is less than the complexity of the popular greedy algorithm Orthogonal Matching Pursuit (OMP). Moreover, in practice we only need m = 2k samples to reconstruct the signal. We present simulation results that demonstrate the RISE framework's ability to recover the original image with higher than 50 dB PSNR, whereas other leading approaches (such as BP) can achieve PSNR values around 30 dB only."
3133055,21239,9616,A feature fusion framework for hashing,2016,"A hash algorithm converts data into compact strings. In the multimedia domain, effective hashing is the key to large-scale similarity search in high-dimensional feature space. A limit of existing hashing techniques is that they typically use single features. In order to improve search performance, it is necessary to utilize multiple features. Due to the compactness requirement, concatenation of hash values from different features is not an optimal solution. Thus a fusion process is desired. In this paper, we solve the multiple feature fusion problem by a hash bit selection framework. Given multiple features, we derive an n-bit hash value of improved performance compared with hash values of the same length computed from each individual feature. The framework utilizes a feature-independent hash algorithm to generate a sufficient number of bits from each feature, and selects n bits from the hash bit pool by leveraging pair-wise label information. The metric bit reliability is used for ranking the bits. It is estimated by bit-level hypothesis testing. In addition, we also take into account the dependence among bits. A weighted graph is constructed for refined bit selection, where the bit reliability is used as vertex weights and the mutual information among hash bits is used as edge weights. We demonstrate our framework with LSH. Extensive experiments confirm that our method is effective, and outperforms several state-of-the-art methods."
1351320,21239,9078,A context adaptive predictor of sensor pattern noise for camera source identification,2012,"Sensor pattern noise (SPN) is a noise-like spread-spectrum signal inherently cast onto every digital image by each imaging device and has been recognised as a reliable device fingerprint for camera source identification (CSI) and image origin verification. It can be estimated as the noise residual between the image content and its denoised version. However, the SPN extracted from a single image can be contaminated largely by image scene because image edge noise is usually much stronger than the SPN. So the identification performance is heavily dependent upon the purity of the estimated SPN, especially for small size images because they have less and weaker SPN. Although there are some existing works dedicated to improving the performance of source camera identification, an effective method to eliminate the contamination of image scene and extract an accurate SPN is currently lacking. In this paper, we will propose an edge adaptive SPN predictor based on context adaptive interpolation (PCAI) to exclude the contamination of image scene. Different from most of the existing methods extracting SPN from wavelet high frequency coefficients, we extract SPN directly from the spatial domain with a pixel-wise adaptive Wiener filter, based on the assumption that the SPN is a white signal. Extensive experiments show that our proposed PCAI method achieves the best receiver operating characteristic (ROC) performance among all of the state-of-the-art CSI schemes on different sizes of images, and has the best performance in resisting JPEG compression (e.g. with a quality factor of 90%) simultaneously."
1436667,21239,21056,RAISE: a raw images dataset for digital image forensics,2015,"Digital forensics is a relatively new research area which aims at authenticating digital media by detecting possible digital forgeries. Indeed, the ever increasing availability of multimedia data on the web, coupled with the great advances reached by computer graphical tools, makes the modification of an image and the creation of visually compelling forgeries an easy task for any user. This in turns creates the need of reliable tools to validate the trustworthiness of the represented information. In such a context, we present here RAISE, a large dataset of 8156 high-resolution raw images, depicting various subjects and scenarios, properly annotated and available together with accompanying metadata. Such a wide collection of untouched and diverse data is intended to become a powerful resource for, but not limited to, forensic researchers by providing a common benchmark for a fair comparison, testing and evaluation of existing and next generation forensic algorithms. In this paper we describe how RAISE has been collected and organized, discuss how digital image forensics and many other multimedia research areas may benefit of this new publicly available benchmark dataset and test a very recent forensic technique for JPEG compression detection."
1924185,21239,9078,Nonlinear distortion correction in endoscopic video images,2000,"Modern video-based endoscopes offer physicians a wide-angle held of view for minimally-invasive procedures. Unfortunately, inherent barrel distortion prevents accurate perception of range. This makes measurement and distance judgment difficult and causes difficulties in emerging applications, such as 3D medical-image registration. Such distortion also arises in other wide field-of-view camera circumstances. This paper presents a distortion-correction technique that can automatically calculate correction parameters, without precise knowledge of horizontal and vertical-orientation. The method is applicable to any camera-distortion correction situation. Based on a least-squares estimation, our proposed algorithm considers line fits in both field-of-view directions and global consistency that gives the optimal image center and expansion coefficients. The method is insensitive to the initial orientation of the endoscope and provides more exhaustive field-of-view correction than previously proposed algorithms. The distortion-correction procedure is demonstrated for endoscopic video images of a calibration test pattern, a rubber bronchial training device, and real human circumstances. The distortion correction is also shown as a necessary component of an image-guided virtual-endoscopy system that matches endoscope images to corresponding rendered 3D CT views."
285077,21239,9078,The VIVA project: digital watermarking for broadcast monitoring,1999,"The main objective of the VIVA project (Visual Identity Verification Auditor) is to investigate and demonstrate a professional broadcast surveillance system. Broadcast material is pre-encoded with an invisible and unique watermark identifier. By automatically monitoring television broadcasts and registering which assets have been pre-encoded, a mechanism for IPR protection is provided. The applications of such a system include copyright protection and proof of ownership, verification of commercial transmissions, assessment of sponsorship effectiveness, protection against illegal transmission, statistical data collection and analysis of broadcast content. The watermarking technology is optimised for the high picture quality needed in a broadcast environment. At the same time, the watermark survives signal processing operations which routinely occur in broadcasting systems such as digital to analogue conversion, editing and compression. The monitoring system detects a 36-bit payload every second, which guarantees an operation time of more than 13 years, without recourse to repeat identifiers. The detection algorithm has reasonably low complexity, enabling real time watermark detection for many broadcast channels simultaneously. We report on the first results of a field trial, using a satellite link between Sweden and Belgium, proving the feasibility of the system."
1545353,21239,9078,Towards generic fitting using multiple features Discriminative Active Appearance Models,2010,"A solution for Discriminative Active Appearance Models is proposed. The model consists in a set of descriptors which are covariances of multiple features evaluated over the neighborhood of the landmarks whose locations are governed by a Point Distribution Model (PDM). The covariance matrices are a special set of tensors that lie on a Riemannian manifold, which make it possible to measure the dissimilarity and to update them, imposing the temporal appearance consistency. The discriminative fitting method produce patch response maps found by convolution around the current landmark position. Since the minimum of the responce map isn't always the correct solution due to detection ambiguities, our method finds candidates to solutions based on a mean-shift algorithm, followed by an unsupervised clustering technique used to locate and group the candidates. A mahalanobis based metric is used to select the best solution that is consistent with the PDM. Finally the global PDM optimization step is performed using a weighted least-squares warp update, based on the Lucas Kanade framework. The weights were extracted from a landmark matching score statistics. The effectiveness of the proposed approach was evaluated on unseen data on the challenging Talking Face video sequence, demonstrating the improvement in performance."
2448648,21239,9078,Wavelet-based multiscale stochastic models for efficient tomographic discrimination of fractal fields,1994,"Proposes a technique for discrimination of fractal fields with different fractal dimensions, directly from the noisy and sparse tomographic projection data. This application is motivated from the medical field, where a change in fractal dimension is used to differentiate between normal and abnormal conditions in many different contexts, including diagnosis of liver abnormalities. The conventional method for discrimination of fractal fields from tomographic data is based on the calculation of the slope of the power spectra of the corresponding projections. This method, derived from the Radon transform results, breaks down in case the projection data are sparse and/or noisy. In order to avoid any restrictions on the duality and quantity of the projection data, we formulate our discrimination problem in a discrete hypothesis testing framework, the solution to which is given by the maximum-log-likelihood discrimination rule. The problem of discriminating fractal fields through likelihood calculations is, however, complicated by the fact that inverses and determinants of large, full, and generally ill conditioned fractal-field data covariance matrices are required. We show that these complications in the likelihood calculations can be removed by a transformation to the multiscale framework. The multiscale data covariance matrices are sparse and in addition, are naturally partitioned into ill conditioned coarsest scale approximation blocks and relatively well conditioned multiscale detail blocks. We simplify our likelihood calculations by using the class of multiscale stochastic models defined on trees to realize accurate approximations of the detail block of the data covariance matrices. >"
2412840,21239,9078,Image-processing approach via nonlinear image-decomposition for a digital color camera,2008,"This paper presents a new image-processing (IP) pipeline adopting an image-processing approach via nonlinear image-decomposition, for a digital color camera. This new IP pipeline is composed of four stages. At the first stage, with the multiplicative BV-G image-decomposition method, each primary color channel of observed raw color data mosaicked with the Bayer color filter array is decomposed as a product of two components. Its structural component corresponds to a cartoon signal-approximation and its texture component collects almost all oscillatory variations representing textures. At the second stage, each separated component is demosaicked with an interpolation method suitable to it. For demosaicing of the structural component, this paper employs our color-TV-regularization super-resolution deblurring-demosaicing method that can remove image blurs without producing ringing artifacts near edges. For interpolation of the texture component gathering most of observation noise, this paper employs a basic averaging-type demosaicing method that is robust against observation noise. At the third stage, white balancing, color enhancement and inverse gamma correction are applied to only the demosaicked structural component. At the final stage, the two processed components are combined. Our proposed IP pipeline succeeds in achieving color interpolation, denoising and deblurring, simultaneously."
2074198,21239,9078,Quantization based nearest-neighbor-preserving metric approximation,2009,"To reduce the computational burden of the nearest neighbor search (NNS) problem, most existing algorithms focus on ‘preprocessing’ the data set to reduce the number of objects to be examined for each querying operation (e.g., efficient data structures, metric space transforms). In this paper we present a quantization based nearest-neighbor-preserving metric approximation algorithm (QNNM) that leads to further complexity reduction by simplifying the metric computation. The proposed algorithm is based on three observations: (i) the query vector is fixed during the entire search process, (ii) the-minimum distance exhibits an extreme value distribution, and (iii) there is high homogeneity of viewpoints. Based on these, QNNM approximates original/benchmark metric in terms of preserving the fidelity of NNS rather than the distance itself, while achieving significantly lower complexity using a query-dependent quantizer. We formulate a quantizer design problem where the goal is to minimize the average NNS error. We show how the query adaptive quantizers can be designed off-line without prior knowledge of the query and present an efficient and specifically tailored off-line optimization algorithm to find such optimal quantizer. Experimental results in a motion estimation (ME) application show minimal performance degradation (average 0.05dB loss) when using optimized 1-bit quantizer."
2128930,21239,9078,Selective Streaming of Multi-View Video for Head-Tracking 3D Displays,2007,"We present a novel client-driven multi-view video streaming system that allows a user watch 3-D video interactively with significantly reduced bandwidth requirements by transmitting a small number of views selected according to his/her head position. The proposed scheme can be used to efficiently stream a dense set of multi-view sequences (light-fields) or wider baseline multi-view sequences together with depth information. The user's head position is tracked and predicted into the future to select the views that best match the user's current viewing angle dynamically. Prediction of future head positions is needed so that views matching the predicted head positions can be requested from the server ahead of time in order to account for delays due to network transport and stream switching. Highly compressed, lower quality versions of some other views are also requested in order to provide protection against having to display the wrong view when the current user viewpoint differs from the predicted viewpoint. The proposed system makes use of multi-view coding (MVC) and scalable video coding (SVC) concepts together to obtain improved compress ion efficiency while providing flexibility in bandwidth allocation to the selected views. Rate-distortion performance of the proposed system is demonstrated under different experimental conditions."
1719344,21239,11470,Complexity Modeling of the Motion Compensation Process of the H.264/AVC Video Coding Standard,2012,"With recent advances in computing and communication technologies, ubiquitous access to high quality multimedia content such as high definition video using smart phones, Net books, or tablets is a fact of our daily life. However, power is still a major concern for any mobile device, and requires optimization of power consumption using a power model for each multimedia application, such as a video decoder. In this paper, a generic decoding complexity model for the motion compensation (MC) process, which constitutes up to 25% of the computational complexity and hence power consumption of an H.264/AVC decoder, has been proposed. For the model to remain independent from a specific implementation or platform, it has been developed by analysing the MC algorithm as described in the standard. Simulation results indicate that the proposed model estimates MC complexity with an average accuracy of 95.63%, for a wide range of test sequences using both JM and x.264 software implementations of H.264/AVC. For a dedicated hardware implementation of the MC module the modeling accuracy is around 89.61%, according to our simulation results. It should be noted that in addition to power consumption control, the proposed model can be used for designing a receiver-aware H.264/AVC encoder, where the complexity constraints of the receiver side are taken into account during compression."
2633101,21239,23735,Safe receding horizon control for aggressive MAV flight with limited range sensing,2015,"Micro Aerial Vehicles (MAVs) are becoming ubiquitous, but most experiments and demonstrations have been limited to slow flight except in open environments or in laboratories with motion capture systems. In this paper, we develop representations and algorithms for aggressive flight in cluttered environments. We incorporate the specific dynamics of the system and generate safe, feasible trajectories for fast navigation in real time. Specifically, we use a polyhedral decomposition of the visible free space and address the generation of safe trajectories that are within the space. Because of the limited field of view, we adopt a receding horizon control policy (RHCP) for planning over a finite time horizon, but with the guarantee that there exists a safe stopping control policy over a second time horizon from the planned state at the end of the horizon. Thus, the robot planning occurs over two horizons. While the robot executes the planned trajectory over the first time horizon, the map of obstacles is refreshed allowing the planner to generate a refined plan, once again over two time horizons. The key algorithmic contribution of the paper lies in the fast planning algorithm that is able to incorporate robot dynamics while guaranteeing safety. The algorithm is also optimal in the sense that the receding horizon control policy is based on minimizing the trajectory snap. Central to the algorithm is a novel polyhedral representation that allows us to abstract the trajectory planning problem as a problem of finding a path through a sequence of convex regions in configuration space."
2389770,21239,11470,Low-latency streaming of pre-encoded video using channel-adaptive bitstream assembly,2002,"Today's Internet video streaming systems employ buffering and retransmission to guarantee the correct reception of each packet. This leads to high latency in media delivery. In this paper, we present an efficient low-latency Internet video streaming system that does not require retransmission of lost packets. We pre-store multiple representations of certain frames on the server such that a representation using a more reliable reference frame will be selected and sent during transmission. The potential mismatch error during bitstream assembly at transmission is avoided by using a novel layered coding structure with coding restrictions. The optimal picture type is determined within a rate-distortion framework during pre-encoding and transmission, adapting to the channel, so that the expected end-to-end distortion is minimized under the given rate constraint. The expected distortion is calculated based on an accurate binary tree modeling with the effects of channel loss and error concealment taken into account. Experiments demonstrate that the proposed scheme provides significant performance gains over a simple INTRA-insertion scheme. The increased error-resilience eliminates the need for retransmission, which allows to reduce the latency of streaming from 10-15 seconds to a few hundred milliseconds, with good video quality maintained."
2899132,21239,9078,Ghost-free dual-exposure HDR for dynamic scenes,2016,"High Dynamic Range Imaging (HDRI) and Exposure Fusion (EF) are methods of choice to computationally extend the dynamic range of images depicting real world scenes. Unfortunately, those methods are still prone to certain artifacts. Among others, the so-called Ghost Effect is the most critical HDR limitation when it comes to dealing with motion (camera or scene motion) in input Low Dynamic Range (LDR) images. This problem becomes more challenging when the input LDR image stack contains only a couple of images with large color differences, which is the case in the mobile phone domain. To address this issue, a de-ghosting step is required to preserve the quality of the final HDR images. In this paper, we propose a robust de-ghosting approach based on the detection and the elimination of the motion induced effects on the final HDR images. The proposed method performs efficiently in all cases even on scenarios where only two differently exposed images with large illumination variations are available as input. Compared to the state-of-the-art, our results exhibit significant visual improvement and artifact reduction. Furthermore, our approach has low computational cost and complexity, which enables an efficient implementation especially for mobile phone-related applications."
2330372,21239,9078,Receiver-initiated resource renegotiation for VBR video transport,1999,"In this paper we address the important issue of providing QoS for VBR video communications in an efficient manner. We show that efficient transmission of VBR video with a high QoS is feasible when using a receiver-initiated resource renegotiation (RIR) scheme. The scheme for RIR is based on RTP and RSVP. RTP's media specific header is used to send video source information to receivers. Receivers utilize this information to estimate the traffic descriptors. Renegotiations are triggered based on the receiver's buffer status and RSVP is used to renegotiate flow parameters with the network. The performance of the proposed scheme is evaluated via simulations using several 20-minute-long MPEG-2 bit streams. Performance metrics considered are video quality and renegotiation overhead for different receiver buffer sizes and network delays. The results show that the proposed RIR scheme provides high video quality with an average renegotiation interval on the order of seconds, a 5-15 frames receiver buffer and network renegotiation delay below 300 msec. We also investigated call admission control (CAC) schemes for renegotiation VBR services. In particular, we studied the performance of the Central Limit Theorem based and the Chernoff bound based CAC algorithms, in terms of error in the calculation of the maximum number of admissible connections. Finally, to facilitate the scaling of the renegotiation scheme to large-scale networks, we studied how to reduce renegotiation overhead."
884497,21239,9078,Iterative tomographic image reconstruction by compressive sampling,2010,"Positron Emission Tomography (PET) and Single Photon Emission Computerized Tomography (SPECT) are essential medical imaging tools with inherent drawback of slow data acquisition process. With the knowledge that radionuclide images are sparse in transform domain, we have applied a novel concept of Compressive Sampling on them. The proposed approach aims to reconstruct images from fewer measurements, significantly reducing scan time and radiopharmaceutical doze, with benefits for patients and health care economics. The reconstruction of tomographic images is realized by compressed sensing the 2-D Fourier projections. These 2-D projections being sparse in transform domain are sensed with fewer samples in k-space and are reconstructed without loss of fidelity. These undersampled Fourier projections can then be backprojected by employing the iterative reconstruction approach for a complete 3-D volume. Our work focuses on the acquisition of 2-D SPECT/PET projections based on compressive sampling and their reconstruction using a non-linear recovery algorithm. Compressive sampling of a phantom image and PET bone scan scintigraph with radial Fourier samples are performed. The reconstructions of these images are compared to conventionally sampled images with MSE, PSNR and a new image quality measure, Structure SIMilarity (SSIM). The results show high quality image reconstruction using considerably few measurements."
2607969,21239,9078,Contour expansion algorithm preprocessed by hough transform and circular shortest path for ovoid objects,2015,"Circular and ovoidal objects such as cells and supramolecular complexes are very common in bio-imaging. In order to do measurements on such objects, the accurate detection of these objects from microscope images is essential. Therefore our objective is finding accurate and reliable methods to extract the contour delineating the object. In earlier work, we have shown to be successful in achieving this objective through dynamic programming, where we used Hough transform and a minimal path algorithm to estimate the contour location. However, due to the inherently fuzzy nature of edges and as microscope imaging can be very delicate, a refinement of the initial estimate is sometimes required. In these cases the minimal path is no longer the ultimate valid representation of the contour. This paper describes an expansion algorithm in the polar image that is created for each object; with this new algorithm we achieve the necessary refinement of contours. From previous results we can assess the improvement to existing segmentation methods, including our own approach. The exact contour contributes in the accomplishment of precise measurements of the objects so that machine learning techniques can better recognize the subtle patterns within the data. As the method is geared to ovoid like objects, it can be easily generalized to other image of other modalities. Here we apply the method in the domain of yeast cell biology."
105826,21239,9078,In-Network View Re-Sampling for Interactive Free Viewpoint Video Streaming,2015,"Interactive free viewpoint video offers the possibility for each user to independently choose the views of a 3D scene to be displayed at de- coder. The visual content is commonly represented by N texture and depth map pairs that capture different viewpoints. A server selects an appropriate subset of M ≤ N views for transmission, so that the user can freely navigate in the corresponding window of viewpoints without being affected by network delay. During navigation, a user can synthesize any intermediate virtual view image in the navigation window via depth-image-based rendering (DIBR) using two nearby camera views as references. When the available bandwidth is too small for the transmission of all camera views needed to synthesize views in the navigation window, we propose to synthesize intermedi- ate virtual views as new references for transmission—a re-sampling of viewpoints for the 3D scene—so that the synthesized view dis- tortion within the navigation window is minimised. We formulate a combinatorial optimization to find the best set of M virtual views to synthesize as new references, and show that the problem is NP- hard. We approximate the original problem with a new reference view equivalence model and derive in this case an optimal dynamic programming algorithm to determine to best set of M views to be transmitted to each user. Experimental results show that synthesiz- ing virtual views as new references for client-side view synthesis can outperform simple selection from camera views by up to 0.73dB in synthesized view quality."
1964105,21239,9078,Automatic suppression of spatially variant translational motion artifacts in magnetic resonance imaging,1998,"This paper summarizes the theory of a novel post-processing approach to automatic motion artifact suppression in magnetic resonance imaging. The main advantage of the new approach is its treatment of practical spatially variant translational motion model that is fundamentally different from previous work in the literature. We first consider a 1-D model for the problem based on differentiated rather than original image. In this model, the motion artifact amounts to blurring of peaks corresponding to the edges in the original image. Observing that the distorted and true images share the same 2-norm, we search for the true image on the hyper-sphere with radius equal to this norm. We show that the solution must have the minimum 1-norm of all vectors on the hyper-sphere and a search strategy based on dynamic programming is used to estimate the motion at a reasonable complexity. Subsequently, this procedure is applied to different regions in the image independently and spatially variant motion model parameters are derived at a resolution of the region sizes. Finally, we show the similarity between this problem and the problem of magnetic field inhomogeneity distortion. Based on this similarity, an image reconstruction strategy and an expression for the point-spread function of the resultant image are derived. The new technique is applied to correct computer simulated images and promising results are obtained."
2427182,21239,9078,Robust image hashing based on radial variance of pixels,2005,"Robust image hashing defines a feature vector that characterizes the image, independently of non-significant distortions of its content. As a consequence, the comparison between robust image hash vectors is able to indicate whether the corresponding images are equivalent or not, independently of visually non-significant distortions due for example to compression or re-sampling. We define a robust image hash based on radial projections of the image pixels. Specifically, our proposed radial hASH (RASH) considers moments of different orders to describe the luminance pdf of the pixels encountered on a set of lines articulated around the center of the image. In short, each RASH component is defined based on the moment of the pixels belonging to a specific line. Our paper provides a careful analysis of the robustness and discriminating capabilities of the RASH vectors computed based on different moment orders. As a first contribution, it demonstrates that the second order moment, i.e. the variance of the pixels on a line, allows for optimal trade-offs between the robustness and the discriminating capabilities of the resulting RASH vector. As a second contribution, extensive simulations prove that a decision engine based on RASH vectors cross-correlations is able to successfully identify pairs of equivalent or distinct images. Bottom line, the RASH assets are a low computational complexity, a strong robustness to both filtering and geometrical distortions, and a risk of collision that is estimated to less than 10 per million of images."
2426030,21239,11470,Human visual system features enabling watermarking,2002,"Digital watermarking consists of hiding subliminal information into digital media content, also called host data. It can be the basis of many applications, including security and media asset management. In this paper, we focus on the imperceptibility requirement for image watermarking. We present the main features of the human visual system (HVS) to be translated into watermarking technology. This paper highlights the need for dedicated inputs from the human vision community. The human visual system (HVS) is very complex and able to deal with a huge amount of information. Roughly speaking, it is composed of a receiver with a pre-processing stage, the eye and the retina, a transmission channel, the optic nerve, and a processing engine, the visual cortex. Mainly because of our lack of knowledge about brain behavior, i.e. about the way a stimulus is processed through its huge neural network, the large effort to understand and model the HVS behavior has partly remained fruitless. The aim of this paper is not to provide a thorough description of the HVS. For complete HVS models and more specific details, the reader is referred to existing literature. Here, we only try to understand, in a synthetic way and from an engineering perspective, the HVS features on which the designer of a watermarking algorithm can rely, i.e. its sensitivity and masking capabilities."
880638,21239,9078,Color palette for screen content coding,2014,"With the prevalence of high speed Internet access, emerging video applications such as remote desktop sharing, virtual desktop infrastructure, and wireless display require high compression efficiency of screen contents. However, traditional intra and inter video coding tools were designed primarily for natural contents. Screen contents have significantly different characteristics compared with nature contents, e.g. sharp edges, less or no noise, which makes those traditional coding tools less sufficient. In this research, a new color palette based video coding tool is presented. Different from traditionally intra and inter prediction that mainly removes redundancy between different coding units, palette coding targets at the redundancy of repetitive pixel values/patterns within the coding unit. In the palette coding mode, a lookup table named palette which maps pixel values into table indices (also called palette indices) is signaled first. Then the mapped indice for a coding unit (which we call index block) are coded with a novel three-mode run-length entropy coding. Some encoder-side optimization for palette coding is also presented in detail in this paper. Simulation has been performed using the common screen content coding test condition defined by JCT-VC and the results show that palette coding can effectively improve screen content coding efficiency for both lossless and lossy scenarios."
1243507,21239,9078,A learning framework for robust hashing of face images,2010,"Robust image hashing has been actively researched over the last decade with varied applications in image content authentication and identification under distortions. In the existing literature on robust image hashing, hash algorithms are ignorant of the class of images being hashed. There are however significant application domains such as that of face image hashing where apriori knowledge of the image class as well as permissible distortions can benefit hash algorithm design. In this paper, we present a two stage cascade of dimensionality reduction constructs for face image hashing. The first stage aims to project the face image to a space where geometric distortions manifest approximately as additive noise. For this purpose, we use the non-negative matrix approximations based hash vector developed by Monga et al. which is known to possess excellent geometric attack robustness. In the second stage, we employ oriented principal component analysis (OPCA) based on estimating signal as well as noise statistics in a learning phase and deriving a projection that mitigates the effect of noise. We obtain both experimentally based ROC curves as well as analytical ones via a detection theoretic analysis of the proposed framework. The ROC curves reveal clearly that incorporating such a learning phase greatly reduces error probabilities."
2662996,21239,9078,DVD technology,1998,"Summary form only given. Starting in 1993 Toshiba made history introducing DVD technology to the industry. Toshiba provided leadership in building DVD alliance and cooperation with many companies in the developing unified DVD format and key enabling technologies. These companies included but are not limited to Time Warner, Matsushita, Pioneer, Dolby, Hitachi, Philips, Sony and many others. Key enabling technologies that make DVD work are: unified DVD format, 0.6 mm thick disk, phase change, MPEG2 video and AC-3 audio compression, DVD encoder and authoring equipment, multi-angle and multistory, double layer, 8:16 modulation, WORM disk, advanced RSPC error correction, commercial 650 nm laser, integrated MPEG2 and AC-3 decoders. Just as the Compact Disc (CD) provided a dramatic leap in quality over vinyl records, DVD provided a powerful shift in quality, interactivity and versatility when compared to traditional media including videocassette and laserdisc. DVD takes advantage of higher capacity discs which are physically the same size as today's CDs but offer tremendous storage capacities-from 4.7 GB to 17 GB-to accommodate all types of data, audio and video. Both DVD-ROM drives and DVD players can read content of all four disc formats: single-sided, single layer 4.7 GB disc, the single sided, double-layer 8.5 GB disc; the double sided, single-layer 9.4 GB disc and the double-sided, double-layer 17 GB disc. DVD-ROM drives will read existing CD-ROMs and audio CDs."
636935,21239,20332,Crowd Access Path Optimization: Diversity Matters,2015,"Quality assurance is one the most important challenges in crowdsourcing. Assigning tasks to several workers to increase quality through redundant answers can be expensive if asking homogeneous sources. This limitation has been overlooked by current crowdsourcing platforms resulting therefore in costly solutions. In order to achieve desirable cost-quality tradeoffs it is essential to apply efficient crowd access optimization techniques. Our work argues that optimization needs to be aware of diversity and correlation of information within groups of individuals so that crowdsourcing redundancy can be adequately planned beforehand. Based on this intuitive idea, we introduce the Access Path Model (APM), a novel crowd model that leverages the notion of access paths as an alternative way of retrieving information. APM aggregates answers ensuring high quality and meaningful confidence. Moreover, we devise a greedy optimization algorithm for this model that finds a provably good approximate plan to access the crowd. We evaluate our approach on three crowdsourced datasets that illustrate various aspects of the problem. Our results show that the Access Path Model combined with greedy optimization is cost-efficient and practical to overcome common difficulties in large-scale crowdsourcing like data sparsity and anonymity."
1838406,21239,9078,Region tracking via local statistics and level set PDEs,2002,"Tracking of regions in image sequences plays a fundamental role in applications (search and retrieval in video databases, object based coding such as in MPEG-4, surveillance), and although numerous approaches to region tracking have been developed, they all suffer from severe constraints imposed on the nature of the image sequence. Some assume a particular motion model or constrain the range of interframe motion, while others constrain both the region tracked and the background to have uniform and contrasting intensities. As a result, these tracking algorithms become byproducts of algorithms for motion or intensity boundary detection, and thus have limited applicability. We propose a novel algorithm for region tracking that uses the Bayesian framework for tracking previously developed. We extend this framework by re-expressing tracking in terms of Kullback-Leibler divergence of specific probability distributions and generalizing these to empirical distributions computed over image neighborhoods, leading to level set equations in terms of local image statistics. The main novelty of our proposed algorithm is that contrary to other tracking algorithms which are expressed as level set PDEs, the motion is not assumed to be small, nor is the background assumed to be stationary, nor is the region supposed to be uniform and have strong contrast with the background. We illustrate the performance of our algorithm on real image sequences with natural motion."
2570762,21239,9078,Piecewise 2D autoregression for predictive image coding,1998,"We propose an on-line piecewise two-dimensional autoregression algorithm (P2AR) for modeling and coding of images. The algorithm assumes no stationarity of the source. The resulting P2AR predictor can be loosely considered as a universal parametric image model in the sense that no prior knowledge about the image are assumed. The predictor parameters are causal and dynamically adapted to the source, hence suits one-pass image coding with no side information. The good fit of the model to a variety of images is demonstrated by its superior performance when being used in lossless image coding. We reduced the bit rate of the benchmark lossless image codec CALIC by three percent averaging over the JPEG set of test images, simply by replacing the context-sensitive, non-linear GAP predictor of CALIC with the P2AR predictor. The performance margin gets much larger when the source is highly nonstationary. In order to make on-line 2D autoregression computationally feasible, we developed a novel and efficient algorithm for computing and updating covariances of regression variables on a pixel-by-pixel basis. This algorithm can also be applied in image segmentation, texture classification, and other image analysis tasks that require heavy computations of second order statistics."
2064563,21239,9078,A covariance adjustment method in compressed domain for noisy image segmentation,2008,"Noise is ubiquitous in real life and changes image acquisition and processing characteristics in an uncontrolled manner. Highly sophisticated image processing algorithms developed for clean images often malfunction when they are used for noisy images. For example, hidden Markov Gauss mixture models (HMGMM) have been shown to perform well in image segmentation applications, but they have also proved to be quite sensitive to uncontrolled noise in test images. To resolve this difficulty, we propose a modified procedure to adjust covariance matrix estimates of test images. We shrink (or expand) the covariance matrix estimates of the noisy image to make them consistent with those in the codebooks. Note that the covariance matrices in the codebooks are those of the noiseless image. The novelty of this paper is that our method is equivalent to adjusting the covariance matrices of codebooks for noiseless images to be consistent wit those of noisy test images without retraining. The adjusted covariance matrices shrink (or expand) the covariance matrix estimates in the codebooks to minimize the overall minimum discrimination information distortion between test images and codebooks. To illustrate the proposed procedure, we apply it to segmenting aerial images with salt and pepper noise and with Gaussian noise. We compare our method with the median filter restoration method and the blind deconvolution method and show that our procedure has better performance than these image-restoration-based techniques in terms of both visual segmentation results and error rate. Further, we find that the suggested procedure performs almost as well as the HMGMM for clean images, which is the benchmark in comparison."
988015,21239,9078,Hierarchical OBB-sphere tree for large-scale range data management,2013,"Recent research has shown that range data acquired from consumer imaging devices are successfully used to build accurate 3D geometric models of surrounding environments very quickly. However, as the volume of modeling space grows, its processing time inevitably becomes huge when a simple linear data structure is used to handle a large range data set. The main difficulty is that pairwise scan matching among all pairs of input data requires quadratic computational complexity. The OBB-sphere tree presented in this paper accelerates the process by intelligently choosing a subset of related pair candidates. The proposed data structure is an enhanced sphere tree in which every leaf node contains a set of 3D points computed from a depth image and bounded by one sphere and one tighter-fitting shape, which is the oriented bounding box (OBB). This approach exploits the simplicity of spheres in hierarchical tree construction and the intersection test which is performed to reject objects that are far apart. The compactness of OBBs is used to refine the results of the intersection test. The number of possible pairs was reduced by more than half in some test datasets in the experiment."
859710,21239,9773,Graphical Figure Classification Using Data Fusion for Integrating Text and Image Features,2013,"This paper describes a multimodal (image + text) learning approach for automatically identifying three graphical figure types commonly found in biomedical literature, namely, diagrams, statistical figures and flow charts. The goal is to improve retrieval of figures from biomedical journal articles. In this article, we describe a data fusion approach to combine information from both text and image sources, believed to contain complementary information. Text information about the image is extracted from the figure caption. The data fusion process includes a hybrid of evolutionary algorithm (EA) and Binary Particle Swarm Optimization (BPSO) called method applied to find an optimal subset of extracted image features. Chi-square statistic and information gain metric are used to select the optimal subset of extracted text features, which along with image features are input to Multi-Layer Perceptron Neural Network classifiers, whose outputs are characterized as fuzzy sets to determine the final classification result. Evaluation performed on 1707 figure images extracted from a test subset of Biome Central® journals extracted from U.S. National Library of Medicine's PubMed Central ® repository yielded classification accuracy as high as 96.1%."
788,21239,21106,The role of facial regions in evaluating social dimensions,2012,"Facial trait judgments are an important information cue for people. Recent works in the Psychology field have stated the basis of face evaluation, defining a set of traits that we evaluate from faces (e.g. dominance, trustworthiness, aggressiveness, attractiveness, threatening or intelligence among others). We rapidly infer information from others faces, usually after a short period of time (<1000ms) we perceive a certain degree of dominance or trustworthiness of another person from the face. Although these perceptions are not necessarily accurate, they influence many important social outcomes (such as the results of the elections or the court decisions). This topic has also attracted the attention of Computer Vision scientists, and recently a computational model to automatically predict trait evaluations from faces has been proposed. These systems try to mimic the human perception by means of applying machine learning classifiers to a set of labeled data. In this paper we perform an experimental study on the specific facial features that trigger the social inferences. Using previous results from the literature, we propose to use simple similarity maps to evaluate which regions of the face influence the most the trait inferences. The correlation analysis is performed using only appearance, and the results from the experiments suggest that each trait is correlated with specific facial characteristics."
651157,21239,9078,Image denoising using optimally weighted bilateral filters: A sure and fast approach,2015,"The bilateral filter is known to be quite effective in denoising images corrupted with small dosages of additive Gaussian noise. The denoising performance of the filter, however, is known to degrade quickly with the increase in noise level. Several adaptations of the filter have been proposed in the literature to address this shortcoming, but often at a substantial computational overhead. In this paper, we report a simple pre-processing step that can substantially improve the denoising performance of the bilateral filter, at almost no additional cost. The modified filter is designed to be robust at large noise levels, and often tends to perform poorly below a certain noise threshold. To get the best of the original and the modified filter, we propose to combine them in a weighted fashion, where the weights are chosen to minimize (a surrogate of) the oracle mean-squared-error (MSE). The optimally-weighted filter is thus guaranteed to perform better than either of the component filters in terms of the MSE, at all noise levels. We also provide a fast algorithm for the weighted filtering. Visual and quantitative denoising results on standard test images are reported which demonstrate that the improvement over the original filter is significant both visually and in terms of PSNR. Moreover, the denoising performance of the optimally-weighted bilateral filter is competitive with the computation-intensive non-local means filter."
1996721,21239,9078,Face recovery in conference video streaming using robust principal component analysis,2011,"Irrecoverable data loss is inevitable for low-delay video conferencing over typical loss-prone networks such as the Internet. A semi-super-resolution (SSR) framework has been previously proposed to supply an additional low-resolution (LR) thumbnail to aid error concealment when the high-resolution (HR) image is lost. Super-resolution is an ill-posed problem, however, and previous block-search based SSR methods tend to produce discontinuities in output images, which can be objectionable, especially in human faces where the focus of a viewer usually lies. In this paper, we propose to recover a human face in a lost frame using the same SSR framework, but by operating on the entire face at a time. We leverage on a recent work called robust principal component analysis (RPCA), where the “salient” features (human face in our scenario) in a sequence of previous HR frames can be recovered despite the presence of gross but sparse errors. We propose and derive various improved methods to solve the SSR problem using RPCA. Beyond robust recovery of the human face, transformations of the face in previous HR frames are also deduced, so that the recovered face can be appropriately transformed in the lost frame for natural viewing. Experimental results show that our face-based approach gives much improved face recovery compared to previous SSR block searches."
1937846,21239,9078,Adaptive motion search with elastic diamond for MPEG-4 video coding,2001,"Video coding is a complex process, comprising a combination of spatial, temporal and statistical data reduction techniques. Of these techniques, motion estimation taking advantage of inter-frame information redundancy, plays the most vital role. The overwhelming complexity of motion estimation using a brute-force search has prompted researchers to propose a myriad of algorithms, yet finding the most efficient algorithm remains an open research problem. Recently, the MPEG-4 committee, after a rigorous evaluation, have recommended a motion estimation algorithm that performs very well in terms of speed and picture quality. We propose a motion estimation algorithm that is a combination of a number of novel ideas for finding more accurate motion vectors and with a faster speed. The proposed algorithm, named as adaptive motion search with elastic diamond (AMSED) algorithm, takes advantages of the correlation between motion vectors in both spatial and temporal domains, and uses special diamond shaped search patterns to accelerate motion search. The new algorithm achieves very close quality compared to the full search, but with several hundred times speedup. In term of speed, compared with the motion estimation technique recommended by the MPEG-4 committee, MVFAST (motion vector field adaptive fast search technique), AMSED can achieve more than 200% speedup."
1342166,21239,9078,Context-adaptive Pansharpening based on binary partition tree segmentation,2014,"Pansharpening is a successful application of data fusion to remotely sensed data. It aims at obtaining a detailed representation of an Earth's zone both in terms of spatial and spectral resolution. This is done through the fusion of a panchromatic and a multispectral image (having complementary spatial and spectral resolutions) that are acquired simultaneously by several optical satellites. The result of the fusion is commonly achieved by introducing the spatial details, modulated opportunely by gains, in the multispectral one. The injection gains can be estimated globally over the image, or locally, thus obtaining spatially variant values. The latter approach has been proven to achieve better results and it is based on windowing the analyzed image in squared blocks. In this paper we propose a more elaborated concept of locality, as it is based on an opportune segmentation of the target scene. In greater details, we propose to estimate the local injection gains on regions composed of pixel with similar spectral characteristic, as defined by a segmentation. Such local approach is compared to the global one and to the conventional local estimation based on overlapping and non-overlapping blocks. The performances have been assessed by using three real datasets, the first acquired by WorldView-2 and the other two by Pleiades. The analysis evidences the appreciable improvements of the performances with respect to classical schemes."
1320330,21239,9078,Learning-based automatic defect recognition with computed tomographic imaging,2013,"The use of image-based automatic defect recognition (ADR) systems in a production line often requires strict processing-time specifications. On the other hand, the typical high-performance requirement of such system calls for the use of sophisticated, computationally-complex algorithms. Addressing the conflicting requirements of fast throughput and high detection performance is a significant challenge. In this paper we present a 3D learning-based ADR approach for industrial parts. The proposed method first extracts defect candidate regions using morphological closing and template matching. Then a local registration-based approach is utilized to produce accurate defect segmentation mask. Finally, 29 features including geometric features and texture features derived from grey level co-occurrence matrix are calculated for each candidate region, and a fast random forests classifier is used to classify the candidate regions as defect or defect-free. This approach was developed into a fully automated system for detecting casting defects in aluminum industrial parts depicted in 3D Computed Tomographic (CT) images. The system was tested on 31 images with 49 cavities and porosities defects, achieving a sensitivity of 94% with an average 3.5 false detections per part."
2265439,21239,9078,Video transport in wireless ATM,1995,"Wireless ATM LANs have the potential to support multi-Mb/s bandwidths to mobile users with guaranteed quality of service. However, the lossy nature of the wireless medium will pose problems for loss-sensitive applications. Techniques to minimize the effect of these losses will therefore be required. In this paper, we examine the use of combined source and channel coding for MPEG video transport in a wireless ATM environment. Although forward error correction (FEC) provides protection against channel bit errors, the bandwidth overhead can become a significant drawback in a fixed bandwidth scenario. Additional protection against losses can be realized by using two-layer video coding. In this work, we compare the performance of a 1-layer main profile and 2-layer data partitioning and SNR scalable MPEG-2 encoders in a system with random channel errors and forward error correction. The results indicate that if the channel bit error rate is known an optimum FEC level can be chosen for the 1-layer case; However, at this fixed FEC level, if a critical bit error rate is exceeded then video quality degrades dramatically. The 2-layer cases appear to lead to more graceful degradation in quality at this critical bit error rates. In particular, SNR scalability may lead to better video quality over a larger range of bit error rates than a 1-layer approach."
770155,21239,9078,A pre-filtering approach to exploit decoupled prediction and transform block structures in video coding,2014,"Recent video coding techniques allow for decoupling of the transform block partition from that employed for prediction. For example, HEVC allows a transform block to overlap multiple prediction blocks. This paper is premised on the observation that in order to truly realize the potential of such enhanced flexibility, it is necessary to account for and mitigate considerable side effects due to stitching together independently predicted blocks, including the emergence of spurious high frequency components from sharp transitions across boundaries, which undermine the transform efficacy. The proposed solution involves an appropriately designed pre-filtering approach to mitigate boundary transition effects whenever a transform spans data from multiple prediction blocks. Moreover, this filtering technique enables extending the flexibility in decoupling prediction and transform structures, as various restrictions may now be eliminated. In particular, it makes it possible and beneficial to allow a transform block to span residual data from both inter and intra predicted blocks, whereas HEVC necessarily forces a single type of prediction in each coding unit. The method is further extended to include motion refinement that accounts for the pre-filtering approach. Experiments provide evidence for consistent coding gains over HEVC and VP9."
1257506,21239,9078,3D automatic anatomy segmentation based on graph cut-oriented active appearance models,2010,"In this paper, we propose a novel 3D automatic anatomy segmentation method based on the synergistic combination of active appearance models (AAM), live wire (LW) and graph cut (GC). The proposed method consists of three main parts: model building, initialization and segmentation. For the model building part, an AAM model is constructed and the LW cost function is trained. For the initialization part, an improved iterative model refinement algorithm is proposed for the AAM optimization, which synergistically combines the AAM and LW method (OAAM). And a multi-object strategy is applied to help the object initialization. A pseudo 3D initialization strategy is employed to segment the organs slice by slice via multi-object OAAM method. The model constraints are applied to the initialization result. For the segmentation part, the object shape information generated from the initialization step is integrated into the GC cost computation. And an iterative GCOAAM method is proposed for object delineation. This method is a general method and can be applied to any organ segmentation. The proposed method was tested on the clinical liver and kidney CT data sets. The results showed the following: (a) an overall segmentation accuracy of true positive fraction>93.5%, and false positive fraction<0.2% can be achieved. (b) The initialization performance is improved by combining the AAM and LW. (c) The multi-object strategy greatly helps the initialization due to inter-object constraints."
2313581,21239,9078,3D motion estimation for on-line MR temperature mapping,2005,"Magnetic resonance (MR) temperature mapping can be used to monitor temperature changes during minimally invasive thermal therapies during the procedure. Robust 3D estimation of organ displacement during the intervention is hardly feasible due to technical limitations (spatial and temporal resolution are not sufficient to perform classic 3D registration methods on anatomical images). However, organ displacements due to physiological activity (heart and respiration) may induce important artifacts on apparent temperature maps and prevent the treatment of a tumor with an external heating device. Recent development has allowed increasing external information for 3D motion estimation. This paper presents a new method that exploits image information for robust 3D motion estimation from MR images, in order to make possible the treatment of organs such as the kidney, and to improve the precision of temperature estimation using the proton resonance frequency (PRF) shift. The motion estimation described in this paper consists of two steps: a reference volume is initially computed in a preparation phase; during the intervention a 3D displacement vector is estimated on-line for each pixel of the acquired images by using this reference volume. This method can be applied to any type of image sequences and thus is not restricted to MR images."
2700510,21239,20411,An Optimization Framework for Remapping and Reweighting Noisy Relevance Labels,2016,"Relevance labels is the essential part of any learning to rank framework. The rapid development of crowdsourcing platforms led to a significant reduction of the cost of manual labeling. This makes it possible to collect very large sets of labeled documents to train a ranking algorithm. However, relevance labels acquired via crowdsourcing are typically coarse and noisy, so certain consensus models are used to measure the quality of labels and to reduce the noise. This noise is likely to affect a ranker trained on such labels, and, since none of the existing consensus models directly optimizes ranking quality, one has to apply some heuristics to utilize the output of a consensus model in a ranking algorithm, e.g., to use majority voting among workers to get consensus labels. The major goal of this paper is to unify existing approaches to consensus modeling and noise reduction within a learning to rank framework. Namely, we present a machine learning algorithm aimed at improving the performance of a ranker trained on a crowdsourced dataset by proper remapping of labels and reweighting of samples. In the experimental part, we use several characteristics of workers/labels extracted via various consensus models in order to learn the remapping and reweighting functions. Our experiments on a large-scale dataset demonstrate that we can significantly improve state-of-the-art machine-learning algorithms by incorporating our framework."
2411730,21239,9078,Wavelet denoising by recursive cycle spinning,2002,"Coupling the periodic time-invariance of the wavelet transform, with a view to thresholding as a projection, yields a simple, recursive, wavelet-based technique for denoising signals. Estimating a signal from a noise-corrupted observation is a fundamental problem of signal processing which has been addressed via many techniques. Previously, R.R. Coifman and D.L. Donoho (see Wavelets and Statistics, Lecture Notes in Statistics, vol.103, p.125-50, 1995) introduced cycle spinning, a technique of estimating the true signal as the linear average of individual estimates derived from wavelet-thresholded translated versions of the noisy signal. We demonstrate that such an average can be improved upon dramatically. The proposed algorithm recursively cycle spins by repeatedly translating and denoising the input via basic wavelet denoising and then translating back; at each iteration, the output of the previous iteration is used as input. Exploiting the convergence properties of projections, our algorithm can be regarded as a sequence of denoising projections that converge to the projection of the original noisy signal to a small subspace containing the true signal. It is proven that the algorithm is guaranteed to converge globally, and simulations on piecewise polynomial signals show marked improvement over both basic wavelet thresholding and standard cycle spinning."
1160629,21239,9078,Latent fingerprint persistence: A new temporal feature space for forensic trace evidence analysis,2014,"In forensic applications, traces are often hard to detect and segment from challenging substrates at crime scenes. In this paper, we propose to use the temporal domain of forensic signals as a novel feature space to provide additional information about a trace. In particular we introduce a degree of persistence measure and a protocol for its computation, allowing for a flexible extraction of time domain information based on different features and approximation techniques. At the example of latent fingerprints on semi-/porous surfaces and a CWL sensor, we show the potential of such approach to achieve an increased performance for the challenge of separating prints from background. Based on 36 earlier introduced spectral texture features, we achieve an increased separation performance (0.01 ≤ Δκ ≤ 0.13, respective 0.6% to 6.7%) when using the time domain signal instead of spatial segmentation. The test set consists of 60 different prints on photographic-, catalogue- and copy paper, acquired in a sequence of ten times. We observe a dependency on the used surface as well as the number of consecutive images and identify the accuracy and reproducibility of the capturing device as the main limitation, proposing additional steps for even higher performances in future work."
1047634,21239,9078,Computer-aided diagnostic system for prostate cancer detection and characterization combining learned dictionaries and supervised classification,2014,"This paper aims at presenting results of a computer-aided diagnostic (CAD) system for voxel based detection and characterization of prostate cancer in the peripheral zone based on multiparametric magnetic resonance (mp-MR) imaging. We propose an original scheme with the combination of a feature extraction step based on a sparse dictionary learning (DL) method and a supervised classification in order to discriminate normal {N}, normal but suspect {NS} tissues as well as different classes of cancer tissue whose aggressiveness is characterized by the Gleason score ranging from 6 {GL6} to 9 {GL9}. We compare the classification performance of two supervised methods, the linear support vector machine (SVM) and the logistic regression (LR) classifiers in a binary classification task. Classification performances were evaluated over an mp-MR image database of 35 patients where each voxel was labeled, based on a ground truth, by an expert radiologist. Results show that the proposed method in addition to being explicable thanks to the sparse representation of the voxels compares well (AUC>0.8) with recent state-of-the-art performances. Preliminary visual analysis of example patient cancer probability maps indicate that cancer probabilities tend to increase as a function of the Gleason score."
2003325,21239,9078,ESUR: A system for Events detection in SURveillance video,2010,"In this paper, we present our eSur (Event detection system on SURveillance video) system, which is derived from TRECVID'09 surveillance tasks. Currently, eSur attempts to detect two categories of events: 1) single-actor events (i.e., PersonRuns and ElevatorNoEntry) irrespective of any interaction between individuals, and 2) pair-activity events (i.e., PeopleMeet, PeopleSplitUp, and Embrace) involves more than one individual. eSur consists of three major stages, i.e., preprocessing, event classification, and post-processing. The preprocessing involves view classification, background subtraction, head-shoulder detection, human body detection and object tracking. Event classification fuses One-vs.-All SVM and rule-based classifiers to identify single-actor and pair-activity events in an ensemble way. To reduce false alarms, we introduce prior knowledge into the post-processing, and in particular, we apply a so-called event merging process over TRECVID dataset. Extensive experiments have been performed over TRECVid'08 and '09 ED data corpus involving in total 144 hours surveillance video of London Gatwick airport. According to the TRECVid-ED formal evaluation, our prototype has yielded fairly promising results over TRECVid'09 dataset, with top Act.DCR of 1.023, 1.025, 1.02, and 0.334 for PeopleMeet, PeopleSplitUp, Embrace, and ElevatorNoEntry, respectively."
2174789,21239,9078,secure media streaming & secure adaptation for non-scalable video,2004,"Two important capabilities in media streaming are (1) adapting the media for the time-varying available network bandwidth and diverse client capabilities, and (2) protecting the security of the media. Providing both end-to-end security and adapting at a (potentially untrusted) sender or mid-network node or proxy can be solved via a framework called secure scalable streaming (SSS) which provides the ability to transcode the content without requiring decryption. In addition, this enables secure transcoding to be performed in a R-D optimized manner. The original SSS work was performed for scalably coded media. This paper examines its potential application to non-scalable media. Specifically, we examine the problems of how to scale non-scalable H.264/MPEG-4 AVC video and how to do it securely. We first show, perhaps surprisingly, (hat the importance of different P-frames in a sequence can vary by two orders of magnitude. Then we propose two approaches for securely streaming and adapting encrypted H.264 video streams in an R-D optimized manner using (1) secure-media R-D hint tracks, and (2) secure scalable packets. While we can not scale the bit rate of encrypted non-scalable H.264 to the same extent possible for scalably coded media, our method does provide some scaling capability and more importantly provides 4-8 dB gain compared to conventional approaches."
1784778,21239,9078,Watermarking ancient documents schema using wavelet packets and convolutional code,2010,"The ancient documents have a major importance in the history of every people and every nation. These documents involve important information that many people need. As a consequence, it is necessary to preserve these documents in order to build a numerical library in the service of the public. Therefore, the necessity of digitizing these documents permits the simultaneous access to the same documents and provides the possibility of the reproduction of these documents existing most of the time in just one example. This task is considered as an important step in the research domain. In fact, many researches have been invested for the processing, compression, segmentation and indexation of these documents. Nevertheless, with a numerical form, there is a threat of hacking, stocking, copying, modifying and finally diffusing these documents in an illegal way without losing their quality. As a consequence, we face the problem of losing the intellectual property because of the lack of methods that concern the protection of data. In order to prevent these frauds, watermarking represents a promising method to protect these images. In this context, our work makes part of protecting ancient documents essentially. In this paper, we have proposed the method of watermarking ancient documents. This method is based on the Wavelet Packet Transform (WPT) and it provides a good robustness which can face different attacks like signal processing (noise, filter and compression) and noticeable signature invisibility."
1689894,21239,9078,Improving image similarity measures for image browsing and retrieval through latent space learning between images and long texts,2010,"The amount of multimedia data on personal devices and the Web is increasing daily. Image browsing and retrieval systems in a low-dimensional space have been widely studied to manage and view large numbers of images. It is essential for such systems to exploit an efficient similarity measure of the images when searching for them. Existing methods use the distance in a low-level image feature space as the similarity measure, and therefore, images with different content may be treated as similar images. In this paper, we propose a novel method to improve the similarity measures for images by considering the text surrounding the images. If there is text describing the images, similarities can be measured more effectively by taking into account the text streams. The proposed method improves the image similarity measures based on the latent semantics obtained from the combination of image and text. It should be noted that the text does not need to be clear tags; indeed, any generic Web text is applicable. Moreover, our method can effectively improve the similarities even if only a small portion of the images include textual descriptions. Additionally, the proposed method is scalable as it has linear computational complexity based on the number of images. In the experiments, we compare our method with previous methods using an original dataset in which a portion of the images are annotated by long text. We show that the proposed method can retrieve semantically similar images more precisely than existing methods."
562261,21239,9078,ML optimality of PDE-based segmentation algorithms,2001,"Summary form only given, as follows. Building on our image restoration and segmentation algorithms developed in Pollak et al. (2000), we present a very simple nonlinear diffusion equation and show its utility both for image segmentation and for the detection of abrupt changes in 1-D. We show that it may be interpreted as a variant of the Perona-Malik (1990) equation, as the steepest descent equation for the total variation (Bouman and Sauer 1991, Rudin et al. 1992), and-in 1-D-as a solver of a simple version of the Mumford-Shah (1985) problem. The analysis of our equation in 1-D reveals it to be an exact solver of certain maximum likelihood detection/estimation problems. The major advantage over other methods to solve these problems is O(N log N) computational complexity in one spatial dimension. Finally, we show our method to be a robust estimator (in the spirit of H-infinity estimation (Nagpal and Khargonekar 1991)) for a restricted class of 1-D problems. Experiments suggest that the 2-D version of our algorithm retains robustness properties of the 1-D version. Moreover, if only a binary segmentation is required, the computational complexity of our 2-D algorithm is still O(N log N). A remaining challenge is to extend our probabilistic analysis of the 1-D algorithm to 2-D, designing fast and optimal image segmentation algorithms."
2027030,21239,9078,Automatic Region Tracking for MR Glomerular Filtration Rate Analysis,2006,"Contrast-enhanced dynamic magnetic resonance imaging (MRI) acquisition is a common method to retrieve functional information from organs in the human body. Applied to the kidney, the observation of the signal evolution in the cortex of a MR-series gives access to the renal perfusion and filtration. The glomerular filtration rate (GFR) is in particular the most useful quantitative index of renal function. Since the rapid bolus passage hampers the use of gated sequences, fast sequences have to be employed to enable a data acquisition while free-breathing. As a result, the acquired data contains motion artifacts caused by the respiratory cycle, spontaneous movements and drifts which limit quantitative analysis of the data. Although these problems can in principle be addressed with motion correction algorithms applied in a post processing step, additional challenges arise from the fact that image amplitude changes not only due to motion but also due to the contrast change during bolus passage. This study proposes a 2D region tracking method for retrospective motion correction without sacrificing temporal resolution which addresses the latter point by a preparative learning phase."
2028190,21239,9078,Optimal MPEG-2 encoder design for low bit-rate HDTV digital broadcasting,2002,"MPEG-2 is an international standard specifying a normative bitstream syntax that decoders must recognize and that allows flexible selection of detailed coding parameters at the encoder. The compression performance is therefore largely dependent on the encoder design and the difference in coded picture quality is present even among encoders developed by different codec makers at identical bit-rates. Conventional MPEG-2 encoders were also designed for high picture quality transmission at an adequate bit-rate, and the overhead portion occupancy in a coded bitstream can be ignored in the final picture quality. However, the unexpected increase in overhead portion occupancy might cause fatal picture quality degradation at extremely low bitrates. To overcome this problem, we investigated an optimal design for an MPEG-2 encoder for use in HDTV digital terrestrial broadcasting where the available bit-rate is limited to less than 0.2 bit/pixel. In our study, advanced key technologies which have not been clearly recognized until now were introduced into possible optimization points, picture type selection macroblock coding mode selection and rate control, and a significant coding gain versus conventional encoding schemes was confirmed in coding experiments."
1859296,21239,9078,Color signal decomposition method using 3-D Gamut boundary of multi-primary display,2005,"Color signal decomposition method is to decompose the conventional three-primary colors (RGB) into the multi-primary control values of multi-primary display (MPD) under the constraints of tri-stimulus matching. For the satisfaction of tri-stimulus matching between input and output display systems, the MPD color signals have to be estimated from a device-independent color space such as CIEXYZ or CIELAB. However, as the target of MPD is to display moving-picture data, it is necessary to simplify the color space conversion between input and output systems and decomposition of the multi-primary control values. In this paper, color signal decomposition method for MPD is proposed using 3-dimensional look-up-table (3D-LUT) in linearized LAB space. The linearized LAB space satisfies the linearity and additivity in color space conversion and easily constructs the 3-D LUT considering lightness, chroma, and hue. For the reproduction of moving-picture data in MPD, the proposed method is based on 3-D LUT structure to reduce the complexity of hardware and processing time. First, 3-D LUT containing gamut boundary data of MPD is created to derive the multi-primary control values. The input images are transformed to linearized LAB values. Then, the corresponding color signals on the gamut boundary points, which have the same lightness and hue as the input point, are calculated. Also, the color signal for a point on the gray axis is calculated. Based on the gamut boundary points and a point on the gray axis, the MPD color signals for the input values can be obtained by interpolating between the gamut boundary points and a point on the gray axis. In particular, the neighboring gamut boundary points are used in the region of occurring abrupt signal change for a smooth change of hue. As a result, the proposed method guarantees computational efficiency and color signal continuity and requires less memory rather than conventional color decomposition methods."
2507046,21239,21056,3D mesh preview streaming,2013,"Publishers of 3D models online typically provide two ways to preview a model before the model is downloaded and viewed by the user: (i) by showing a set of thumbnail images of the 3D model taken from representative views (or keyviews); (ii) by showing a video of the 3D model as viewed from a moving virtual camera along a path determined by the content provider. We propose a third approach called preview streaming for mesh-based 3D object: by streaming and showing parts of the mesh surfaces visible along the virtual camera path. This paper focuses on the preview streaming architecture and framework, and presents our investigation into how such a system would best handle network congestion effectively. We study three basic methods: (a)  stop-and-wait , where the camera pauses until sufficient data is buffered; (b)  reduce-speed , where the camera slows down in accordance to reduce network bandwidth; and (c)  reduce-quality , where the camera continues to move at the same speed but fewer vertices are sent and displayed, leading to lower mesh quality. We further propose a keyview-aware method that trades off mesh quality and camera speed appropriately depending on how close the current view is to the keyviews. A user study reveals that our keyview-aware method is preferred over the basic methods."
2181747,21239,9078,A robust RGB-D SLAM system for 3D environment with planar surfaces,2013,"With the increasing popularity of RGB-depth (RGB-D) sensors such as the Microsoft Kinect, there have been much research on capturing and reconstructing 3D environments using a movable RGB-D sensor. The key process behind these kinds of simultaneous location and mapping (SLAM) systems is the iterative closest point or ICP algorithm, which is an iterative algorithm that can estimate the rigid movement of the camera based on the captured 3D point clouds. While ICP is a well-studied algorithm, it is problematic when it is used in scanning large planar regions such as wall surfaces in a room. The lack of depth variations on planar surfaces makes the global alignment an ill-conditioned problem. In this paper, we present a novel approach for registering 3D point clouds by combining both color and depth information. Instead of directly searching for point correspondences among 3D data, the proposed method first extracts features from the RGB images, and then back-projects the features to the 3D space to identify more reliable correspondences. These color correspondences form the initial input to the ICP procedure which then proceeds to refine the alignment. Experimental results show that our proposed approach can achieve better accuracy than existing SLAMs in reconstructing indoor environments with large planar surfaces."
991389,21239,9078,Residue role assignment based transform partition predetermination on HEVC,2013,"The transform unit (TU), combining with the rate-distortion optimized quantization (RDOQ), as a part of the important features of HEVC brings much performance gain by adding more computation burden to the encoder. Different from the previous standards, the transform and quantization (T&Q) lead into a quadtree structure which requires to be split and calculated recursively for one coding unit (CU). The complexity issues limit the development of both software and hardware video code engine. To solve this problem, this paper proposes a residue role assignment based transform partition predetermination method by introducing the all zero block detection principle. The proposal generates from the observation that the rate-distortion optimized TU partitions generally depend on the distribution of the low frequency residue blocks. 3 different roles are assigned to the residue blocks based on 3 level of residue frequency value. Each role of residue relates to a concrete T&Q calculation. Therefore, instead of traversal calculating of full T&Q size and T&Q depth, the residue blocks are categorized previously and only the corresponding T&Q are calculated. The evaluation results state that the proposal can achieve 12.8% and 26.3% total encoding time reduction with only 1.00% and 0.80% BD bit-rate loss in average with both RDOQ off and on cases."
664049,21239,9078,Feature coincidence trees for registration of ultrasound breast images,2001,"Registration of an image, the query or reference, to a database of rotated and translated exemplars constitutes an important image retrieval and indexing application which arises in biomedical imaging, digital libraries, georegistration, and other areas. Two important issues are the specification of a class of discriminatory and generalizable image features and determination of an appropriate image-dissimilarity measure to rank the closeness of the query image with respect to images in the database. The theoretically best set of features and dissimilarity measure are those which can be implemented with the lowest misregistration error rate. We study a method based on feature discrimination using feature coincidence trees and mutual /spl alpha/-information measures of feature correlation. Feature coincidence trees represent the commonality between pairs of images using joint histograms of many simple features, or tags, which are organized in a data structure similar to that of Y. Amit and D. Geman's randomized trees for shape recognition (see Neural Computation, vol.9, p.1545-88, 1997). The mutual alpha-information measure is a ranking discriminant applied to the joint histograms which is motivated by a large deviations framework for detection error rates. We illustrate the methodology in the context of registering ultrasound scans of human breast images."
2276155,21239,21056,ALD: adaptive layer distribution for scalable video,2013,"Bandwidth constriction and datagram loss are prominent issues that affect the perceived quality of streaming video over lossy networks, such as wireless. The use of layered video coding seems attractive as a means to alleviate these issues, but its adoption has been held back in large part by the inherent priority assigned to the critical lower layers and the consequences for quality that result from their loss. The proposed use of forward error correction ( FEC ) as a solution only further burdens the bandwidth availability and can negate the perceived benefits of increased stream quality.   In this paper, we propose Adaptive Layer Distribution ( ALD ) as a novel scalable media delivery technique that optimises the tradeoff between the streaming bandwidth and error resiliency. ALD is based on the principle of layer distribution, in which the critical stream data is spread amongst all datagrams thus lessening the impact on quality due to network losses. Additionally, ALD provides a parameterised mechanism for dynamic adaptation of the scalable video, while providing increased resilience to the highest quality layers. Our experimental results show that ALD improves the perceived quality and also reduces the bandwidth demand by up to 36% in comparison to the well-known Multiple Description Coding ( MDC ) technique."
899124,21239,9078,Singularity Preserving Fingerprint Image Adaptive Filtering,2006,"Accurate and reliable detection of minutiae from the fingerprint images is an important factor in the performance of automatic fingerprint identification systems (AFIS). Fingerprint image quality evaluation and appropriate enhancement technique are critical steps for accuracy of minutiae detection algorithm. Most fingerprint enhancement algorithms rely heavily on local orientation of ridge flows. However, significant orientation changes occur around the delta and core points in the fingerprint images, and this poses a challenge to the enhancement of ridge flows in those high-curvature regions. Instead of identifying the singular points, we calculate an orientation coherence map and determine minimum coherence regions as high-curvature areas. Gaussian filter window sizes are adaptively chosen to smooth the local orientation map. Because the smoothing operation is applied to local ridge shape structures, it efficiently joins broken ridges without destroying essential singularities and enforces continuity of directional fields even in creases. To the best the authors' knowledge, the coherence has not previously been used to estimate ridge curvature, and curvature has not been used to select filter scale in this field. These two strategies are the primary contributions of this paper. Experimental results demonstrate the effectiveness of the proposed method."
302312,21239,22035,Perceptual video quality estimation by regression with myopic experts,2015,"Objective video quality metrics can be viewed as myopic expert systems that focus on particular aspects of visual information in video, such as image edges or motion parameters. We conjecture that the combination of many such high-level metrics leads to statistically-significant improvement in the prediction of reference-based perceptual video quality in comparison to each individual metric. To examine this hypothesis in a systematic and rigorous manner, we use: (i) the LIVE and the EPFL/PoliMi databases that provide the difference mean opinion scores (DMOS) for several video sequences under encoding and packet-loss errors; (ii) ten well-known metrics that range from mean-squared error based criteria to sophisticated visual quality estimators; (iii) five variants of regression-based supervised learning. For 400 experimental trials with random (non-overlapping) estimation and prediction subsets taken from both databases, we show that the best of our regression methods: (i) leads to statistically-significant improvement against the best individual metrics for DMOS prediction for more than 97% of the experimental trials; (ii) is statistically-equivalent to the performance of humans rating the video quality for 36.75% of the experiments with the EPFL/PoliMi database. On the contrary, no single metric achieves such statistical equivalence to human raters in any of the experimental trials."
591539,21239,8494,A novel DCT-based bit plane error resilient entropy coding scheme and codec for wireless image communication,2002,"We propose a novel bit plane error resilient entropy coding scheme for DCT-based image compression, which can control and minimize the error propagation effect. The compressed rate is similar to the JPEG standard. However, it uses only 18 VLC symbols. Hardware implementation cost and power consumption can then be minimized. Base on simulation results, the proposed coding scheme can achieve high image quality (PSNR=29.82 dB) even at bit error rate of 10/sup -3/. An image codec has been implemented for verifying the proposed bit-plane EREC coding technique. It can compress and decompress CIF size (352/spl times/288, 4:2:0 format) images at the rate of 30 frames per second using 20 MHz clock rate. It only occupies 36 k gate count and 1.90/spl times/1.90 mm/sup 2/ silicon area in a 0.35 /spl mu/m CMOS process. With 3.3 V power supply, the simulated power consumption is only 27 mWatt and 0.41 mA/MHz. This performance can meet various wireless portable multimedia system requirements."
1636989,21239,9078,Randomized texture flow estimation using visual similarity,2014,"Exploring underlying texture flows defined with orientation and scale is of a great interest on a variety of vision-related tasks. However, existing methods often fail to capture accurate flows due to over-parameterization of texture deformation or employ a costly global optimization which makes the algorithm computationally demanding. In this paper, we address this inverse problem by casting it as a randomized correspondence search along with a locally-adaptive vector field smoothing. When a small example patch is given as a reference, a randomized deformable matching is performed on the very densely quantized label space, enabling an efficient estimation of texture deformation without quality degeneration, e.g., due to quantization artifacts which often appear in the optimization-driven discrete approaches. The visual similarity with respect to the deformation parameters is directly measured with an input texture image on an appearance space. The locally-adaptive smoothing is then applied to the intermediate flow field, resulting in a good continuation of the resultant texture flow. Experimental results on both synthetic and natural images show that the proposed method improves the performance in terms of both runtime efficiency and/or visual quality, compared to the existing methods."
1411270,21239,9078,Robust video fingerprinting via structural graphical models,2012,"Applications of video fingerprinting range from traditional video retrieval and authentication to the more recent problem of anti-piracy search brought about by the emergence of video websites such as Youtube. Video fingerprints offer the potential of identifying in a robust and scalable manner - illegal or undesirable uploads of copyrighted video content. The principal challenge in video fingerprinting is to extract reduced dimensionality descriptors that can withstand incidental spatial and temporal distortions to the video while still allowing the discrimination of distinct videos. To address this fundamental problem, we propose to first represent a video as a graphical structure which can encode temporal relationships between video shots that are crucial to uniquely identifying the video. Next, we leverage ideas from graph theory, namely the normalized cuts graph partitioning method to divide the video representation into sub-graphs. Robust dimensionality reduction applied to these sub-graphs yields the final video hash/fingerprint. Experimental results in the form of receiver operating characteristic (ROC) curves on video databases acquired from YouTube reveal that the proposed video fingerprinting can enable a much more favorable robustness vs. discriminability trade-off over state-of-the art algorithms in video hashing."
856646,21239,9078,Directional intra frame interpolation for HEVC compressed video,2014,"Image interpolation is one of the most elementary imaging research topics. A number of image interpolation methods have been developed and tested on uncompressed images in the literature. However, a lot of videos have already been stored or have to be transmitted in compressed format due to the storage limitation or the bandwidth limitation. The existed image interpolation methods may not be efficient when directly applied to compressed images or videos. Inspired by the success of the intra prediction in HEVC and the edge-directed image interpolation methods, a directional intra frame interpolation for HEVC compressed video is proposed. The main idea is to use the directional prediction information in compressed low-resolution video bitstreams to estimate the associated high-resolution video. For intra frames, the prediction direction information is taken into account as context in the directional interpolation. When a pixel is decompressed with a small prediction residual, the interpolation is performed along its block direction. The interpolation weight for each block direction is off-line trained by the Wiener filter based on the representative video sequences. For each pixel with a large prediction residual, a piecewise autoregressive model is used as a regularization term into the interpolation function. Extensive experiments demonstrate that the proposed method achieves better performance than the traditional methods such as Bicubic, KR, LAZA, NEDI and SAI."
2346335,21239,9078,A fast structural matching and its application to pattern analysis of 2-D electrophoresis images,1998,"Image pattern matching is essential and important in such applications as registration, stereo-matching and model-based recognition. Although a variety of pattern matching algorithms have been proposed so far, almost all of them are time-consuming and sensitive to geometrical distortion of an image. Thus, from the practical point of view, we have investigated a fast and reliable structural matching algorithm. Our approach to pattern matching deals with the correspondence of two kinds of structured graphs, that is, the Delaunay net and relative neighbourhood graph with attributes, rather than two sets of points. The algorithm estimates a matching merit and uses thresholds to cut off the redundant path and therefore perform a fast matching. Then, graph traverse is carried out by using not only real arcs of the relative neighbourhood graph but also virtual arcs which are virtually extended to the look-ahead node in order to deal with geometrical distortion and thus improve the reliability of matching. We apply the algorithm to the pattern matching problem of 2-D gel electrophoresis images often appearing in the area of biochemistry. As a result, it takes about ten minutes to detect thousands of spots and compare them with those on a reference pattern even with a cheaper DOS/V computer running with Linux, while it takes about two or three hours by visual inspection."
346976,21239,9078,Inverting input scanner vibration errors,1995,"Images scanned in the presence of mechanical vibrations are subject to artifacts such as brightness fluctuation and geometric warping. The goal of this work is to develop an algorithm to invert these distortions and produce an output digital image consistent with a scanner operating under ideal uniform motion conditions. The image restoration algorithm described in this paper applies to typical office scanners that employ a moving linear sensor array (LSA) or moving optics. The velocity of the components is generally not constant in time. Dynamic errors are introduced by gears, timing belts, motors, and structural vibrations. In this work, we make use of the instantaneous LSA velocity to reconstruct an underlying piecewise constant or piecewise linear model of the image irradiance function. The control points for the underlying model are obtained by solving a system of equations derived to relate the observed area samples with the instantaneous LSA velocity and a spatially-varying sampling kernel. An efficient solution exists for the narrow band diagonal matrix that results. The control points computed with this method fully define the underlying irradiance function. That function is then suitable for resampling under ideal scanning conditions to produce a restored image."
1209998,21239,9078,Mapping data on a rotated grid in high-dimensions for lossless compression,2011,"Interactive navigation of large high-dimensional media datasets aims at allowing viewers to freely navigate content, selecting a subset of the high-dimensional visual data of interest for display. An example application would be remote visualization of an arbitrary 2-D planar cut from a large volumetric dataset with random access. In our previous work, we proposed a server-client based data representation and retrieval system using overlapping rotated tiles to represent the dataset, which lower the bandwidth required for accessing a random plane from large volume data. This leads to the question of how best to represent these rotated tiles for compression. In this paper we present a non-interpolated symmetric mapping algorithm, which maps each voxel in the original image to a rotated Cartesian grid point. We will show that this approach outperforms tile representation methods based on interpolation and non-symmetric mapping. In particular, the lack of interpolation means that complexity is significantly lower. Moreover, especially at high rates, remapping without interpolation will be shown to lead to overall better RD performance and the more symmetric the mapping is, the better RD performance will be achieved. Furthermore, a metric is proposed for automatically checking the mapping symmetry and measuring the percentage of the non-symmetric mapped points in the non-symmetric mapping algorithms."
2162526,21239,9078,Generalized ELL for detecting and tracking through illumination model changes,2008,"In previous work, we developed the Illum-PF-MT, which is the PF-MT idea applied to the problem of tracking temporally and spatially varying illumination change. In many practical problems, the rate at which illumination changes varies over time. For e.g. when a car transitions from shadow to sunlight or vice-versa the rate of illumination change is much higher than when it is in shadow or in sunlight. One way to model illumination change in such problems is using a Gaussian random walk model with two values of the change covariance - a large covariance when a transition is detected and a much smaller one when no transition is detected. But to use such a model, one needs to first detect the transition. The transition is a natural one and so it happens gradually (unlike a sudden manual dimming of the light in the room) and thus existing change detection statistics which are designed only for sudden changes are unable to detect the transition. In this paper, we propose to use the recently proposed generalized ELL (gELL) idea which uses the tracked part of the change to detect it and hence detects such partially trackable changes very quickly. Since gELL detects much before loss of track occurs, one is able to transition to the transition model and back without ever losing track. Also, for the first time, we demonstrate the use of gELL in combination with the PF-MT algorithm which is more stable to model change than the original PF."
402317,21239,9078,Tagged cardiac MRI: detection of myocardial boundaries by texture analysis,2003,"The noninvasive evaluation of the cardiac function presents a great interest for the diagnosis of cardiovascular diseases. Cardiac tagged MRI allows the measurement of anatomical and functional myocardial parameters. This protocol generates a dark grid which is deformed with the myocardium. Tracking the grid allows the displacement estimation in the myocardium. The work described in this paper aims to automate the myocardial contours detection and the following of the grids of tags on short-axis and long-axis time sequences, in order to firstly optimize the 3D+T study of the parietal contractions and secondly make possible its clinical use. The method we have developed for endocardial and epicardial contours detection is based on the use of texture analysis and active contours models. Texture analysis allows us to define energy maps more efficient than those usually used in active contours methods where attractor is often based on gradient and which were useless in our case of study. The follow-up of the grid of tags that we have implemented is based on a grid of active contours (B-snakes) which part of the energy is calculated in the Fourier's domain. The results obtained with our method is fully automatic and correct on short-axis as well as on long-axis sequences, when previous works on cardiac tagged MR images analysis always used manual contours detection."
2885256,21239,9078,Scale-adaptive EigenEye for fast eye detection in wild web images,2016,"Detecting eyes in images is fundamental for many computer vision applications including face detection, face recognition, and human-computer interaction. Most existing methods are designed and tested on datasets acquired under controlled lab settings (e.g., fixed scale, known poses, clean background, etc.), leaving their performance to be further examined on real-world, uncontrolled images, such as on-line images. This paper presents an effort on developing a fast and accurate eye detector for on-line images for which the acquisition condition is unknown and varies from one image to another, resulting in unpredictable background and variable scales for the eyes/faces. The key idea is to develop a scale-adaptive EigenEye approach, which employs an approximate scale estimated from face detection to modulate the pre-trained EigenEye basis in searching for the best match in a test image. The effort also includes building a 2845-image dataset with accurately-annotated eye locations and size, which will be made public to the community for future comparative study. Evaluation using this dataset, with comparison with a few leading state-of-the-art approaches, demonstrates the advantages of the proposed method."
1788671,21239,9078,Image coding based on zero-crossing and energy information,1994,"The purpose of this paper is twofold first to present two other variants of the algorithm shown by Basso, Geurtz, and Kunt (see SPIE-VCIP 93, volume SPIE 2094-I, p.976-986, Boston, USA, Nov. 8-11, 1993) that we call data restoring and error distribution algorithms. Second to discuss a new coding philosophy, based on modelization of the sign of 2D bandpass signals. The data restoring algorithm improves the reconstruction quality and convergence speed while the error distribution one can convergence faster with an acceptable reconstruction quality. In the proposed system a bandpass 2D signal is viewed as a baseband signal that is modulating a carrier. The great part of the information relative to the carrier component is known a priori. Furthermore it constitutes a major part of the sign information of the 2D signal. The method models the sign of carrier component and subtracts it from the sign of the original signal. This technique is applied in a subband based still image coding system. The image is decomposed in subbands that are modeled in terms of the carrier and modulation components. For each subband the sign of logical XOR between a carrier refined model and the original subband is coded, together with the carrier model parameters and refinement parameters. Interesting coding results have been obtained. >"
1523091,21239,9078,Re-ranking using compression-based distance measure for Content-based Commercial Product Image Retrieval,2012,"With the prevalence of E-Commerce sites such as eBay, Content-based Commercial Product Image Retrieval (CBCPIR) has become an emerging application-oriented field of Content-based Image Retrieval (CBIR). Though a number of traditional CBIR techniques and evaluation criterions have been applied directly or with minor modifications, they tend to neglect one critical factor that greatly affects user experience: users usually care about the exact ranks of the results, especially few top ones, which should share very high similarity with the query image. In this work, we propose a novel two-stage retrieval framework that uses a compression-based re-ranking method and a new subjective retrieval evaluation criterion to address such a problem. More specifically, we extend the state-of-art texture descriptor Campana-Keogh (CK) method from data mining in several aspects and validate the superiority of our framework via extensive experiments and real-world user feedback. We also make our code and CBCPIR dataset publicly available. The number of images of the latter is much larger than current freely accessible ones and better represents real-world commercial product images."
2464364,21239,9078,Multi-level pixel difference classification methods,1995,"Block matching motion estimation algorithms are useful in many video applications such as the block-based video coding scheme employed in MPEG1/2. A single-chip implementation of a motion estimator (ME) for high quality video compression domains has been the goal of many ongoing research projects. There are several complementary directions along which we can reduce hardware complexity, for example, (1) reduction of search points, and (2) simplification of criterion functions. The last category is what this paper focuses on. We study the algorithmic and architectural potentials of the pixel difference classification (PDC) method and propose a generalisation called multi-level PDC (MPDC). The goal is to examine different hardware-complexity vs performance trade-offs. Moreover, we identify a subset of MPDC, the bit-truncation (BT) method which has the most potential for hardware saving. Experimental results show that it offers attractive trade-offs. Under fixed bit rate constraints, it gives picture quality degradation of less than 0.5 dB, which is non-perceivable, for up to 6-bit truncation. BT results in no complicated data or control flows. Hence the consequent hardware reduction is straightforward. The estimated overall encoder hardware saving ranges from 12% to 35% for 6-bit truncation."
692859,21239,9078,Video multicast over fair queueing networks,2000,"We consider the problem of video multicast over networks that enforce fair bandwidth allocations in the routers. State-of-the-art layered IP multicast systems perform well for moderately sized sessions, but suffer from three basic problems that prevent their massive deployment: (a) lack of fairness among different multicast sessions, (b) lack of fairness to competing TCP flows, and (c) high complexity requirements to scale up to potentially large numbers of users and sessions. In this work we present an entirely different approach to the design of these systems, in which sources obliviously inject packets into the network (disregarding congestion), whereas routers obliviously drop the amount of bandwidth that exceeds the fair share of a flow (disregarding packet contents). We model our system as a broadcast channel, with receivers connected to the source via erasure channels of different capacities, and we design a video coder to operate in this environment. Our results suggest that a combination of fair queueing routers and appropriate coding is indeed able to overcome certain drawbacks of current IP multicast systems. Furthermore, we also find that such systems would benefit significantly from being able to re-encode the video signal at internal nodes of the multicast tree."
2696033,21239,9078,SSSC-AM: A unified framework for video co-segmentation by structured sparse subspace clustering with appearance and motion features,2016,"Video co-segmentation typically refers to the task to jointly segment common objects existing in a given group of videos. In practice, high-dimensional data such as videos are often conceptually thought of being drawn from a union of subspaces corresponding to multiple categories. Therefore, segmenting data into respective subspaces, known as subspace clustering, has widespread applications in computer vision, including co-segmentation. State-of-the-art methods via subspace clustering seek to solve the problem in two steps: learning an affinity matrix, followed by applying spectral clustering to the affinity matrix. However, it is insufficient to obtain an optimal solution since it does not take into account the interdependence of the affinity matrix and the segmentation. In this paper, we present a new unified video co-segmentation framework inspired by Structured Sparse Subspace Clustering (S 3 C), which yields more consistent segmentation results. In order to improve the detectability of motion features with missing trajectories, we add an extra signature to motion trajectories. Moreover, we reformulate the S 3 C algorithm by adding the affine subspace constraint in order to make it more suitable to segment rigid motions lying in affine subspaces of dimension at most 3. Experiments on MOViCS dataset demonstrate the effectiveness of our approaches and robustness with heavy noise."
2856763,21239,9078,A dynamic motion vector referencing scheme for video coding,2016,"Video codecs exploit temporal redundancy in video signals, through the use of motion compensated prediction, to achieve superior compression performance. The coding of motion vectors takes a large portion of the total rate cost. Prior research utilizes the spatial and temporal correlation of the motion field to improve the coding efficiency of the motion information. It typically constructs a candidate pool composed of a fixed number of reference motion vectors and allows the codec to select and reuse the one that best approximates the motion of the current block. This largely disconnects the entropy coding process from the block's motion information, and throws out any information related to motion consistency, leading to sub-optimal coding performance. An alternative motion vector referencing scheme is proposed in this work to fully accommodate the dynamic nature of the motion field. It adaptively extends or shortens the candidate list according to the actual number of available reference motion vectors. The associated probability model accounts for the likelihood that an individual motion vector candidate is used. A complementary motion vector candidate ranking system is also presented here. It is experimentally shown that the proposed scheme achieves about 1.6% compression performance gains on a wide range of test clips."
230859,21239,9078,Cross-layer adaptive video coding to reduce energy on general-purpose processors,2003,"Traditionally, video encoders have been designed assuming that the more redundancy is removed, the better the encoder. However, on current laptops, reducing the compression efficiency of the video encoder by reducing the number of instructions used to perform compression can actually reduce the total energy used to encode and transmit a sequence. The correct balance between computation and compression efficiency may change dynamically, motivating adaptive encoders. At the same time, recent general-purpose processors also employ energy-driven adaptations. For best gains, the adaptations in the hardware and application layers must be coordinated. From a system design viewpoint, this coordination must happen through minimal, well-defined interfaces. This paper develops (1) an adaptive video encoder for general-purpose processors that trades computational complexity for compression efficiency to minimize total system energy, and (2) a method for determining the best configuration for such an encoder when running on a processor that is also adaptive. Our adaptive processor employs recent energy saving techniques of dynamic voltage and frequency scaling and architectural adaptation. Using a detailed simulator, we show that our cross-layer adaptive application algorithm reduces energy significantly, when employed on a fixed or adaptive processor."
2279999,21239,9078,Landmines recognition system using thermovision techniques,2009,"Sub-surface and buried landmines, with the surrounding environment constitute a complex system with variable characteristics. Infrared thermography techniques are attractive candidates for this kind of applications. They can be used from a considerable standoff distance to provide information on several mine properties, and they can also rapidly survey large areas. This paper presents a robust method for landmine detection and recognition. It uses the mean-shift algorithm to segment the acquired infrared image. The segmented image retains pixels associated with mines together with background clutters. To determine which pixels represent the mines, a second phase of segmentation is applied to the output of the mean-shift algorithm by using a self-organizing maps (SOM) algorithm. Depending on the resulted cluster intensity variations, the chips extracted from the segmented image are processed to extract mine signatures. After that, the extracted signatures are scanned horizontally, vertically, and diagonally to build a cluster intensity variation profile. This profile is statistically compared with the known mine signature profiles v. The proposed system is applied on series of time variant mid-wave infrared images (MWIR), and the test result show that the system can effectively recognize the mines with low false alarm rate."
1866973,21239,9078,Optimized frame structure using distributed source coding for interactive multiview video streaming,2009,"While multiview video coding typically focuses on the rate-distortion performance of compressing all frames of all views, we address the problem of designing a pre-encoded frame structure for a streaming server to enable a new functionality—interactive multiview switching, where a streaming client can send requests periodically to a server to switch to different views while continuing uninterrupted temporal playback of streaming video. We observe that providing bandwidth-efficient interactive view switching usually comes at the price of additional overall storage. Thus, our goal is to find a frame structure that minimizes the expected transmission rate during interactive multiview streaming, subject to a storage constraint. Noting that standard tools for random access (i.e., I-frame insertion) can be bandwidth-inefficient for this functionality, we propose to automatically generate a structure, combining I-frames, redundant P-frames and Distributed Source Coded (DSC) frames, in a near-optimal fashion to facilitate view switching. We present three new DSC techniques for view switching and discuss how these techniques can be integrated into an optimization framework. We show experimentally that near-optimal coding structures using DSC frames, in addition to I- and P-frames, reduce transmission cost over structures using I-frames only for view switching by up to 28%, and over structures using I- and P-frames only by up to 20% for the same storage cost."
1915301,21239,9078,Motion Correction Strategies for Interventional Angiography Images: A Comparative Approach,2007,"Digital subtraction angiography (DSA) is an important tool in interventional procedures and enables the surgeon to visualize the blood vessels in the projection X-ray images. Due to the motion of patient as well as motion of internal tissues constituting the background anatomy of the patient, the difference images may contain motion artifacts. The artifacts due to motion may be severe enough to make the visualization useless or erroneous and the images need to be motion compensated. Image registration is used for motion correction of DSA images such that the background mask image is placed in coordinates of the blood vessel enhanced image. The background structures are aligned as a result of image registration and are therefore removed from the subtraction image. There exist a large number of image registration techniques depending upon the application and the available information. In this paper, we compare two intensity based image registration techniques for motion correction of DSA images using signal-to-noise ratio as the evaluation metric. The methods discussed are: inverse consistent linear elastic image registration using B-splines and modified demons method, using a hierarchical strategy that focuses on region of intensity differences, like HAMMER. Both methods derive the driving function from image intensities, but impose different kind of constraints."
769096,21239,9078,A fast intra-frame prediction algorithm for MPEG-2/H.264 video transcoders,2005,"The H.264 standard, jointly developed by the ITU-T and the MPEG committees, is highly efficient offering perceptually equivalent quality video at about 1/3 to 1/2 of the bitrates offered by the MPEG-2 format. These significant bandwidth savings open the market to new products and services, including HDTV services at lower bitrates. It is expected that the H.264/AVC takes over the digital video market, replacing the use of MPEG-2 in most digital video applications. The complete migration to the new video-coding algorithm takes several years given the wide scale use of MPEG-2 in the market place today. This creates an important need for transcoding technologies for converting the large volume of existent video material from the MPEG-2 into the H.264 format and vice versa. However, given the significant differences between the MPEG-2 and the H.264 encoding algorithms, the transcoding process of such systems is much more complex to other heterogeneous video transcoding processes. In this paper, we introduce and evaluate a novel fast intra-frame prediction algorithm to be used as part of a high-efficient MPEG-2 to H.264 transcoder. Our results show that the proposed algorithm is able to maintain a good picture quality while considerably reducing the number of operations to be performed."
1169948,21239,9078,Motion-blur-free exposure fusion,2010,"We present a novel approach to HDR (high-dynamic-range) image fusion that copes with image blur degradation often present in long-exposed images. The proposed approach can deal with both camera and object motion blur in a computationally efficient manner suitable for implementation on a mobile device. The main idea is to exploit the differences between the image degradations that affect images captured for HDR fusion. Short-exposed images are mainly affected by sensor noise and less affected by motion blur, whereas longer exposed images are less noisy but potentially blurry due to motion during their exposure. Our approach consists of two steps. First we calculate an HDR representation of the scene by applying a typical HDR fusion approach. This representation could be blurry if some of the input images with long exposure time are blurry. We then fuse the HDR result with a photometrically modified version of the image with the shortest exposure time, which allows us to retain the sharpness of the short-exposed image and the noise-free characteristics of the HDR-fused image. The method does not assume an invariant blur PSF over any of the input images, and hence it can solve both for local and global blur, due to object or camera motion, respectively. We demonstrate the algorithm through a series of experiments and comparisons on natural images."
2064336,21239,9078,Rate-distortion optimized delivery of JPEG2000 compressed video with hierarchical motion side information,2008,"Streaming video as a sequence of JPEG2000 images provides the scalability, flexibility, and accessibility at a wide range of bit-rates that is lacking from the current motion-compensated predictive video coding standards; however, streaming this sequence requires considerably more bandwidth. The authors have recently proposed a novel approach that reduces the required bandwidth; this approach uses motion compensation and conditional replenishment of the JPEG2000 code-blocks, aided by server-optimized selection of these code-blocks. This work extends the previous work to the case of hierarchical arrangement of frames, similar to the hierarchical B-frames of the SVC scalable video coding extension of the H.264/AVC standard. We employ a Lagrangian-style rate-distortion optimization procedure to the server transmission problem and compare the performance to that of streaming individual frames and also to that of predictive video coding. The proposed approach can serve a diverse range of client requirements and can adapt immediately to interactive changes in client interests, such as forward or backward playback and zooming into individual frames. This paper introduces the concepts, formulates the optimization problem, proposes a solution, and compares the performance to alternate strategies."
1264354,21239,9078,Frequency-based local content adaptive filtering algorithm for automated photoreceptor cell density quantification,2012,"Photoreceptor cells in the human eye play a vital role in vision. Certain retinal diseases cause the photoreceptor cells to degenerate and may lead to vision loss. Quantification of photoreceptor cell density from adaptive optics (AO) retinal images can provide valuable information and aid in the screening, diagnosis, and follow-up of retinal diseases. In this paper we describe an image model using a windowed two-dimensional (2D) lattice of pulses representing the cells and characterize the frequency content as decaying frequency domain pulses on the reciprocal lattice. Based on this model we propose a novel method for detection of cone photoreceptor cells by analyzing the discrete-space Fourier transform (DSFT) of AO retinal images. This method uses a small-extent block-based 2D discrete Fourier transform (DFT) to determine cell frequency content in order to obtain parameters of an adaptive circularly symmetric band-pass filter that is applied to the image. The filter extracts the underlying cellular structure and removes high-frequency noise as well as very low frequency contamination manifested as slow variations in the image. Subsequent detection yields an automated cell count that compares well with actual and manual counts on test and retinal images and demonstrates the accuracy of the method."
1089898,21239,9078,A new image super resolution by texture transfer,2014,"This paper proposes the new super resolution algorithm for improving the texture of low-quality image. Conventional methods have restored the high-resolution image based on the linear blur model with a variety of priors, which boost the frequency powers of the images while preventing from being noisy or having jagged edges. Previous methods show good performance for restoring the sharp and jagged-free edges. However, in the point of texture, they often fail to restore and result in cartoon-like images. To solve this problem, proposed method focuses on the property of natural textures and tries to create the fine texture from self-image by texture transfer, which modulates the phase of the high-frequency components of image in spatial and frequency domains. The sign of DCT coefficients are modulated to change the phase of image in frequency domain, and the high-frequency components of image are reshaped with an auto regressive filter in spatial domain. Moreover, the external noise sources are added to enhance the pixel-precision textures. Experimental results show that the texture of low-quality images is highly improved than that of previous methods. The fine textures created by proposed method give more natural feeling of highresolution image than that of results restored without a consideration of restoring textures."
2491432,21239,9078,Partial Volume Estimation and the Fuzzy C-means Algorithm,1998,"Partial volume averaging (PVA) is present in nearly all practical imaging situations, medical imaging in particular One method that has been used to account for the effects of PVA is the fizzy c-means algorithm (FCM). We propose a new method for estimating the partial volume coefJicient of each class at each voxel in a given image using a Bayesian statistical model. A prior probability on the partial volume coefficients is used to reflect how most voxels in the image are expected to be pure. We then show that the results obtained by this method are quite similar and in some cases equivalent to results obtaine,d using FCM. Both algorithms are demonstrated on a magnetic resonance image of the brain. the PVCs in an image. Their results focussed mainly on multichannel images, which typically are not very high resolution and are therefore less accurate for the purposes of volumetric quantification. In this paper, we present a new model for the estimation of partial volume coefficients. The model is similar to the one used in [6]. However, we focus primarily on single channel images and do not assume any spatial dependence between pixels. This leads to an algorithm that is more easily implemented and computationally efficient but is less robust to noise. Furthermore, we compare the algorithm with FCM and discuss FCM s relation to correcting PVA. An example is shown applying both algorithms to magnetic resonance (MR) images of the brain."
1647016,21239,9078,A Model-Based Approach to Correlation Estimation Inwavelet-Based Distributed Source Coding with Application to Hyperspectral Imagery,2006,"In many practical distributed source coding (DSC) applications correlation information has to be obtained at the encoder in order to determine the encoding rate. Coding efficiency depends strongly on the accuracy of this correlation estimation, which often has to be performed under rate and complexity constraints. In this paper we focus on correlation estimation for wavelet-based DSC. We extend our previously proposed model-based estimation techniques, which provided accurate estimates of bit-plane level correlation under rate constraints, in the simple case where bit-planes are generated from the binary representation of the sources. To extend the model-based approach to wavelet-based DSC, we need to address two issues. Firstly, in order to improve coding efficiency, bit-planes are typically generated by more sophisticated algorithms in wavelet-based DSC (e.g., by deciding on the bitplane scan order based on coefficient significance), which makes model-based estimation more challenging. Secondly, certain wavelet subbands may not have enough coefficients for reliable model estimation, so that model-based techniques alone may not be sufficiently accurate. We propose solutions to these problems and, using a DSC-based hyperspectral image system as example, we demonstrate that model-based estimation can lead to efficient system implementation with lower computational and data exchange requirements, and improved parallelism, while incurring only small degradation in coding efficiency."
2233948,21239,9078,Automated magnetic resonance assisted echocardiographic motion analysis,2008,"Competing technologies have recently emerged across the spectrum of cardiovascular imaging modalities that study intra-myocardial function, probing promising new parameters such as cardiac strain, twist, and torsion. Displacement encoding with stimulated echoes (DENSE) in cardiac magnetic resonance (cMR) imaging achieves accurate spatiotemporal measurements of tissue motion, but remains expensive and time-consuming. Speckle tracking in echocardiography uses an inexpensive and ubiquitous imaging modality, but often fails to fully capture tissue motion. This project strives to combine the desirable aspects of DENSE cMR and speckle tracking via a novel automated analysis technique termed active trajectory field models (ATFMs). The proposed ATFM analysis characterizes cardiac motion within a training set of DENSE cMR data, and uses this characterization to subsequently recover motion from a noisy and incomplete echocardiographic sequence. We demonstrate the effectiveness of the proposed technique via 2D short-axis murine image acquisitions, achieving results comparable to existing semi-automatic echocardiographic motion analysis methods."
571912,21239,9078,Joint source-channel coding for scalable video over DS-CDMA multipath fading channels,2001,"We extend our previous work on joint source-channel coding to scalable video transmission over wireless direct-sequence code-division-multiple-access (DS-CDMA) multipath fading channels. A SNR scalable video coder is used and unequal error protection (UEP) is allowed for each scalable layer. At the receiver-end an adaptive antenna array auxiliary-vector (AV) filter is utilized that provides space-time RAKE-type processing and multiple-access interference suppression. The choice of the AV receiver is dictated by realistic channel fading rates that limit the data record available for receiver adaptation and redesign. Our problem is to allocate the available bit rate of the user of interest between source and channel coding and across scalable layers, while minimizing the end-to-end distortion of the received video sequence. The optimization algorithm that we propose utilizes universal rate-distortion characteristic curves that show the contribution of each layer to the total distortion as a function of the source rate of the layer and the residual bit error rate (the error rate after channel coding). These plots can be approximated using appropriate functions to reduce the computational complexity of the solution."
1006671,21239,9078,Near-Lossless Compression of Hyperspectral Images,2006,"Research Supported By National Science Foundation Grant Number Ccr-0104800. Richard Ladner was supported in part by the Boeing Professor-ship in Computer Science and Engineering. Contact Information: Professor Agnieszka Miguel, Department of Electrical & Computer Engineering Seattle University, 901 12th Avenue, P.O. Box 222000, Seattle, WA 98122-1090, (206)296-5965, amiguel@seattleu.edu. ABSTRACT Algorithms for near-lossless compression of hyperspectral images are presented. They guarantee that the intensity of any pixel in the decompressed image(s) differs from its original value by no more than a user-specified quantity. To reduce the bit rate required to code images while providing significantly more compression than lossless algorithms, linear prediction between the bands is used. Each band is predicted by a previously transmitted band. The prediction is subtracted from the original band, and the residual is compressed with a bit plane coder which uses context-based adaptive binary arithmetic coding. To find the best prediction algorithm, the impact of various band orderings and optimization techniques on the compression ratios is studied."
820444,21239,9078,On the Optimum Multiplicative Watermark Detection in the Transform Domain,2006,"This paper presents an investigation on the optimum detection of multiplicative watermarks based on statistical behaviour of image contents in the transform domain. In recent works, the problem of watermark detection is viewed as a binary decision where the observation is the possibly watermarked transformed coefficients. Indeed, the detector verifies whether the watermark presented to its input is actually embedded in the input image (hypothesis H1) or not (hypothesis H0). Such a detection scheme relies on the Neyman-Pearson criterion to derive a decision threshold by minimising the probability of missed detection with respect to a given probability of false alarm. Previous works approximate the probability density function (pdf) of the observation when hypothesis H0 holds by the pdf when hypothesis of having no watermark embedded in the input image is in force by assuming that the watermark strength is weak to some extent. However, the weakness of the watermark does fulfil the requirement on the robustness. Moreover, from the viewpoint of the decision theory, the smaller the embedding depth, the worse the watermark detection. This paper describes the drawback behind this approximation in the general case and proposes an efficient solution closer to the theoretical derivation. To validate the proposed technique, we consider a special case in which the Laplace statistical model is used."
1759887,21239,9078,Wavelet-based color texture retrieval using the independent component color space,2008,"In this paper, we propose a wavelet based color texture retrieval method using the independent component color space. In color texture retrieval, the product of low dimensional marginal distributions of wavelet coefficients from different color layers are preferred to substitute or approximate their high dimensional joint distributions in order to avoid the curse of dimensionality. However, the RGB color spaces is a highly correlated color space and the extracted wavelet coefficients from different layers are also correlated, which means such a substitution or approximation will not be adequate. To solve the problem, we use independent component analysis to decor- relate the R, G and B layers into three new independent layers before applying wavelet decomposition on the color texture images. In the feature extraction (FE) step of the proposed method, generalized Gaussian density (GGD) are used to model the marginal distribution of wavelet coefficients, and the extracted model parameters are used as features. In the similarity measurement (SM) step of the proposed method, the Kullback-Leibler distance(KLD) is calculated as feature distance, using the extracted model parameters of the query texture images and those of the images in the database. Experimental results on a database of 1120 color texture images indicate that the proposed method greatly overperforms its RGB based counterpart that ignores the inter-layer correlation, and its counterpart which uses the I1I2I3 colorspace."
1885810,21239,9078,Color Image Superresolution Based on a Stochastic Combinational Classification-Regression Algorithm,2007,"The proposed algorithm in this work provides superresolution for color images by using a learning based technique that utilizes both generative and discriminant approaches. The combination of the two approaches is designed with a stochastic classification-regression framework where a color image patch is first classified by its content, and then, based on the class of the patch, a learned regression provides the optimal solution. For good generalization, the classification portion of the algorithm determines the probability that the image patch is in a given class by modeling all possible image content (learned through a training set) as a Gaussian mixture, with each Gaussian of the mixture portraying a single class. The regression portion of the algorithm has been chosen to be a modified Support Vector Regression, where the kernel has been learned by solving a semi definite programming (SDP) and quadratically constrained quadratic programming (QCQP) problem. The SVR is further modified by scaling the training points in the SDP and QCQP problems by their relevance and importance to the examined regression. The result is a weighted average of different regressions depending on how much a single regression is likely to contribute, where advantages include reduced problem complexity, specificity with regard to image content, added degrees of freedom from a nonlinear approaches, and excellent generalization that a combined methodology has over its individual counterparts."
1059148,21239,9078,Hyperspectral super-resolution of locally low rank images from complementary multisource data,2014,"Remote sensing hyperspectral images (HSI) are quite often locally low rank, in the sense that the spectral vectors acquired from a given spatial neighborhood belong to a low dimensional subspace/manifold. This has been recently exploited for the fusion of low spatial resolution HSI with high spatial resolution multispectral images (MSI) in order to obtain super-resolution HSI. Most approaches adopt an unmixing or a matrix factorization perspective. The derived methods have led to state-of-the-art results when the spectral information lies in a low dimensional subspace/manifold. However, if the subspace/manifold dimensionality spanned by the complete data set is large, the performance of these methods decrease mainly because the underlying sparse regression is severely ill-posed. In this paper, we propose a local approach to cope with this difficulty. Fundamentally, we exploit the fact that real world HSI are locally low rank, to partition the image into patches and solve the data fusion problem independently for each patch. This way, in each patch the subspace/manifold dimensionality is low enough to obtain useful super-resolution. We explore two alternatives to define the local regions, using sliding windows and binary partition trees. The effectiveness of the proposed approach is illustrated with synthetic and semi-real data."
381362,21239,9078,A neural network approach to interactive content-based retrieval of video databases,1999,"A neural network scheme is presented in this paper for adaptive video indexing and retrieval. First, a limited but characteristic amount of frames are extracted from each video scene, by minimizing a cross-correlation criterion. Low level features are extracted to indicate the frame characteristics, such as color and motion segments. This is due to the fact that extraction of high-level, semantic, features from any kind of images is too hard to be implemented. After the key frame extraction, the video queries are implemented directly on this small number of frames. To reduce, however, the limitation of low-level features the human is considered as a part of the process, meaning that he/she is able to assign a degree of appropriateness for each retrieved image of the system and then restart the searching. A feedforward neural network structure is proposed as a parametric distance for the retrieval, mainly due to the highly non linear capabilities. An adaptation mechanism is also proposed for updating the network weights, each time a new image selection is performed by the user. This mechanism can modify the network weights so that the output of the network, after the adaptation, is as much as close to the user's selection while simultaneously performing a minimal degradation of the previous learned data."
2876852,21239,9078,Atmospheric lidar imaging and poisson inverse problems,2016,"This paper describes an atmospheric lidar photon-limited imaging problem in which observations are contaminated with Poisson noise. The observations are a nonlinear function of two spatially varying physical parameters. The first parameter, called the transmittance, is known to be a bounded monotonic non-increasing function. The second parameter, called the backscatter cross-section, is non-negative and can be approximated with a piecewise constant function. Current statistical estimators in the lidar community do not take these constraints in account, and at times the estimates violate the physical properties. The standard practice is such that estimation of the parameters are not treated as a statistical inference imaging problem, except that the only imaging technique that is used is averaging over non-overlapping blocks to reduce the noise variance. The proposed method of this paper leverages novel Poisson image reconstruction algorithms with monotonicity constraints. Specifically, an isotonic regression algorithm called PAV is incorporated into total-variation regularized Poisson estimation methods. Simulations demonstrate that the proposed approach — relative to competitor algorithms — consistently yields improved estimates (smaller MSE) of the transmittance parameter, with a negligible deterioration of the backscatter cross-section."
2490610,21239,9078,Matching pursuits video coding using generalized bit-planes,2002,"The matching pursuits (MP) coding method has been employed successfully in video coding. In it, the motion compensated frame difference is decomposed on an overcomplete dictionary of atoms, generating a sequence of pairs specifying the atoms used and their corresponding coefficients. A particular rate-distortion trade-off is achieved by setting both the number of atoms and the quantizers of their coefficients. Several strategies have been presented in order to set this trade-off. In this paper we propose a novel method for performing matching pursuits quantization, based on the notion of decomposition in generalized bit-planes. Such decompositions generate just a sequence of indexes. They provide an elegant solution to the trade-off between quantization of coefficients and number of passes in the MP algorithm. We show that they can be regarded as generalizations of decompositions followed by linear quantization of the coefficients. In addition, we state a theorem that sets bounds for their R-D performance. We test the effectiveness of the proposed method using the framework of Neff and Zakhor's MP video encoder. The results obtained are promising, presenting, without any ad-hoc assumptions about the R-D behavior of the coded frames or any increase in computational complexity, a significant improvement over the classical MP video coders. Also, the results are as good as the ones obtained employing more sophisticated strategies."
1639769,21239,9078,GPU accelerated view synthesis from multiple RGB-D images,2012,"This paper proposes a GPU-based approach to generate novel views from a set of RGB-D images captured by static multiple depth cameras. The proposed method consists mainly of two steps: 1) construction of 3D structures by non-rigid registration and 2) an image-based rendering procedure. For seamless registration, we apply a thin plate spline-based deformation onto a 3D point cloud rather than a conventional rigid-body transformation which would result in inaccurate registration due to the low precision of depth measurements from a distance. For rendering, our approach first draws a non-rigidly registered point cloud onto a depth buffer in the GPU and then fills holes and removes noise. Finally, we project an RGB color image onto reconstructed 3D points using the projective texture technique. Since most of the procedures are implemented using programmable CPUs, our methods fit well with modern graphics hardware and therefore can accelerate computationally heavy processes. Experiments showed high-quality seamlessly rendered results of multiple RGB-D images compared with the previous point-based rendering and registration techniques"
1532972,21239,11491,Supervised Multi-scale Locality Sensitive Hashing,2015,"LSH is a popular framework to generate compact representations of multimedia data, which can be used for content based search. However, the performance of LSH is limited by its unsupervised nature and the underlying feature scale. In this work, we propose to improve LSH by incorporating two elements - supervised hash bit selection and multi-scale feature representation. First, a feature vector is represented by multiple scales. At each scale, the feature vector is divided into segments. The size of a segment is decreased gradually to make the representation correspond to a coarse-to-fine view of the feature. Then each segment is hashed to generate more bits than the target hash length. Finally the best ones are selected from the hash bit pool according to the notion of bit reliability, which is estimated by bit-level hypothesis testing.   Extensive experiments have been performed to validate the proposal in two applications: near-duplicate image detection and approximate feature distance estimation. We first demonstrate that the feature scale can influence performance, which is often a neglected factor. Then we show that the proposed supervision method is effective. In particular, the performance increases with the size of the hash bit pool. Finally, the two elements are put together. The integrated scheme exhibits further improved performance."
2570955,21239,9078,Multiple-description coding for robust image watermarking,2004,"If we treated image watermarking as a communication problem using the image as the communication channel, applying error-correcting codes (ECC) to the watermarking system should increase the robustness significantly and there has been a lot of works done in this direction. For fading channels, using diversity or signal repetition could be more effective than other ECC methods. However, twice the signal repetition needed twice the bandwidth which can be excessive because the available bandwidth in watermarking was usually small. Multiple-description coding was a mean of trading off between transmission bandwidth and bit error rate. Thereby, we could increase diversity without increasing as much bandwidth as signal repetition. In this paper, we proposed a new image watermarking method using multiple-description coding to increase robustness. Since traditional multiple-description coding was considered in on-off channels where channels were not marred by bit errors but occasional connection outages such as dropped packets, to apply multiple-description coding in watermarking, we needed a form of multiple-description coding for noisy channels instead of on-off channels. Iterative coding of multiple descriptions (ICMD) has been used in noisy channels. Here, we used ICMD together with spread spectrum watermarking to form our watermarking system. We found that bit error did not happen until we compressed the test image in JPEG to 12.7% of the original size (PSNR 37.25 dB). We concluded that ICMD was a very good way to increase robustness for image watermarking."
616281,21239,9078,Space-color quantization of multispectral images in hierarchy of scales,2001,"In this paper a novel model for multiscale space-color quantization of multispectral images, is described. The approach is based on the hierarchical clustering technique, derived from the statistical physics model of free energy (Jovovic 1996, Jovovic et al. 1999). The group vectors for image color are computed on the adaptively selected windows of computation, as contrasted to the block-size windows, optimizing the accuracy of the computation of the group vectors with the density of sampling an image by the group windows. The algorithm is suitable for implementation in parallel computer architectures. The results of quantization of color images by our algorithm are compared with 3 image compression techniques: 1) wavelets, 2) discrete cosine transform (DCT), and, 3) quad tree (QT). Contextual information of spatial coherency of the data is used in the segmentation process, in our algorithm. As a result, much better spatial resolution and small size of compressed images are obtained by our algorithm, as compared to the other techniques, for any error level of compression selected. Major spatial features are optimally color-coded along the hierarchy of scales of computation. The images quantized with our algorithm are suitable for the run-length encoding scheme of the hierarchy of binary images."
2260252,21239,9078,Signature pattern recognition using pseudo Zernike moments and a fuzzy logic classifier,1996,"We have introduced a new method, taking advantage of an image moment transformation combined with a fuzzy logic approach. For this purpose first we tried to model the noise embedded in signature patterns inherently and separate that from environmental effects. On the basis of the first step results, we have extracted the most optimum mapping to a unit circle using LMS criteria. Then we derived some orientation invariant moments introduced in former reports and studied their own statistical properties in our special input space, using a new defined criterion. Afterwards we defined an error matrix for signature patterns and studied its behavior and concluded that a fuzzy classifier seems to be the best choice for our application. Then we defined a fuzzy complex space and also a fuzzy complex similarity measure in this space, and constructed a training algorithm to learn the fuzzy classifier. Thus any input pattern could be compared to the learned prototypes through a pre-defined fuzzy similarity measure and attributed to one of the learned classes. The fuzzy classifier is applied to each of the above derived moments which constituted an individual feature space separately and miss-classifications were detected as a measure of the error magnitude. Finally a comparison is made between the above considered image transformations and we have pointed out some of the advantages of this method."
655087,21239,9078,Mid-level vision: new directions in vision and video,1994,"Human vision, machine vision, and image coding, all have to deal with the problem of finding representations that are useful and efficient. The best-known techniques are based on low-level processing, using signal processing concepts such as filters, transforms, and simple non-linearities. Low-level concepts are at the heart of standard vision systems for computing optic flow, texture, etc. Low-level image coding techniques include DCTs, pyramids, wavelets, etc. To advance to a new generation of image coding architectures one must work with new image representations that involve such concepts as surfaces, lighting, transparency, etc. These representations fall in the domain of mid-level vision. By representing images with these more sophisticated vocabularies one can increase the flexibility and efficiency of the vision and image coding systems. The authors decompose an image sequence into a set of overlapping layers, rather like the cels used by a traditional animator. These layers are ordered in depth, sliding over one another and being combined according to the rules of transparency and occlusion. For some test sequences the authors achieve data compression far better than is possible with standard techniques such as MPEG. >"
2077979,21239,9078,Automatic liver tumor diagnosis with Dynamic-Contrast Enhanced MRI,2008,"Dynamic-contrast enhanced MRI is currently used as a complementary diagnosis tool to assess the malignancy of the liver tumor, called hepatoma or adult primary liver cancer. This paper proposes a set of features and computation methods to extract them in order to design a classifier for automatic diagnosis of the hepatoma. A dynamic discrete linear pharmacokinetic model is used to estimate the perfusion curves from the noisy observations, based on the multi-compartment paradigm. The arterial response to the contrast agent bolus injection, called arterial input function, is also estimated since no arteries are available in the neighborhood of the tumor. The compensation of involuntary movements of the patient, such as the respiratory activity, during the acquisition process is performed using a mutual information based registration algorithm with non-rigid transformations. The tumor is isolated in a small region of interest in order to speed up the analysis and the tumor itself is segmented using an active contour algorithm. The classification of the tumor can be based on the mean and variance values of the Maximum, Washln and WashOut rates of the perfusion curves inside the tumor as it is shown that these rates are adequate discriminative features to automatically classify the tumors with respect to its malignancy. Tests using real data are used to illustrate the ability of using these features to classify the tumor with respect to its malignancy."
1587968,21239,9078,Image reconstruction from compressed linear measurements with side information,2011,"This paper proposes a joint reconstruction algorithm for compressed correlated images that are given under the form of linear measurements. We consider the particular problem where one image is selected as the reference image and it is used as a side information for decoding the compressed correlated images. These compressed images are given under the form of random measurements that are further quantized and entropy coded. The joint decoder estimates the correlation model based on the geometric transformation of features captured by a structured dictionary. We observe that the high frequency components are not efficiently captured in the estimated image when the correlation information is used alone for image prediction. Hence, we propose a reconstruction strategy that uses the information in the measurements to recover the missing visual information in the predicted image. The reconstruction is based on an optimization algorithm that enforces the reconstructed image to be consistent with the quantized measurements. We further add additional constraints to ensure that the reconstructed image is close to the image predicted from the correlation estimation. The non-linearities introduced due to quantization are considered on both correlation and reconstruction algorithms in order to improve the performance. Experimental results demonstrate the benefit of the reconstruction algorithm as it brings improved coding performance especially at high rate and outperforms independent coding solutions based on JPEG 2000."
1025926,21239,9078,Free-View Watermarking for Free-View Television,2006,"The recent advances in image based rendering (IBR) has pioneered a new technology, free-view television, in which TV-viewers select freely the viewing position and angle by the application of IBR on the transmitted multi-view video. Noting that the TV-viewer might also record a personal video for this arbitrarily selected view and misuse this content, it is apparent that copyright and copy protection problems also exist and should be solved for free-view TV. In this paper, we focus on this problem by proposing a watermarking method for free-view video. The watermark is embedded into every frame of multiple views by exploiting the spatial masking properties of the human visual system (HVS). Assuming that the position and rotation for the imagery view is known, the proposed method extracts the watermark successfully from an arbitrarily generated image. In order to extend the method for the case of an unknown imagery camera position and rotation, the modifications on the watermark pattern due to image based rendering operations are also analyzed. Based on this analysis, a camera position and homography estimation method is proposed considering the operations in image based rendering. The results show that the watermark detection is achieved successfully for the cases in which the imagery camera is arbitrarily located on the camera plane."
1739799,21239,9078,Applications of Gaussian mixture models and mean squared error within DatSCAN SPECT imaging,2014,"This work highlights the exploitation of Gaussian Mixture Model (GMM) and Mean squared Error (MSE) in DaTSCAN SPECT brain images for intensity normalization purposes over two proposed approaches. The first proposed methodology is based on a nonlinear image filtering by means of GMM, which considers not only the intensity levels of each voxel but also its coordinates inside the so-defined spatial Gaussian functions. It is achieved according to a probability threshold that measures the weight of each kernel or cluster on the striatum area, the voxels in the non-specific regions are intensity normalized by removing clusters whose likelihood is negligible. The second normalization method based on MSE which is performed by a linear intensity transformation in each voxel. This approach is based on predicting jointly different intensity normalization parameters that leads to the joint minimization of the squared sum errors between the template image and the optimal linear estimated image (normalized image). We compare these methods of normalization together with another approach widely used based on specific-to-non-specific binding ratio. This comparison is based on DaTSCAN image analysis and classification for the development of a computer aided diagnosis (CAD) system for Parkinsonian syndrome detection."
2025293,21239,9078,A novel framework for object removal from digital photograph,2005,"This work aims for a novel function for smart camera-redundant object removal from digital photograph. The proposed novel framework can fill the left lacuna region in the digital image. In previous related researches, texture synthesis and image inpainting construct the fundamentals of filling the lost image region. Texture synthesis can be used to fill the large hole of input texture, while image inpainting can be used to repair the small image gaps. In this paper, we propose an object removal framework by the sub-patch texture synthesis algorithm and weighted interpolation method with automatic repainting mechanism. In the filling process, the color distribution analysis is used to choose different methods. The exhaustive computation time is reduced by the weighted interpolation method. In order to repaint the faulty texture region intelligently, we use the color ratio gradients to detect the synthesized artifact region. The automatic artifact detection can lead repainting the faulty region without user intervention. The proposed algorithm can achieve better performance with seamless output images. The regular computation is also suitable for hardware architecture different from previous existing algorithms."
2212049,21239,11470,Fast motion estimation and mode decision for H.264 video coding in packet loss environment,2008,"H.264 coding standard offers several coding modes, including intra mode and inter prediction mode with different block sizes. Since intra mode does not need reference from previous frames, in packet loss environment, intra mode can be a good choice to stop error propagation and enhance decoded video quality. To achieve best coding efficiency, such best mode decision is often made upon optimizing rate distortion (RD) constraints. Unfortunately, this RD optimization process incurs a considerable complexity increase of the encoder. To solve the problem, we propose a fast motion estimation and mode decision algorithm for H.264 video coding in packet loss environment. Unlike previous algorithms, our method takes advantage of channel distortion estimation as assistance to decide if an early termination of existing coding mode choices is necessary. Experiment results show that, compared with error resilient coding method proposed in the work of Stockhammer et al. (2002) and Yuan Zhang et al. (2004), the proposed algorithm can enhance speed up to 10 times while maintaining similar average picture quality."
1020497,21239,9078,Extending Single-View Scalable Video Coding to Multi-View Based on H.264/AVC,2006,"An extension of single-view scalable video coding to multi-view is presented in this paper. Scalable video coding is recently developed in the Joint Video Team of ISO/IEC MPEG and ITU-T VCEG named Joint Scalable Video Model. The model includes temporal, spatial and quality scalability enhancing a H.264/AVC base layer. To remove redundancy between views a hierarchical decomposition in a similar way to the temporal direction is applied. The codec is based on this technology and supports open-loop as well as closed-loop controlled encoding. The advantage of this approach lies in its compatibility to the state of the art single-view video codec H.264/AVC and its simple decomposition structure. Encoding a base view using H.264/AVC syntax, any standard single-view decoder is able to decode the data. The hierarchical decomposition structure allows efficient access to all views and frames inside a view. This is especially important for video-based-rendering and multi-view displays, which have different requirements. The chosen decomposition structure also supports parallel processing. Gain in objective as well as subjective quality was achieved for some test sequences using a single layer. The results were compared to JSVM 5.1 (simulcast)."
395506,21239,9078,A feature-assisted search strategy for block motion estimation,1999,"Block motion estimation using the exhaustive full search is computationally intensive. Previous fast search algorithms tend to reduce the computation by limiting the number of locations to be searched. Nearly all of these algorithms rely on the assumption: the mean absolute distortion (MAD) function increases monotonically as the search location moves away from the global minimum. Unfortunately, this is usually not true in real-world video signals. However, we can reasonably assume that it is monotonic in a small neighbourhood around the global minimum. Consequently, one simple, but perhaps the most efficient and reliable strategy, is to put the checking point as close as possible to the global minimum. In this paper, some image features are suggested to locate the initial search points. Such a guided scheme is based on the location of some feature points. After a feature detecting process was applied to each frame to extract a set of feature points as matching primitives, we studied extensively the statistical behaviour of these matching primitives and found that they are highly correlated with the MAD error surface of real-world motion vectors. These correlation characteristics are extremely useful for fast search algorithms. The results are robust and the implementation could be very efficient."
1759263,21239,9078,Reconfigurable data flow engine for HEVC motion estimation,2014,"High Efficiency Video Coding (HEVC) standard achieves enhanced compression efficiency in comparison to previous standards, at the cost of a dramatic increase of the computational load. In order to cope with such computational requirements, and to challenge the real-time encoding of High Definition (HD) video sequences with the HEVC standard, we propose herein a reconfigurable architecture design for the most computationally demanding motion estimation module, considering highly efficient Full-Search Block-Matching algorithm. The proposed architecture supports Prediction Blocks (PBs) sizes ranging from 8×8 to 64×64 pixels (also considering non-square shapes), and search areas as large as 256×256 pixels. Furthermore, this reconfigurable approach leverages the trade-off between maximum performance and minimum resource usage. Experimental results show that the proposed architecture is able of achieving real-time motion estimation with more than 26.9 fps, for 1080p video formats, a 64×64 pixels search area and 1 reference frame, by relying on a Xilinx Virtex 5 FPGA implementation. Moreover, a performance superior to the NVIDIA Fermi-based GPU implementation, for up to 25%, was achieved."
2822691,21239,9078,A universal image coding approach using sparse steered Mixture-of-Experts regression,2016,"Our challenge is the design of a “universal” bit-efficient image compression approach. The prime goal is to allow reconstruction of images with high quality. In addition, we attempt to design the coder and decoder “universal”, such that MPEG-7-like low-and mid-level descriptors are an integral part of the coded representation. To this end, we introduce a sparse Mixture-of-Experts regression approach for coding images in the pixel domain. The underlying stochastic process of the pixel amplitudes are modelled as a 3-dimensional and multi-modal Mixture-of-Gaussians with K modes. This closed form continuous analytical model is estimated using the Expectation-Maximization algorithm and describes segments of pixels by local 3-D Gaussian steering kernels with global support. As such, each component in the mixture of experts steers along the direction of highest correlation. The conditional density then serves as the regression function. Experiments show that a considerable compression gain is achievable compared to JPEG for low bitrates for a large class of images, while forming attractive low-level descriptors for the image, such as the local segmentation boundaries, direction of intensity flow and the distribution of these parameters over the image."
2211633,21239,9078,Optical imaging with scanning MEMS mirror - A single photodetector approach,2009,"This paper describes an optical system for low-complexity optical image acquisition based on a single scanning MEMS mirror and a single photodetector. The overall aim of the research is to investigate techniques for image acquisition at electromagnetic wavelengths where the cost and/or technical maturity of detector arrays pose a limitation. In contrast to similar systems built using a digital micromirror device (DMD), the present configuration has advantages of lower cost and potential applicability across a wide spectrum, ranging from visible to Terahertz frequencies. In the present arrangement, light at visible wavelengths from the object passes through a telescope and falls onto a small, scanning MEMS micromirror. The entire image of the object is projected onto the mirror surface and reflected towards a single photodetector with a pinhole at its entrance. Similarly to conventional scanning, by finely changing the tilt-angle of the mirror, the detector sees different areas of the projected image, thereby building up an image pixel-by-pixel. Resolution is increased by allowing for an overlap between neighbouring scanned areas. Iterative bilinear interpolation and wavelet denoising are employed to enhance image quality."
479139,21239,9078,Automatic video object segmentation via 3D structure tensor,2003,"3D structure tensor is an effective representation of the local motion information of video object (VO) and has been exploited for performing VO segmentation. However, existing 3D structure tensor-based VO segmentation approaches often yield inaccurate objects' boundaries, and high computation is needed for estimating dense motion field. To address these concerns, a new scheme is proposed in this paper by generating the spatial-constrained motion masks without computing dense motion field. For that, scale-adaptive spatio-temporal filtering steered by the condition number is developed to handle multiple motions contributed from different VOs. As rigid, and nonrigid VO motions need to be handled differently on mask generation, rigidity analysis is conducted based on standard deviation of correlation coefficients over a range of successive video frames in order to identify whether each video sequence frame contains rigid or nonrigid motion. Various masks, such as eigenmaps, coherency-measurement maps, and change-detection maps, are produced and fused for generating the final VO motion masks. With boundary refinement by graph-based spatial segmentation, experimental results present accurately segmented moving VOs using different kinds of test sequences."
2350242,21239,9078,Temporal resolution scalable video coding,1994,"Scalable video coding is important in a number of applications where video needs to be decoded and displayed at a variety of spatial and temporal resolution scales or when resilience to errors is necessary. In this paper, we investigate a temporal-domain approach for resolution scalable video coding. This approach was originally proposed to and since then has been included in the Moving Picture Experts Group (MPEG-2) standard. Temporal scalability, although not limited to, is particularly useful for applications such as, HDTV as well as for high quality telecommunication applications that may require high frame rate (e.g., 60 Hz) progressive video. Furthermore, it allows flexibility in use of progressive or interlaced video formats for coding even though the source video may be progressive with high frame rate. Our technique builds on the motion-compensated DCT framework of nonscalable (single layer) MPEG-2 video coding and adds motion compensated inter-layer coding. In our simulations we focus on the 2- layer temporally scalable coding and compare the performance of using progressive video format for both layers to that when interlaced video format is used for both layers; all interim video formats used are derived from original high temporal resolution progressive video. In each case we compare the performance of two inter-layer prediction configurations and evaluate their prediction efficiency. >"
2292506,21239,9078,A new rate control scheme using quadratic rate distortion model,1996,"A new rate control scheme is used to calculate the target bit rate for each frame based on a quadratic formulation of the rate distortion function. The distortion measure are assumed to be either the average quantization scale of a frame or mean square error. The rate distortion function is modeled as a second order function of the inverse of the distortion measure. We presented a closed form solution for the target bit allocation which applies to both the digital storage media (DSM) applications (e.g., MPEG-2) and very low bit rate (VLBR) applications (e.g., MPEG-4). We perform experiments on a wavelet-based coder to demonstrate the versatility of our scheme. Model parameters are estimated using statistical linear regression analysis. Since the estimation uses the past encoded frames of the same picture prediction type (I, P and B pictures), the proposed approach is a single pass rate control technique. For the MPEG-2 coder the fluctuations of the bit counts are significantly reduced by 20 percent to 65 percent in the standard deviation of the bit count while the picture quality remains the same. For the MPEG-4 VM coder, the PSNR is improved by almost one dB as compared to the best MPEG-4 VM rate control using exhaustive combinations of quantization scale parameters."
678965,21239,9078,Video rate control using conditional mean,2002,This paper presents an efficient rate control algorithm based on estimation theory. We measure a conditional mean by estimating a joint probability density function (PDF) using Parzen's window. The training data can estimate the nonlinear rate-distortion (R-D) relationship between the quantization parameter (QP) and the bits spent for each macroblock depending on the sum of absolute differences (SAD). We increase the accuracy of this joint PDF by clustering the training data depending on the QP values. This localization helps the estimation process to be very accurate. Next we apply a mean operator to simplify the conditional mean estimation of the rate given the SAD and QP values. This information is stored into three look-up tables depending on picture types. We use these tables to find the optimal QP values in least-mean-square (LMS) sense for a given bit budget of the current frame. Simulation results show that the proposed algorithm outperforms the informative MPEG-4 rate control algorithm in terms of reproduced image quality and coding efficiency while requiring much less implementation complexity. Most of all it keeps the bit rate very close to the required bit-rate due to the accuracy of the conditional mean estimator that solves the nonlinear R-D function.
2262749,21239,9078,Real-time optimal-memory image rotation for embedded systems,2009,"Skew-corrected document images are necessary for subsequent downstream operations such as archiving, printing or improving OCR performance. Image rotation is a necessary and more expensive step in achieving skew correction of document images. Other applications of rotation include, image registration and orientation correction. Traditional image rotation algorithms [2-4] such as three-shear rotation [1] require three separable shears of the image. The embedded use of such techniques in scanners/printers presents technical challenges, since the memory available is limited and/or the document image is only available progressively in chunks of say 32 or 64 rows (swaths). Traditional image rotation algorithms require the entire image to be available before commencing the rotation operation. This paper presents an approach that allows image rotation using swaths of the image thus minimizing the overall memory requirement. We theoretically prove that the number of image swaths that are to be buffered is independent of the image size and depends only on the rotation angle. This approach enables rotation of any arbitrary sized image on memory constrained devices. The memory savings realized is at least 80%, for an A4-sized document image rotated 15°. Our progressive approach demonstrates real-time image rotation and hence improves on the state-of-the-art approaches for reduction of rotation complexity [5-10]."
1861910,21239,9078,An automatic scheme for stereoscopic video object-based watermarking using qualified significant wavelet trees,2002,"A fully automatic system for embedding visually recognizable watermark patterns to video objects is proposed. The architecture consists of 3 main modules. In the first module, unsupervised video object extraction is performed, by analyzing stereoscopic pairs of frames. Then each video object is decomposed into three levels with ten subbands, using the discrete wavelet transform (DWT) and three pairs of subbands are formed (HL/sub 3/, HL/sub 2/), (LH/sub 3/, LH/sub 2/) and (HH/sub 3/, HH/sub 2/). Next qualified significant wavelet trees (QSWTs) are estimated for the specific pair of subbands that contains the highest energy content compared to the other two pairs. QSWTs are derived from the embedded zerotree wavelet (EZW) algorithm and they are high-energy coefficient paths within the selected pair of subbands. Finally, in the third module, visually recognizable watermark patterns are redundantly embedded to the coefficients of the highest energy QSWTs and the inverse DWT is applied to provide the watermarked video object. The performance of the proposed video object watermarking system is tested under various signal distortions such as JPEG lossy compression, sharpening, blurring and adding different types of noise. Experimental results on real life stereoscopic images are presented to indicate the efficiency and robustness of the proposed scheme."
2479779,21239,9078,Image interpolation with edge-preserving differential motion refinement,2009,"Motion estimation (ME) methods based on differential techniques provide useful information for video analysis, and moreover it is relatively easy to embed into them regularity constraints enforcing for example, contour preservation. On the other hand, these techniques are rarely employed for video compression since, though accurate, the dense motion vector field (MVF) they produce requires too much coding resource and computational effort. However, this kind of algorithm could be useful in the framework of distributed video coding (DVC), where the motion vector are computed at the decoder side, so that no bit-rate is needed to transmit them. Moreover usually the decoder has enough computational power to face with the increased complexity of differential ME. In this paper we introduce a new image interpolation algorithm to be used in the context of DVC. This algorithm combines a popular DVC technique with differential ME. We adapt a pel-recursive differential ME algorithm to the DVC context; moreover we insert a regularity constraint which allows more consistent MVFs. The experimental results are encouraging: the quality of interpolated images is improved of up to 1.1 dB w.r.t. to state-of-the-art techniques. These results prove to be consistent when we use different GOP sizes."
2949532,21239,11104,Region-based classification of remote sensing images with the morphological tree of shapes,2016,"Satellite image classification is a key task used in remote sensing for the automatic interpretation of a large amount of information. Today there exist many types of classification algorithms using advanced image processing methods enhancing the classification accuracy rate. One of the best state-of-the-art methods which improves significantly the classification of complex scenes relies on Self-Dual Attribute Profiles (SDAPs). In this approach, the underlying representation of an image is the Tree of Shapes, which encodes the inclusion of connected components of the image. The SDAP computes for each pixel a vector of attributes providing a local multiscale representation of the information and hence leading to a fine description of the local structures of the image. Instead of performing a pixel-wise classification on features extracted from the Tree of Shapes, it is proposed to directly classify its nodes. Extending a specific interactive segmentation algorithm enables it to deal with the multi-class classification problem. The method does not involve any statistical learning and it is based entirely on morphological information related to the tree. Consequently, a very simple and effective region-based classifier relying on basic attributes is presented."
2237015,21239,9078,Exploiting the residual redundancy in motion estimation vectors to improve the quality of compressed video transmitted over noisy channels,1998,"Many of the existing and emerging low-bit-rate video coding techniques employ motion compensation to exploit the high correlation between successive frames in real-world image sequences-the translation being represented by motion estimation vectors (MEVs). When transmitted over a noisy channel, errors in the MEVs can severely degrade performance, especially when the MEVs are entropy coded using a variable-length code prior to transmission. Techniques commonly used to mitigate the effect of these errors often employ some form of spatio-temporal error masking which relies on the relatively high degree of correlation between MEVs in neighboring macroblocks. This paper presents an alternate approach which exploits this correlation to reduce the probability of errors, rather than try and mask them when they do occur. The new approach is to perform maximum a posteriori probability (MAP) detection using a new method for joint source-channel MAP decoding applicable to data encoded using a variable length code followed by an FEC code. A first-order Markov model is used to model the inter-frame correlation between MEVs. Results presented show that the proposed approach may result in significant improvement in performance at low-to-mid SNR."
1919328,21239,9078,Design optimization of a global/local tone mapping processor on arm SOC platform for real-time high dynamic range video,2008,"As the advance of high quality displays such as organic light- emitting diode (OLED) or laser TV, the importance of a real-time high dynamic range (HDR) data processing for display devices increases significantly. Many tone mapping algorithms are proposed for rendering HDR images or videos on display screens. The choice of tone mapping algorithm depends on characteristics of displays such as luminance range, contrast ratio and gamma correction. An ideal HDR tone mapping processor should include several tone mapping algorithms and be able to select an appropriate one for different kind of devices and applications. Such a HDR tone mapping processor has characteristics of robust core functionality, high flexibility, and low area consumption. An ARM core based system on chip (SOC) platform with HDR tone mapping ASIC is suitable for such applications. In this paper, we present a systematic methodology to develop an optimized architecture for tone mapping processor in the ARM SOC platform. We illustrate the approach by a HDR tone mapping processor that can handle both photographic and gradient compression. The optimization is achieved through four major steps: common module extraction, computation power enhancement, hardware/software partition and cost function analysis. Based on the proposed scheme, we develop an integrated photographic and gradient compression HDR tone mapping processor that can process 1024times768 images at 60 fps. This design runs at 100 MHz clock and consumes area of 13.8 mm 2  under TSMC 0.13 mum technology with 50% improvement in speed and area compared with previous results."
1516634,21239,9078,Image processing challenges in weak gravitational lensing,2011,"The field of weak gravitational lensing, which measures the basic properties of the Universe by studying the way that light from distant galaxies is perturbed as it travels towards us, is a very active field in astronomy. This short article presents a broad overview of the field, including some of the important questions that cosmologists are trying to address, such as understanding the nature of dark energy and dark matter. To do this, there is an increasing feeling within the weak lensing community that other disciplines, such as computer science, machine learning, signal processing and image processing, have the expertise that would bring enormous advantage if channelled into lensing studies. To illustrate this point, the article below outlines some of the key steps in a weak lensing analysis chain. The challenges are distinct at each step, but each could benefit from ideas developed in the signal processing domain. This article also gives a brief overview of current and planned lensing experiments that will soon bring about an influx of data sets that are substantially larger than those analysed to date. It is, therefore, inevitable that current techniques are likely to be insufficient, thus leading to an exciting era where new methods will become crucial for the continued success of the field."
2558650,21239,9078,Detection of line scratch using energy regularity of spatio-temporal video cube,2015,"Scratch is the most dominant defect in an old age video film and its most common form is the vertical line scratch. Restoration of video film corrupted by these defects is totally depends on their absolute detection. Generally, line scratch is persistent in a sequence of frames and their pattern is almost regular as compared to the other defects. Detection of regular pattern is easy as compared to the irregular pattern and this is the key point of the proposed detection method. In the proposed method, non-traditional TY and TX frames has been used rather than a traditional XY frame of video. It is a non traditional approach of line scratch detection method. In the proposed method, modelling of video is formulated by using spatio-temporal regularity flow. Spatio-Temporal regularity flow (SPREF) framework is used here for video modelling, which assumes video as cube and process whole video cube at a time. The pixel intensity varies the least for regular pattern and irregularity produces large variation. Such irregularity is detected by the energy of the frame. The cause of irregularity is not only due to the large intensity variation, but also due to the deviation of flow vectors. Energy of frames incorporates both the intensity and flow vectors of the frame. Experiments have been formulated on the standard test videos and achieved higher detection accuracy that proves its robustness."
1235956,21239,9078,Graphical Models for real-time capable gesture recognition,2010,"In everyday live head gestures such as head shaking or nodding and hand gestures like pointing gestures form important aspects of human-human interaction. Therefore, recent research considers integrating these intuitive communication cues into technical systems for improving and easing human-computer interaction. In this paper we present a vision-based system to recognize head gestures (nodding, shaking, neutral) and dynamic hand gestures (hand moving right/left/up/down, fist moving right/left) in real-time. The gestural input delivers a communication modality for a human-robot interaction scenario situated in an assistive household environment. The use of fast low-level image-feature extraction methods contributes to the real-time capability of the system and advanced classification approaches relying on Graphical Models provide high robustness. Graphical Models offer the possibility to group the input features in several sub-nodes resulting in a better classification than obtained via a traditional Hidden Markov Model classification. The applied grouping can regard interdependencies owing to, either physical constraints (like for the head gestures), or interrelations between shape and motion (like for the hand gestures)."
2211324,21239,9078,"Secure spread spectrum watermarking for images, audio and video",1996,"We describe a digital watermarking method for use in audio, image, video and multimedia data. We argue that a watermark must be placed in perceptually significant components of a signal if it is to be robust to common signal distortions and malicious attack. However, it is well known that modification of these components can lead to perceptual degradation of the signal. To avoid this, we propose to insert a watermark into the spectral components of the data using techniques analogous to spread spectrum communications, hiding a narrow band signal in a wideband channel that is the data. The watermark is difficult for an attacker to remove, even when several individuals conspire together with independently watermarked copies of the data. It is also robust to common signal and geometric distortions such as digital-to-analog and analog-to-digital conversion, resampling, quantization, dithering, compression, rotation, translation, cropping and scaling. The same digital watermarking algorithm can be applied to all three media under consideration with only minor modifications, making it especially appropriate for multimedia products. Retrieval of the watermark unambiguously identifies the owner, and the watermark can be constructed to make counterfeiting almost impossible. We present experimental results to support these claims."
1800788,21239,11470,3-D multiple description video coding for packet switched networks,2003,"Multiple description coding (MDC) schemes have become popular as robustness tools in packet switched networks where multiple paths with varying packet loss rates are available. In this paper we propose a new MDC scheme for video using 3D wavelet decomposition. The spatio-temporal coefficients are then grouped using either a random, uniform scheme or a hierarchical scheme. The descriptions are constructed by arranging a subset of these groups in two positions: primary and the secondary. The bit-allocation problem between the primary and secondary groups is formulated and solved using the generalized BFOS [E.A. Riskin, 1991] algorithm. The bits allocated to the secondary position may be zero, in which case, the description is said to be constructed with no redundancy. Experiments using bi-orthogonal wavelet for the spatial and Haar wavelet for the temporal decomposition, along with embedded block coding with optimal truncation for the different groups have shown significant performance for test video sequences. We expect that, for finite packet loss probability, the MDC scheme with redundancy does better than the scheme that does not have redundancy. In this paper, we also present a discussion of the differences between our work with the most recent 3D based video MDC scheme [Joohie Kim et al., 2002]."
2207044,21239,9078,A wavelet-based video coding scheme using image warping prediction,1998,"This paper describes a video coding algorithm that combines new ideas in motion estimation and wavelet-based coding techniques. A motion estimation technique using an image warping model is employed to reduce temporal redundancies in a given image sequence. The underlying continuous motion model is based on a bilinear warp of a grid of quadrangles. It has the advantage of dealing with more complex motion than simple block matching schemes. In addition, a block adaptive high-quality texture interpolation is used to further improve the quality of the prediction. The prediction error image is decorrelated using a wavelet transform coupled with a framework of highly efficient pre-coding techniques utilizing the concepts of partitioning, aggregation and conditional coding (PACC). This combined coding strategy has proved to be very efficient both with respect to subjective and objective measures. The improvements achieved with our proposed coder compared to the H.263 coder (with all options applied) range from 0.5 to 1.2 dB PSNR. Especially at very low bit-rates where reconstructed videos of block-based coders suffer from visually annoying blocking artifacts the proposed coding scheme produces a superior subjective quality."
540876,21239,9078,Dual domain interactive image restoration: basic algorithm,1996,"This paper describes a new fast, iterative algorithm for interactive image noise removal. Given the locations of noisy pixels and a prototype image, the noisy pixels are to be restored in a natural way. Most existing image noise removal algorithms use either frequency domain information (e.g. low pass filtering) or spatial domain information (e.g median filtering or stochastic texture generation). However, for good noise removal, both spatial and frequency information must be used. The existing algorithms that do combine the two domains (e.g. Gerchberg-Papoulis and related algorithms) place the limitation that the image be band-limited and the band limits be known. Also, some of these may not work well when the noisy pixels are contiguous and numerous. Our algorithm combines the spatial and frequency domain information by using projection onto convex sets (POCS). But unlike previous methods it does not need to know image band limits and does not require the image to be band-limited. Results given here show noise removal from images with texture and prominent lines. The detailed textures as well as the pixels representing prominent lines are created by our algorithm for the noise pixels. The algorithm is fast, the cost being a few iterations (usually under 10), each requiring an FFT, IFFT and copying of a small neighborhood of the noise."
1267006,21239,9078,Improved image interpolation using bilateral filter for weighted least square estimation,2010,"New edge-directed interpolation (NEDI) consists two steps. The two steps are parameter and data estimation. The second step can be replaced by a recently proposed technique called soft-decision to consider the consistency of image structure during this data estimation. The original idea of both steps is to assume equal variances for all estimation errors, such that an ordinary least squares (OLS) estimator can be used. Due to the existence of noise, different object layers, changing in image structures, different spatial distance to the missing data, etc, we observe that the estimation errors of data samples have unequal variances. Hence, a weighted least square (WLS) estimator should be used for both steps. The bilateral filter, which can accurately remove noise and preserve image structure, has been used to model successfully the weights of squared residuals, such that we can apply it to both steps of the estimation. Experimental results show that the average PSNR of this improved interpolation method is 0.47 dB and 0.23 dB higher than two similar approaches, NEDI and Soft-decision Adaptive Interpolation (SAI) using 24 natural images from Kodak. The subjective results show improvement as well."
523300,21239,9078,Wavelet-based contourlet transform and its application to image coding,2004,"In this paper, we first propose a new family of geometrical image transforms that decompose images both radially and angularly. Our construction comprises two stages of filter banks that are non-redundant and perfect reconstruction and therefore lead to an overall non-redundant and perfect reconstruction transform. Using the wavelet transform as the first stage, we apply directional filter banks to the wavelet coefficients in such a way to maintain the anisotropy scaling law. Furthermore, we propose a new image coding scheme based on the proposed transform, the wavelet-based contourlet transform (WBCT), using a new contourlet-based set partitioning in hierarchical trees (CSPIHT) algorithm that provides an embedded code. Due to differences in parent-child relationships between the WBCT coefficients and wavelet coefficients, under CSPIHT, we developed an elaborated repositioning algorithm for the WBCT coefficients in such a way that we could scan spatial orientation trees that are similar to the original SPIHT algorithm. Our experiments demonstrate that the proposed approach is efficient in coding images that possess mostly textures and contours. Our simulation results also show that this new coding approach is competitive to the wavelet coder in terms of the PSNR-rate curves, and is visually superior to the wavelet coder for the mentioned images."
828236,21239,9078,State-Space Reconstruction of Pet Parametric Maps,2006,"The primary goal of dynamic positron emission tomography (PET) is to quantify the physiological and biological processes through tracer kinetics analysis. However, the process is difficult and complicated because of the compromising imaging data quality, i.e. either longer scans with good counting statistics but poor temporal resolution, or noisy shorter scans with good temporal resolution. In this paper, we explore the usage of state space principles for physiological parameter estimation in dynamic PET imaging. The system equation is constructed from particular tracer kinetic models, with the number and relationship between tissue compartments dictated by the physiological and biochemical properties of the process under study. And the observation equation on measurement data is formed based on the specific types of imaging or image-derived data. Once the Poisson distributed PET data are converted to Gaussian ones through the Anscombe transformation, an extended Kalman filter is adopted to estimate the tracer kinetics parameters from the system and observations. More appropriate estimation strategies which better take care of the PET statistics are also under development. The framework is tested on simulated digital phantom data, and the results are of sufficient accuracy and robustness."
2881019,21239,9078,Model based image reconstruction with physics based priors,2016,"Computed tomography is increasingly enabling scientists to study physical processes of materials at micron scales. The MBIR framework provides a powerful method for CT reconstruction by incorporating both a measurement model and prior model. Classically, the choice of prior has been limited to models enforcing local similarity in the image data. In some material science problems, however, much more may be known about the underlying physical process being imaged. Moreover, recent work in Plug-And-Play decoupling of the MBIR problem has enabled researchers to look beyond classical prior models, and innovations in methods of data acquisition such as interlaced view sampling have also shown promise for imaging of dynamic physical processes. In this paper, we propose an MBIR framework with a physics based prior model — namely the Cahn-Hilliard equation. The Cahn-Hilliard equation can be used to describe the spatiotemporal evolution of binary alloys. After formulating the MBIR cost with Cahn-Hilliard prior, we use Plug-And-Play algorithm with ICD optimization to minimize this cost. We apply this method to simulated data using the interlaced-view sampling method of data acquisition. Results show superior reconstruction quality compared to the Filtered Back Projection. Though we use Cahn-Hilliard equation as one instance, the method can be easily extended to use any other physics-based prior model for a different set of applications."
754877,21239,9078,Approaching optimality in spatially scalable video coding: From resampling and prediction to quantization and entropy coding,2013,"This paper builds on our recent work on optimal prediction in spatially scalable video coding, and is inspired by earlier work in our lab on optimal approaches for quality (or SNR) scalability. The approach we propose herein complements the optimal enhancement-layer prediction, enabled by transform domain resampling that ensures the base layer information is maximally accessible and usable at the enhancement layer despite their differing signal resolutions, with an optimal approach to quantization and entropy coding that exploits all available information, encapsulated in the appropriate conditional distribution for transform coefficients, to yield a unified coding engine for spatial scalability. For such quantizers to fully exploit base layer information, the enhancement layer transform block size must proportionally match the signal block transformed at the base layer. The overall system incorporates switching that applies the full estimation-theoretic quantizer and entropy coder at the right block size, but may optionally employ other block sizes where it defaults to optimal prediction followed by standard quantization. It is experimentally shown that the proposed scheme provides considerable performance gains over conventional codec and other leading competitors."
505757,21239,9078,Unsupervised motion detection using a Markovian temporal model with global spatial constraints,2004,"In this work, we propose an unsupervised Bayesian model for the detection of moving objects from dynamic scenes. This unsupervised solution is a three-step approach that uses a statistical model of an interframe gradient norm field (as likelihood model) with a local regularization term (as prior model) combined with strong intraframe spatial constraints. In the first step, the spatial constraints are estimated by making an unsupervised Markovian spatial over-segmentation of two input frames. In the second step, the interframe gradient (derived from the input frames) is restored to minimize undesired noise. In the last step, an unsupervised Markovian temporal segmentation (with global spatial constraints) is performed to generate the desired motion label field. The maximum a posteriori (MAP) estimation of the label field associated with the spatial segmentations (in the first step) and the motion label field (in the third step) is performed by a classical Iterative Conditional Mode (ICM) algorithm. An Iterative Conditional Estimation (ICE) procedure is exploited for estimating the parameters of the spatial model and the region-constrained temporal model. This new statistical method of motion detection has been successfully applied to real dynamic scenes and seems to be well suited for the temporal detection of noisy image sequences."
2151698,21239,9078,Confocal Disparity Estimation and Recovery of Pinhole Image for Real-Aperture Stereo Camera Systems,2007,"A single dense depth estimation using stereo or defocus cannot produce a reliable result due to the ambiguity problem. In this paper, we propose a novel anisotropic disparity estimation embedding a stereo confocal constraint for real-aperture stereo camera systems. If the focal length of a real-aperture stereo camera is just changed, the depth range is localized in a focused object which can be discriminated from defocused blurring. The focal depth plane is estimated by the displacement of tensors which are derived from generalized 2D Gaussian, since the point spread functions (PSF) in defocused blurring can be approximated by a shift-invariant Gaussian function. We localize the isotropic propagation in blurring over invariance by a sparse Laplacian kernel in Poisson solution. The matching of real-aperture stereo images is performed by observing the focal consistency. However, the isotropic propagation cannot exactly hold a non-parallel surface to the lens plane i.e., unequifocal surface. An anisotropic regularization term is employed to suppress the isotropic propagation near the non-parallel surface boundary. Our method achieves an accurate dense disparity map by sampling the disparities in focal points from multiple defocus stereo images. The pels in focal points are utilized to recover the pinhole image (i.e. an ideally focused image for all different depths)."
714496,21239,9078,Robust adaptive Wiener filtering,2012,"A recent paper by Anderes and Paul [1] analyze a regression characterization of a new estimator of lensing from cosmic microwave observations, developed by Hu and Okamoto [2, 3, 4]. A key tool used in that paper is the application of the robust generalized shrinkage priors developed 30 years ago in [5, 6, 7] to the problem of adaptive Wiener filtering. The technique requires the user to propose a fiducial model for the spectral density of the unknown signal but the resulting estimator is developed to be robust to misspecification of this model. The role of the fiducial spectral density is to give the estimator superior statistical performance in a “neighborhood of the fiducial model” while controlling the statistical errors when the fiducial spectral density is drastically wrong. One of the main advantages of this adaptive Wiener filter is that one can easily obtain posterior samples of the true signal given the unknown data. These posterior samples are particularly advantageous when studying non-linear functions of the signal, cross correlating with other independent measurements of the same signal and can be used to propagate uncertainty when the filtering is done in a scientific pipeline. In this paper we explore these advantages with simulations and examine the possibility of widespread application in more general image and signal processing problems."
1683288,21239,9078,3D-SSIM for video quality assessment,2012,"Effective and efficient objective video quality assessment (VQA) methods are highly desirable in modern visual communication systems for performance evaluation, quality control and resource allocation purposes. Simple VQA algorithms may be developed by direct extensions of still image quality assessment (IQA) approaches on a frame-by-frame basis. Advanced VQA methods take into account the temporal correlation and motion information contained in video signals but often lead to significantly increased computational complexity. Here we use a different approach to examine a video signal by considering it as a three-dimensional (3D) volume image. Specifically, we propose a 3D structural similarity (3D-SSIM) approach, which first creates a 3D quality map by applying SSIM evaluations within local 3D blocks, and then use local information content and local distortion based weighting methods to pool the quality map into a single quality measure. The resulting 3D-SSIM algorithm is computationally efficient and demonstrates highly competitive performance in comparison with state-of-the-art VQA algorithms when tested using four publicly available video quality databases."
2382256,21239,9078,View Synthesis for Robust Distributed Video Compression in Wireless Camera Networks,2007,"We propose a method for delivering error-resilient video from wireless camera networks in a distributed fashion over lossy channels. Our scheme is based on distributed source coding that exploits inter-view correlation among cameras with overlapping views. The main focus in this work is on robustness which is imminently needed in a wireless setting. The proposed approach has low encoding complexity, is robust while satisfying tight latency constraints, and requires no inter-camera communication. Our system is built on and is a multi-camera extension of PRISM[1], an earlier proposed single-camera distributed video compression system. Decoder motion search, a key attribute of single-camera PRISM, is extended to the multi-view setting by using estimated scene depth information when it is available. In particular, dense stereo correspondence and view synthesis are utilized to generate side-information. When combined with decoder motion search, our proposed method can be made insensitive to small errors in camera calibration, disparity estimation and view synthesis. In experiments over a simulated wireless channel, the proposed approach achieves up to 2.1 dB gain in PSNR over a system using H.263+ with forward error correction."
1782562,21239,9078,Modelling visual attention and motion effect for visual quality evaluation,2004,"The perceptual visual quality evaluation of human visual system (HVS) is very complex. It concerns almost all aspects of visual processing in the vision path, from low-level neuron activities to high-level visual perception. Existing perceptual visual quality metrics (VQMs) only considered several of the mechanisms of HVS, and many others are ignored. In this paper, two global modulatory factors, visual attention and motion suppression, are modelled and combined to form a mathematic expression - perceptual quality significant level (PQSL). To a certain extent, it is believed that PQSL values reflect the processing ability of the human brain on local visual content. To evaluate their effects on visual quality evaluation, two VQMs are proposed. One is a MSE-like VQM based in a PQSL-modulated JND profile, which was proposed in (Z. K. Lu et al, Proc. ICASSP'2004, v.3, p.705-708, 2004); the other VQM is based on Wang's visual quality assessment (Sig. Proc.:Image Comm., v.19, n.2, p.121-132, 2004), PQSL values are used to adjust the weights of his structural similarity index. Experimental results show that introduction of the global modulatory factors can improve the performance of current visual quality metrics."
935192,21239,9078,Dual-view medical image visualization based on spatial-temporal psychovisual modulation,2014,"Medical imaging technologies such as magnetic resonance imaging (MRI) and computerized tomographic (CT) are used to diagnose a wide range of medical diseases. Medical images are generated by detecting density differences between different tissues in the body. Multiple medical image visualization is of critical importance to diagnosis. This paper introduces a dual-view medical image visualization prototype based on spatial-temporal psychovisual modulation (STPVM). Temporal psychovisual modulation (TPVM) enables a single display to generate multiple visual content for different viewers. Spatial psychovisual modulation (SPVM) extends the idea of TPVM to spatial domain. STPVM combines TPVM and SPVM by exploiting both temporal and spatial redundancy of modern displays. Based on STPVM technology, one display can present even more images simultaneously. In this demo, two kinds of medical images e.g. T1 and T2 weighted MRI images, are presented simultaneously. Physicians can switch between either image by just moving the eye fixations. Since T1 and T2 are shown simultaneously and are aligned on the screen, it is more convenient for the physicians to get different information of the same spot from the T1 and T2 images. The developed demo is useful for physicians during surgery navigation and effectively reduces the burden of mental transfer."
565786,21239,9078,Effects of channel delays on underflow events of compressed video over the Internet,2002,"This paper presents an extensive statistical study and analysis of the effects of channel delays in the current (best-effort) Internet on underflow events in MPEG-4 video streaming. Two types of network delays are considered: end-to-end round-trip delays and delay jitter. Our data were collected in a seven-month real-time streaming experiment, which was conducted between a number of unicast dialup clients in more than 600 major USA cities and a backbone video server. Among other findings, our analysis shows that startup delays approximately 15-20 times the average roundtrip time (RTT) are required for the client to avoid 90% of late packets caused by delay jitter. Meanwhile, startup delays of only 3-4 times the average RTT are needed to achieve lost-packet recovery rates of 90% or more. Hence, a key finding of our study is that delay jitter represents a more challenging problem for video streaming applications than round-trip delays. We also show that the probability density function (PDF) of RTT samples, which are associated with retransmitted video packets, can be modeled using a Pareto distribution. This observation indicates that the upper tail of the RTT PDF decays slower than reported in earlier studies."
2427592,21239,9078,Online detection of snowcoverage and swing angles of electrical insulators on power transmission lines using videos,2009,"Apotential fatal problem for electrical power delivery through power lines in Northern countries is when snow or ice accumulates on electrical insulators. This could lead to snow or ice-induced outages and voltage collapse, causing huge economic loss. Further, large swing angles due to wind may cause short circuits. This paper presents a novel video surveillance system for detecting snow coverage on electric insulators and swing angles of insulators using videos from a remote outdoor 420 kV power transmission line. To the best of our knowledge, it is the first insulator snow surveillance system base on automatic image analysis techniques. We propose using hybrid techniques by combining histograms, boundaries and template cross-correlations for analyzing a broad range of scenarios caused by changing weather and lighting conditions. Experiments on videos captured during several month periods have shown promising and valuable estimation results. For image pixels related to snow on insulators, our system has yielded an average detection rate of 93% for good quality images and 67.6% for poor quality images, and a corresponding average false alarm of 9% and 18.1%."
1535848,21239,9078,Simultaneous sparsity model for multi-perspective video anomaly detection,2014,"Recently, sparsity based classification has been applied to video anomaly detection. A linear model is assumed over video features (e.g. trajectories) such that the feature representation of a new event is written as a sparse linear combination of existing feature representations in the dictionary. Sparsity based video anomaly detection has shown promise over alternate video anomaly detection methods in that the sparse representations exhibit excellent robustness under noise (common in surveillance videos) and missing or corrupted features, e.g. vehicle occlusion in transportation videos. One limitation of existing sparsity based video anomaly detection techniques is that they are based on only a single feature representation (known formally as video event encoding). One can easily envision that different event representations such as object trajectories and spatio-temporal volumes often contain correlated yet complementary information. In this paper, we propose to extend sparsity models based on single feature representations to simultaneous sparse representations of multiple feature representations. In this model, the matrix of sparse coefficients does not confirm to the commonly seen row-sparsity and a modified greedy heuristic approach that extends simultaneous orthogonal matching pursuit (SOMP) is needed to solve the resulting optimization problem. Experiments on two benchmark video datasets reveal that our method significantly outperforms state-of-the art approaches that utilize only a single-perspective or event encoding."
766301,21239,9078,Measuring the strength of partial encryption schemes,2005,"Partial encryption (PE) of compressed multimedia can greatly reduce the computational complexity by encrypting only a fraction of the data bits. It can also easily provide users with low-quality versions, while maintaining the high-quality version inaccessible to unauthorized users. However, it is necessary to realistically evaluate its security strength. Some of the cryptanalysis done for these techniques ignored important characteristics of the multimedia files, and used overly optimistic assumptions. We demonstrate potential weaknesses of such techniques studying attacks that exploit the information provided by non-encrypted bits, and the availability of side information (e.g., from analog signals). We show that a more useful measure of encryption strength is the complexity to reduce distortion, instead of recovering the encryption key. We consider attacks on PE that avoid error propagation (standard-compliant PE), and PE that try to exploit that property for security. In both cases we show that attacks that require complexity much lower than exhaustive enumeration of encrypted/key bits can successfully yield good quality content. Experimental results are shown for images, but the conclusions can be extended to partial encryption of video and other types of media."
660083,21239,8494,Region of interest determined by perceptual-quality and rate-distortion optimization in JPEG 2000,2004,"The region-of-interest (ROI) coding of Joint Photographic Experts Group (JPEG) 2000 is an important option to encode pictures on a limited bit rate. This work presents a scheme to automatically generate the ROI mask during the encoding process of JPEG 2000. In the proposed scheme, the embedded block coding with optimized truncation (EBCOT) is utilized to provide information of the significant states, rates and distortions at bit-planes for analyzing picture contents, and then determining an appropriate ROI mask that is optimized by the perceptual quality and rate-distortion function. During the bit-plane coding, the coefficients of discrete wavelet transform (DWT) in the magnitude refinement pass of the EBCOT are grouped into the interested or uninterested ones by using their pass scanning results from perceptual point of view. Furthermore, the rate-distortion function is applied to automatically determine the optimized ROI mask from the probable masks that are formed by the interested sub-blocks, according to the required bit rate. As compared to the scheme using the fixed-square or fixed-region ROI mask, the proposed scheme can provide clearer object contours and better image quality. Additionally, the proposed ROI determination scheme can be easily realized and embedded in JPEG 2000 encoding process."
709182,21239,9078,A robust Hough transform technique for description of multiple line segments in an image,1998,"The process of using the Hough transform (HT) to detect lines in an image involves the computation of the HT for the entire image, accumulating votes in an accumulator array and searching the array for peaks which hold information of potential lines present in the input image. The process of peak formation generates a butterfly shaped spread of votes in the accumulator array. The authors have used this property to adaptively define windows of interest around a detected peak to facilitate the description of multiple line segments within an image in terms of the coordinates of their end points. The developed technique has been employed to test several images composed of multiple line segments and the results in terms of accuracy of the determination of line segment mid points are presented. While most methods which employ the HT to detect line segments cannot handle the case of separate line segments formed by a colinear set of points, it is shown that the developed method can successfully do so. This algorithm would find applications in different areas of machine vision like robotics and manufacturing systems. Results of the application of the developed method, to detect lane markers and curve signs from a road scene captured by a CCD camera, to aid in the maneuvering of autonomous vehicles are presented."
2416244,21239,9078,Multiterminal Video Coding,2007,"Following recent works on the rate region of the quadratic Gaussian two-terminal source coding problem and limit-approaching code designs, this paper examines multiterminal source coding of two correlated video sequences to save the sum rate over independent coding. Specifically, the first video sequence is coded by H.264 and used at the joint decoder to facilitate Wyner-Ziv coding of the second video sequence. The first I-frame of the right sequence is successively coded by H.264 and Slepian-Wolf coding. An efficient stereo matching algorithm based on loopy belief propagation is then adopted at the decoder to produce pixel-level disparity maps between the corresponding frames of the two decoded video sequences on the fly. Based on the disparity maps, side information for both motion vectors and motion-compensated residual frames of the second sequence are generated at the decoder before Wyner-Ziv encoding. Experimental results on stereo video sequences using H.264, LDPC codes for Slepian-Wolf coding of the motion vectors and scalar quantization in conjunction with LDPC codes for Wyner-Ziv coding of the residual coefficients show savings in terms of the sum-rate when compared to separate H.264 coding at the same video quality."
2481559,21239,11470,COSMOS: Peer-to-Peer Collaborative Streaming Among Mobiles,2006,"In traditional mobile streaming networks such as 3G cellular networks, all users pull streams from a server. Such pull model leads to high streaming cost and problem in system scalability. In this paper, we propose and investigate a scalable and cost-effective protocol to distribute multimedia content to mobiles in a peer-to-peer manner. Our protocol, termed Collaborative Streaming among Mobiles (COSMOS), makes use of multiple description coding (MDC) and data sharing to achieve high performance. In COSMOS, only a few peers pull video descriptions through a telecommunication channel. Using a free broadcast channel (such as Wi-Fi and bluetooth), they share the descriptions to nearby neighbors in an ad-hoc manner. This way reduces greatly the telecommunication cost and cellular bandwidth requirement. As video descriptions are supplied by multiple peers, COSMOS is robust to peer failure. Since broadcasting is used to distribute video data, the protocol is highly scalable to large number of users. By taking turns to pull descriptions, we show through simulation that peers can effectively share, and hence substantially reduce, streaming cost. As peers can often obtain a number of descriptions from nearby neighbors, they enjoy lower delay as compared to a recent scheme CHUM."
1742661,21239,9078,Efficient communication of video using metadata,2011,"In traditional video coding schemes, motion information is tightly coupled to the prediction strategy. In this preliminary work, we depart from this model by utilizing metadata to convey motion information to the client; in particular, metadata conveys crude boundaries of objects together with motion information for these objects. Here, we are interested in applications where metadata itself carries semantics that the client is interested in, such as tracking information in surveillance applications. To keep things simple, we focus on the case where we have a single object to track. Therefore, we model each frame as a background region with a foreground region/object, enclosing each region by a quadrilateral that identifies it. The foreground quadrilateral does not follow the exact boundaries of the foreground object; it leaves the task of identifying these boundaries to the client. The advantages of metadata is that it provides a global representation of motion, which allows predicting a given object from potentially all the frames that contain that object. The approach is applicable in fully open loop systems such as in the case of the JPEG2000-Based Scalable Interactive Video (JSIV) paradigm. In this work, we present the concepts behind the proposed approach and detail the modifications introduced to the JSIV server and client policies, presenting some promising preliminary results."
1303233,21239,9078,Covariance-based adaptive deinterlacing method using edge map,2010,"The purpose of this article is to discuss deinterlacing results in a computationally constrained and varied environment. The proposed covariance-based adaptive deinterlacing method using edge map (CADEM) consists of two methods: the modified edge-based line averaging (MELA) method for plain regions and the covariance-based adaptive deinterlacing (CAD) method along the edges. The proposed CADEM uses the edge map of the interlaced input image for assigning the appropriate method between MELA and the modified CAD (MCAD) methods. We first introduce the MCAD method. The principle idea of the MCAD is based on the correspondence between the high-resolution covariance and the low-resolution covariance. The MCAD estimates the local covariance coefficients from an interlaced image using Wiener filtering theory and then uses these optimal minimum mean squared error interpolation coefficients to obtain a deinterlaced image. However, the MCAD method, though more robust than most known methods, was not found to be very fast compared with the others. To alleviate this issue, we propose an adaptive selection approach rather than using only one MCAD algorithm. The proposed hybrid approach of switching between the MELA and MCAD is proposed to reduce the overall computational load. A reliable condition to be used for switching the schemes is established by the edge map composed of binary image. The results of computer simulations showed that the proposed methods outperformed a number of methods presented in the literature."
1890605,21239,9078,Sirface vs. Fisherface: recognition using class specific linear projection,2003,"Using a novel data dimension reduction method proposed in statistics, we develop an appearance-based face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as coordinate in a high-dimensional space. However, since faces are not truly Lambertian surfaces and indeed produce self-shadowing, images deviates from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation using Sliced inverse regression (SIR) [K.C. Li, 1991]. Our face recognition algorithm termed as Sirface produces well-separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expression. Sirface is shown to be equivalent to the well known Fisherface algorithm [P.N. Belhumeur, et al., 1997] in the subspace sense. However, Sirface is shown to produce the optimal reduced subspace (with the fewest dimensions) resulting in a lower error rate and reduced computational expense. Experimental results comparing Sirface to Fisherface on the Yale face database are presented."
1257396,21239,9078,Automatic detection of small spherical lesions using multiscale approach in 3D medical images,2013,"Automated detection of small, low level shapes such as circular/spherical objects in images is a challenging computer vision problem. For many applications, especially microbleed detection in Alzheimer's disease, an automatic pre-screening scheme is required to identify potential seeds with high sensitivity and reasonable specificity. A new method is proposed to detect spherical objects in 3D medical images within the multi-scale Laplacian of Gaussian framework. The major contributions are(1)breaking down 3D sphere detection into 1D line profile detection along each coordinate dimension, (2) identifying center of structures bynormalizing the line response profile and (3) employing eigenvalues of the Hessian matrix at optimum scale for the center points to determine spherical objects. The method is validated both on simulated data and susceptibility weighted MRI images with ground truth provided by a medical expert. Validation results demonstrate that the current approach has higher performance in terms of sensitivity and specificity and is effective in detecting adjacent microbleeds, with invariance to intensity, orientation, translation and object scale."
322137,21239,9078,CART-based feature selection of hyperspectral images for crop cover classification,2003,"In this paper, we propose a procedure to reduce data dimensionality while preserving relevant information for posterior crop cover classification. The huge amount of data involved in hyperspectral image processing is one of the main problems in order to apply pattern recognition techniques. We propose a dimensionality reduction strategy that eliminates redundant information and a subsequent selection of the most discriminative features based on classification and regression trees (CART). CART allow feature selection based on the classification success, it is a non-linear method and specially allows knowledge discovery. The main advantage of our proposal relies on model interpretability, since we can get qualitative information by analyzing the surrogate and main splits of the tree. This method is tested with a crop cover recognition application of six hyperspectral images from the same area acquired with the 128-bands HyMap spectrometer. Even though CART do not provide the best results in classification it is useful for a previous pre-processing step of feature selection. Finally, we analyze the selected bands of the input space in order to gain knowledge on the problem and to give a physical interpretation of results."
408157,21239,9078,Low complexity lossless video compression,2004,"We present a line-based adaptive lossless video compression (LALVC) algorithm for interactive multimedia applications that demand low complexity and low latency. Communications between high-resolution display and storage devices require high bandwidth for exchanging raw data. To reduce cost of video transmission without losing data accuracy, lossless video compression is necessary. Considering low complexity and low delay, LALVC adopts a simple and efficient architecture that adopts one-pass, raster-scan, transform-free coding process and a simple predictor. For low latency, zero-motion prediction and one-frame buffer are used to reduce temporal redundancy. In addition, to maximize the coding efficiency for both natural and computer-generated video sequences, LALVC adaptively selects the best coding mode for each line of a frame. The entropy coding of each line is based on Golomb code that can enhance coding efficiency with less computation load and is easy for hardware realization. The simulation results show that temporal preprocessing and line-based mode decision of LALVC can increase compression ratio with properly increased complexity as compared to that of JPEG-LS."
230528,21239,9078,JPEG2000-based shape adaptive algorithm for the efficient coding of multiple regions-of-interest,2004,"The JPEG2000 standard supports two methods to code regions-of-interest (ROIs) in an image-the maxshift method and the scaling-based method. Compared to the maxshift method, the scaling-based method is preferable in many applications because it allows the nonROI (background) region to be partially coded prior to coding all of the ROIs. This is accomplished by supporting arbitrary bitplane shift factors and by filling the most significant bit-planes of the nonROI region with zeros after the ROI bitplanes have been shifted up in value. In both of these methods, the embedded block coding with optimum truncation (EBCOT) algorithm is then applied to code the wavelet coefficients. However, when multiple regions-of-interest need to be coded, the performance of the scaling-based method degrades significantly since the EBCOT coder is no longer able to efficiently code the nonROI region, which is now less contiguous as compared to the single-ROI case. Also, the large header information that is required in JPEG2000 for each ROI degrades the coding performance. This paper presents an improved scaling-based method for the efficient coding of multiple, arbitrarily-shaped ROIs using a modified EBCOT algorithm. The proposed method utilizes the shape information of the different ROIs to generate a stripe mask, which is then used to optimize the coding performance. Coding results and comparisons with the JPEG2000 scaling-based method are presented to illustrate the improved performance of the proposed scheme."
745638,21239,9078,Biophysical Active Contours for Cell Tracking I: Tension and Bending,2006,"Automatic segmentation and tracking of biological objects from dynamic microscopy data is of great interest for quantitative biology. A successful framework for this task are active contours, curves that iteratively minimize a cost function, which contains both data-attachment terms and regularization constraints reflecting prior knowledge on the contour geometry. However the choice of these latter terms and of their weights is largely arbitrary, thus requiring time-consuming empirical parameter tuning and leading to sub-optimal results. Here, we report on a first attempt to use regularization terms based on known biophysical properties of cellular membranes. The present study is restricted to 2D images and cells with a simple cytoskeletal cortex underlying the membrane. We describe our new active contour model and its implementation, and show a first application to real biological images. The obtained segmentation is slightly better than standard active contours, however the main advantage lies in the self-consistent and automated determination of the weights of regularization terms. This encouraging result will lead us to extend the approach to 3D and more complex cells."
2561192,21239,9078,Parallel rate-distortion optimized intra mode decision on multi-core graphics processors using greedy-based encoding orders,2009,"Rate-distortion (RD) optimized intra-prediction mode selection can lead to significant improvement in coding efficiency in intra-frame encoding. However, it would incur considerable increase in encoding complexity. In this paper, we investigate how multi-core Graphics Processing Units (GPUs) can be efficiently utilized to undertake the task of RD optimized intra mode selection in AVS and H.264 video encoding. Achieving efficient GPU-based intra mode decision, however, could be non-trivial. It is because the mode decision of the current block would depend on the reconstructed data of the neighboring blocks. Therefore, the coding modes of neighboring blocks would need to be computed first before that of the current block can be determined. This dependency poses challenge to computation on multi-core GPUs, which rely heavily on parallel data processing to achieve superior speedups. To address this issue, we analyze the data dependency in intra mode decision, and propose novel greedy-based encoding orders to achieve highly parallel processing. We also prove that the proposed greedy-based orders are optimal in terms of execution time. Experimental results suggest that the proposed GPU-based intra mode decision compares favorably to the counterpart implemented on a single-core CPU."
887068,21239,9078,Pilot study of applying shape analysis to liver cirrhosis diagnosis,2013,"This paper explores the potential of applying shape analysis to classify normal/cirrhotic liver and in addition estimate the severity of abnormal cases. Conventional Computer-Aided Diagnosis (CAD) systems are developed for automatically providing a binary output as a second opinion to assist radiologists to draw conclusions about the condition of the pathology (normal or abnormal). After the disease is diagnosed, grasping the proceeding stage of the abnormal degree is essential for adopting the appropriate strength of treatment. However, none of existing CAD system is well established for such a challenging task. Liver cirrhosis has an important feature: morphological changes of the liver and the spleen occur during the clinical course of liver cirrhosis. In this study we constructed liver, spleen and their joint Statistical Shape Models (SSMs) to quantitatively assess the global shape variation and selected several modes from the SSMs. Then we learnt a mapping function between coefficients of selected modes and the ground truth staging label by Support Vector Regression (SVR). Using this mapping function, the proceeding stage of new input data can be estimated. Experimental results have validated the potential of our method on assisting the cirrhosis diagnosis."
526053,21239,11470,Structure-preserving Image Quality Assessment,2015,"Perceptual Image Quality Assessment (IQA) has many applications. Existing IQA approaches typically work only for one of three scenarios: full-reference, non-reference, or reduced-reference. Techniques that attempt to incorporate image structure information often rely on hand-crafted features, making them difficult to be extended to handle different scenarios. On the other hand, objective metrics like Mean Square Error (MSE), while being easy to compute, are often deemed ineffective for measuring perceptual quality. This paper presents a novel approach to perceptual quality assessment by developing an MSE-like metric, which enjoys the benefit of MSE in terms of inexpensive computation and universal applicability while allowing structural information of an image being taken into consideration. The latter was achieved through introducing structure-preserving kernelization into a MSE-like formulation. We show that the method can lead to competitive FR-IQA results. Further, by developing a feature coding scheme based on this formulation, we extend the model to improve the performance of NR-IQA methods. We report extensive experiments illustrating the results from both our FR-IQA and NR-IQA algorithms with comparison to existing state-of-the-art methods."
1497945,21239,9078,Human tracking & visual spatio-temporal statistical analysis,2014,"In this work, a novel, multi-space, real-time and robust human tracking system is going to be presented. The system exploits a multi-camera network monitoring the multi-space dynamic environment under interest, detecting and tracking the humans in it. The system is able to handle the dynamic changes of the environment, as well as partial occlusions utilizing virtual top cameras. Furthermore, the system is able to real-time visualize the detection and tracking results on the architectural map of the dynamic environment, as well as a variety of statistics. The visual spatio-temporal analysis of the tracked data are presented in a consolidated form for the overall monitoring area and analytically for each space separately and for each tracked human. These statistics could be also combined with the energy consumption in the area, as well as with other environmental data providing semantic information such as comfort. The overall system is equipped with a number of visual interactive tools providing real-time spatio-temporal human presence analysis offering to the user the opportunity to capture and isolate the areas/spaces with high human presence, the days and times of high human presence, to correlate this information with the potential energy consumption and indicators such as comfort."
2225118,21239,9078,Video transcoding for fast forward/reverse video playback,2002,"Fast forward and fast reverse playbacks are two common video browsing functions provided in many analog and digital video players. They help users quickly find and access video segments of interest by scanning through the content of a video at a faster than normal playback speed. We propose a video transcoding approach to realizing fast forward and reverse video playbacks by generating a new compressed video from a pre-coded video. To reduce the computational requirements, we design and compare several fast algorithms for estimating the motion vectors required in transcoded video. To accommodate changes due to frame skipping for fast video playback, we also alter the group-of-pictures structure of transcoded video. In addition, subjective tests are conducted to assess the minimum video peak-signal-to-noise-ratio degradation that is perceptible to viewers at different fast playback speeds. To this end, we obtain an adaptive video transcoding method, which combines intra-coding and inter-coding with a fast motion vector reestimation method to strike a good balance between computational complexity and transcoded video quality. Experimental results are reported to show the efficacy of the proposed method."
611036,21239,11052,Compressive acquisition of dynamic scenes,2010,"Compressive sensing (CS) is a new approach for the acquisition and recovery of sparse signals and images that enables sampling rates significantly below the classical Nyquist rate. Despite significant progress in the theory and methods of CS, little headway has been made in compressive video acquisition and recovery. Video CS is complicated by the ephemeral nature of dynamic events, which makes direct extensions of standard CS imaging architectures and signal models infeasible. In this paper, we develop a new framework for video CS for dynamic textured scenes that models the evolution of the scene as a linear dynamical system (LDS). This reduces the video recovery problem to first estimating the model parameters of the LDS from compressive measurements, from which the image frames are then reconstructed. We exploit the low-dimensional dynamic parameters (the state sequence) and high-dimensional static parameters (the observation matrix) of the LDS to devise a novel compressive measurement strategy that measures only the dynamic part of the scene at each instant and accumulates measurements over time to estimate the static parameters. This enables us to considerably lower the compressive measurement rate considerably. We validate our approach with a range of experiments including classification experiments that highlight the effectiveness of the proposed approach."
1888790,21239,9078,Multirate coding of 3D medical data,2000,"The last generation medical imaging equipment produce multidimensional (3D or 3D+time) data distributions. On a coding perspective, it is reasonable to expect that the exploitation of the full dimensional correlation among data samples would lead to a sensible improvement in compression performances, especially for isotropic datasets. We propose a fully three-dimensional wavelet-based coding system providing a finely-graded up to lossless data representation in a single bistream. The data are first decorrelated by a 3D discrete wavelet transform, performed by the non-linear lifting scheme mapping integers to integers. This enables the lossless mode and permits the in-place implementation of the transform at a reduced computational complexity. The coding scheme is inspired to the layered-zero coding proposed by Taubman and Zakhor (1994), extended to handle fully 3D subband structures. Performances are characterized with respect to both the 2D version of the same algorithm and the JPEG standard. The rate-saving is strongly influenced by the amount of the data correlation in the z dimension, ranging between 16.5% and 5.5% for the considered datasets."
653443,21239,9078,Phase Distortion Correction for See-Through-The-Wall Imaging Radar,2006,"See through the wall (STTW) applications have become of high importance to law enforcement, homeland security and defense needs. In this work surface penetrating radar is simulated using basic physical principles of radar propagation. Wavenumber migration is employed to form 2D images of objects found behind a wall. It is shown that this technique cannot properly image with the wall present because of an unknown phase delay experienced by the electromagnetic waves as they pass through the wall. Two approaches are taken to estimate this phase by looking at the direct backscatter signal from the wall. The first is a dual phase approach, which uses a non-parametric technique to find the phase at every frequency. The second method is a dual frequency approach. The two frequencies are close enough together that the reflection coefficients are approximately equal. This approximation allows for more observations than unknown parameters. The surface reflection coefficient, back wall coefficient, and phase are simultaneously determined using an iterative, non-linear (Newton-Raphson) successive approximation algorithm. Comparisons are performed for a simple scenario of three point scatterers with and without phase correction."
2012634,21239,9078,Switched error concealment and robust coding decisions in scalable video coding,2000,"This work introduces two complementary techniques to improve the packet loss resilience of scalable video coding systems. First, a switch per-pixel error concealment (SPEC) scheme is proposed, which allows the decoder to exploit information from both the current base layer and previous enhancement-layer frame for the reconstruction of missing enhancement-layer blocks. Based on the packet loss history and the quantized base-layer data, the algorithm switches per pixel between the two information sources. SPEC is shown to consistently outperform standard concealment methods. The second main contribution is concerned with encoder decision optimization. Enhancement layer prediction modes are selected so as to minimize the overall decoder reconstruction distortion, which is due to quantization, packet loss and error propagation. The distortion computation uses a recursive optimal per-pixel estimate (ROPE) to accurately account for the effects of error concealment as well as spatial and temporal error propagation. Simulation results show that ROPE-based mode selection substantially outperforms conventional prediction mode selection schemes. Finally, the combination of SPEC at the decoder and ROPE-based mode selection at the encoder is shown to achieve significant additional performance gains."
586998,21239,9078,A case study in using human similarity measure for automated object recognition,1999,"Image understanding often involves object recognition, where a basic question is how to decide whether a match is correct. Typically the best match (among a set of prestored objects) is assumed to be the correct match. This may work well in controlled environments (closed world). But, in uncontrolled environments (open world), the test object may not belong to the prestored object classes. In uncontrolled environments, a metric similarity measure (e.g. Euclidean) in conjunction with a threshold is used. However, based on psychophysical studies this is very different from, and far inferior to, human capabilities. To accept or reject a match, we introduce an approach that avoids metric similarity measures and the use of thresholds as it attempts to employ similarity measures used by humans. In the absence of sufficient real data, the approach allows to specifically generate an arbitrarily large number of training exemplars projecting near classification boundary. For aircraft detection, the performance of a neural network trained on such a training set, was comparable to that of a human expert, and far better than a network trained only on the available real data. Furthermore, the results were considerably better than those obtained using a Euclidean discriminator."
2478490,21239,9078,Data-rate constrained lattice vector quantization: a new quantizing algorithm in a rate-distortion sense,1995,"This paper describes an image coding scheme using lattice vector quantizers where the emphasis is put on a new lattice vector quantization approach. The adaptive signal decomposition uses wavelet packets which allow to best match the decomposition to the signal non-stationary characteristics. The quantizers used here are data rate constrained lattice vector quantizers. A classical rate-distortion algorithm [Ramchandran and Vetterli, 1993] based on two nested optimization processes allows to jointly optimize transformation and quantization and is used as a first step in the quantization procedure described here. It allows to define in each subband the lattice spacing (or scaling factor) minimizing the overall distortion for a given bit rate and to choose by a pruning algorithm the best structure of decomposition. An additional procedure developed here allows (by exploiting the lattice properties) to project a vector on a lattice point providing a better rate-distortion tradeoff. In addition, a new partitioning of the vector space allowing to divide the source of vectors into three sub-sources is introduced in order to improve the coding efficiency. In the rate distortion plane, this new quantization procedure brings a significant improvement of about 0.5-0.6 dB with respect to the classical rate-distortion algorithm."
1089492,21239,9078,Adaptive loop filter technology based on analytical design considering local image characteristics,2011,"Based on Wiener algorithm, the adaptive loop filter (ALF) technology is applied to minimize coding distortion over a frame. As the ALF is designed based on the Wiener algorithm, coding performance can be improved by spatially adaptive selection of filter coefficients dependent on a local image feature. However, since the conventional ALF approaches designed only a single set of frame basis filter coefficients, the filter coefficients are not necessarily optimal to achieve the significant coding gain. As the conventional work, the modified adaptive loop filter approach that employs the segmentation pattern has been proposed. This approach divides the current frame into a few segments based on the structure of image texture, and the filter coefficients are determined for each segment type. Although 14 candidates are defined as available segmentation modes, further improvement can be expected because such segmentation can not handle the complicated combination of various texture characteristics within a video frame. From this perspective, we propose an enhanced ALF approach based on spatially adaptive control of filter design dependent on the texture property of the local image region. Experimental results show that the bit reduction performance is improved by 1.6 points in the maximum case against the conventional scheme. Furthermore, the proposed scheme contributes to improving the subjective picture quality."
1594610,21239,9078,"Factorizing speaker, lexical and emotional variabilities observed in facial expressions",2012,"An effective human computer interaction system should be equipped with mechanisms to recognize and respond to the affective state of the user. However, spoken message conveys different communicative aspects such as the verbal content, emotional state and idiosyncrasy of the speaker. Each of these aspects introduces variability that will affect the performance of an emotion recognition system. If the models used to capture the expressive behaviors are constrained by the lexical content and speaker identity, it is expected that the observed uncertainty in the channel will decrease, improving the accuracy of the system. Motivated by these observations, this study aims to quantify and localize the speaker, lexical and emotional variabilities observed in the face during human interaction. A metric inspired in mutual information theory is proposed to quantify the dependency of facial features on these factors. This metric uses the trace of the covariance matrix of facial motion trajectories to measure the uncertainty. The experimental results confirm the strong influence of the lexical information in the lower part of the face. For this facial region, the results demonstrate the benefit of constraining the emotional model on the lexical content. The ultimate goal of this research is to utilize this information to constrain the emotional models on the underlying lexical units to improve the accuracy of emotion recognition systems."
2852372,21239,11470,Efficient plenoptic imaging representation: Why do we need it?,2016,"The 3D representation of the world visual information has been a challenge for a long time both in the analogue and digital domains. At least in the past decade, 3D stereo-based solutions have become very common. However, several constraints and limitations ended up causing a negative impact on its user popularity and market deployment. Recent developments in terms of acquisition and display devices have shown that it is possible to offer more immersive and powerful 3D experiences by adopting higher dimensional representations. In this context, the so-called plenoptic function offers an excellent framework to analyze and discuss the recent and future developments towards improved 3D imaging representations, functionalities and applications. Since they are associated to huge amounts of data, the new imaging modalities such as light fields and point clouds critically ask for appropriate efficient coding solutions. In this context, the main objective of this paper is to present, organize and discuss the recent trends and future developments on 3D visual data representation in a plenoptic function framework. This is critical to effectively plan the next research and standardization steps on 3D imaging representation and coding."
354538,21239,11187,Feature Selection for Neural-Network Based No-Reference Video Quality Assessment,2009,"Design of algorithms that are able to estimate video quality as perceived by human observers is of interest for a number of applications. Depending on the video content, the artifacts introduced by the coding process can be more or less pronounced and diversely affect the quality of videos, as estimated by humans. In this paper we propose a new scheme for quality assessment of coded video streams, based on suitably chosen set of objective quality measures driven by human perception. Specifically, the relation of large number of objective measure features related to video coding artifacts is examined. Standardized procedure has been used to calculate the Mean Opinion Score (MOS), based on experiments conducted with a group of non-expert observers viewing SD sequences. MOS measurements were taken for nine different standard definition (SD) sequences, coded using MPEG-2 at five different bit-rates. Eighteen different published approaches for measuring the amount of coding artifacts objectively were implemented. The results obtained were used to design a novel no-reference MOS estimation algorithm using a multi-layer perceptron neural-network."
2038719,21239,9078,Wavelet decomposition and reconstruction using arbitrary kernels: a new approach,1997,"In a previous paper, we presented a method to design perfect reconstruction filters using arbitrary lowpass filter kernels and presented fast filters with compression performance surpassing the well-known 9/7 biorthogonal filters. This paper improves the results obtained by Polyak and Pearlman (see Proc. IEEE International Conference on Image Processing, Santa Barbara, CA, vol.1, p.660-63, 1997), presenting filters comparable in compression performance and faster than the biorthogonal 10/18 filters. Furthermore, we combine our approach with the lifting and prediction schemes similar to the ones discussed by Said and Pearlman (see IEEE Trans. on Image Processing, vol.5, p.1303-10, 1996) in deriving the SS-P filters and later extended by Sweldens (see Appl. Comput. Harm. Anal., vol.3, no.2, p.186-200, 1996), thus obtaining integer to integer transforms whose performance is comparable to one of the S+P filters. At this stage, our algorithms are, however considerably slower. In any case it seems that the flexibility of our method shows some promise to serve as a basis for finding new integer to integer filters."
2092602,21239,9078,Threshold-free pattern-based low bit rate video coding,2008,"Pattern-based video coding (PVC) has already established its superiority over recent video coding standard H.264, at low bit rate because of an extra pattern-mode to segment out the arbitrary shape of the moving region within the macroblock (MB). To determine the pattern-mode, the PVC however uses three thresholds to reduce the number of MBs coded using the pattern- mode. By setting these content-sensitive thresholds to any predefined values, the technique risks ignoring some MBs that would otherwise be selected by the rate-distortion optimization function for this mode. Consequently, the ultimate achievable performance is sacrificed to save motion estimation times. In this paper, a novel PVC scheme is proposed by removing all thresholds to determine this mode and hence more efficient performance is achieved without knowing the content of the video sequences. To keep computational complexity in check, pattern motion is approximated from the motion vector of the MB. In addition, efficient pattern similarity metric and new Lagrangian multipliers are also developed. The experimental results confirm that this new scheme improves the image quality by at least 0.5 dB and 1.0 dB compared to the existing PVC and the H.264 respectively."
1305054,21239,9078,"IRIS Segmentation: Detecting Pupil, Limbus and Eyelids",2006,"This paper presents an active contour model to accurately detect pupil boundary in order to improve the performance of iris recognition systems. The contour model takes into consideration that an actual pupil boundary is a near-circular contour rather than a perfect circle. Two types of controlling force models, introduced as internal and external forces, are designed to properly activate the contour and locate it over the pupil boundary. The internal forces are designed to smooth the curve as well as to keep it close to a circular shape by pushing the contour vertices to their local radial mean. The external forces, which are responsible for pulling the contour vertices toward the pupil boundary, are designed based on a circular-curve gradient measurement with a proper angular range with respect to the contour center. In addition, an iterative algorithm has been developed in order to capture limbus and eyelids. The developed algorithm iteratively searches the limbus and eyelids boundaries and excludes the detected eyelids areas that cover the iris. Excluding the eyelids leads to a more precise search for limbus in the next iteration and the search is completed when the circular parameters of the limbus converge to fixed values. The eyelid contours are modeled as elliptic curves considering the spherical shape of an eyeball and the search is based on the expected contour in different degrees of eye openness."
2396300,21239,9616,Super-resolution enhancement of text image sequences,2000,"The objective of this work is the super-resolution enhancement of image sequences. We consider in particular images of scenes for which the point-to-point image transformation is a plane projective transformation. We first describe the imaging model, and a maximum likelihood (ML) estimator of the super-resolution image. We demonstrate the extreme noise sensitivity of the unconstrained ML estimator. We show that the Irani and Peleg (1991, 1993) super-resolution algorithm does not suffer from this sensitivity, and explain that this stability is due to the error back-projection method which effectively constrains the solution. We then propose two estimators suitable for the enhancement of text images: a maximum a posteriori (MAP) estimator based on a Huber prior and an estimator regularized using the total variation norm. We demonstrate the improved noise robustness of these approaches over the Irani and Peleg estimator. We also show the effects of a poorly estimated point spread function (PSF) on the super-resolution result and explain conditions necessary for this parameter to be included in the optimization. Results are evaluated on both real and synthetic sequences of text images. In the case of the real images, the projective transformations relating the images are estimated automatically from the image data, so that the entire algorithm is automatic."
2087650,21239,9078,"Semantic based categorization, browsing and retrieval in medical image databases",2002,"Content-based retrieval (CBIR) methods in medical databases have been designed to support specific tasks, such as retrieval of digital mammograms or 3D MRI images. These methods cannot be transferred to other medical applications since different imaging modalities require different types of processing. To enable content-based queries in diverse collections of medical images, the retrieval system must be familiar with the current image class prior to the query processing. We describe a novel approach for the automatic categorization of medical images according to their modalities. We propose a semantically based set of visual features, their relevance and organization for capturing the semantics of different imaging modalities. The features are used in conjunction with a new categorization metric, enabling intelligent annotation, browsing/searching of medical databases. Our algorithm provides basic semantic knowledge about the image, and may serve as a front-end to the domain specific medical image analysis methods. To demonstrate the effectiveness of our approach, we have designed and implemented an Internet portal for browsing/querying online medical databases, and applied it to a large number of images. Our results demonstrate that accurate categorization can be achieved by exploiting the important visual properties of each modality."
827729,21239,9078,Efficient depth map coding using linear residue approximation and a flexible prediction framework,2012,"The importance to develop more efficient 3D and multiview data representation algorithms results from the recent market growth for 3D video equipments and associated services. One of the most investigated formats is video+depth which uses depth image based rendering (DIBR) to combine the information of texture and depth, in order to create an arbitrary number of views in the decoder. Such approach requires that depth information must be accurately encoded. However, methods usually employed to encode texture do not seem to be suitable for depth map coding. In this paper we propose a novel depth map coding algorithm based on the assumption that depth images are piecewise-linear smooth signals. This algorithm is designed to encode sharp edges using a flexible dyadic block segmentation and hierarchical intra-prediction framework. The residual signal from this operation is aggregated into blocks which are approximated using linear modeling functions. Furthermore, the proposed algorithm uses a dictionary that increases the coding efficiency for previously used approximations. Experimental results for depth map coding show that synthesized views using the depth maps encoded by the proposed algorithm present higher PSNR than their counterparts, demonstrating the method's efficiency."
1171973,21239,9078,2-D non-separable GenLOT with trend vanishing moments,2010,"A novel design method of 2-D non-separable linear-phase paraunitary filter banks (LPPUFBs) is proposed. The trend vanishing moment (TVM) condition is newly derived for the lattice structure. The TVM condition can be regarded as a natural extension of 1-D VM to 2-D one and alternative of the conventional directional vanishing moments(DVMs). The structure is based on the 2-D DCT and can be regarded as an extension of GenLOT to the 2-D non-separable case. Because of the non-separability, the bases are allowed to be anisotropic and directional in addition to the fixed-subsampling, overlapping, orthogonal, real-valued, symmetric and compact-support property. As a previous work, the authors have shown that the block-wise implementation is appealing for the image processing application since the technique serves boundary operation for size-limitation, seamless basis alteration and compatibility with the block DCT. This work further contributes to yield the TVM condition so that piece-wise smooth images with smooth contours are compactly approximated. The condition is imposed directly on the design parameters and, therefore, the desirable properties of the lattice structure are maintained. From an experimental result of coding, it is shown that the TVM condition gives smooth reconstruction of a diagonal texture image."
1141639,21239,9078,One-round renormalization based 2-bin/cycle H.264/AVC CABAC encoder,2011,"Context-based Adaptive Binary Arithmetic Coder (CABAC) is the advanced entropy coding tool employed by main and higher profiles of H.264/AVC. As compared with Context-based Adaptive Variable Length Coding (CAVLC), under the same bit rate, CABAC achieves up to 0.5dB PSNR gain. On the other hand, the high complexity of CABAC severely hinders the whole encoder throughput. To overcome the throughput bottleneck of CABAC, the authors devise the one-round renormalization and the associated VLSI architecture to omit the multiple-iteration operation of one bin's encoding. The proposed full-context CABAC hardwired encoder garners the constant 2-bin/cycle throughput. Using TSMC one-poly nine-metal 90nm CMOS technology, the prototyping is implemented with 33.9k logic gates and 1562-bit on-chip SRAM. In the worst operating conditions (0.9V, 125 ° C), the operating frequency is 238.1MHz, which can support HDTV720p real-time encoding at 329fps frame rate with the quantization parameter (QP) not less than 18."
2454370,21239,9078,Image Processing for Teaching: a national dissemination program,1994,"The Image Processing for Teaching (IPT) project provides a powerful medium to excite students about science and mathematics, especially children from minority groups and others whose needs have not been met by traditional coded ways of teaching these subjects. IPT offers open-ended opportunities for exploration, discovery, and quantitative analysis. Using professional quality software on microcomputers, students explore a variety of scientific data sets, including biomedical imaging, Earth remote sensing and meteorology data, and planetary exploration images. They also learn about the mathematical concepts that underlie image processing. IPT curriculum materials cover all areas of mathematics and science for elementary school through college levels, allowing this tool to be used across a variety of subjects and student interests. Through the non-profit Center for Image Processing in Education (CIPE), teachers across the country are learning image processing and how it can be incorporated into their classes. CIPE provides materials, implementation support, and teacher education for school districts, for colleges, and for other education projects that want to incorporate image processing into their educational programs. >"
2513315,21239,9078,Bayesian wavelet-domain image modeling using hidden Markov trees,1999,"Wavelet-domain hidden Markov models have proven to be useful tools for statistical signal and image processing. The hidden Markov tree (HMT) model captures the key features of the joint statistics of the wavelet coefficients of real-world data. One potential drawback to the HMT framework is the need for computationally expensive iterative training (using the EM algorithm, for example). In this paper, we propose two reduced-parameter HMT models that capture the general structure of a broad class of grayscale images. The image HMT (iHMT) model leverages the fact that for a large class of images the structure of the HMT is self-similar across scale. This allows us to reduce the complexity of the iHMT to just nine easily trained parameters (independent of the size of the image and the number of wavelet scales). In the universal HMT (uHMT) we take a Bayesian approach and fix these nine parameters. The uHMT requires no training of any kind. While simple, we show using a series of image estimation/denoising experiments that these two new models retain nearly all of the key structures modeled by the full HMT. Based on these new models, we develop a shift-invariant wavelet denoising scheme that outperforms all algorithms in the current literature."
2339590,21239,9078,Tracking fast-rolling leukocytes in vivo with active contours,2002,"We propose and demonstrate an active contour technique to track fast-rolling leukocytes observed in vivo from video microscopy. A rolling leukocyte is an activated white blood cell that interacts with the vessel wall (the endothelium) in the inflammatory process. Tracking is enhanced here to accommodate fast-moving cells. To tackle the task of tracking wherein only low temporal resolution is possible, we have introduced an energy-minimizing framework and obtained a partial differential equation (PDE) based active contour evolution technique. The proposed PDEs are shown to be an initialization-insensitive version of the gradient vector flow (GVF) proposed by Xu and Prince (1998). We modify the GVF-PDEs by adding a Dirichlet type boundary condition (BC) based on the initial position of the active contour and the direction of cell movement. Using actual intravital experiments, we compare the performance of the proposed active contour tracker with the Dirichlet BC, the active contour tracker without the BC, the correlation tracker and the centroid tracker. The comparative results provide evidence of the advantages of the proposed method in terms of increased number of frames successfully tracked and reduced localization error."
2280324,21239,9078,A generic method for generating multispectral filter arrays,2004,"The technology of color filter arrays (CFA) has been widely used in the digital camera industry since it provides several advantages like low cost, exact registration, and strong robustness. The same motivations also drive the design of multi-spectral filter arrays (MSFA), in which more than three color bands are used (e.g. visible and infrared). Although considerable research has been reported to optimally reconstruct the full-color image using various interpolation algorithms, studies on the intrinsic properties of these filter arrays as well as the underlying design principles have been very limited. In this paper, we identify the properties a CFA should possess and extend the design philosophy to MSFA. Based on these discussions, we develop a generic MSFA generation method starting from a checkerboard pattern with both rectangular and hexagonal tessellations. By manipulating this pattern through a combination of decomposition and subsampling steps, we can generate MSFAs that satisfy all the design requirements. We show, through case studies, that most of the CFAs currently used by the industry can be derived as special cases. To evaluate the performance of MSFAs, we design a metric, referred as the static coefficient (SC), to measure the uniformity of MSFAs."
2360954,21239,11470,Wavelet-based Bayesian estimator for Poisson noise removal from images,2003,"Images are, in many cases, degraded even before they are encoded. Emission and transmission tomography images, X-ray films, and photographs taken by satellites are usually contaminated by quantum noise, which is Poisson distributed. Poisson shot noise is a natural generalization of a compound Poisson process when the summands are stochastic processes starting at the points of the underlying Poisson process. Unlike additive Gaussian noise, Poisson noise is signal-dependent and separating signal from noise is a difficult task. A wavelet-based maximum likelihood for a Bayesian estimator that recovers the signal component of the wavelet coefficients in original images by using an alpha-stable signal prior distribution is extended to the Poisson noise removal from a previous investigation. As we discussed in our earlier papers that Bayesian estimator can approximate impulsive noise more accurately than other models and that in the general case the Bayesian processor does not have a closed-form expression. The parameters relative to Bayesian estimators of the model are carefully investigated after an investigation of a-stable simulations for a maximum likelihood estimator. As an example, an improved Bayesian estimator that is a natural extension of other wavelet denoising (soft and hard threshold methods) via a colour image is presented to illustrate our discussion."
977110,21239,9078,Analysis of green noise error diffusion,2009,"Unlike blue-noise halftone patterns which are composed of isolated pixels, green noise patterns are composed of pixel-clusters. This makes green noise halftones robust to printer distortions due to the poor isolated pixel reproduction. Levien modified error diffusion by adding a hysteresis filter that feeds a filtered version of the past outputs into the quantizer input in classic error diffusion in order to promote dot-clustering. The patterns produced by the Levien algorithm were analyzed by Lau, Arce and Galagher who coined the term “green-noise” since the pattern power spectrum was mid-frequency rather than high-frequency (characteristic of “blue-noise” halftones). However no work has been done to study Leviens algorithm directly in the frequency domain and understand why it works as a green-noise generator. In this paper we will first analyze the Levien green noise error diffusion based on the gain model proposed by Kite et al.. We isolate the noise shaping behavior of the Levien algorithm in the frequency domain and show that it indeed shapes noise into the mid-frequency regions. We then develop two new green noise generators from frequency domain considerations by first by modifying how the error term is computed in conventional error diffusion and then by replacing the error filter in conventional error diffusion by an infinite impulse response (IIR) filter."
1762854,21239,9078,Sparse representation based band selection for hyperspectral images,2011,"Hyperspectral images consist of large number of spectral bands but many of which contain redundant information. Therefore, band selection has been a common practice to reduce the dimensionality of the data space for cutting down the computational cost and alleviating from the Hughes phenomenon. This paper presents a new technique for band selection where a sparse representation of the hyperspectral image data is pursued through an existing algorithm, K-SVD, that decomposes the image data into the multiplication of an overcomplete dictionary (or signature matrix) and the coefficient matrix. The coefficient matrix, that possesses the sparsity property, reveals how importantly each band contributes in forming the hyperspectral data. By calculating the histogram of the coefficient matrix, we select the top K bands that appear more frequently than others to serve the need for dimensionality reduction and at the same time preserving the physical meaning of the selected bands. We refer to the proposed band selection algorithm based on sparse representation as SpaBS. Through experimental evaluation, we first use synthetic data to validate the sparsity property of the coefficient matrix. We then apply SpaBS on real hy-perspectral data and use classification accuracy as a metric to evaluate its performance. Compared to other unsupervised band selection algorithms like PCA and ICA, SpaBS presents higher classification accuracy with a stable performance."
2963457,21239,9078,Automatic modeling and classification of vitreomacular traction pathology stages,2016,"Retinal pathologies that are detected too late and/or left untreated can seriously damage eyesight. It is important to monitor the retina and react to any pathological changes. A fast, accurate, non-invasive, and even three-dimensional retina examination is the optical coherence tomography (OCT). In this paper we propose a new automated classification method for evaluation of vitreomacular interface (VRI) in human eyes. We present an approach for modelling changes in retina structure during the progression of vitreomacular traction (VMT) pathology. Presented experiments were performed on volumetric data acquired from adult patients with the use of Avanti RTvue device. Advanced digital image processing algorithms were subsequently applied to each OCT cross-section (B-scan) for image denoising and flattening, as well as retina layers segmentation. The proposed solution has a good accuracy and almost all subjects were successfully classified into one of 4 groups corresponding to various stages of VMT. The developed models of VMT stages show a high potential of the proposed method to support ophthalmologists in making appropriate clinical decisions."
2386374,21239,23735,Genetic MRF model optimization for real-time victim detection in search and rescue,2007,"One primary goal in rescue robotics is to deploy a team of robots for coordinated victim search after a disaster. This requires robots to perform sub- tasks, such as victim detection, in real-time. Human detection by computationally cheap techniques, such as color thresholding, turn out to produce a large number of false-positives. Markov Random Fields (MRFs) can be utilized to combine the local evidence of multiple weak classifiers in order to improve the detection rate. However, inference in MRFs is computational expensive. In this paper we present a novel approach for the genetic optimizing of the building process of MRF models. The genetic algorithm determines offline relevant neighborhood relations with respect to the data, which are then utilized for generating efficient MRF models from video streams during runtime. Experimental results clearly show that compared to a Support Vector Machine (SVM) based classifier, the optimized MRF models significantly reduce the false-positive rate. Furthermore, the optimized models turned out to be up to five times faster then the non-optimized ones at nearly the same detection rate."
510540,21239,9078,Large-scale infographic image downsizing,2004,"'Content repurposing' is a way to convert existing multimedia content to suit mobile devices (handphones, PDAs, handheld PCs etc.), which have a lower display capability. As long as images are concerned, the main task is to downscale images and images of different types must be tackled differently in order to preserve as much visual content as possible. While in general most images stand fairly well downscaling, infographics (charts, maps, etc.) and other images containing thin structures are delicate to handle. Conventional methods tend to average out thin structures which usually convey critical information. Specifically designed techniques present various kinds of limitations. In this paper, we present a novel approach for infographic image downsizing. It relies on the concept of thin structure', the determination of which is related to the required downsizing scale. The algorithm is general, robust, without any limitation on the images. It is efficient because all operations are local. It works in an iterative way to achieve arbitrary large-scale downsizing. Result examples on real-world images are given to show the effectiveness of the approach."
2913884,21239,11470,Designing coding structures with merge frames for interactive multiview video streaming,2016,"In interactive multiview video streaming (IMVS), a client periodically requests switches to neighboring views for uninterrupted temporal video playback from a server storing a large number of pre-encoded views. The technical challenge is that the navigation path taken by a client is not known at encoding time, and thus it is difficult to employ differential coding to lower code rate without knowing exactly what frames are available at the client buffer as predictors. In a previous work, a new frame type called merge frame was designed to efficiently merge different side information (SI) frames S n  from different possible decoding paths into a unique construction M, so that the following frame(s) in time can be differentially coded using M as predictor without coding drift. In this paper, we design new coding structures using two variants of merge frame for different view-switching probabilities and desired rate-distortion (RD) tradeoff points. Experimental results show that our proposed frame structure designs outperform view-switching mechanisms in the literature, such as SP-frames in H.264, in RD performance."
1410327,21239,9078,Solving the out-of-gamut problem in image composition,2010,"Existing digital image composition algorithms neglect the out-of-gamut problem, i.e. some pixel values in a composited image exceed the displayable or printable range. The commonly used solutions, including hard clipping or linear scaling, result in either detail loss or global contrast reduction. Directly applying the existing high dynamic range (HDR) compression algorithms cannot achieve pleasant visual quality either. In our previous work, we proposed a gamut fitting algorithm by formulating gamut fitting as an energy-minimization problem and used the cubic polynomial in the Bernstein-Bezier form to compute the optimal mapping curve. Despite the good performance achieved, the problem of the previous algorithm lies in the necessity of fine tuning the weighting parameter in the proposed energy function. In this paper, we further improve our previous method by using piecewise mapping curves with multiple Bernstein polynomials to find the optimal mapping curve. The proposed approach can be regarded as a post-process to enhance the visual quality of the resulting images from image composition applications. The performance of the proposed method is compared with our previous method and the state-of-the-art HDR compression algorithms."
2114775,21239,9078,Total variation restoration of speckled images using a split-bregman algorithm,2009,"Multiplicative noise models occur in the study of several coherent imaging systems, such as synthetic aperture radar and sonar, and ultrasound and laser imaging. This type of noise is also commonly referred to as speckle. Multiplicative noise introduces two additional layers of difficulties with respect to the popular Gaussian additive noise model: (1) the noise is multiplied by (rather than added to) the original image, and (2) the noise is not Gaussian, with Rayleigh and Gamma being commonly used densities. These two features of the multiplicative noise model preclude the direct application of state-of-the-art restoration methods, such as those based on the combination of total variation or wavelet-based regularization with a quadratic observation term. In this paper, we tackle these difficulties by: (1) using the common trick of converting the multiplicative model into an additive one by taking logarithms, and (2) adopting the recently proposed split Bregman approach to estimate the underlying image under total variation regularization. This approach is based on formulating a constrained problem equivalent to the original unconstrained one, which is then solved using Bregman iterations (equivalently, an augmented Lagrangian method). A set of experiments show that the proposed method yields state-of-the-art results."
496163,21239,9078,Perceptually based color texture features and metrics for image retrieval,1999,"We propose a perceptually-based system for pattern retrieval and matching. The central idea of the work is that similarity judgment has to be modeled along perceptual dimensions. Hence, we detect basic visual categories that people use in judgment of similarity, and design a computational model which accepts patterns as input, and depending on the query, produces a set of choices that follow human behavior in pattern matching. To understand how humans perceive color patterns we performed a subjective experiment The experiment yielded five perceptual criteria used in comparison between color patterns (vocabulary), as well as a set of rules governing the use of these criteria in similarity judgment (grammar). This paper describes the actual implementation of the perceptual criteria and rules in the image retrieval system. Following the processing typical for human vision, we designed a system to: (a) extract perceptual features from the vocabulary and (b) perform the comparison between the patterns according to the grammar rules. We propose new color and texture features, as well as new distance functions that correlate with human performance. The performance of the system is illustrated with numerous examples from image databases from different application domains."
2422452,21239,9078,Automatic scene comparison and matching in multimodal cytopathological microscopic images,2005,"The potential of cytopathologic diagnoses to detect cancer of a variety of types non-invasively, cost-efficiently and up to three years ahead of conventional histopathologic diagnoses can be increased by the application of adjuvant methods, i.e. the combination of different stainings. For further improvement of cytopathology we introduced multimodal cell analysis (MMCA) which combines specific information about identical cells in different stainings successively applied to the same microscope slide. This requires a precise relocation and coregistration of individual cells under scrutiny. As a precondition for application in daily routine and screening settings the crucial relocation of cells has to be automated. The paper describes a method for an automatic retrieval of images of cells which have already been selected and recorded in a preceding staining together with their coordinates. Due to inevitable mechanical inaccuracies the geometric match is insufficient. The comparison of nuclear constellations based on segmentations in both stains facilitates an automatic correction of the position even if there is a subscene matching only. The process furthermore generates the initial guess for the succeeding coregistration which thereby gains robustness. The success rate of the method described is about 85%."
2219638,21239,9078,New image processing challenges for jointly designed electro-optical imaging systems,2009,"Still-image processing algorithms are tailored to and depend crucially upon the properties of the class of images to which they are applied, for instance natural images in consumer digital cameras, medical images in fMRI machines, and binary text images in some photocopiers. We describe a new and possibly very important class of images and tasks for which traditional algorithms seem ill-suited, and for which new algorithms and general methods and concepts are required. This new class of images arises in imaging systems designed through new, joint optimization methods where the optics and the image processing are designed simultaneously in order to yield a high-quality digital image. These new design methods yield intermediate optical images that have unusual spatial, noise and chromatic properties ill-served by traditional image methods. Moreover, these new images present a number of novel challenges in image processing hardware implementations such as constrained space-variance. We describe these briefly new, joint methods for designing digital-optical imaging systems, characterize the intermediate optical images they yield, and some of the digital image processing challenges for producing high-quality still images from these sensed optical images."
2859790,21239,9078,Non-parametric bounds on the nearest neighbor classification accuracy based on the Henze-Penrose metric,2016,"Analysis procedures for higher-dimensional data are generally computationally costly; thereby justifying the high research interest in the area. Entropy-based divergence measures have proven their effectiveness in many areas of computer vision and pattern recognition. However, the complexity of their implementation might be prohibitive in resource-limited applications, as they require estimates of probability densities which are very difficult to compute directly for high-dimensional data. In this paper, we investigate the usage of a non-parametric distribution-free metric, known as the Henze-Penrose test statistic, to estimate the divergence between different classes of vehicles. In this regard, we apply some common feature extraction techniques to further characterize the distributional separation relative to the original data. Moreover, we employ the Henze-Penrose metric to obtain bounds for the Nearest Neighbor (NN) classification accuracy. Simulation results demonstrate the effectiveness and the reliability of this metric in estimating the inter-class separability. In addition, the proposed bounds are exploited for selecting the least number of features that would retain sufficient discriminative information."
577943,21239,9078,Computational optical-sectioning microscopy for 3D quantification of cell motion: results and challenges,1994,"How cells move and navigate within a 3D tissue mass is of central importance in such diverse problems as embryonic development, wound healing and metastasis. This locomotion can now be visualized and quantified using computational optical-sectioning microscopy, which permits non-destructive 3D imaging of living specimens over long time periods. This technique, however, presents several technical challenges. Image restoration methods must be fast enough to process numerous I Gbyte time-lapse data sets (16 Mbytes per 3D image/spl times/60 time points). Because some cells are weakly labeled and background intensity is often high due to unincorporated dye, the SNR in some of these images is poor. Also required are accurate, automated-tracking procedures to generate both 3D trajectories for individual cells and 3D flows for a group of cells. Finally, sophisticated visualization techniques are needed to view the 3D movies of cell locomotion. Here, I discuss our current approaches to these problems and note present limitations. >"
1555901,21239,9078,Biometrics on visual preferences: A “pump and distill” regression approach,2014,"We present a statistical behavioural biometric approach for recognizing people by their aesthetic preferences, using colour images. In the enrollment phase, a model is learnt for each user, using a training set of preferred images. In the recognition/authentication phase, such model is tested with an unseen set of pictures preferred by a probe subject. The approach is dubbed “pump and distill”, since the training set of each user is pumped by bagging, producing a set of image ensembles. In the distill step, each ensemble is reduced into a set of surrogates, that is, aggregates of images sharing a similar visual content. Finally, LASSO regression is performed on these surrogates; the resulting regressor, employed as a classifier, takes test images belonging to a single user, predicting his identity. The approach improves the state-of-the-art on recognition and authentication tasks in average, on a dataset of 40000 Flickr images and 200 users. In practice, given a pool of 20 preferred images of a user, the approach recognizes his identity with an accuracy of 92%, and sets an authentication accuracy of 91% in terms of normalized Area Under the Curve of the CMC and ROC curve, respectively."
1706633,21239,9078,Interpolation of Multi-Spectral Images Inwavelet Domain for Satellite Image Fusion,2006,"This paper presents an image interpolation technique for satellite image fusion in the wavelet domain. For the fusion of satellite images, we need to interpolate and match the low resolution multi-spectral images (MSIs) to the high resolution panchromatic images. But since the typical spline-based interpolation methods employed in the conventional image fusion entails blurriness of edges, we propose a wavelet-domain image interpolation method that creates high frequency details based on the estimation of non-existent higher band coefficients from the relationship of available coefficients in lower scales. We model the relationship of coefficients in vertical as well as horizontal direction by the Markov stochastic model, and also find the coefficients of higher scale in this respect. The estimated coefficients are further refined by adding the maximum a posteriori (MAP) estimation process. The proposed interpolation technique is employed into the most popular image fusion algorithms, namely wavelet, principle component analysis (PCA), and intensity-hue-saturation (IHS) transformation based algorithms, instead of the conventional bilinear or bicubic interpolation methods. The experimental results show that the fused image based on the proposed interpolation instead of the conventional bilinear and bicubic interpolation algorithms employed in the conventional wavelet, principle component analysis (PCA), and intensity-hue-saturation (IHS) transformation based fusion algorithms, we incorporate the proposed wavelet based interpolation method."
2391409,21239,9078,Multiple shape models for simultaneous object classification and segmentation,2009,"Shape models (SMs), capturing the common features of a set of training shapes, represent a new incoming object based on its projection onto the corresponding model. Given a set of learned SMs representing different objects, and an image with a new shape, this work introduces a joint classification-segmentation framework with a twofold goal. First, to automatically select the SM that best represents the object, and second, to accurately segment the image taking into account both the image information and the features and variations learned from the on-line selected model. A new energy functional is introduced that simultaneously accomplishes both goals. Model selection is performed based on a shape similarity measure, determining which model to use at each iteration of the steepest descent minimization, allowing for model switching and adaptation to the data. High-order SMs are used in order to deal with very similar object classes and natural variability within them. The presentation of the framework is complemented with examples for the difficult task of simultaneously classifying and segmenting closely related shapes, stages of human activities, in images with severe occlusions."
617579,21239,9078,Color documents on the Web with DjVu,1999,"We present a new image compression technique called DjVu that is specifically geared towards the compression of scanned documents in color at high resolution. With DjVu, a magazine page in color at 300 dpi typically occupies between 40 KB and 80 KB, approximately 5 to 10 times better than JPEG for a similar level of readability. Using a combination of hidden Markov model techniques and MDL-driven heuristics, DjVu first classifies each pixel in the image as either foreground (text, drawings) or background (pictures, photos, paper texture). The pixel categories form a bitonal image which is compressed using a pattern matching technique that takes advantage of the similarities between character shapes. A progressive, wavelet-based compression technique, combined with a masking algorithm, is then used to compress the foreground and background images at lower resolutions while minimizing the number of bits spent on the pixels that are not visible in the foreground and background planes. Encoders, decoders, and real-time, memory efficient plug-ins for various web browsers are available for all the major platforms."
1402595,21239,9078,A unified framework for spectral domain prediction and end-to-end distortion estimation in scalable video coding,2011,"A novel scalable coding approach is proposed for video transmission over lossy networks, which builds on two estimation-theoretic (ET) paradigms previously developed by our group: (1) an ET approach to enhancement layer prediction in scalable video coding (ET-SVC) that optimally combines all available information from both the current base layer and prior enhancement layer frames, and (2) the spectral coefficient-wise optimal recursive estimate (SCORE) of end-to-end distortion. SCORE provides the encoder with an estimate of distortion per decoder-reconstructed transform coefficient, accounting for the effects of quantization, concealment, packet loss and error propagation via the prediction loop. The current work significantly extends the scope of SCORE to encompass the setting of ET-SVC, whose prediction involves non-linear operations. This advance enables optimization of ET-SVC systems for transmission over lossy networks, thereby combining optimal prediction with optimal mode decisions at the enhancement layer. Experiments first demonstrate the estimation accuracy of SCORE in the settings of the ET-SVC coder. They then show considerable gains when SCORE is incorporated into ET-SVC to optimize encoding decisions under a wide range of packet loss and bit rates."
1520939,21239,9078,A 3D reconstruction of the human jaw from a single image,2013,"Accurate 3D modeling of the human teeth/jaw helps patients avoid the discomfort of the mold process, and improves the data accuracy for oral orthodontists and dental care personnel. Since the surface of the human tooth is almost textureless, Shape from Shading (SFS) has been successfully adopted in solving this problem. In this paper, we attempt to improve the limitations of previous 3D tooth reconstruction algorithms by developing a new approach for shape reconstruction from single image shading with Two-Dimensional Principle Component Analysis (2D-PCA) shape priors. The surface reflectance is modeled using Oren-Nayar-Wolff model which accounts for the retro-reflection case and we experimentally prove that the teeth surface follows the microfacet theory. Our formulation exploits the shape priors as extracted from a set of training CT scans of real human jaws. Our experiments provide promising quantitative metric results for the proposed approach. This work is fundamental for establishing an optical-based approach for reconstructing the human jaw that is inexpensive and does not use ionizing radiation."
1794831,21239,9078,Colour image compression with anisotropic diffusion,2014,"Schmaltz et al. (2009) have shown that for reasonably high compression rates, diffusion-based codecs can exceed the quality of transformation-based methods such as JPEG 2000. They store only data at a few optimised pixel locations and in-paint missing data with edge-enhancing anisotropic diffusion (EED). However, research on compression with diffusion methods has mainly focussed on grey-value images, and colour images have been compressed in a straightforward way using anisotropic diffusion in RGB space. So far, there is no sophisticated diffusion-based counterpart to the colour mode of JPEG 2000. To address this shortcoming we introduce an advanced colour compression codec that exploits properties of the human visual system in YCbCr space. Since details in the luma channel Y are perceptually relevant, we invest a large fraction of our bit budget in its encoding with high fidelity. For the chroma channels Cb and Cr, the stored information can be very sparse, if we guide the EED-based inpainting with the high quality diffusion tensor from the luma reconstruction. Experiments demonstrate that our novel codec outperforms JPEG 2000 and compression with RGB-diffusion, both visually and quantitatively."
288099,21239,9078,"A directional, shift insensitive, low-redundancy, wavelet transform",2001,"Shift sensitivity and poor directionality, two major disadvantages of the discrete wavelet transform, have previously been circumvented either by using highly redundant, non-separable wavelet transforms or by using restrictive designs to obtain a pair of wavelet trees with a transform-domain redundancy of 4.0 in 2D. We demonstrate that excellent shift-invariance properties and directional selectivity may be obtained with a transform-domain redundancy of only 2.67 in 2D. We achieve this by projecting the wavelet coefficients from Selesnick's (see Wavelet Applications VII, Proceedings of SPIE, 2000) shift-insensitive, double-density wavelet transform so as to separate approximately the positive and negative frequencies, thereby increasing directionality. Subsequent decimation and a novel inverse projection maintain the low redundancy while ensuring perfect reconstruction. Although our transform generates complex-valued coefficients that provide valuable phase information, it may be implemented with a fast algorithm that uses only real arithmetic. To demonstrate the efficacy of our new transform, we show that it achieves state-of-the-art performance in a seismic image-processing application."
2271774,21239,9078,Sampling in practice: is the best reconstruction space bandlimited?,2005,"Shannon's sampling theory and its variants provide effective solutions to the problem of reconstructing a signal from its samples in some shift-invariant space, which may or may not be bandlimited. In this paper, we present some further justification for this type of representation, while addressing the issue of the specification of the best reconstruction space. We consider a realistic setting where a multidimensional signal is prefiltered prior to sampling and the samples corrupted by additive noise. We consider two formulations of the reconstruction problem. In the first deterministic approach, we determine the continuous-space function that minimizes a variational, Tikhonov-like criterion that includes a discrete data term and a suitable continuous-space regularization functional. In the second formulation, we seek the minimum mean square error (MMSE) estimation of the signal assuming that the input signal is a realization of a stationary random process. Interestingly, both approaches yield a solution included in some optimal shift-invariant space that is generally not bandlimited. The solutions can be made equivalent by choosing a regularization operator that corresponds to the whitening filter of the process. We present some practical examples that demonstrate the optimality of the approach."
788465,21239,9078,A novel JSCC scheme for scalable video transmission over MIMO systems,2012,"MIMO recently emerges as one of promising techniques for wireless video streaming. It is still a challenge to provide un-equal error protections by joint source-channel coding (JSCC) over multiple diverse MIMO sub-channels. In this paper, a joint source-channel coding and antenna mapping scheme for scalable video transmission over MIMO systems is proposed. Bandwidth are elaborately allocated between video source and channel protections by layer extracting and FEC coding. For the extracted layers, we determine i) which antenna will they be transmitted over and ii) how much redundancy bits will be added for error protections. We formulate this scheme into a non-linear integer optimization problem, whose complexity is very high. Instead, a low-complexity branch-and-bound algorithm is presented. Source layers are partitioned into subsets of layers, and the selected layer are mapped to antennas using Min-max scheduling algorithm. By branching and pruning, the computation complexity are reduced significantly. We carry out extensive numerical experiments under various network conditions. The results demonstrate our algorithm's efficiency and the overall transmission quality is improved significantly."
1558215,21239,9078,Windowed Image Registration for Robust Mosaicing of Scenes with Large Background Occlusions,2006,"We propose an enhanced window-based approach to local image registration for robust video mosaicing in scenes with arbitrarily moving foreground objects. Unlike other approaches, we estimate accurately the image transformation without any pre-segmentation even if large background regions are occluded. We apply a windowed hierarchical frame-to-frame registration based on image pyramid decomposition. In the lowest resolution level phase correlation for initial parameter estimation is used while in the next levels robust Newton-based energy minimization of the compensated image mean-squared error is conducted. To overcome the degradation error caused by spatial image interpolation due to the warping process, i.e. aliasing effects from under-sampling, final pixel values are assigned in an up-sampled image domain using a Daubechies bi-orthogonal synthesis filter. Experimental results show the excellent performance of the method compared to recently published methods. The image registration is sufficiently accurate to allow open-loop parameter accumulation for long-term motion estimation."
1819305,21239,9099,Modeling 3D articulated motions with conformal geometry videos (CGVs),2011,"3D articulated motions are widely used in entertainment, sports, military, and medical applications. Among various techniques for modeling 3D motions, geometry videos (GVs) are a compact representation in that each frame is parameterized to a 2D domain, which captures the 3D geometry (x, y, z) to a pixel (r, g, b) in the image domain. As a result, the widely studied image/video processing techniques can be directly borrowed for 3D motion. This paper presents conformal geometry videos (CGVs), a novel extension of the traditional geometry videos by taking into the consideration of the isometric nature of 3D articulated motions. We prove that the 3D articulated motion can be uniquely (up to rigid motion) represented by (»,H), where » is the conformal factor characterizing the intrinsic property of the 3D motion, and H the mean curvature characterizing the extrinsic feature (i.e., embedding or appearance). Furthermore, the conformal factor » is pose-invariant. Thus, in sharp contrast to the GVs which capture 3D motion by three channels, CGVs take only one channel of mean curvature H and the first frame of the conformal factor », i.e., approximately 1/3 the storage of the GVs. In addition, CGVs have strong spatial and temporal coherence, which favors various well studied video compression techniques. Thus, CGVs can be highly compressed by using the state-of the-art video compression techniques, such as H.264/AVC. Our experimental results on real-world 3D motions show that CGVs are a highly compact representation for 3D articulated motions, i.e., given CGVs and GVs of the same file size, CGVs show much better visual quality than GVs."
1888411,21239,9078,Combined dynamic tracking and recognition of curves with application to road detection,2000,"We present an algorithm that extracts the largest shape within a specific class, starting from a set of image edgels. The algorithm inherits the best-first segmentation approach. However, instead of being applicable only to shapes defined within a given class of curves, we have extended our approach to tackle more general-and complex-shapes. For example, we can now process shapes obtained from sets defined over different kinds of curves and related to one another by estimated parameters. Therefore, we go from a segmentation problem to a recognition problem. In order to reduce the complexity of the searching algorithm, we work with a linearly parameterized class of shapes. This allows us, first, to use a recursive least-squares fitting, second, to cast the problem as the search of a largest edgel subset in a directed acyclic graph, and, third, to easily introduce a priori information on the location of the edgels of the searched subset. This leads us to propose a unified approach where recognition and tracking are combined. Experiments on recognizing and tracking both left and right road boundaries demonstrate that real-time processing is achievable."
1302388,21239,9078,Rate-distortion optimization and adaptation of intra prediction filter parameters,2014,"Conventional “pixel copying” prediction used in current video standards was shown in previous work to be sub-optimal compared to 2-D non-separable Markov model based recursive extrapolation approaches. The premise of this paper is that in order to achieve the full potential of these approaches it is necessary to account for several requirements, namely, the design of prediction modes (and respective extrapolation filters) must optimize a rate-distortion cost rather than minimize the mean squared prediction error; the filters must be of sufficient complexity to cover all necessary directions; and the approach must include adaptation to available information indicative of local statistics. Hence, the proposed system employs four-tap recursive extrapolation filters that can predict from all standard directions, combined with a filter design method that accounts for the overall rate-distortion cost in conjunction with the codec decisions, along with adaptation of filter coefficients to relevant local information provided by encoder decisions on target bit rate and block size. Experimental evidence is provided for substantial coding gains over conventional intra coding."
692808,21239,9078,Gaze correction in video communication with single camera,2002,"In face-to-face video communication, we commonly use only a single camera placed on top of a monitor screen. This general configuration gives poor eye contact problem to decrease the feeling of natural conversation since the user stares at the monitor screen rather than directly staring at the camera lens. In this paper we present a new approach for natural feeling in video communication using image-based modeling and rendering techniques. Our facial modeling approach has two components. The first component is to estimate the eye position from an input image to find the gaze-correction angle and generate the basic model to represent the user's facial shape. The second component is a model-based shape approximation from motion which generates the user's simple facial shape to be used in rendering. To render a good eye-contact image, we propose 3D mesh warping technique, a method to rotate input image with the correction angle and the facial model. Our approach is effective and convenient since it uses the characteristic information about a facial scene. Preliminary experimental results with real facial image shows the enhanced naturalness which the face-to-face video communication has to offer."
2241929,21239,9078,Stego image quality and the reliability of PSNR,2010,"Digital image steganography is the art of hiding information in other digital images. Moreover, image quality evaluation has many difficulties such as the amount of degradation or distortion induced in the reconstructed image. The peak signal-to-noise ratio (PSNR) is the most common metric used to evaluate the stego image quality. However, subjective evaluation is the most reliable method to measure the image quality. Therefore, we try to give an answer to the following question: “does the PSNR value of a stego image reflect its actual quality?”. However, JPEG steganography represents a distortion source in addition to the image compression. Therefore, this paper investigates the relationship, if there any, between the PSNR and the subjective quality of stego images. Four steganography methods and five grayscale images are used in this paper. Moreover, an adapted double stimulus continuous quality scale (DSCQS) method has been adopted. As a result, PSNR can not be reliably used because it has poor correlation with the mean opinion score (MOS). Moreover, conclusions derived from only PSNR values of different stego images are quite different from that derived from the MOS values. Additionally, MOS shows that a particular steganography method modifies different test images quality in different ways."
402426,21239,21106,A New Image Fusion Method for Estimating 3D Surface Depth,2008,"Creation of virtual reality models from photographs is very complex and time-consuming process, that requires special equipment like laser scanners, a large number of photographs and manual interaction. In this work we present a method for generating of surface geometry of photographed scene. Our approach is based on the phenomenon of shallow depth-of-field in close-up photography. Representing such surface details is useful to increase the visual realism in a range of application areas, especially biological structures or microorganisms.#R##N##R##N#For testing purposes a set of images of the same scene is taken from a typical digital camera with macro lenses with a different depth-of-field. Our new image fusion method employs discrete Fourier transform to designate sharp regions in this set of images, combine them together into a fully focused image and finally produce a height field map. Further image processing algorithms approximate three dimensional surface using this height field map and a fused image. Experimental results show that our method works for wide range of cases and gives a good tool for acquiring surfaces from a few photographs."
2469519,21239,9078,Super-resolution mosaicking of UAV surveillance video,2008,"This paper explains and implements our efficient multi-frame super-resolution mosaicking algorithm. In this algorithm, feature points between images are matched using SIFT, and then random M-least squares is used to estimate the homography between frames. Next, separate frames are registered and the overlapping region is extracted. A generative model is then adopted and combined with maximum a posteriori estimation to construct the underdetermined sparse linear system. To solve the ill-posed large-scale inverse system, we derive and implement a new hybrid regularization (bilateral total variance Hubert) method. Cross validation is utilized to estimate the derivative of the blur kernel as well as the regularization parameter. Super-resolution is then applied to the individual sub-frames from the overlapping region. Finally, multi-band blending is used to stitch these resolution-enhanced frames to form the final image. The whole process is semi-real time (roughly 30 seconds for 35 frames) and the effectiveness of our algorithm is validated by applying it to real and synthetic UAV video frames."
1347375,21239,9078,DTI based structural damage characterization for Disorders of Consciousness,2012,"MRI Diffusion Tensor Imaging (DTI) has been recently proposed as a highly discriminative measurement to detect structural damages in Disorders of Consciousness patients (Vegetative State/Unresponsive Wakefulness Syndrome (VS/UWS) and Minimally Consciousness State (MCS)). In the DTI analysis, certain tensor features are often used as simplified scalar indices to represent these alterations. Those characteristics are mathematically and statistically more tractable than the full tensors. Nevertheless, most of these quantities are based on a tensor diffusivity estimation, the arithmetic average among the different strengths of the tensor orthogonal directions, which is supported on a symmetric linear relationship among the three directions, an unrealistic assumption for severely damaged brains. In this paper, we propose a new family of scalar quantities based on Generalized Ordered Weighted Aggregations (GOWA) to characterize morphological brain damages. The main idea is to perform a water diffusivity estimation in the damaged tissue by weighting and combining differently each tensor orthogonal strength. Using these new scalar quantities we constructed an affine invariant DTI tensor feature based on regional tissue histograms that outperforms state-of-the-art tensor based scalar representations for discrimination problems."
2495397,21239,9078,Correlation estimation for distributed source coding under information exchange constraints,2005,"Distributed source coding (DSC) depends strongly on accurate knowledge of correlation between sources. Previous works have reported capacity-approaching code constructions when exact knowledge of correlation is available at the encoder. However, in many applications exact correlation information may not be available, and correlation estimation is necessary. While error in estimation is inevitable, the impact of estimation error on compression efficiency has not been sufficiently studied for the DSC problem. In this paper we study correlation estimation subject to complexity constraints, and its impact on coding efficiency in a DSC framework. In particular, we consider the case where estimation entails information exchange between spatially separate sources and thus correlation estimation is subject to rate constraints. We first derive optimal strategies for information exchange that minimize the rate penalty due to inaccurate estimation, under constraints on the number of bits that can be exchanged between sources. Experimental results show that significant gain is possible by optimally exchanging information. We then derive analytical expressions to quantify the rate penalty, and analyze how rate penalty changes with a priori knowledge of correlation. In addition, we present a model-based estimation method which can achieve more accurate estimation results compared to directly inspecting the data."
881415,21239,9078,Underwater image enhancement using guided trigonometric bilateral filter and fast automatic color correction,2013,"This paper describes a novel method to enhance underwater optical images by guided trigonometric bilateral filters and color correction. Scattering and color distortion are two major problems of distortion for underwater optical imaging. Scattering is caused by large suspended particles, like fog or turbid water which contains abundant particles. Color distortion corresponds to the varying degrees of attenuation encountered by light traveling in the water with different wavelengths, rendering ambient underwater environments dominated by a bluish tone. Our key contributions are proposed a new underwater model to compensate the attenuation discrepancy along the propagation path, and to propose a fast guided trigonometric bilateral filtering enhancing algorithm and a novel fast automatic color enhancement algorithm. The enhanced images are characterized by reduced noised level, better exposedness of the dark regions, improved global contrast while the finest details and edges are enhance significantly. In addition, our enhancement method is comparable to higher quality than the state-of-the-art methods by assuming in the latest image evaluation systems."
1349675,21239,9078,Low-Complexity Transcoding of Inter Coded Video Frames from H.264 to H.263,2006,"The presented work addresses the reduction of computational complexity for transcoding of interframes from H.264 to H.263 baseline profiles maintaining the quality of a full search approach. This scenario aims to achieve fast backward compatible interoperability inbetween new and existing video coding platforms, e.g. between DVB-H and UMTS. By exploiting side information of the H.264 input bitstream the encoding complexity of the motion estimation is strongly reduced. Due to the possibility to divide a macroblock (MB) into partitions with different motion vectors (MV), one single MV has to be selected for H.263. It will be shown, that this vector is suboptimal for all sequences, even if all existing MVs of a MB of H.264 are compared as candidate. Also motion vector refinement with a fixed ?-pel refinement window as used by transcoders throughout the literature is not sufficient for scenes with fast movement. We propose an algorithm for selecting a suitable vector candidate from the input bitstream and this MV is then refined using an adaptive window. Using this technique, the complexity is still low at nearly optimum rate-distortion results compared to an exhaustive full-search approach."
1345547,21239,9078,Dorsal hand veins based person identification,2014,"Biometrics is a way that identifies people with the help of physical human features. There are many ways of bio-metric identification and recognition systems such as fingerprints, face, iris and veins etc. However, these conventional methods have some problems with respect of performance and convenience. Every human hand has unique veins patterns. Hand veins based recognition is most feasible than all of other conventional methods especially because of its easy acquisition process and also difficult to forge hand vein pattern. Patterns are taken from inside the body rather than obtaining from outside the body. Due to no physical contact, internal features and patterns from live body makes it more secure than other methods. In this research, we present a new method for person identification based on hand veins. The proposed system consists of pre-processing, vein enhancement and segmentation, feature extraction and finally matching. A new filter bank based method for hand veins enhancement is presented here. The proposed system is tested and evaluated using Bosphorus hand vein dataset which consists of 1200 hand images from 100 different people with 12 images per person. The proposed system has achieved 1.3% false acceptance and 1.75% false rejection rate respectively at a threshold of 0.85. Overall accuracy achieved by proposed system is 96.97%."
2458340,21239,9078,Preconditioning methods for shift-variant image reconstruction,1997,"Preconditioning methods can accelerate the convergence of gradient-based iterative methods for tomographic image reconstruction and image restoration. Circulant preconditioners have been used extensively for shift-invariant problems. Diagonal preconditioners offer some improvement in convergence rate, but do not incorporate the structure of the Hessian matrices in imaging problems. For inverse problems that are approximately shift-invariant (i.e. approximately block-Toeplitz or block-circulant Hessians), circulant or Fourier-based preconditioners can provide remarkable acceleration. However, in applications with nonuniform noise variance (such as arises from Poisson statistics in emission tomography and in quantum-limited optical imaging), the Hessian of the (penalized) weighted least-squares objective function is quite shift-variant, and the Fourier preconditioner performs poorly. Additional shift-variance is caused by edge-preserving regularization methods based on nonquadratic penalty functions. This paper describes new preconditioners that more accurately approximate the Hessian matrices of shift-variant imaging problems. Compared to diagonal or Fourier preconditioning, the new preconditioners lead to significantly faster convergence rates for the unconstrained conjugate-gradient (CG) iteration. Applications to position emission tomography (PET) illustrate the method."
2224567,21239,23735,Terrain surface classification for autonomous ground vehicles using a 2D laser stripe-based structured light sensor,2009,"To increase autonomous ground vehicle (AGV) safety and efficiency on outdoor terrains the vehicle's control system should have settings for individual terrain surfaces. A first step in such a terrain-dependent control system is classification of the surface upon which the AGV is traversing. This paper considers vision-based terrain surface classification for the path directly in front of the vehicle (≪ 1 m). Most visionbased terrain classification has focused on terrain traversability and not on terrain surface classification. The few approaches to classifying traversable terrain surfaces, with the exception of the use of infrared cameras to classify mud, have relied on stand-alone cameras that are designed for daytime use and are not expected to perform well in the dark. In contrast, this research uses a laser stripe-based structured light sensor, which uses a laser in conjunction with a camera, and hence can work at night. Also, unlike most previous results, the classification here does not rely on color since color changes with illumination and weather, and certain terrains have multiple colors (e.g., sand may be red or white). Instead, it relies only on spatial relationships, specifically spatial frequency response and texture, which captures spatial relationships between different gray levels. Terrain surface classification using each of these features separately is conducted by using a probabilistic neural network. Experimental results based on classifying four outdoor terrains demonstrate the effectiveness of the proposed methods."
2196220,21239,9078,On optimal royalty costs for video compression,2008,"Modern video compression includes a mature set of codecs, tools, and techniques that correspond to a variety of coding efficiencies and royalty costs in video delivery. In early work we showed that there can be significant benefits to employing a royalty-aware encoder that jointly optimizes rate, distortion, and royalty cost by choosing among several codecs having different royalty costs. In this paper we assume the existence of such an encoder and concentrate on how royalty costs should be assigned to benefit a system of users, video service providers, and tool/codec owners. Our work operates on the intersection of rate-distortion principles with basic concepts from economics and tries to address issues that are rapidly becoming very relevant in media delivery. Using a simple model that captures user preferences of video quality levels (given costs associated with these levels), we formulate the assignment of royalty costs as an optimization problem and solve it for optimal costs for several scenarios. The main trade-offs that govern our results are the cost of bandwidth and previously unachievable quality levels (if any) that a state-of-the-art codec gives access to. We show that content-based cost assignment not only increases user utilization but also the royalty earnings for tool owners compared to content-unaware pricing policies."
868842,21239,9078,Fusing multi-feature representation and PSO-Adaboost based feature selection for reliable frontal face detection,2013,"We propose a reliable frontal face detector based on multifeature descriptors and feature selection using PSO-Adaboost. Utilization of multiple heterogeneous feature descriptors enriches the diversity of feature types for face modeling and feature learning. To speed up the training process of face detector, we also propose a PSO-Adaboost algorithm that replaces exhaustive search used in original Adaboost framework with Particle Swarm Optimization (PSO) technique for efficient feature selection. Finally, a three-stage cascade classifier is developed to remove background rapidly. In particular, an initial stage is designed to detect candidate face regions more quickly by using a large size window with a large moving step. Radial Basis Function (RBF) SVM classifiers are used instead of decision stump functions in the last stage to remove those remaining complex non-face patterns that can not be rejected in the previous two stages. Combining these three effective modules, our face detector achieves a detection rate of 96.50% at ten false positives on the CMU+MIT frontal face dataset."
2477633,21239,9078,A direction-adaptive in-loop deartifacting filter for video coding,2008,"Recent video coding strategies, such as H.264/AVC, incorporate an in-loop deblocking filter in order to reduce the effects of quantization noise. These techniques are limited to treating blocky artifacts on smooth regions. In order to solve this, sparsity-based filtering techniques have been recently proposed for efficient filtering of edge and textured areas. More recently, direction-adaptive sparsity-based filtering has also allowed to exploit directional features on intra encoded pictures. This paper proposes a high-performance in-loop filter for deartifacting intra and inter encoded video data. This work extends for Inter frames the use of direction-adaptive sparsity-based filtering techniques and it improves performance by adaptively selecting filtering thresholds consistent with quantization noise statistics, local encoding conditions, compression requirements and the original signal. Thresholds are both spatially and temporally adapted to optimize video quality and/or coding cost. Selected thresholds are encoded and transmitted as side information to the decoder. Experimental results show significant bit rate savings and visual quality enhancement when compared to the state-of-the- art H.264/AVC codec using in-loop deblocking filtering."
1960114,21239,9078,Macroblock-based progressive fine granularity scalable (PFGS) video coding with flexible temporal-SNR scalablilities,2001,"We proposed a flexible and efficient architecture for scalable video coding, namely, the macroblock (MB)-based progressive fine granularity scalable video coding with temporal-SNR scalabilities (PFGST). The proposed architecture can provide not only much improved coding efficiency but also simultaneous SNR scalability and temporal scalability. Building upon the original frame-based progressive fine granularity scalable (PFGS) coding approach, the MB-based PFGS scheme is first proposed. Three INTER modes and the corresponding mode selection mechanism are presented for coding the SNR enhancement MBs in order to make a good trade-off between low drifting errors and high compression efficiency. Furthermore, temporal scalability is introduced into the MB-based PFGS, which forms the MB-based PFGST scheme. Two coding modes are proposed for coding the temporal enhancement MBs. Since it would not cause any error propagation if using the high quality reference in the temporal enhancement MB coding, the coding efficiency of the PFGST is highly improved by always choosing the most suitable reference for the temporal scalable coding. Experimental results show that the MB-based PFGST video coding scheme can significantly improve the coding efficiency up to 2.8 dB compared with the FGST scheme adopted in MPEG-4, while supporting full SNR, full temporal, and hybrid SNR-temporal scalabilities according to the different requirements from the channels, the clients or the servers."
264625,21239,21106,Creatools: a framework to develop medical image processing software: application to simulate pipeline stent deployment in intracranial vessels with aneurysms,2012,"The paper presents a collaborative project that offers stand-alone software applications for end-users and a complete open-source platform to rapidly develop/prototype medical image processing work-flows with sophisticated visualization and user interactions. It builds on top of a flexible cross-platform framework (Linux, Windows and MacOS) developed in C++, which guarantees an easy connection of heterogeneous C++ modules and provides the user with libraries of high-level components to construct graphical user interfaces (GUI) including input/output (file management), display, interaction, data processing, etc.#R##N##R##N#In this article, we illustrate the usefulness of this framework through a research project dealing with the study of thrombosis in intra-cranial aneurysms. Algorithms developed by the researchers, such as image segmentation, stent model generation, its interactive virtual deployment in the segmented vessels, as well as the generation of meshes necessary to simulate the blood flow through thus stented vessels, have been implemented in a user-friendly GUI with 3D visualization and interaction."
2489388,21239,9078,A novel MPEG-4 based hybrid temporal-SNR scalability for Internet video,2000,"Transmission of video over bandwidth-varying networks (e.g. the Internet) requires a highly scalable solution that is capable of adapting to the network condition in real-time. To address this requirement, scalable video coding schemes with multiple enhancement-layers have been proposed. However, under this multiple-layer paradigm, the transmission bitrate of each layer has to be predetermined at encoding time. Consequently, the range of bitrates that can be covered with these compression schemes is limited and often lower-than or different from the desired range required at transmission time. In this paper, a novel scalable video-coding framework and a corresponding compression method for Internet video streaming is introduced. Building upon the MPEG-4 fine-granular-scalability (FGS) approach, the proposed framework provides a new level of abstraction between the encoding and transmission-process by supporting both SNR and temporal scalability through a single enhancement layer. Consequently, our proposed approach enables streaming systems to support full SNR, full temporal, and hybrid SNR-temporal scalability in real-time depending on available bandwidth, packet-loss patterns, user preferences, and/or receiver complexity."
552677,21239,9078,Content-based selective enhancement for streaming video,2001,"Video transmission over bandwidth-varying networks is becoming increasingly important due to emerging applications such as streaming of video over the Internet. The fundamental obstacle in designing such systems resides in the varying characteristics of the Internet (i.e. bandwidth variations and packet-loss patterns). In MPEG-4, a new scalability scheme, called fine-granular-scalability (FGS), was standardized, which is able to adapt to bandwidth variations in real-time while using the same pre-encoded stream. This paper presents a novel technique that enables the FGS coding scheme to perform content-based enhancement and prioritized transmission of specific regions. The proposed selective enhancement mechanism gives the FGS framework the flexibility to perform differentiated bit-allocation to enhance specific objects, such that an improved visual image quality can be obtained at various bit-rates. In our system, we employed a novel real-time face detection algorithm, that in conjunction with the proposed content-based selective enhancement method consistently leads to better subjective visual quality of streaming video under various transmission bit-rates. The FGS selective enhancement method presented has been adopted in the MPEG-4 standard."
1880920,21239,9078,Spatial scalable HDTV coding,1995,"In this paper, we discuss the coding strategy when a two-layer digital HDTV service is considered. We propose to use-three different strategies in performing compression to yield the best quality based on the respective requirements on bandwidth, picture quality and efficiency. These three modes are as follows. Strategy A: if the bandwidth for the enhancement layer is not sufficient to achieve high quality, the base layer encoder should perform the compression at a reduced spatial resolution using the combined bandwidth. The top layer video signal will be obtained from upsampling the base layer. Strategy B: if the bandwidth is sufficient to support quality requirements at both layers, the typical two-layer spatial scalable coding scheme is used. Strategy C: if the quality requirement at the top layer is critical, a single layer compression at a full spatial resolution is used. To receive the low resolution signal, certain bit stream scaling can be used to get a usable quality signal. A theoretical qualitative analysis gives an insight of our experimental results. Such a strategy is useful in designing future HDTV services for television sets with different capabilities, resolutions and complexities. The results are also applicable to the problem of HDTV/standard TV compatibility."
1797791,21239,9078,Evaluation and benchmark for biological image segmentation,2008,"This paper describes ongoing work on creating a benchmarking and validation dataset for biological image segmentation. While the primary target is biological images, we believe that the dataset would be of help to researchers working in image segmentation and tracking in general. The motivation for creating this resource comes from the observation that while there are a large number of effective segmentation methods available in the research literature, it is difficult for the application scientists to make an informed choice as to what methods would work for her particular problem. No one single tool exists that is effective on a diverse set of application contexts and different methods have their own strengths and limitations. We describe below three different classes of data, ranging in scale from subcellular to cellular to tissue level images, each of which pose their own set of challenges to image analysis. Of particular value to the image processing researchers is that the data comes with associated ground truth information that can be used to evaluate the effectiveness of different methods. The analysis and evaluation are also integrated into a database framework that is available online at http://dough.ece.ucsb.edu."
2952592,21239,11470,Light field imaging coding: Performance assessment methodology and standards benchmarking,2016,"It is well known that the conventional ways to capture the light around us are limited and thus provide a limited user experience, notably in terms of parallax capabilities. As this has been preventing 3D systems to explode in the market, significant advances are emerging in terms of light capturing technologies among which is relevant to highlight the socalled light field cameras which capture a richer representation of the visual scene by measuring the light intensity for each direction and for each pixel position. Considering the huge amount of light intensity data involved, efficient compression becomes a must. In this context, this paper addresses the light field coding challenge by using available image coding standard solutions. With this target in mind, this paper proposes first a compression performance assessment methodology and presents after the compression performance of the direct, fully compatible usage of the main image coding standards available, notably JPEG, JPEG 2000, H.264/AVC Intra and HEVC Intra for a representative set of light field images. While the expected conclusion is that HEVC Intra is the most efficient codec, other less expected conclusions emerge."
2055424,21239,9078,Scalable stereo matching with Locally Adaptive Polygon Approximation,2008,"We present a scalable stereo matching algorithm based on a Locally Adaptive Polygon Approximation (LAPA) technique. For accurate local stereo matching, pixel-wise adaptive polygon-based support windows are constructed to approximate spatially varying image structures. Central to building these pixel-wise polygons is a fast algorithm that adaptively decides a set of directional scales, utilizing intensity and spatial information. Thanks to the locally adaptive support window, the proposed method achieves high stereo reconstruction quality both in depth-discontinuity regions and homogenous regions. Moreover, our LAPA-based method offers flexible scalability in terms of quality-complexity trade-off. As a specific instantiation favoring high-quality stereo estimation, our 8-direction stereo method outperforms most of the other local stereo methods and even some global optimization techniques. Another low-complexity alternative is also presented, achieving a significant speedup of up to a factor 20 with graceful accuracy degradation. Within a unified LAPA framework, our stereo method hence facilitates more flexibility in conciliating different algorithm design needs with processing performance issues."
1613073,21239,9078,Comparative study to analyze the effect of aging on microvascular blood flow by processing laser speckle contrast images when Lorentzian and Gaussian velocity profiles are assumed for moving scatterers,2014,"It is well known that microcirculation alters with age. These alterations can be evaluated by monitoring microvas-cular blood flow. Laser speckle contrast imaging (LSCI) is a recent optical imaging modality to monitor microvascular blood flow. From LSCI, the average velocity of microvascular moving scatterers (mainly red blood cells from the microcirculation) can be evaluated when assumptions on the scatterers velocity distribution are made. The objective of this study is twofold: 1) Process experimental LSCI data to compare velocity of red blood cells from the microcirculation when a Lorentzian profile and when a Gaussian profile are assumed. The experimental images that are processed have been recorded in healthy subjects, at rest, during a vascular occlusion and post-occlusive reactive hyperaemia, in two populations: young and aged subjects. 2) Analyze if alterations of microcirculation with age can be evaluated by processing LSCI data. Moreover, the presence of static scatterers (like skin) is taken into account in our work. Our results show that red blood cells velocity values determined when a Lorentzian velocity profile is assumed are lower than the ones determined when a Gaussian velocity profile is assumed, both for the young and the aged subjects. This is true with or without the presence of static scatterers. Furthermore, at rest, during biological zero, and post-occlusive reactive hyperaemia, the velocity of the moving scatterers increases with age. It also increases when the part of static scatterers increases. LSCI is therefore a new imaging modality that can be used to assess alterations of microvascular blood flow with age."
2326681,21239,9078,Embedded zerotree based lossless image coding,1995,"In this paper the problem of progressive lossless image coding is addressed. Many applications require a lossless compression of the image data. The possibility of progressive decoding of the bitstream adds a new functionality for those applications using data browsing. In practice, the proposed scheme can be of intensive use when accessing large databases of images requiring a lossless compression (especially for medical applications). The international standard JPEG allows a lossless mode. It is based on an entropy reduction of the data using various kinds of estimators followed by source coding. The proposed algorithm works with a completely different philosophy summarized in the following four key points: 1) a perfect reconstruction hierarchical morphological subband decomposition yielding only integer coefficients, 2) prediction of the absence of significant information across scales using zerotrees of wavelet coefficients, 3) entropy-coded successive-approximation quantization, and 4) lossless data compression via adaptive arithmetic coding. This approach produces a completely embedded bitstream. Thus, it is possible to decode only partially the bitstream to reconstruct an approximation of the original image."
2850245,21239,9078,Variable rate adaptive color-based particle filter tracking,2016,"Effective tracking of highly maneuvering objects while preserving robustness to illumination changes is a challenging problem. Conventionally the particle filter based color trackers (CPF) are efficiently used for non-linear estimation problems. Difficulties arise from the non-stationarity in lighting conditions through long video sequences that prevents efficient tracking of highly maneuvering objects. In order to improve the tracking performance we introduce a model that integrates the variable rate particle filtering (VRPF) into the CPF. The integrated tracking model called as variable rate color-based particle filtering (VRCPF) employs the non-uniform state update scheme of VRPF enhanced by an adaptive target update mechanism. Unlike the existing methods the VRCPF applies an exponential weighting scheme on the similarity metric between the target and candidate models and assigns the state points in such a way that allowing adaptive control of illumination changes in temporal domain. It is shown that although it relies on conventional color histograms, the VRCFP highly improves the maneuvering target tracking performance under severe illumination changes while provides comparable performance on BoBoT benchmarking dataset."
1934494,21239,9078,Content and transformation effect matching for automated home video editing,2004,"While camcordcrs havc bccome a commodity home appliance, few watch the recorded vidcos or sharc thcm with friends and relatives duc to the difficulty of turning thc raw footagc into a compclling vidco story. Previous works on Automated Video Editing (AVE) dcvelopcd an automatic solution for vidco contcnt selection and video-music matching. Howcvcr, to automatically apply vidco transformation effects, such a fastislow motion, thresholding, binariration and watercolor, has not been solved or even addressed. In this paper, we proposed several automatic video effect and contcnt matching schemes, which facilitate generating more compelling and intcrcsting AVE rcsults."
604536,21239,9078,Portable traveling support system using image processing for the visually impaired,2001,"We have previously proposed a portable navigation system for the visually impaired (see Nakamura, K. et al., IEICE Trans. of Japan, vol.J79, p.1610-18, 1996; Takuno, S. et al., IEICE Trans. of Japan, vol.J83, p.293-302, 2000). However, there are some places where this system is unable to provide proper guidance. Since the traveling direction of the traveler in this system is measured by a terrestrial magnetism sensor, there is a possibility that the wrong direction is measured, due to the distortion of the terrestrial magnetism by vehicles, etc. Road markings, e.g., pedestrian crossing, white line of the road, are reliable information to measure the correct direction. Furthermore, street landmarks, e.g., electric pole, traffic-control sign and post box, are also useful for a traveler to confirm his position. Therefore, we have now adequately applied image processing to our portable navigation system to make it more reliable."
882109,21239,9078,Automated detection of polysomes in cryoelectron tomography,2014,"Ribosomes and messenger RNA assemble to polysomes during protein synthesis. Cryoelectron tomography enables detection and identification of large macromolecular complexes under physiological conditions making the method uniquely suitable to study the supercomplexes that govern translation of mRNA into proteins. Here, we describe a method for automated assignment of polysomes in cryoelectron tomograms using the positions and orientations of ribosomes, as localized by template matching on tomographic data, as input. On the basis of a training dataset of expert-curated polysomes in cryoelectron tomograms, we define the relative 3D arrangements of neighboring ribosomes in polysomes. This prior distribution is used in a probabilistic framework for polysome assignment: the localized ribosomes from a tomogram are represented as a graph of which the edge weights are defined by the prior distribution. A Markov Random Field is embedded on the graph structure, and a message-passing algorithm is used to infer a polysome-label for each ribosome, i.e., to cluster ribosomes into polysomes. The performance of the method is assessed based on simulated tomograms and experimental tomograms indicating that polysome detection is reliable for typical signal-to-noise ratios of cryoelectron tomograms."
1573412,21239,20358,Quizz: targeted crowdsourcing with a billion (potential) users,2014,"We describe Quizz, a gamified crowdsourcing system that simultaneously assesses the knowledge of users and acquires new knowledge from them. Quizz operates by asking users to complete short quizzes on specific topics; as a user answers the quiz questions, Quizz estimates the user's competence. To acquire new knowledge, Quizz also incorporates questions for which we do not have a known answer; the answers given by competent users provide useful signals for selecting the correct answers for these questions. Quizz actively tries to identify knowledgeable users on the Internet by running advertising campaigns, effectively leveraging the targeting capabilities of existing, publicly available, ad placement services. Quizz quantifies the contributions of the users using information theory and sends feedback to the advertisingsystem about each user. The feedback allows the ad targeting mechanism to further optimize ad placement.   Our experiments, which involve over ten thousand users, confirm that we can crowdsource knowledge curation for niche and specialized topics, as the advertising network can automatically identify users with the desired expertise and interest in the given topic. We present controlled experiments that examine the effect of various incentive mechanisms, highlighting the need for having short-term rewards as goals, which incentivize the users to contribute. Finally, our cost-quality analysis indicates that the cost of our approach is below that of hiring workers through paid-crowdsourcing platforms, while offering the additional advantage of giving access to billions of potential users all over the planet, and being able to reach users with specialized expertise that is not typically available through existing labor marketplaces."
2978839,21239,9078,De-identifying people in videos using neural art,2016,"We propose a computer vision-based de-identification pipeline that enables automated segmentation of humans in videos and effective protection of their identities. Due to the ubiquity of video surveillance, many jurisdictions implement strict regulations for the protection of personal data in publicly collected video sequences, requiring the data to be de-identified. However, soft biometric and non-biometric features like clothing, hair color, personal items, skin marks, etc., are often overlooked in the process. Assuming a surveillance scenario, we combine GMM-based background subtraction with an improved version of the GrabCut algorithm to find and segment pedestrians. We use the responses of a deep neural network to de-identify soft and non-biometric features through style mixing with images of other pedestrians. Our method produces de-identified versions of the input frames while preserving the naturalness and utility of the de-identified data."
1427247,21239,8502,An Innovative Model of Tempo and Its Application in Action Scene Detection for Movie Analysis,2008,"In this paper, we present an innovative model of tempo and its application in action scene detection for movie analysis. For the first time, we clearly propose that tempo indicates the rhythm of both movie scenarios and human perception. By thoroughly analyzing both aspects, we classify the factors of tempo into two sorts. The first is based on the film grammar and we use the low level features of shot length and camera motion to describe filmmaking by directors. The second is based on the human perception and we originally propose the information measure for perception depending on the cognitive informatics, a newly emerging and significative subject. With the information in both visual and auditory modalities, the low level features of motion intensity, motion complexity, audio energy and audio pace are integrated for the formulation of information to describe the viewers' emotional changes to continuously developing storyline. With both aspects, tempo is defined and tempo flow plot is derived as the clue of storyline. On the basis of video structuralization and movie tempo analysis, we build a system for hierarchical browse and edit with action scene annotation. The large-scale experiments demonstrate the effectiveness and generality of tempo for action movie analysis.In this paper, we present an innovative model of tempo and its application in action scene detection for movie analysis. For the first time, we clearly propose that tempo indicates the rhythm of both movie scenarios and human perception. By thoroughly analyzing both aspects, we classify the factors of tempo into two sorts. The first is based on the film grammar and we use the low level features of Shot Length and Camera Motion to describe filmmaking by directors. The second is based on the human perception and we originally propose the information measure for perception depending on the cognitive informatics, a newly emerging and significative subject. With the information in both visual and auditory modalities, the low level features of Motion Intensity, Motion Complexity, Audio Energy and Audio Pace are integrated for the formulation of information to describe the viewers' emotional changes to continuously developing storyline. With both aspects, tempo is defined and tempo flow plot is derived as the clue of storyline. On the basis of video structuralization and movie tempo analysis, we build a system for hierarchical browse and edit with action scene annotation. The large-scale experiments demonstrate the effectiveness and generality of tempo for action movie analysis."
1090218,21239,9078,A deformation model for biotissues behaviour simulation,2000,"This paper presents a deformable model able to simulate the structural modification of anatomical structures under some force field. The simulation is able to perform a realistic modelling of interacting objects considering both their geometrical and physical properties. A multi-spring based model, where the springs are organised to fit the geometry of the system, is used for this purpose. In particular, the properties of the biosolid tissues define the characteristic parameters of the corresponding springs. The viscoelastic behaviour of the objects considered is formalised by means of differential equations which represent the stress-strain relation of a generic tissue under dynamic solicitation. The model is applied to the skull-encephalon architecture with an intracranial growing lesion and tested with tomographic image pairs of the same patient, representing both healthy and pathologic conditions."
183733,21239,8231,Quality Control for Crowdsourced POI Collection,2015,"Crowdsourcing allows human intelligence tasks to be out- sourced to a large number of unspecified people at low costs. However, because of the uneven ability and diligence of crowd workers, the quality of their submitted work is also uneven and sometimes quite low. There- fore, quality control is one of the central issues in crowdsourcing research. In this paper, we consider a quality control problem of POI (points of interest) collection tasks, in which workers are asked to enumerate loca- tion information of POIs. Since workers neither necessarily provide correct answers nor provide exactly the same answers even if the answers indicate the same place, we propose a two-stage quality control method consist- ing of an answer clustering stage and a reliability estimation stage. Imple- mented with a new constrained exemplar clustering and a modified HITS algorithm, the effectiveness of our method is demonstrated as compared to baseline methods on several real crowdsourcing datasets."
2041717,21239,8494,A baseball exploration system using spatial pattern recognition,2008,"Despite a lot of research efforts in baseball video processing, little work has been done in analyzing the detailed process and ball movement of the batting content. This paper proposes a novel system to automatically summarize the progress of each batting in baseball videos. Utilizing the strictly-defined specifications of the baseball field, the system recognizes the spatial patterns in each frame and identifies what region of the baseball field is currently focused. Finally, an annotation string which abstracts the batting content is generated. With the annotation strings, the system is able to make descriptions and provide exploration for baseball videos, so that users can be given a further insight into the game quickly. The experiments on broadcast baseball videos of MLB and JPB show promising results."
3183989,21239,9078,Short local descriptors from 2D connected pattern spectra,2015,"We propose a local region descriptor based on connected pattern spectra, and combined with normalized central moments. The descriptors are calculated for MSER regions of the image, and their performance compared against SIFT. The MSER regions were chosen because they can be efficiently selected by constructing a max-tree, a structure used to calculate both descriptors and region moments. Experiments on the UCID database show an improvement over SIFT in two out of five experimental setups, and comparable performance in two other experiments. The new descriptors are only half the size of SIFT, resulting in 4 times faster query times when performing exact search on descriptor index built from 262 images."
1253968,21239,9078,Introducing the Monash vision group's cortical prosthesis,2013,"Monash Vision Group is developing a bionic eye based on implanting 7-11 small tiles into the visual cortex. Each tile has 43 active electrodes on its base, and a wirelessly powered electronic system to decode control signals and drive the electrodes with biphasic pulses. The tiles are fed with power and data using a common transmitting coil at the back of the patient's head. Sophisticated image processing, described in a companion paper, ensures that the user experiences maximum benefit from the small number of electrodes. This paper describes the progress in the first three years (2010-2012) of this four-year project."
1386357,21239,8228,On the extraction of spread-spectrum hidden data in digital media,2012,"This paper considers the problem of blindly extracting data embedded over a wide band in a spectrum (transform) domain of a digital medium (image, audio, video). We first develop a multi-signature iterative generalized least-squares (M-IGLS) core procedure to seek unknown data hidden in hosts via multi-signature direct-sequence spread-spectrum embedding. Neither the original host nor the embedding signatures are assumed available. Then, cross-correlation enhanced M-IGLS (CC-M-IGLS), a procedure described herein in detail that is based on statistical analysis of repeated independent M-IGLS processing of the host, is seen to offer most effective hidden message recovery. Experimental studies on images show that the proposed CC-M-IGLS algorithm can achieve recovery probability of error close to what may be attained with known embedding signatures and host autocorrelation matrix."
1321676,21239,8494,A collusion-free key assignment scheme for hierarchical access control using recursive hash chains,2013,"We propose an efficient key assignment scheme for flexible access control of scalable media consisting of multidimensional scalability. The proposed scheme manages one key composed of a single key segment and assigns keys for each entity through hash chains. This scheme is also resilient to collusion attacks by which malicious users illegally access many more portions than they can legally. We introduce recursive hash chains to decrease the number of key segments to one, whereas the conventional schemes that have the above mentioned features must use multiple key segments to compose each key. The managed key is not delivered to any user for security against key leakage. Moreover, our scheme inhibits the amount of hash calculation. Performance analysis validates the proposed scheme."
2411168,21239,8494,Efficient collusion attack-free access control for multidimensionally hierarchical scalability content,2009,"This paper proposes an efficient access control method for content with multiple dimensions of hierarchical scalability. The proposed method versatilely serves various quality content by simultaneous control in every dimensions of scalability. This method manages single entity for the content and single encipher key, though it prevents users from colluding. A user receives single ciphered content and single decipher key to obtain the content with the permitted quality. In comparison to the conventional collusion attack-resistant access control methods, the sophisticated key generation manner in the proposed method reduces the required length of the managed key and that of the managed codestream."
2156789,21239,8960,Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?,2013,"We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd's answers is that workers' reliabilities and biases are usually unknown and highly diverse. Control items with known answers can be used to evaluate workers' performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods."
1004185,21239,422,A multiple tree algorithm for the efficient association of asteroid observations,2005,"In this paper we examine the problem of efficiently finding sets of observations that conform to a given underlying motion model. While this problem is often phrased as a tracking problem, where it is called track initiation, it is useful in a variety of tasks where we want to find correspondences or patterns in spatial-temporal data. Unfortunately, this problem often suffers from a combinatorial explosion in the number of potential sets that must be evaluated. We consider the problem with respect to large-scale asteroid observation data, where the goal is to find associations among the observations that correspond to the same underlying asteroid. In this domain, it is vital that we can efficiently extract the underlying associations.We introduce a new methodology for track initiation that exhaustively considers all possible linkages. We then introduce an exact tree-based algorithm for tractably finding all compatible sets of points. Further, we extend this approach to use multiple trees, exploiting structure from several time steps at once. We compare this approach to a standard sequential approach and show how the use of multiple trees can provide a significant benefit."
595910,21239,8494,A review of video registration methods for watermark detection in digital cinema applications,2004,"To deter constant theft in digital cinema applications, forensic watermarking has been proposed to track a digital content and to determine the time and the place where the theft occurs. However, to be effective in digital cinema applications, a forensic watermark must be invisible and secure. The detection of the forensic watermark must also be robust to attacks and processing, such as camcorder capture, low bit rate compression, D/A/D conversion, cropping, resizing and frame rate conversion. To achieve the required robustness detection, many proposed forensic watermarking algorithms adopt reference-based (informed or nonblind) watermark detection where the original video is used to assist the detection. To use the original video, one must first register it to the candidate video, where the forensic watermark needs to be extracted. We will review a number of video registration algorithms proposed for forensic watermark detection in digital cinema applications. Among all the proposed registration algorithms, we will discuss and compare the following three algorithms: (1) a semiautomatic temporal, spatial and volumetric video registration algorithm proposed by IRISA, (2) a key frame based temporal alignment algorithm proposed by Universite Catholique de Louvain and (3) a spatial, temporal and volumetric video registration algorithm proposed by Sarnoff."
2436598,21239,8960,Bayesian Model of Surface Perception,1998,"Image intensity variations can result from several different object surface effects, including shading from 3-dimensional relief of the object, or paint on the surface itself. An essential problem in vision, which people solve naturally, is to attribute the proper physical cause, e.g. surface relief or paint, to an observed image. We addressed this problem with an approach combining psychophysical and Bayesian computational methods.#R##N##R##N#We assessed human performance on a set of test images, and found that people made fairly consistent judgements of surface properties. Our computational model assigned simple prior probabilities to different relief or paint explanations for an image, and solved for the most probable interpretation in a Bayesian framework. The ratings of the test images by our algorithm compared surprisingly well with the mean ratings of our subjects."
2314477,21239,8960,The Multidimensional Wisdom of Crowds,2010,"Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a method for estimating the underlying value (e.g. the class) of each image from (noisy) annotations provided by multiple annotators. Our method is based on a model of the image formation and annotation process. Each image has different characteristics that are represented in an abstract Euclidean space. Each annotator is modeled as a multidimensional entity with variables representing competence, expertise and bias. This allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, as well as groups of images that differ qualitatively. We find that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods. Experiments also show that our model, starting from a set of binary labels, may discover rich information, such as different schools of thought amongst the annotators, and can group together images belonging to separate categories."
2701211,21239,30,Restoration of vision using wireless cortical implants: The Monash Vision Group project,2015,"Monash Vision Group is developing a bionic vision system based on implanting several small tiles in the V1 region of the visual cortex. This cortical approach could benefit a greater proportion of people with total blindness than other approaches, as it bypasses the eyes and optic nerve. Each tile has 43 active electrodes on its base, and a wirelessly powered electronic system to decode control signals and drive the electrodes with biphasic pulses. The tiles are fed with power and data using a common transmitting coil at the back of the patient's head. Sophisticated image processing, described in a companion paper, ensures that the user experiences maximum benefit from the small number of electrodes. This paper describes key features of this system."
1823519,21239,9078,Blind iterative recovery of spread-spectrum steganographic messages,2005,"We propose an iterative generalized least squares procedure to recover unknown messages hidden in image hosts via spread-spectrum embedding. Neither the original host nor the embedding signature is assumed available. We demonstrate that for hidden messages of sufficient length (data sample support), recovery can be achieved with probability of error close to what may be attained with known embedding signature and known original host autocorrelation matrix. For small hidden messages, the signature estimate calculated by the iterative generalized least squares procedure can be fed as initial value to a (computationally costly) expectation-maximization signature identification scheme that we derive. Message recovery can again be carried out successfully by means of a linear sample-matrix-inversion minimum-mean-square-error receiver."
2234274,21239,8960,Inferring Ground Truth from Subjective Labelling of Venus Images,1995,"In remote sensing applications ground-truth data is often used as the basis for training pattern recognition algorithms to generate thematic maps or to detect objects of interest. In practical situations, experts may visually examine the images and provide a subjective noisy estimate of the truth. Calibrating the reliability and bias of expert labellers is a non-trivial problem. In this paper we discuss some of our recent work on this topic in the context of detecting small volcanoes in Magellan SAR images of Venus. Empirical results (using the Expectation-Maximization procedure) suggest that accounting for subjective noise can be quite significant in terms of quantifying both human and algorithm detection performance."
1121941,21239,8806,Semantic event detection in baseball videos based on a multi-output hidden Markov model,2011,"In this paper, we proposed an event detection method in baseball videos based on a multi-output HMM (hidden Markov model), using high-level audio/video features. For the video part, we use eight kinds of semantic scenes detected from baseball videos in our previous work. For the audio part, we extract the audio shots from corresponding video scenes, and cut an audio shot into N one-second clips. Then, the MFCC and ZCR of a one-second clip are extracted and fed into the SVM for classifying it as acclaim and silence. Based on the classification results, the type of an audio shot can be determined in the post-classification. Next, a multi-output HMM modified from the original HMM is used to combine video and audio features to detect baseball video events. Finally, the experimental results show, the multi-output HMM has good event detection accuracy."
3005972,21239,9078,"Automatic detection, tracking and counting of birds in marine video content",2016,"Robust automatic detection of moving objects in a marine context is a multi-faceted problem due to the complexity of the observed scene. The dynamic nature of the sea caused by waves, boat wakes, and weather conditions poses huge challenges for the development of a stable background model. Moreover, camera motion, reflections, lightning and illumination changes may contribute to false detections. Dynamic background subtraction (DBGS) is widely considered as a solution to tackle this issue in the scope of vessel detection for maritime traffic analysis. In this paper, the DBGS techniques suggested for ships are investigated and optimized for the monitoring and tracking of birds in marine video content. In addition to background subtraction, foreground candidates are filtered by a classifier based on their feature descriptors in order to remove non-bird objects. Different types of classifiers have been evaluated and results on a ground truth labeled dataset of challenging video fragments show similar levels of precision and recall of about 95% for the best performing classifier. The remaining foreground items are counted and birds are tracked along the video sequence using spatio-temporal motion prediction. This allows marine scientists to study the presence and behavior of birds."
2967041,21239,9078,Super pixel based classification using conditional random fields for hyperspectral images,2016,"Classification plays a significant role in analyzing remotely sensed imagery. In order to obtain an optimized classier, following aspects are rather challenging: 1) complexity in dealing with the overwhelming amount of data information from an advanced high resolution hyperspectral imaging sensor; 2) difficulty in leveraging spectral and spatial information across the sensed wavelengths; 3) struggles in obtaining adequate dataset as in the same modalities with labeled ground truth in the training process. Therefore, we propose a novel classification approach to tackle these issues by utilizing probabilistic graphical model on super-pixel segmentation. This method is capable of compacting hyperspectral information efficiently which decreases computing complexity. Moreover, the employment of probabilistic graphical models that weighs the strong dependency in spatial and spectral neighbors improves accuracy. One of the most successful probabilistic graphical models is Conditional Random Fields (CRFs). Conventional methods utilize all spectral bands and assign the corresponding raw intensity values into the feature functions in CRFs and build the grid graph. These methods, however, require significant computational efforts and yield an ambiguous summary from the data. To mitigate these problems, we cooperate a non-linear kernel based classier to provide the meaningful probability features for CRFs and learn the non-grid graph from super pixel segmentation."
1567877,21239,422,Semantic representation: search and mining of multimedia content,2004,"Semantic understanding of multimedia content is critical in enabling effective access to all forms of digital media data. By making large media repositories searchable, semantic content descriptions greatly enhance the value of such data. Automatic semantic understanding is a very challenging problem and most media databases resort to describing content in terms of low-level features or using manually ascribed annotations. Recent techniques focus on detecting semantic concepts in video, such as indoor, outdoor, face, people, nature, etc. This approach works for a fixed lexicon for which annotated training examples exist. In this paper we consider the problem of using such semantic concept detection to map the video clips into semantic spaces. This is done by constructing a  model vector  that acts as a compact semantic representation of the underlying content. We then present experiments in the semantic spaces leveraging such information for enhanced semantic retrieval, classification, visualization, and data mining purposes. We evaluate these ideas using a large video corpus and demonstrate significant performance gains in retrieval effectiveness."
2514537,21239,8960,Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery,2006,"In this paper we consider the problem of finding sets of points that conform to a given underlying model from within a dense, noisy set of observations. This problem is motivated by the task of efficiently linking faint asteroid detections, but is applicable to a range of spatial queries. We survey current tree-based approaches, showing a trade-off exists between single tree and multiple tree algorithms. To this end, we present a new type of multiple tree algorithm that uses a variable number of trees to exploit the advantages of both approaches. We empirically show that this algorithm performs well using both simulated and astronomical data."
1081970,21239,9078,Complexity-scalable H.264/AVC in an IPP-based video encoder,2010,"Real-time high-definition video encoding is a computation-hungry task that challenges software-based solutions. For that, in this work we adopted an Intel software implementation of an H.264 video encoder and optimized its prediction stage in the complexity sense (C). Thus, besides looking for the coding options which lead to the best coded representation in terms of rate and distortion, we constrain the process to fit within a certain time budget. We present an RDC-optimized framework which allows for real-time HD video compression."
2007184,21239,8494,Intermediate view synthesis from binocular images for stereoscopic applications,2001,"A new method was proposed in this paper for synthesizing intermediate views from binocular images captured by parallel cameras. The main techniques include: (1) predictive and multistage block-matching procedure for disparity estimation, (2) disparity interpolation for ill-correspondence and occlusion regions, and (3) image synthesis by hypothesized-and-tested block image rendering. The method features speedy (3 times) and accurate estimation of disparity map, as well as a tradeoff between synthesis quality (pixel-based disparities) and synthesis speed (4/spl times/4 block-based rendering). Experiments show that our synthesis results are generally good without distinctive defects and should be promising in the applications of 3-D virtual reality in entertainment, simulation, etc."
2954700,21239,9078,Empirical reliability analysis of disparity-based autofocus,2016,"Conventional autofocus methods based on contrast detection are often unable to reliably decide the direction of initial lens movement. In this paper, we show that even using the disparity data obtained from blurry stereo images can effectively solve the problem. This approach is developed for stereo cameras with adjustable focal distance. Such stereo cameras provide sharp images over a wide range of object distance. The disparity-based autofocus approach allows the lens to instantly come to a position near the in-focus position or inside the rising zone of the focus profile. We investigate the impact of disparity variation on the first lens movement driven by disparity-based autofocus and suggest how it can be integrated with contrast detection autofocus."
2039573,21239,8228,Energy-Aware Adaptive Watermarking for Real-Time Image Delivery in Wireless Sensor Networks,2008,"A secure image transmission mechanism is necessary when malicious intruders intend to access and modify content delivery over wireless networks. To assure data integrity, authentication is required for multimedia data delivered over wireless image sensor networks. Watermarking technique is an effective vehicle to assert and assure the image data authentications. There have been recent works reported on watermarking, but few with the consideration of energy cost in terms of the data communication and processing, which is a key constraint to many embedded systems and wireless sensor networks. Most watermarking systems only target at minimizing watermarked image distortion and increasing robustness at the source coding site for lossy image processing. The watermarked image distortion caused by error-prone wireless environments during transmission has not been fully considered. In this paper, an innovative energy-aware adaptive watermarking scheme for realtime image delivery is proposed in wireless multimedia sensor networks. This new scheme allocates network resource to protect the watermarked image transmission while embedding watermark coding redundancies into the images. Dynamic watermark thresholds are applied to be adaptive to the network condition (packet loss ratio) and the inter-frame correlation is exploited to reduce processing delay. The simulation results show that the proposed adaptive watermark system can achieve considerable energy efficiency and assure the data integrity."
1595186,21239,8228,Silhouette coefficient based approach on cell-phone classification for unknown source images,2012,"Cell-phones have become a necessary communication accessory in daily life. MMS (Multimedia Messaging Service) used by smart phones has caused higher requirement on mobile image manipulation. Classifying image source cell-phones has become a major issue in the cell-phone communication forensics. There are two ways usually used for tracing and identifying the source device: image characteristics and equipment fingerprint. Both of the above schemes require a set of images captured by known source cell-phones for training a classification model. To avoid using any prior knowledge in practical scenarios, a graph based approach was proposed to classify the source cell-phones. Though an acceptable result has been obtained, a problem of incomplete classification appears in the case that one image is classified wrong into a single subset. In this paper, a silhouette coefficient based algorithm is proposed for source cell-phone classification. The spectral clustering algorithm is adopted in graph partitioning and the silhouette coefficient is used to extract the optimal classification from all the possibilities of classification. Experimental results show the validity of the proposed method."
2928406,21239,9078,"Figaro, hair detection and segmentation in the wild",2016,"Hair is one of the elements that mostly characterize people appearance. Being able to detect hair in images can be useful in many applications, such as face recognition, gender classification, and video surveillance. To this purpose we propose a novel multi-class image database for hair detection in the wild, called Figaro. We tackle the problem of hair detection without relying on a-priori information related to head shape and location. Without using any human-body part classifier, we first classify image patches into hair vs. non-hair by relying on Histogram of Gradients (HOG) and Linear Ternary Pattern (LTP) texture features in a random forest scheme. Then we obtain results at pixel level by refining classified patches by a graph-based multiple segmentation method. Achieved segmentation accuracy (85%) is comparable to state-of-the-art on less challenging databases."
2863430,21239,9078,Pareto-based energy control for the HEVC encoder,2016,"The current state-of-art video coding standard, the High Efficiency Video Coding (HEVC), brings many innovations as a way to improve the coding performance. However, the improvement on performance also brought higher computational effort and energy consumption. Since most of devices that handle digital videos are battery powered, the energy consumption became an important issue that demands efficient solutions. This way, controlling energy consumption is strongly desirable to adapt the encoding process to the energy availability. This goal is a hard task due the heterogeneous dynamic behavior of HEVC encoder. This work presents the development of a Pareto-based dynamic energy controller for the HEVC encoder, reaching up to 70% energy saving with small losses on coding efficiency for most of the cases."
2038402,21239,8228,Tracing watermarking for multimedia communication quality assessment,2002,"Multimedia data hiding by digital watermarking is usually employed for copyright protection purposes. In this contribution, a new application of watermarking is presented. Specifically, watermarking is here employed as a technique for testing the quality of service in multimedia mobile communications. A fragile known watermark is embedded in a MPEG-like host data video transport stream using a spread-spectrum technique to avoid visual interference. Like a tracing signal, a (known) tracing watermark tracks the (unknown) information stream that follows the same communication link. The detection of the tracing watermark allows dynamically evaluating the effective quality of the provided video services, depending on the whole physical layer (including the employed image co/decoder). The performed method is based on the mean-square-error between estimated and actual watermarks. The devised technique has been usefully applied to typical scenarios of mobile wireless multimedia communication systems, in presence of multipath channel and interfering users."
1289765,21239,8502,Adaptive Keyframe Selection for Video Summarization,2015,"The explosive growth of video data in the modern era has set the stage for research in the field of video summarization, which attempts to abstract the salient frames in a video in order to provide an easily interpreted synopsis. Existing work on video summarization has primarily been static - that is, the algorithms require the summary length to be specified as an input parameter. However, video streams are inherently dynamic in nature, while some of them are relatively simple in terms of visual content, others are much more complex due to camera/object motion, changing illumination, cluttered scenes and low quality. This necessitates the development of adaptive summarization techniques, which adapt to the complexity of a video and generate a summary accordingly. In this paper, we propose a novel algorithm to address this problem. We pose the summary selection as an optimization problem and derive an efficient technique to solve the summary length and the specific frames to be selected, through a single formulation. Our extensive empirical studies on a wide range of challenging, unconstrained videos demonstrate tremendous promise in using this method for real-world video summarization applications."
2910773,21239,9078,Active-disc-based Kalman filter technique for tracking of blood cells in microfluidic channels,2016,"Identification and tracking of red blood cells and leukocytes in micro-circulation is very important for drug validation and inflammation. Manual tracking is usually used for this purpose. However, due to its excessive time consumption and inaccuracy in the case of overlapped cells, there is a need for automated methods. In this paper, we propose a Kalman filter and active-disc-based method for automatic and accurate detection of cells. In addition to tracking both slow and fast moving blood cells, the proposed method is also able to track overlapped cells successfully with an overall tracking accuracy of 95.7% and at a processing rate of 25 ms per frame. Comparisons with five state-of-the-art methods show that the proposed method is better in terms of the detection rate and accuracy of instantaneous speed estimation."
2906409,21239,9078,Depth estimation from focus and disparity,2016,"This paper explores how focusing and defocusing can be used in combination with stereoscopy to enhance the accuracy and speed of depth estimation. The proposed method for combining focus information with stereo is inspired from how the human brain perceives depth using both. The outline of the method is to first estimate depth from focus using a high focal length camera. Next, we perform disparity estimation from the stereo images by only searching in the neighborhood of the disparity from focus (derived from depth from focus) for matching features. The matching is faster as the search range is only a small neighborhood of the expected disparity map. It also increases the accuracy because searching over a small range of high confidence rules out the possibility of wrong matching, especially if there are multiple areas with similar features. Depth from focus can be coarsely approximated by depth from defocus using multiple cameras focused at complementary distances. Hence, the proposed method also works with depth from defocus."
2181876,21239,390,RELIABLE MOTION DETECTION AND ANALYSIS IN LIVE-CELL IMAGING,2007,"We describe an alternative method to tracking for analyzing the motion of fluorescent spots in 3D+time video-microscopy. First, we realize motion detection by viewing the 3D+T spatio-temporal as a single 4D volume and detecting simultaneous significant change in the fluorescence of groups of neighboring voxels as spatio-temporal locations with high intensity. This step outputs spatio-temporal locations of motion and are next used to detect the moving spots and making statistical analysis of their motion. We show in particular that we are able to estimate the histogram of the motion speed using only the so segmented spatio-temporal locations. Simulated sequences are used to learn some of the parameters of this second step of our method. Results obtained on simulated sequences as well as on video-microscopic sequences are shown"
2649170,21239,30,Computed tomography image source identification by discriminating CT-scanner image reconstruction process.,2015,"In this paper, we focus on the identification of the Computed Tomography (CT) scanner that has produced a CT image. To do so, we propose to discriminate CT-Scanner systems based on their reconstruction process, the footprint or the signature of which can be established based on the way they modify the intrinsic sensor noise of X-ray detectors. After having analyzed how the sensor noise is modified in the reconstruction process, we define a set of image features so as to serve as CT acquisition system footprint. These features are used to train a SVM based classifier. Experiments conducted on images issued from 15 different CT-Scanner models of 4 distinct manufacturers show it is possible to identify the origin of one CT image with high accuracy."
1653716,21239,8494,Analysis of template matching prediction and its application to parametric overlapped block motion compensation,2010,"Template matching prediction (TMP), which estimates the motion for a target block by using its surrounding pixels, has been observed to perform efficiently in inter-frame coding. In this paper, we expose, from a more theoretical viewpoint, the factors that determine the prediction efficiency of TMP. It is shown that the motion estimate found by template matching tends to be the motion associated with the template centroid and that TMP consistently outperforms SKIP prediction, but hardly competes with block motion compensation (BMC) unless both the motion and intensity fields are less random or have high spatial correlation. We also demonstrate how template and block motion estimates can jointly be applied in a parametric overlapped block motion compensation (OBMC) framework to further improve temporal prediction. Preliminary results show that combining TMP with OBMC can yield 2–16% reductions in mean-square prediction error, as compared with the single use of OBMC. The gain is even higher (18%) when the performance is compared with that of the standard BMC."
721313,21239,8228,Global motion compensation and spectral entropy bit allocation for low complexity video coding,2012,"Most standard video compression schemes such as H.264/AVC involve a high complexity encoder with block motion estimation (ME) engine. However, applications such as video reconnaissance and surveillance using unmanned aerial vehicles (UAVs) require a low complexity video encoder. Additionally, in such applications, the motion in the video is primarily global and due to the known movement of the camera platform. Therefore in this work, we propose and investigate a low complexity encoder with global motion based frame prediction and no block ME. We show that for videos with mostly global motion, this encoder performs better than a baseline H.264 encoder with ME block size restricted to 8×8. Furthermore, the quality degradation of this encoder with decreasing bit rate is more gradual than that of the baseline H.264 encoder since it does not need to allocate bits across motion vectors (MVs) and residue data. We also incorporate a spectral entropy based coefficient selection and quantizer design scheme that entails latency and demonstrate that it helps achieve more consistent frame quality across the video sequence."
707879,21239,8494,Low-bit motion estimation with edge enhanced images for lowpower MPEG encoder,2001,"This paper proposes a method for improving the image quality when we use low-bit motion estimation (ME) using linear quantization. By using edge enhanced images for quantization, we can increase the accuracy of the low-bit ME and improve the image quality. It is known that using low-bit images for ME is effective on saving power but it slightly degrades image quality. The quality of the encoded image depends on the thresholds for data quantization, and algorithms for determining thresholds are studied. The proposed method uses linear quantization, which simply truncates the least significant bits. This method is simple without any complicated threshold calculations, and the image quality improves as much as the methods with threshold calculations. To evaluate the effectiveness, we simulate results for image quality and estimate the power consumption using synthesis results from a VHDL model motion estimator."
2880375,21239,9078,Transform-coded pel-recursive video compression,2016,"We propose an algorithm that accomplishes transform-coded, spatiotemporal, pel-recursive video compression. Traditional pel-recursive coders obtain sophisticated spatio-temporal predictions for the current pixel based on previously decoded data. The resulting per-pixel prediction errors are encoded independently so that the decoder can use previously-encoded pixels in the prediction of the current. It is well-known that pel-recursive coders significantly under-perform modern hybrid coders which use comparatively very simple predictors but transform code the prediction errors. Our algorithm combines the accurate predictors of pel-recursive techniques with transform coders so that the strengths of both approaches can be taken advantage of. In the proposed work, the decoder transform decodes the residuals for a block and then acts like a simple pel-recursive decoder for the pixels of that block. We show that the proposed algorithm can be seen as the use of a pel-recursion enabling transform at the encoder, which generates and encodes the correct transform coefficients to avoid error propagation at the decoder. A straightforward implementation of our work (implemented as an extension of HEVC that preserves independent decodability of INTER blocks) shows compression improvements over the baseline HEVC."
2403011,21239,8960,Learning Mixture Hierarchies,1999,"The hierarchical representation of data has various applications in domains such as data mining, machine vision, or information retrieval. In this paper we introduce an extension of the Expectation-Maximization (EM) algorithm that learns mixture hierarchies in a computationally efficient manner. Efficiency is achieved by progressing in a bottom-up fashion, i.e. by clustering the mixture components of a given level in the hierarchy to obtain those of the level above. This clustering requires only knowledge of the mixture parameters, there being no need to resort to intermediate samples. In addition to practical applications, the algorithm allows a new interpretation of EM that makes clear the relationship with non-parametric kernel-based estimation methods, provides explicit control over the trade-off between the bias and variance of EM estimates, and offers new insights about the behavior of deterministic annealing methods commonly used with EM to escape local minima of the likelihood."
276077,21239,8494,A texture-based tamper detection scheme by fragile watermark,2004,"A texture-based tamper detection scheme by a fragile watermarking technique is proposed in this paper. In comparison with other fragile watermarking schemes, the highlight of our scheme is that it is rather sensitive to malicious tamper such as replacing one's face in the image by another's and at the same time it is insensitive to other legal processing such as lossy JPEG compression and brightness/contrast changes. So it is more suitable for tamper detection in practical use."
2235109,21239,8494,Complexity-constrained rate-distortion optimization for h.264/avc video coding,2011,"In order to enable real-time software-based video encoding, in this work we optimized the prediction stage of an H.264 video encoder, in the complexity sense. Thus, besides looking for the coding options which lead to the best coded representation in terms of rate and distortion (RD), we constrain to a complexity (C) budget. We present a complexity optimized framework (RDC-optimized) which allows for real-time video compression and that does not make use of frame-skipping to comply to the desired encoding speed. We developed our framework around an open source software implementation of the H.264/AVC, the the ×264 encoder. Results show that tight complexity control is attainable in practice, with very little loss in RD performance."
395610,21239,8494,Complexity reduction for the 3D-HEVC depth maps coding,2015,"This paper presents a qualitative discussion of the depth maps properties that can be considered to achieve complexity reduction for 3D-High Efficiency Video Coding (3D-HEVC) depth maps coding. Both intra and inter-frame predictions are considered in this discussion that conduced to the proposition of two simple complexity reduction techniques: the Simplified Edge Detector (SED) and the Diamond Search (DS) simplified inter-prediction. The SED anticipates the blocks that are likely to be better predicted by the HEVC intra-prediction, avoiding evaluations of Depth Modeling Modes (DMM). The DS and SED were compared to anchor results and experimental analysis showed that the proposed algorithms are able to achieve a time saving of 11.3% encoding time reduction, with acceptable impact on the BD-Rate of the synthesized views of 0.6%."
2922270,21239,9078,Optimizing block-coded motion parameters with block-partition graphs,2016,"We address the problem of optimizing block-coded motion parameters for use inside typical motion-compensating video encoders. We cast the given discrete problem as a nonsmooth nonconvex optimization problem which is defined over some graph, and solve it using the split primal-dual hybrid gradient algorithm. Although computational efficiency is not the main focus of this paper, an efficient, parallelized implementation of our proposed approach can be used as a way of performing rate-distortion optimal motion estimation in video encoders such as those following the H.264 or HEVC standard. Results from our experiments highlight the degree of sub-optimality demonstrated by motion parameters that have been computed by H.264 block matching algorithms."
1774525,21239,8494,Local computation and estimation of wavelet coefficients in the dual-tree complex wavelet transform,2008,"The dual-tree complex wavelet transform (DT CWT) was introduced to overcome the disadvantages of the traditional fully decimated discrete wavelet transform (DWT), namely the shift-variance and the poor directional selectivity properties. Because of its improvements in this regards, the dual-tree has been successfully demonstrated in many image processing applications such as denoising, motion estimation, image classification, and even compression despite its redundancy nature. Our goal is to be able to predict the wavelet coefficients of one tree knowing those of the other in the dual-tree complex wavelet transform. In other words, given a subset of real coefficients in the DTCWT, how can we compute or estimate accurately the imaginary coefficients in the same local neighborhood and vice versa? The proposed method is based on exploiting the orthogonality properties of one of the nicest dual-tree designs - the Q-shift complex wavelets."
2845728,21239,9078,Constant-time bilateral filter using spectral decomposition,2016,This paper presents an efficient constant-time bilateral filter where constant-time means that computational complexity is independent of filter window size. Many state-of-the-art constant-time methods approximate the original bilateral filter by an appropriate combination of a series of convolutions. It is important for this framework to optimize the performance tradeoff between approximate accuracy and the number of convolutions. The proposed method achieves the optimal performance tradeoff in a least-squares manner by using spectral decomposition under the assumption that images consist of discrete intensities such as 8-bit images. This approach is essentially applicable to arbitrary range kernel. Experiments show that the proposed method outperforms state-of-the-art methods in terms of both computational complexity and approximate accuracy.
2906730,21239,9078,Dehazing of color image using stochastic enhancement,2016,"Images captured in presence of fog, haze or snow usually suffer from poor contrast and visibility. In this paper we propose a novel dehazing method to increase visibility from a single view without using any prior knowledge about the outdoor scene. The proposed method estimates a visibility map of the scene from the input image and uses stochastic iterative algorithm to remove fog and haze. The method can be applied to color and grayscale images. Experimental results show that the proposed algorithm outperforms most of the state-of-the-art algorithms in terms of contrast, colorfulness and visibility."
2000555,21239,9078,Implementation and application of local computation of wavelet coefficients in the dual-tree complex wavelets,2009,"The dual-tree complex wavelet transform (DT CWT) was introduced to overcome the disadvantages of the traditional fully decimated discrete wavelet transform (DWT), namely the shift-variance and the poor directional selectivity properties. Because of its improvements in these aspects, the dual-tree has been widely used in many image processing applications such as denoising, motion estimation, image classification and even compression despite its redundant representation. In our previous work, we were able to accurately and locally estimate the wavelet coefficients of one tree in the DT CWT, given a subset of the other tree coefficients. Our method is based on exploiting the orthogonality properties of one of the nicest dual-tree designs - the Q-shift complex wavelets. In this paper, we demonstrate the implementation of multiple level of decomposition as well as the two dimensional realization with application to region of interest (ROI) imaging applications such as denoising."
