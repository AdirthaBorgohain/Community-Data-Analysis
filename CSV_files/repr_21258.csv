ID_Article,communityId,ID_RelatedVenue,title,year,abstract
1244105,21258,9099,PodCastle and songle: crowdsourcing-based web services for spoken content retrieval and active music listening,2012,"In this keynote talk, we describe two crowdsourcing-based web services, PodCastle (http://en.podcastle.jp for the English version and http://podcastle.jp for the Japanese version) and Songle (http://songle.jp). PodCastle and Songle collect voluntary contributions by anonymous users in order to improve the experiences of users listening to speech and music content available on the web. These services use automatic speech-recognition and music-understanding technologies to provide content analysis results, such as full-text speech transcriptions and music scene descriptions, that let users enjoy content-based multimedia retrieval and active browsing of speech and music signals without relying on metadata.   When automatic content analysis is used, however, errors are inevitable. PodCastle and Songle therefore provide an efficient error correction interface that let users easily correct errors by selecting from a list of candidate alternatives. Through these corrections, users gain a real sense of contributing for their own benefit and that of others and can be further motivated to contribute by seeing corrections made by other users.   Our services promote the popularization and use of speech-recognition and music-understanding technologies by raising user awareness. Users can grasp the nature of those technologies just by seeing results obtained when the technologies applied to speech data and songs available on the web."
2822801,21258,9804,Uncontrolled Manifolds in Vowel Production: Assessment with a Biomechanical Model of the Tongue,2016,"Motor equivalence is a key feature of speech motor control, since speakers must constantly adapt to various phonetic contexts and speaking conditions. The Uncontrolled Manifold (UCM) idea offers a theoretical framework for considering motor equivalence. In this framework coordination among motor control variables is separated into two subspaces, one in which changes in control variables modify the acoustic output and another one in which these changes do not influence the output. Our work develops this concept for speech production using a 2D biomechanical model of the tongue, coupled with a jaw and lip model, for vowel production. We first propose a representation of the linearized UCM based on orthogonal projection matrices. Next we characterize the UCMs of various vocal tract configurations of the 10 French oral vowels using their perturbation responses. We then investigate whether these UCMs describe phonetic classes like phonemes, front/back vowels, rounded/unrounded vowels, or whether they significantly vary across representatives of these different classes. We found they clearly differ between rounded and unrounded vowels, but are quite similar within each category. This suggests that similar motor equivalence strategies can be implemented within each of these classes and that UCMs provide a valid characterization of an equivalence strategy."
2182616,21258,535,Using temporal information for improving articulatory-acoustic feature classification,2009,"This paper combines acoustic features with a high temporal and a high frequency resolution to reliably classify articulatory events of short duration, such as bursts in plosives. SVM classification experiments on TIMIT and SVArticulatory showed that articulatory-acoustic features (AFs) based on a combination of MFCCs derived from a long window of 25ms and a short window of 5ms that are both shifted with 2.5ms steps (Both) outperform standard MFCCs derived with a window of 25 ms and a shift of 10 ms (Baseline). Finally, comparison of the TIMIT and SVArticulatory results showed that for classifiers trained on data that allows for asynchronously changing AFs (SVArticulatory) the improvement from Baseline to Both is larger than for classifiers trained on data where AFs change simultaneously with the phone boundaries (TIMIT)."
1907307,21258,9099,Unfolding speaker clustering potential: a biomimetic approach,2009,"Speaker clustering is the task of grouping a set of speech utterances into speaker-specific classes. The basic techniques for solving this task are similar to those used for speaker verification and identification. The hypothesis of this paper is that the techniques originally developed for speaker verification and identification are not sufficiently discriminative for speaker clustering. However, the processing chain for speaker clustering is quite large - there are many potential areas for improvement. The question is:  where  should improvements be made to improve the  final  result? To answer this question, this paper takes a biomimetic approach based on a study with human participants acting as an automatic speaker clustering system. Our findings are twofold: it is the stage of modeling that has the highest potential, and information with respect to the temporal succession of frames is crucially missing. Experimental results with our implementation of a speaker clustering system incorporating our findings and applying it on TIMIT data show the validity of our approach."
1977747,21258,8960,Static and Dynamic Error Propagation Networks with Application to Speech Coding,1988,"Error propagation nets have been shown to be able to learn a variety of tasks in which a static input pattern is mapped onto a static output pattern. This paper presents a generalisation of these nets to deal with time varying, or dynamic patterns, and three possible architectures are explored. As an example, dynamic nets are applied to the problem of speech coding, in which a time sequence of speech data are coded by one net and decoded by another. The use of dynamic nets gives a better signal to noise ratio than that achieved using static nets."
1679606,21258,535,NMF-based keyword learning from scarce data,2013,"This research is situated in a project aimed at the development of a vocal user interface (VUI) that learns to understand its users specifically persons with a speech impairment. The vocal interface adapts to the speech of the user by learning the vocabulary from interaction examples. Word learning is implemented through weakly supervised non-negative matrix factorization (NMF). The goal of this study is to investigate how we can improve word learning when the number of interaction examples is low. We demonstrate two approaches to train NMF models on scarce data: 1) training word models using smoothed training data, and 2) training word models that strictly correspond to the grounding information derived from a few interaction examples. We found that both approaches can substantially improve word learning from scarce training data."
1385960,21258,535,Any questions? Automatic question detection in meetings,2009,"In this paper, we describe our efforts toward the automatic detection of English questions in meetings. We analyze the utility of various features for this task, originating from three distinct classes: lexico-syntactic, turn-related, and pitch-related. Of particular interest is the use of parse tree information in classification, an approach as yet unexplored. Results from experiments on the ICSI MRDA Corpus demonstrate that lexico-syntactic features are most useful for this task, with turn-and pitch-related features providing complementary information in combination. In addition, experiments using reference parse trees on the Broadcast Conversation portion of the OntoNotes release 2.9 data set illustrate the potential of parse trees to outperform word lexical features."
61402,21258,9804,Generation of fundamental frequency contours for Thai speech synthesis using tone nucleus model.,2013,報告番号: ; 学位授与日: 2013-03-25 ; 学位の種別: 修士 ; 学位の種類: 修士（情報理工学） ; 学位記番号: ; 研究科・専攻: 情報理工学系研究科・電子情報学専攻
299997,21258,9804,Ingressive Speech As An Indication That Humans Are Talking To Humans (And Not To Machines),2002,Ingressive Speech As An Indication That Humans Are Talking To Humans (And Not To Machines)
354932,21258,9804,Using statistical language modelling to identify new vocabulary in a grammar-based speech recognition system.,2003,Using Statistical Language Modelling to Identify NewVocabulary in a Grammar-Based Speech Recognition System
392164,21258,9804,On the time variability of vocal tract for speaker recognition.,2004,Chalmers Publication Library (CPL).#N#    Forskningspublikationer fran Chalmers Tekniska Hogskola.
63760,21258,9804,Vector quantization of glottal pulses.,1995,Chalmers Publication Library (CPL).#N#    Forskningspublikationer fran Chalmers Tekniska Hogskola.
291933,21258,9804,Model or Non-Model Based Classifiers,1991,Chalmers Publication Library (CPL).#N#    Forskningspublikationer fran Chalmers Tekniska Hogskola.
209215,21258,9804,Theory for speaker recognition over IP,2004,Chalmers Publication Library (CPL).#N#    Forskningspublikationer fran Chalmers Tekniska Hogskola.
436275,21258,9804,A Posterior Probability-based System Hybridisation and Combination for Spoken Term Detection,2009,"Proceedings of the 10th Annual Conference of the International Speech Communication Association, Brighton"
2731279,21258,9804,Durational information in word-initial lexical embeddings in spoken Dutch,2015,"16th Annual Conference of the International Speech Communication Association, 6 september 2015"
203335,21258,9804,Enhanced processing of a lost language. Linguistic knowledge or linguistic skill,2015,"16th Annual Conference of the International Speech Communication Association, 6 september 2015"
2753178,21258,9804,Confusability in L2 vowels. Analyzing the role of different features,2015,"16th Annual Conference of the International Speech Communication Association, 6 september 2015"
2583400,21258,9804,Auris populi: crowdsourced native transcriptions of Dutch vowels spoken by adult Spanish learners,2015,"16th Annual Conference of the International Speech Communication Association, 6 september 2015"
203882,21258,9804,Perceptual learning of /f/-/s/ by older listeners,2012,"Interspeech 2012, the 13th Annual Conference of the International Speech Communication Association"
2314908,21258,9804,Modeling cue trading in human word recognition,2012,Interspeech 2012: the 13th Annual Conference of the International Speech Communication Association
2310004,21258,9804,Hearing loss and the use of acoustic cues in phonetic categorisation of fricatives,2012,"Interspeech 2012, the 13th Annual Conference of the International Speech Communication Association"
261919,21258,9804,Constrained speaker linking,2014,"15th Annual Conference of the International Speech Communication Association, 14 september 2014"
53716,21258,9804,Training Log-Linear Acoustic Models in Higher-Order Polynomial Feature Space for Speech Recognition,2013,"14th Annual Conference of the International Speech Communication Association, 25 augustus 2013"
204806,21258,9804,Word Identification Using Phonetic Features: Towards a Method to Support Multivariate fMRI Speech Decoding,2013,"14th Annual Conference of the International Speech Communication Association, 25 augustus 2013"
2213355,21258,9804,Detecting Words in Speech Using Linear Separability in a Bag-of-Events Vector Space,2013,"14th Annual Conference of the International Speech Communication Association, 25 augustus 2013"
50405,21258,9804,Smile with a smile,2012,"13th Annual Conference of the International Speech Communication Association (Interspeech 2012), Portland, OR, USA"
527664,21258,9804,The efficient generation of pronunciation dictionaries: machine learning factors during bootstrapping.,2004,"8th International Conference on Spoken Language Processing, Jeju Island, Korea, 4 - 8 October 2004"
376309,21258,9804,Unsupervised Noisy Environment Adaptation Algorithm Using MLLR and Speaker Selection,2001,"EUROSPEECH2001: the 7th European Conference on Speech Communication and Technology, September 3-7,  2001, Aalborg,  Denmark."
194823,21258,9804,High Quality Voice Conversion Based on Gaussian Mixture Model with Dynamic Frequency Warping,2001,"EUROSPEECH2001: the 7th European Conference on Speech Communication and Technology, September 3-7,  2001, Aalborg,  Denmark."
349040,21258,9804,Statistical sound source identification in a real acoustic environment for robust speech recognition using a microphone array.,2001,"EUROSPEECH2001: the 7th European Conference on Speech Communication and Technology, September 3-7,  2001, Aalborg,  Denmark."
532881,21258,9804,Blind Source Separation for Speech Based on Fast-convergence Algorithm with ICA and Beamforming,2001,"EUROSPEECH2001: the 7th European Conference on Speech Communication and Technology, September 3-7,  2001, Aalborg,  Denmark."
470182,21258,9804,Elderly acoustic model for large vocabulary continuous speech recognition.,2001,"EUROSPEECH2001: the 7th European Conference on Speech Communication and Technology, September 3-7,  2001, Aalborg,  Denmark."
189769,21258,9804,Speech Extraction in a Car Interior Using Frequency-Domain ICA with Rapid Filter Adaptations,2005,"INTERSPEECH2005: the 9th European Conference on Speech Communication and technology, September 4-8,  2005, Lisbon,  Portugal."
419269,21258,9804,Investigating the role of the Lombard reflex in non-audible murmur (NAM) recognition.,2005,"INTERSPEECH2005: the 9th European Conference on Speech Communication and technology, September 4-8,  2005, Lisbon,  Portugal."
205904,21258,9804,Remodeling of the Sensor for Non-Audible Murmur (NAM),2005,"INTERSPEECH2005: the 9th European Conference on Speech Communication and technology, September 4-8,  2005, Lisbon,  Portugal."
471345,21258,9804,Operating a public spoken guidance system in real environment.,2005,"INTERSPEECH2005: the 9th European Conference on Speech Communication and technology, September 4-8,  2005, Lisbon,  Portugal."
488095,21258,9804,Rapid Unsupervised Speaker Adaptation Based on Multi-Template HMM Sufficient Statistics in Noisy Environments,2005,"INTERSPEECH2005: the 9th European Conference on Speech Communication and technology, September 4-8,  2005, Lisbon,  Portugal."
510750,21258,9804,GMM-Based Voice Conversion Applied to Emotional Speech Synthesis,2003,"EUROSPEECH2003: 8th European Conference on Speech Communication and Technology, September 1-4,  2003, Geneva,  Switzerland."
279249,21258,9804,Unsupervised Speaker Adaptation Based on HMM Sufficient Statistics in Various Noisy Environments,2003,"EUROSPEECH2003: 8th European Conference on Speech Communication and Technology, September 1-4,  2003, Geneva,  Switzerland."
172980,21258,9804,An Efficient Keyword Spotting Technique Using a Complementary Language for Filler Models Training,2003,"EUROSPEECH2003: 8th European Conference on Speech Communication and Technology, September 1-4,  2003, Geneva,  Switzerland."
167652,21258,9804,Non-Audible Murmur Recognition,2003,"EUROSPEECH2003: 8th European Conference on Speech Communication and Technology, September 1-4,  2003, Geneva,  Switzerland."
420977,21258,9804,A semi-blind source separation method for hands-free speech recognition of multiple talkers.,2003,"EUROSPEECH2003: 8th European Conference on Speech Communication and Technology, September 1-4,  2003, Geneva,  Switzerland."
466733,21258,9804,Model-Integration Rapid Training Based on Maximum Likelihood for Speech Recognition,2003,"EUROSPEECH2003: 8th European Conference on Speech Communication and Technology, September 1-4,  2003, Geneva,  Switzerland."
521567,21258,9804,An Agent-based Framework for Speech Investigation,2005,"The 9th International Conference on Speech Communication and Technology (Interspeech 2005), Lisbon, Portugal, September, 2005"
354899,21258,9804,Noise robust real world spoken dialogue system using GMM based rejection of unintended inputs.,2004,"ICSLP2004: the 8th International Conference on Spoken Language Processing, October 4-8,  2004, Jeju Island,  Korea."
218004,21258,9804,Interface for barge-in free spoken dialogue system using adaptive sound field control.,2004,"ICSLP2004: the 8th International Conference on Spoken Language Processing, October 4-8,  2004, Jeju Island,  Korea."
361117,21258,9804,Rapid EM training based on model-integration.,2004,"ICSLP2004: the 8th International Conference on Spoken Language Processing, October 4-8,  2004, Jeju Island,  Korea."
306031,21258,9804,Robust Speech Recognition with Spectral Subtraction in Low SNR,2004,"ICSLP2004: the 8th International Conference on Spoken Language Processing, October 4-8,  2004, Jeju Island,  Korea."
280304,21258,9804,Recent Progress of Open-Source LVCSR Engine Julius and Japanese Model Repository,2004,"ICSLP2004: the 8th International Conference on Spoken Language Processing, October 4-8,  2004, Jeju Island,  Korea."
414971,21258,9804,Robust Dependency Parsing of Spontaneous Japanese Speech and Its Evaluation,2004,"Grant-in-Aids for Young Scientists of the Ministry of Education, Science, Sports and Culture, Japan;#R##N#The Tatematsu Foundation"
538054,21258,9804,An effect of adaptive beamforming on hands-free speech recognition based on 3-d viterbi search.,1998,"ICSLP1998: the 5th International Conference on Spoken Language Processing,  November 30 - December 4,  1998,  Sydney,  Australia."
373285,21258,9804,Evaluation of model adaptation by HMM decomposition on telephone speech recognition.,1998,"ICSLP1998: the 5th International Conference on Spoken Language Processing,  November 30 - December 4,  1998,  Sydney,  Australia."
393136,21258,9804,Blind source separation based on subband ICA and beamforming,2000,"ICSLP2000: the 6th International Conference on Spoken Language Processing, October 16-20,  2000, Beijing,  China."
286925,21258,9804,Stream Weight Optimization of Speech and Lip Image Sequence for Audio-Visual Speech Recognition,2000,"ICSLP2000: the 6th International Conference on Spoken Language Processing, October 16-20,  2000, Beijing,  China."
2080677,21258,9804,Free software toolkit for Japanese large vocabulary continuous speech recognition,2000,"ICSLP2000: the 6th International Conference on Spoken Language Processing, October 16-20,  2000, Beijing,  China."
470060,21258,9804,Investigation of Analysis and Synthesis Parameters of Straight by Subjective Evaluation,2000,"ICSLP2000: the 6th International Conference on Spoken Language Processing, October 16-20,  2000, Beijing,  China."
293957,21258,9804,Manipulating Speech Pitch Periods According to Optimal Insertion/Deletion Position in Residual Signal for Intonation Control in Speech Synthesis,2000,"ICSLP2000: the 6th International Conference on Spoken Language Processing, October 16-20,  2000, Beijing,  China."
98710,21258,9804,Robust Fundamental Frequency Estimation Using Instantaneous Frequencies of Harmonic Components,2000,"ICSLP2000: the 6th International Conference on Spoken Language Processing, October 16-20,  2000, Beijing,  China."
146679,21258,9804,Straight-Based Voice Conversion Algorithm Based on Gaussian Mixture Model,2000,"ICSLP2000: the 6th International Conference on Spoken Language Processing, October 16-20,  2000, Beijing,  China."
363665,21258,9804,Close Speaker Cancellation for Suppression of Non-Stationary Background Noise for Hands-Free Speech Interface,2010,"INTERSPEECH2010: 11th Annual Conference of the International Speech Communication Association, September 26-30,  2010, Chiba,  Japan."
482700,21258,9804,Comparison of Methods for Topic Classification in a Speech-Oriented Guidance System,2010,"INTERSPEECH2010: 11th Annual Conference of the International Speech Communication Association, September 26-30,  2010, Chiba,  Japan."
442882,21258,9804,Study on Speaker Verification with Non-Audible Murmur Segments,2007,"INTERSPEECH2007: 8th Annual Conference of the International Speech Communication Association, August 27-31,  2007, Antwerp,  Belgium."
417944,21258,9804,Rapid Unsupervised Speaker Adaptation Using Single Utterance Based on MLLR and Speaker Selection,2007,"INTERSPEECH2007: 8th Annual Conference of the International Speech Communication Association, August 27-31,  2007, Antwerp,  Belgium."
379793,21258,9804,DYNAMIC ADAPTATION OF VOCABULARY INDEPENDENT HMMS TO AN APPLICATION ENVIRONMENT,2000,"In this paper, the authors present a software architecture for collecting, selecting, and using speech data and applying the method to a train timetable information system."
421528,21258,9804,Using Start/End Timings of Spectral Transitions Between Phonemes in Concatenative Speech Synthesis,2002,"ICSLP2002: the 7th International Conference on Spoken Language Processing , September 16-20,  2002, Denver,  Colorado,  USA."
453404,21258,9804,Evaluation of cross-language voice conversion using bilingual and non-bilingual databases.,2002,"ICSLP2002: the 7th International Conference on Spoken Language Processing , September 16-20,  2002, Denver,  Colorado,  USA."
83417,21258,9804,Designing Japanese Speech Database Covering Wide Range in Prosody for Hybrid Speech Synthesizer,2002,"ICSLP2002: the 7th International Conference on Spoken Language Processing , September 16-20,  2002, Denver,  Colorado,  USA."
400494,21258,9804,Speech Enhancement in Car Environment Using Blind Source Separation,2002,"ICSLP2002: the 7th International Conference on Spoken Language Processing , September 16-20,  2002, Denver,  Colorado,  USA."
503827,21258,9804,Selective Multi-Path Acoustic Model Based on Database Likelihoods,2002,"ICSLP2002: the 7th International Conference on Spoken Language Processing , September 16-20,  2002, Denver,  Colorado,  USA."
85059,21258,9804,Spectral Subtraction in Noisy Environments Applied to Speaker Adaptation Based on HMM Sufficient Statistics,2002,"ICSLP2002: the 7th International Conference on Spoken Language Processing , September 16-20,  2002, Denver,  Colorado,  USA."
286323,21258,9804,Low-Delay Voice Conversion Based on Maximum Likelihood Estimation of Spectral Parameter Trajectory,2008,"INTERSPEECH2008: 9th Annual Conference of the International Speech Communication Association, September 22-26,  2008, Brisbane,  Australia."
282216,21258,9804,Question and Answer Database Optimization Using Speech Recognition Results,2008,"INTERSPEECH2008: 9th Annual Conference of the International Speech Communication Association, September 22-26,  2008, Brisbane,  Australia."
586704,21258,9804,An improved one-to-many eigenvoice conversion system.,2008,"INTERSPEECH2008: 9th Annual Conference of the International Speech Communication Association, September 22-26,  2008, Brisbane,  Australia."
413278,21258,9804,Evaluation of speaking-aid system with voice conversion for laryngectomees toward its use in practical environments.,2008,"INTERSPEECH2008: 9th Annual Conference of the International Speech Communication Association, September 22-26,  2008, Brisbane,  Australia."
80336,21258,9804,Speaker Verification with Non-Audible Murmur Segments by Combining Global Alignment Kernel and Penalized Logistic Regression Machine,2008,"INTERSPEECH2008: 9th Annual Conference of the International Speech Communication Association, September 22-26,  2008, Brisbane,  Australia."
317304,21258,9804,Simultaneous conversion of duration and spectrum based on statistical models including time-sequence matching.,2008,"INTERSPEECH2008: 9th Annual Conference of the International Speech Communication Association, September 22-26,  2008, Brisbane,  Australia."
350105,21258,9804,Maximum a posteriori adaptation for many-to-one eigenvoice conversion.,2008,"INTERSPEECH2008: 9th Annual Conference of the International Speech Communication Association, September 22-26,  2008, Brisbane,  Australia."
500975,21258,9804,Rapid Unsupervised Speaker Adaptation Robust in Reverberant Environment Conditions,2008,"INTERSPEECH2008: 9th Annual Conference of the International Speech Communication Association, September 22-26,  2008, Brisbane,  Australia."
252413,21258,9804,Development and evaluation of hands-free spoken dialogue system for railway station guidance.,2008,"INTERSPEECH2008: 9th Annual Conference of the International Speech Communication Association, September 22-26,  2008, Brisbane,  Australia."
2615036,21258,9804,Speaker-Adaptive Speech Synthesis Based on Eigenvoice Conversion and Language-Dependent Prosodic Conversion in Speech-to-Speech Translation.,2011,"INTERSPEECH 2011: 12th Annual Conference of the International Speech Communication Association, 28-31 August, 2011, Florence, Italy."
2657901,21258,9804,Evaluation of Many-to-Many Alignment Algorithm by Automatic Pronunciation Annotation Using Web Text Mining,2012,"INTERSPEECH 2012: The 13th Annual Conference of the International Speech Communication Association, September 9-13, 2012, Portland, Oregon, USA."
2634461,21258,9804,Spoken Inquiry Discrimination Using Bag-of-Words for Speech-Oriented Guidance System,2012,"INTERSPEECH 2012: The 13th Annual Conference of the International Speech Communication Association, September 9-13, 2012, Portland, Oregon, USA."
450980,21258,9804,Microprosodic study of isolated French word corpora,1995,"Keywords: speech Reference EPFL-CONF-82308 Record created on 2006-03-10, modified on 2016-08-08"
474133,21258,9804,Acoustic-to-Articulatory Inversion based on Local Regression,2010,"This paper presents an Acoustic-to-Articulatory inversionmethod based on local regression. Two types of local regression,a non-parametric and a local linear regression have beenapplied on a corpus  ..."
2885918,21258,11321,Efficient Parallel Learning of Word2Vec,2016,"ICML '16: The 33rd International Conference on Machine Learning, June 24, New York, 2016. ML Systems Workshop"
2583406,21258,9804,The relationship between voice source parameters and the Maxima Dispersion Quotient (MDQ),2015,"Science Foundation Ireland, Grant 09/IN.1/I2631 (FASTNET) and the ABAIR project funded by the Department of Arts, Heritage and the Gaeltacht, Ireland."
417391,21258,9804,Speaker verification with non-audible murmur segments.,2006,"INTERSPEECH2006: the 9th International Conference on Spoken Language Processing (ICSLP), September 17-21,  2006, Pittsburgh,  Pennsylvania,  USA."
1869776,21258,9804,The acoustics of word stress in English as a function of stress level and speaking style,2015,"This study of lexical stress in English is part of a series of studies, the goal of which is to describe the acoustics of lexical stress for a number of typologically different languages. When full ..."
2822281,21258,9804,Respiratory belts and whistles : A preliminary study of breathing acoustics for turn-taking,2016,This paper presents first results on using acoustic intensity of inhalations as a cue to speech initiation in spontaneous multiparty conversations. We demonstrate that inhalation intensity signific ...
2222124,21258,9804,The Effect of Using Normalized Models in Statistical Speech Synthesis.,2011,This work was partly supported by the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement 213845 (EMIME).
199545,21258,9804,Evidence for demodulation in speech perception,2000,"According to the Modulation Theory, speakers modulate their voice with linguistic gestures, and listeners demodulate the signal in order to separate the linguistic from the expressive and organic i ..."
2584237,21258,9804,New Nonsense Syllables Database -- Analyses and Preliminary ASR Experiments,2004,"The paper presents analyses, modifications, and first experiments with a new nonsense syllables database. Results of preliminary experiments with phoneme recognition are given and discussed."
575428,21258,9804,Advances in regional accent clustering in Swedish,2005,The regional pronunciation  variation in Swedish is analysed on a large database. Statistics over each phoneme and for each region of Sweden are computed using the EM algorithm in a hidden Markov m ...
605312,21258,9804,Multiresolution time-sequency speech processing based on orthogonal wavelet packet pulse forms,1993,"Keywords: LTS1 Reference LTS-CONF-1993-052 Record created on 2006-06-14, modified on 2016-08-08"
441971,21258,9804,Multiresolution speech analysis using fast time- varying orthogonal wavelet packet transform algorithms,1995,"Keywords: LTS1 Reference LTS-CONF-1995-037 Record created on 2006-06-14, modified on 2016-08-08"
675045,21258,9804,Entropy Based Voice Activity Detection in Very Noisy Conditions,2001,"Keywords: LTS1 Reference LTS-CONF-2001-039 Record created on 2006-06-14, modified on 2016-08-08"
415744,21258,9804,JavaSpeakerRecognition - Interactive Workbench for Visualizing Speaker Recognition Concepts on the WWW,2001,"Keywords: LTS1 Reference LTS-CONF-2001-027 Record created on 2006-06-14, modified on 2016-08-08"
334808,21258,9804,Pitch-Dependent GMMs for Text-Independent Speaker Recognition Systems,2001,"Keywords: LTS1 Reference LTS-CONF-2001-011 Record created on 2006-06-14, modified on 2016-08-08"
567466,21258,9804,eLite-HTS: a NLP tool for French HMM-based speech synthesis,2014,"This paper presents eLite-HTS, a web service which generates input files for the training and synthesis stages of a French HMM-based synthesizer using the HTS toolkit"
616588,21258,9804,Correlates to intelligibility in deviant child speech - comparing clinical evaluations to audience response system-based evaluations by untrained listeners.,2013,"The severity of speech impairments can be measured in different ways; whereas some metrics focus on quantifying the specific speech deviations, other focus on the functional effects of the speech i ..."
211349,21258,9804,Synthetic correction of deviant speech - children's perception of phonologically modified recordings of their own speech.,2012,This report describes preliminary data from a study of how children with phonological impairment (PI) perceive automatically corrected versions of their own deviant speech. The results from 8 child ...
377874,21258,9804,Vocal imitation in early language acquisition,2008,This paper presents a study of vocal imitation during the early stages of the language acquisition process. Utterances were extracted from recordings of adult-infant interactions in controlled but  ...
130879,21258,9804,Tracking pitch contours using minimum jerk trajectories,2011,"This paper proposes a fundamental frequency tracker, with the specific purpose of comparing the automatic estimates with pitch contours that are sketched by trained phoneticians. The method uses a  ..."
457233,21258,9804,Collision threshold pressure before and after vocal loading.,2009,The phonation threshold pressure (PIP) has been found to increase during vocal fatigue. In the present study we compare PTP and collision threshold pressure (CTP) before and after vocal loading in  ...
2843561,21258,9804,The Acoustics of Lexical Stress in Italian as a Function of Stress Level and Speaking Style.,2016,"The study is part of a series of studies, describing the acoustics of lexical stress in a way that should be applicable to any language. The present database of recordings includes Brazilian Portug ..."
2680853,21258,9804,Detecting Repetitions in Spoken Dialogue Systems Using Phonetic Distances,2015,"Repetitions in Spoken Dialogue Systems can be a symptom of problematic communication. Such repetitions are often due to speech recognition errors, which in turn makes it harder to use the output of ..."
524354,21258,9804,On the effect of the acoustic environment on the accuracy of perception of speaker orientation from auditory cues alone,2012,"The ability of people, and of machines, to determine the position of a sound source in a room is well studied. The related ability to determine the orientation of a directed sound source, on the ot ..."
579179,21258,9804,"The phrase-final accent in Kammu: effects of tone, focus and engagement",2009,"The phrase-final accent can typically contain a multitude of simultaneous prosodic signals. In this study, aimed at separating the effects of lexical tone from phrase-final intonation, phrase-final ..."
555256,21258,9804,Influence of lexical tones on intonation in Kammu,2010,"The aim of this study is to investigate how the presence of lexical tones influences the realization of focal accent and sentence intonation. The language studied is Kammu, a language particularly  ..."
2114559,21258,9804,Speaker diarization using gesture and speech,2014,"Interspeech 2014: 15th Annual Conference of the International Speech Communication Association, 14-18 Sept 2014, MAX Atria @ Singapore EXPO"
2506355,21258,9804,Communicative needs and respiratory constraints,2015,This study investigates timing of communicative behaviour with respect to speaker’s respiratory cycle. The data is drawn from a corpus of multiparty conversations in Swedish. We find that while lon ...
355758,21258,9804,A Tree-Trellis N-best Decoder for Stochastic Context-Free Grammars,2000,"In this paper a decoder for continuous speech recognition using stochastic context-free grammars is described. It forms the backbone of the ACE recognizer, which is a modular system for real-time s ..."
497366,21258,9804,Automatic Recognition of Anger in Spontaneous Speech,2008,"Automatic detection of real life negative emotions in speech has been evaluated using Linear Discriminant Analysis, LDA, with classic emotion features and a classifier based on Gaussian Mixture M ..."
255350,21258,9804,Visual correlates to prominence in several expressive modes,2006,"In this paper, we present measurements of visual, facial parameters obtained from a speech corpus consisting of short, read utterances in which focal accent was systematically varied. The utterance ..."
347644,21258,9804,A Comparison of Disfluency Distribution in a Unimodal and a Multimodal Speech Interface,2000,"In this paper, we compare the distribution of disfluencies in two human--computer dialogue corpora. One corpus consists of unimodal travel booking dialogues, which were recorded over the telephone. ..."
348948,21258,9804,Can audio-visual instructions help learners improve their articulation? : an ultrasound study of short term changes,2008,This paper describes how seven French subjects change their pronunciation and articulation when practising Swedish words with a computer-animated virtual teacher. The teacher gives feedback on the  ...
334844,21258,9804,Improving the Phase Vocoder Approach to Pitch-Shifting,2007,A class of methods known as phase vocoders allows for implementing pitch shifting in the spectral domain. We extend the approach of shifting the isolated harmonies of the spectrum by introducing a  ...
218088,21258,9804,Prosody in public speech : analyses of a news announcement and a political interview,2005,"The study concerns informative and argumentative functions of prosody in the public domain. Analyses were based on speech samples from a professional news announcer and a well-known politician, ea ..."
693116,21258,9804,Effect of MPEG audio compression on HMM-based speech synthesis,2013,"In this paper, the effect of MPEG audio compression on HMMbased speech synthesis is studied. Speech signals are encoded with various compression rates and analyzed using the GlottHMM vocoder. Objec ..."
2670007,21258,9804,"The Effect of Soft, Modal and Loud Voice Levels on Entrainment in Noisy Conditions",2015,Conversation partners have a tendency to adapt their vocal in- tensity to each other and to other social and environmental fac- tors. A socially adequate vocal intensity level by a speech syn- thes ...
492940,21258,9804,"Intra-, Inter-, and Cross-cultural Classification of Vocal Affect",2011,"We present intra-, inter- and cross-cultural classifications of vocal expressions. Stimuli were selected from the VENEC corpus and consisted of portrayals of 11 emotions, each expressed with 3 leve ..."
470758,21258,9804,On the potential threat of using large speech corpora for impostor selection in speaker verification.,2000,In order to evaluate the risk in SV systems one should take into account the possible perpetrator who knows whom he/she is attacking. This paper thus evaluated if a large speech corpus can be used  ...
2880637,21258,9804,Using a Biomechanical Model and Articulatory Data for the Numerical Production of Vowels,2016,"We introduce a framework to study speech production using a biomechanical model of the human vocal tract, ArtiSynth. Electromagnetic articulography data was used as input to an inverse tracking sim ..."
391109,21258,9804,Virtual Speech Reading Support for Hard of Hearing in a Domestic Multi-Media Setting,2009,"In this paper we present recent results on the development of the SynFace lip synchronized talking head towards multilinguality, varying signal conditions and noise robustness in the Hearing at Hom ..."
559724,21258,9804,Pitch similarity in the vicinity of backchannels,2010,Dynamic modeling of spoken dialogue seeks to capture how interlocutors change their speech over the course of a conversation. Much work has focused on how speakers adapt or entrain to different asp ...
2880700,21258,9804,Respiratory turn-taking cues,2016,This paper investigates to what extent breathing can be used as a cue to turn-taking behaviour. The paper improves on ex- isting accounts by considering all possible transitions between speaker sta ...
82108,21258,9804,Speech training for deaf and hearing-impaired people,1999,"We describe the results of the ISAEUS project (TIDE DE 3004) under development. Its objective is to develop a prototype for training deaf people in three languages: French, German, and Spanish."
316576,21258,9804,Super-Dirichlet Mixture Models using Differential Line Spectral Frequencies for Text-Independent Speaker Identification,2011,A new text-independent speaker identification (SI) system is proposed. This system utilizes the line spectral frequencies (LSFs) as alternative feature set for capturing the speaker characteristics ...
345642,21258,9804,A Comparative Study of Pauses in Dialogues and Read Speech.,2001,"This study aims to investigate the length, frequency and position of various types of pauses in three different speaking styles: elicited spontaneous dialogues, professional reading and non-professional reading."
2835530,21258,9804,Sequence Student-Teacher Training of Deep Neural Networks,2016,This is the final version of the article. It first appeared from the International Speech Communication Association via http://dx.doi.org/10.21437/Interspeech.2016-911
46760,21258,9804,Jitter and shimmer measurements for speaker recognition.,2007,Comunicacio presentada a:  8th Annual Conference of the International Speech Communication Association a Antwerp (Belgium) celebrada del 27 al 31 d'agost de 2007.
330198,21258,9804,AN EMPIRICAL MODEL OF EMPHATIC WORD DETECTION,2015,"Keywords: probabilistic amplitude demodulation ; speech emphasis Reference EPFL-REPORT-209098 Record created on 2015-06-19, modified on 2016-08-09"
1189097,21258,11470,Expert Talk for Time Machine Session: Dynamic Time Warping New Youth,2012,"This time machine expert talk describes the recent comeback of acoustic pattern matching algorithms, such as DTW. These are particularly suited for applications where little (or no) transcribed training data is available."
661440,21258,9804,Cross-Cultural Perception of Discourse Phenomena,2009,"We discuss perception studies of two low level indicators of discourse phenomena by Swedish. Japanese, and Chinese native speakers. Subjects were asked to identify upcoming prosodic boundaries and  ..."
516378,21258,9804,A Bayesian approach to non-intrusive quality assessment of speech,2009,A Bayesian approach to non-intrusive quality assessment of narrow-band speech is presented. The speech features used to assess quality are the sample mean and variance of band-powers evaluated from ...
2748529,21258,9804,Speech Technologies for African Languages: Example of a Multilingual Calculator for Education,2015,This paper presents our achievements after 18 months of the ALFFA project dealing with African languages technologies. We focus on a multilingual calculator (Android app) that will be demonstrated during the Show and Tell session.
10519,21258,9804,Robustness of prosodic features to voice imitation,2008,Comunicacio presentada a 9th Annual Conference of the International Speech Communication Association celebrada a Brisbane (Australia) del  22 al 26 de setembre de 2008.
372050,21258,9804,A front-end using the harmonicity cue for speech enhancement in loud noise,2000,"Keywords: speech Note: no IDIAPRR, see RESPITE www Reference EPFL-CONF-82650 Record created on 2006-03-10, modified on 2016-08-08"
2347039,21258,9804,The virtual guide: a direction giving embodied conversational agent,2007,"We present the Virtual Guide, an embodied conversational agent that can give directions in a 3D virtual environment. We discuss how dialogue management, language generation and the generation of appropriate gestures are carried out in our system."
30108,21258,9804,Reproducing laryngeal mechanisms with a two-mass model,2003,Evidence is produced for the correspondence between the oscillation regimes of an up-to-date two-mass model and laryngeal mechanisms. Features presented by experimental electroglottographic signals during transition between laryngeal mechanisms are shown to be reproduced by the model.
579355,21258,9804,Recognizing and Modelling Regional Varieties of Swedish,2008,"Our recent work within the research project SIMULEKT (Simulating Intonational Varieties of Swedish) includes two approaches. The first involves a pilot perception test, used for detecting tendencie ..."
2430266,21258,9804,Weighted Linear Prediction for Speech Analysis in Noisy Conditions,2009,"1 k p; where E = X n (s n p X k=1 a k s n k ) 2 I WLP is a generalization of LP, introducing temporal weighting of the squared prediction error [2]: E = X n (s n p X k=1 a k s n k ) 2 W n IW n is the weighting function I For constant W n , WLP becomes identical to LP! Stabilized Weighted Linear Prediction (SWLP)"
52732,21258,9804,Gaze patterns in turn-taking,2012,This paper investigates gaze patterns in turn-taking. We focus on differences between speaker changes resulting in silences and overlaps. We also investigate gaze patterns around backchannels and around silences not involving speaker changes.
229790,21258,9804,Speech recognition based on a text-to-speech synthesis system.,1987,A major problem in large-vocabulary speech recognition is the collection of reference data and speaker normalization. In this paper we propose the use of synthetic speech as a means of handling this problem. An experimental scheme for such a system will be described.
281461,21258,9804,The INTERSPEECH 2012 Speaker Trait Challenge,2012,"Keywords: Computational Paralinguistics ; Speaker Traits ; Personality ; Likability ; Pathology Reference EPFL-CONF-174360 Record created on 2012-01-23, modified on 2012-03-20"
2785017,21258,9804,I 2 r speech2singing perfects everyone's singing.,2014,This paper recieved Best Show & Tell Paper Award at Interspeech 2014 (The 15th Annual Conference of the International Speech Communication Association); Full paper can be downloaded from the Publisher's URL provided.
1569534,21258,21089,A general scheme for broad-coverage multimodal annotation,2009,We present in this paper a formal and computational scheme in the perspective of broad-coverage multimodal annotation. We propose in particular to introduce the notion of annotation hypergraphs in which primary and secondary data are represented by means of the same structure.
2845882,21258,9804,Automatic paragraph segmentation with lexical and prosodic features,2016,"Comunicacio presentada a la Interspeech 2016, celebrada per la International Speech Communication Association (ISCA) els dies 8 a 12 de septembre de 2016 a San Francisco (EUA)."
25915,21258,9804,SPeaker and Language Characterization (SpLC): A Special Interest Group (SIG) of ISCA,2001,"Last year, SpLC - an ISCA Special Interest Group (SIG) centered around Speaker and Language Characterization was born. The aims of this paper are to present the SpLC SIG, its objectives, and the work done during the first year."
2516352,21258,9804,The non-native consonant challenge for European languages,2008,"This paper reports on a multilingual investigation into the effects of different masker types on native and non-native perception in a VCV consonant recognition task. Native listeners outperformed 7 other language groups, but all groups showed a similar r"
3133596,21258,9804,A real-time framework for visual feedback of articulatory data using statistical shape models,2016,"We present a novel open-source framework for visualizing electromagnetic articulography (EMA) data in real-time, with a modular framework and anatomically accurate tongue and palate models derived by multilinear subspace learning."
289058,21258,9804,On Speaker-Independent Personality Perception and Prediction from Speech,2012,"Keywords: extra-linguistic speech properties ; personality modeling from speech ; speaker characteristics Reference EPFL-CONF-178309 Record created on 2012-06-19, modified on 2016-08-09"
322989,21258,9804,A new approach for multi-band speech recognition based on probabilistic graphical models,2000,"In this paper, we introduce a new approach for multi-band speech recognition which allows interaction between sub-bands and does not require a recombination step. Moreover, this approach is a natural generalization of the HMMs paradigm and leads to fast learning and recognition algorithms."
2476453,21258,9804,"Prosody, emotions, and… ‘whatever’",2007,"Weexaminetheroleofprosodyincueingascaleofnegative� meaningsassociatedwiththeuseof� whatever.�Theanalysisof� acorpusofelicitedexamplesshowsthatthemorenegative� thetoken,�themorelikelyitistohaveanadditionalpitch� accent,�extendedduration,�andexpandedpitchrangeonthe� firstsyllable.�Thesefindingsareanalyzedasalinkbetween� pragmaticmeaningandthestrengthoftheprosodicboundary� betweenthefirsttwosyllables�( what#ever).�Theresultsof� perceptionexperimentsshowthattheprosodyofwhatever� itselfisasystematiccueforthedegreeofnegative� connotationassociatedwiththeutteranceinwhich� whatever� occurs.� Potentialapplicationsofthisresultforspoken� dialoguesystemsandsynthesisofemotionalspeechare� discussed.�"
437493,21258,9804,"Comparing word, character, and phoneme n-grams for subjective utterance recognition",2008,"In this paper, we compare the performance of classifiers trained using word n-grams, character n-grams, and phoneme n-grams for recognizing subjective utterances in multiparty conversation. We show that there is value in using very shallow linguistic representations, such as character n-grams, for recognizing subjective utterances, in particular, gains in the recall of subjective utterances. Copyright © 2008 ISCA."
1832146,21258,9804,"Influences on tone in Sepedi, a Southern Bantu language",2008,"Tone in Bantu languages is rarely studied experimentally. This paper reports a production study which reveals the intricate interaction of tonal context and morphological structure in surface tone realization in Sepedi, a South African Bantu language. Index Terms: tone, morphological structure, Bantu language"
538720,21258,9804,Phoneme Background Model for Information Bottleneck based Speaker Diarization,2014,"Reference EPFL-CONF-203864 Related documents: http://publications.idiap.ch/index.php/publications/showcite/Yella_INTERSPEECH_2014 Record created on 2014-12-19, modified on 2016-08-09"
2099749,21258,21102,A proposed fuzzy pattern verification system,2001,Proposes a fuzzy pattern verification system based on fuzzy if-then rules and fuzzy membership functions in fuzzy clustering. The proposed fuzzy system is applied to speaker verification and is compared with current probabilistic verification systems. Experiments performed on the ANDOSL and YOHO speech corpora show better results for the proposed system.
2629146,21258,23735,Analysis and semantic modeling of modality preferences in industrial human-robot interaction,2015,"Intuitive programming of industrial robots is especially important for small and medium-sized enterprises. We evaluated four different input modalities (touch, gesture, speech, 3D tracking device) regarding their preference, usability, and intuitiveness for robot programming."
2078376,21258,9804,VOCALOID - Commercial singing synthesizer based on sample concatenation,2007,"The song submitted here to the “Synthesis of Singing Challenge” is synthesized by the latest version of the singing synthesizer “Vocaloid”, which is commercially available now. In this paper, we would like to present the overview of Vocaloid, its product lineups, description of each component, and the synthesis technique used in Vocaloid. Index Terms: singing synthesis"
343354,21258,9804,Production and Perception of Pauses and their Linguistic Context in Read and Spontaneous Speech in Swedish.,2002,We investigate the relationship between prosodic phrase boundaries in terms of pausing and the linguistic structure on morpho-syntactic and discourse levels inspontaneous dialogues as well as in read aloud speech in Swedish. Both the speakers' production and the listeners' perception of pausing are considered and mapped to the linguistic structure.
376148,21258,235,A maximum entropy approach for spoken Chinese understanding,2003,"In this paper, we present a spoken language understanding method based on the maximum entropy model. We first extract certain features from the corpus, and then train the maximum entropy model with an annotated corpus. We use this model to analyze spoken Chinese into semantic frames. Experiments show that the model can work effectively."
2640160,21258,9677,Evaluation of the Scusi? Spoken Language Interpretation System -- A Case Study,2013,"We present a performance evaluation framework for Spoken Language Understanding (SLU) modules, focusing on three elements: (1) characterization of spoken utterances, (2) experimental design, and (3) quantitative evaluation metrics. We then describe the application of our framework to Scusi?— our SLU system that focuses on referring expressions."
584603,21258,9804,Integrating Online I-vector extractor with Information Bottleneck based Speaker Diarization system,2015,"Reference EPFL-CONF-209082 Related documents: http://publications.idiap.ch/index.php/publications/showcite/Madikeri_Idiap-RR-20-2015 Record created on 2015-06-19, modified on 2016-08-09"
593884,21258,9804,Analysis of CNN-based Speech Recognition System using Raw Speech as Input,2015,"Reference EPFL-CONF-210029 Related documents: http://publications.idiap.ch/index.php/publications/showcite/Palaz_Idiap-RR-23-2015 Record created on 2015-07-19, modified on 2016-08-09"
227107,21258,9804,A data-driven approach to understanding spoken route directions in human-robot dialogue,2012,"In this paper, we present a data-driven chunking parser for automatic interpretation of spoken route directions into a route graph that is useful for robot navigation. Different sets of features and machine learning algorithms are explored. The results indicate that our approach is robust to speech recognition errors."
303388,21258,9804,A Timbre Space for Speech,2005,"We describe a perceptual space for timbre, dene an objective metric that takes into account perceptual orthogonality and measure the quality of timbre interpolation. We discuss two timbre representations and measure perceptual judgments. We determine that a timbre space based on Mel-frequency cepstral coefcients (MFCC) is a good model for perceptual timbre space."
136750,21258,9804,How similar are clusters resulting from schwa deletion in French to identical underlying clusters,2009,"Clusters resulting from the deletion of schwa in French are compared with identical underlying clusters in words and pseudowords. Both manual and automatic acoustical comparisons suggest that clusters resulting from schwa deletion in French are highly similar to identical underlying clusters. Furthermore, cluster duration is not longer for clusters resulting from schwa deletion than for identical underlying clusters. Clusters in pseudowords show a different acoustical and durational pattern from the two other clusters in words."
443038,21258,9804,AUTOMATIC DETECTION OF AUDIO ADVERTISEMENTS,2010,"A method, apparatus, and computer-readable medium for editing a data stream based on a corpus are provided. The data stream includes stream words. A sequence includes a predetermined number of sequential words of the stream words. The method, apparatus, and computer-readable medium determine whether the sequence exists in the corpus at least at a predetermined minimum frequency. When the sequence exists in the corpus at least at the predetermined minimum frequency, the sequence is edited in the data stream."
2202479,21258,9804,OASIS Natural Language Call Steering Trial,2001,"A recent trial of natural language call steering on live UK calls to the operator is described along with its results. The characteristics of the problem are described along with the acoustic, language, semantic and dialogue modelling approaches employed. Natural language call steering is found to be viable, with recognition and semantic accuracy the current limiting factors."
2682341,21258,9804,Adding a Speech Cursor to a Multimodal Dialogue System,2011,"This paper describes Dico II+, an in-vehicle dialogue system demonstrating a novel combination of flexible multimodal menu-based dialogue and a “speech cursor” which enables menu navigation as well as browsing long list using haptic input and spoken output. Index Terms: dialogue systems, multimodality"
560495,21258,9804,Perceived boundary strength,2002,"In this paper, results from a perception experiment on perceived prosodic boundary strength in spontaneous speech are presented. The analysis of the listeners’ responses reveals a good agreement in rating the strength of prosodic phrase boundaries on a visual analogue scale (VAS), and a strong correlation between perceived boundary strength and pause duration."
379560,21258,9804,Test of several external posterior weighting functions for multiband Full Combination ASR,2000,"Keywords: speech Note: published in ICSLP 2000 Reference EPFL-REPORT-82646 URL: http://publications.idiap.ch/downloads/reports/2000/rr00-27.pdf Record created on 2006-03-10, modified on 2016-08-08"
2324694,21258,9804,Robust Parsing of Utterances in Negotiative Dialogue,2003,"This paper presents an algorithm for domain-dependent parsing of utterances in negotiative dialogue. To represent such utterances, the algorithm outputs semantic expressions that are more expressive than propositional slot-filler structures. It is very fast and robust, yet precise and capable of correctly combining information from different utterance fragments."
38040,21258,9804,Bayesian methods for HMM speech recognition with limited training data,2001,"This paper presents a Bayesian approach to learning for HMMs in speech recognition. The implementation of Bayesian learning for HMMs in speech recognition is discussed, including the requirement of maintaining the original HMM constraints, choice of prior and utterance recognition. This work shows that the Bayesian learning approach can be successfully applied to complex models when the amount of training data is small."
58582,21258,9804,Using Linear Interpolation to Improve Histogram Equalization for Speech Recognition,2004,This paper presents a novel approach to speech data normalization by introducing interpolation for histogram equalization. We study different ways of histogram interpolation that inhence this normalization technique. We found that using a special weighting factor to combine current and past test sentence statistics improved speech recognition performance.
544887,21258,9804,Multimodality and speech technology: Verbal and non-verbal communication in talking agents,2003,"This paper presents methods for the acquisition and modelling of verbal and non-verbal communicative signals for the use in animated talking agents. This work diverges from the traditional focus on the acoustics of speech in speech technology and will be of importance for the realization of future multimodal interfaces, some experimental examples of which are presented at the end of the paper."
362296,21258,9804,Classifying emotions in speech: a comparison of methods,2001,"A number of recent studies have attempted classification of emotional speech using various methods. In this paper we compare the performance of two algorithms: a classification algorithm based on euclidean distances, and a classification algorithm based on the use of neural networks. Both perform the classification using an identical feature set, on a database of emotional speech which has been validated through subjective listening tests."
728931,21258,9463,Learning Pronunciation Dictionaries: Language Complexity and Word Selection Strategies,2006,The speed with which pronunciation dictionaries can be bootstrapped depends on the efficiency of learning algorithms and on the ordering of words presented to the user. This paper presents an active-learning word selection strategy that is mindful of human limitations. Learning rates approach that of an oracle system that knows the final LTS rule set.
136826,21258,9804,Pseudo-Articulatory Representations and the Recognition of Syllable Patterns in Speech.,2001,This paper presents an account of syllable structure as the basis for organizing articulatory activity. This contrasts with the serial organization of more conventional phonetic segments. It is demonstrated that working with syllables in this way can provide the basis for linguistically motivated speech recognition using the previously reported notion of the Pseudo-Articulatory Representation (PAR).
2050101,21258,9804,Broad Focus Across Sentence Types in Greek,2003,"In Greek main sentence stress is located on the rightmost constituent in ‘all new’ declaratives, but for all-new negatives, polar questions, and wh-questions it is located on the negative particle, main verb, and whword respectively. I discuss the implications of this pattern for the focus projection rules and for the accentedness of discourse new constituents."
6746,21258,9804,Development of a stochastic dialog manager driven by semantics,2003,"We present an approach for the development of a dialog manager based on stochastic models for the representation of the dialogue structure and strategy. This dialog manager processes semantic representations and, when it is integrated with our understanding and answer generation modules, it performs natural language dialogs. It has been applied to a Spanish dialogue system which answers telephone queries about train timetables."
159657,21258,9804,Estimation of Glottal Area Function Using Stereo-endoscopic High-Speed Digital Imaging,2010,"In this paper, a novel stereo-endoscopic high-speed digital imaging system and a method to estimate the glottal area function are proposed. Glottal length, width, and area of one female participant were estimated in three different fundamental frequencies (F0s). Index Terms: glottal area function, stereoscopy, high-speed imaging"
100020,21258,9804,NIST 2003 language recognition evaluation.,2003,"This paper discusses NIST’s 2007 evaluation of language recognition. Some history of earlier NIST language evaluations is covered, and the test procedures and protocols, evaluation data used, and planned measures of performance for the 2007 evaluation are described. The participants and submissions of the 2007 evaluation are described, and preliminary information is included on the evaluation performance results after brief initial analysis."
1913625,21258,21089,Dialog Navigator : A Spoken Dialog Q-A System based on Large Text Knowledge Base,2003,"This paper describes a spoken dialog Q-A system as a substitution for call centers. The system is capable of making dialogs for both fixing speech recognition errors and for clarifying vague questions, based on only large text knowledge base. We introduce two measures to make dialogs for fixing recognition errors. An experimental evaluation shows the advantages of these measures."
2284736,21258,21089,INPRO_iSS: A Component for Just-In-Time Incremental Speech Synthesis,2012,"We present a component for incremental speech synthesis (iSS) and a set of applications that demonstrate its capabilities. This component can be used to increase the responsivity and naturalness of spoken interactive systems. While iSS can show its full strength in systems that generate output incrementally, we also discuss how even otherwise unchanged systems may profit from its capabilities."
2480641,21258,9804,Comparing word and syllable prominence rated by naïve listeners,2011,Prominence has been widely studied on the word level and the syllable level. An extensive study comparing the two approaches is missing in the literature. This study investigates how word and syllable prominence relate to each other in German. We find that perceptual ratings based on the word level are more extreme than those based on the syllable level. The correlations between word prominence and acoustic features are greater than the correlations between syllable prominence and acoustic features.
21728,21258,9804,Prosodic Cues to Disengagement and Uncertainty in Physics Tutorial Dialogues,2012,"This paper focuses on the analysis and prediction of student disengagement and uncertainty, using a corpus of dialogues collected with a spoken tutorial dialogue system in the STEM domain of qualitative physics. We first compare and contrast the prosodic characteristics of dialogue turns exhibiting disengagement or not, and those exhibiting uncertainty or not. We then compare the utility of using multiple prosodic features to predict both disengagement and uncertainty."
50346,21258,9804,Topic styles in IR and TDT: effect on system behavior.,2001,"The TREC Spoken Document Retrieval Track (SDR) and the Topic Detection and Tracking (TDT) project have annotated the same corpus with difference styles of relevance judgements, using differenct notions of topic. We compare the behavior of a topic tracking system using relevance judgements from TDT with that of the same system using relevance from the SDR in order to investigate the influence of differences document relevance judgements on the behavior of the tracking system."
390063,21258,9804,SpeechDat-E: Five Eastern European Speech Databases for Voice-Operated Teleservices Completed,2001,"In the Speechdat-E project five medium large telephone speech databases have been collected for Czech, Hungarian, Polish, Russian, and Slovak. The project was recently concluded. This paper reports briefly on the contents of the databases, elaborates on experiences gained from the data recordings and from the validation of the databases. The availability of the databases to the public is addressed, too."
416384,21258,9804,Connected speech processes in Warlpiri,2008,"Connected speech processes (CSP) in Warlpiri, an indigenous language of central Australia, taken from two fluent 'dreaming' monologues are analyzed with the aim of observing the influence of language particular phonological constraints and prosody upon phonetic processes of lenition, co-articulation and selective segmental enhancement."
1691298,21258,9463,Exploring Affect-Context Dependencies for Adaptive System Development,2007,"We use X2 to investigate the context dependency of student affect in our computer tutoring dialogues, targeting uncertainty in student answers in 3 automatically monitorable contexts. Our results show significant dependencies between uncertain answers and specific contexts. Identification and analysis of these dependencies is our first step in developing an adaptive version of our dialogue system."
2318143,21258,21089,A Web-based Evaluation Framework for Spatial Instruction-Giving Systems,2012,"We demonstrate a web-based environment for development and testing of different pedestrian route instruction-giving systems. The environment contains a City Model, a TTS interface, a game-world, and a user GUI including a simulated street-view. We describe the environment and components, the metrics that can be used for the evaluation of pedestrian route instruction-giving systems, and the shared challenge which is being organised using this environment."
2694768,21258,9804,Automatic Segmentation of Spanish Speech Into Syllables,1989,"This paper presents an algorithm that provides a syllabic segmentation of speech following the syllabification rules of Spanish language. The implemented algorithm is divided into two parts. First, an initial segmentation is made based on energy contour, sonority and duration. Second, a fine adjustement of syllable boundaries and final segmentation is made by applying syllabic rules."
2134065,21258,21089,Is word-to-phone mapping better than phone-phone mapping for handling English words?,2013,"In this paper, we relook at the problem of pronunciation of English words using native phone set. Specifically, we investigate methods of pronouncing English words using Telugu phoneset in the context of Telugu Text-to-Speech. We compare phone-phone substitution and wordphone mapping for pronunciation of English words using Telugu phones. We are not considering other than native language phoneset in all our experiments. This differentiates our approach from other works in polyglot speech synthesis."
706517,21258,8840,Exploiting Discourse Structure for Spoken Dialogue Performance Analysis,2006,"In this paper we study the utility of discourse structure for spoken dialogue performance modeling. We experiment with various ways of exploiting the discourse structure: in isolation, as context information for other factors (correctness and certainty) and through trajectories in the discourse structure hierarchy. Our correlation and PARADISE results show that, while the discourse structure is not useful in isolation, using the discourse structure as context information for other factors or via trajectories produces highly predictive parameters for performance analysis."
180371,21258,9804,Unit Size in Unit Selection Speech Synthesis,2003,"In this paper, we address the issue of choice of unit size in unit selection speech synthesis. We discuss the development of a Hindi speech synthesizer and our experiments with different choices of units: syllable, diphone, phone and half phone. Perceptual tests conducted to evaluate the quality of the synthesizers with different unit size indicate that the syllable synthesizer performs better than the phone, diphone and half phone synthesizers, and the half phone synthesizer performs better than diphone and phone synthesizers."
25633,21258,9804,Distinctive features for use in an automatic speech recognition system.,2001,"In this paper we develop a method of representing the speech waveform in terms of a set of abstract, linguistic distinctions in order to derive a set of discriminative features for use in a speech recognizer. By combining the distinctive feature representation with our original waveform representation we are able to achieve a reduction in word error rate of 33% on an automatic speech recognition task."
2187098,21258,9804,Intensive Gestures in French and their Multimodal Correlates,2007,"This paper relates a pilot study on intensive gestures in French – e.g. gestures which accompany speech and participate in the highlighting of some discourse elements which the paper means to determine. The study is based on spontaneous French informal conversation and the intensive gestures correlates we looked at pertained to the morphological, prosodic and gestural dimensions. Index Terms: Reinforcement, intensive gestures, prosody, morphology, hand gestures"
2234297,21258,9804,Named Entity Extraction from Word Lattices,2003,"We present a method for named entity extraction from word lattices produced by a speech recogniser. Previous work by others on named entity extraction from speech has used either a manual transcript or 1-best recogniser output. We describe how a single Viterbi search can recover both the named entity sequence and the corresponding word sequence from a word lattice, and further that it is possible to trade off an increase in word error rate for improved named entity extraction."
2166375,21258,21089,"Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System",2010,"In-vehicle dialogue systems often contain more than one application, e.g. a navigation and a telephone application. This means that the user might, for example, interrupt the interaction with the telephone application to ask for directions from the navigation application, and then resume the dialogue with the telephone application. In this paper we present an analysis of interruption and resumption behaviour in human-human in-vehicle dialogues and also propose some implications for resumption strategies in an in-vehicle dialogue system."
345681,21258,9804,An EPG therapy protocol for remediation and assessment of articulation disorders.,2002,"This paper describes technical and methodological advances in#R##N#the development of a procedure for measuring changes in#R##N#accuracy and stability of linguapalatal (tongue-palate) contact#R##N#patterns during a course of visual feedback therapy using#R##N#electropalatography (EPG). The procedure is exemplified by a#R##N#case in which therapy was aimed at resolving a pattern of velar#R##N#fronting whereby phonetic targets /k, g,"
87983,21258,9804,Measuring Unsupervised Acoustic Clustering through Phoneme Pair Merge-and-Split Tests,2005,"Subphonetic discovery through segmental clustering is a central step in building a corpus-based synthesizer. To help decide what clustering algorithm to use we employed mergeand-split tests on English fricatives. Compared to reference of 2%, Gaussian EM achieved a misclassification rate of 6%, Kmeans 10%, while predictive CART trees performed poorly."
284277,21258,9804,Design strategies for a virtual language tutor,2004,In this paper we discuss work in progress on an interactive talking agent as a virtual language tutor in CALL applications. The ambition is to create a tutor that can be engaged in many aspects of language learning from detailed pronunciation to conversational training. Some of the crucial components of such a system is described. An initial implementation of a stress/quantity training scheme will be presented.
480655,21258,9804,Using Information State to Improve Dialogue Move Identification in a Spoken Dialogue System,2007,"In this paper we investigate how to improve the performance of a dialogue move and parameter tagger for a taskoriented dialogue system using the information-state approach. We use a corpus of utterances and information states from an implemented system to train and evaluate a tagger, and then evaluate the tagger in an on-line system. Use of information state context is shown to improve performance of the system. Index Terms: spoken dialogue systems, dialogue management, tagging"
1154075,21258,535,The IBM keyword search system for the DARPA RATS program,2013,"The paper describes a state-of-the-art keyword search (KWS) system in which significant improvements are obtained by using Convolutional Neural Network acoustic models, a two-step speech segmentation approach and a simplified ASR architecture optimized for KWS. The system described in this paper had the best performance in the 2013 DARPA RATS evaluation for both Levantine and Farsi."
104921,21258,9804,ATREUS: a speech recognition front-end for a speech translation system.,1993,"This paper describes the continuous speech recognition subsystem ATREUS which is used as the speech input stage in the experimental speech translation system ASURA. The speech recognition algorithm is SSS-LR/VFS which consists of context-dependent phone models (HMnet), a generalized LR parser, and vector field smoothing for speaker / environment adaptation."
874055,21258,9099,Audio Puzzler: piecing together time-stamped speech transcripts with a puzzle game,2008,We have developed an audio-based casual puzzle game which produces a time-stamped transcription of spoken audio as a by-product of play. Our evaluation of the game indicates that it is both fun and challenging. The transcripts generated using the game are more accurate than those produced using a standard automatic transcription system and the time-stamps of words are within several hundred milliseconds of ground truth.
561426,21258,9804,Spirantization of /p t k/ in Sienese Italian and so-called semi-fricatives,2005,"This paper presents the results of a first acoustic phonetic investigation into voiceless spirantization in the variety of Tuscan Italian spoken in Siena. Based on spontaneous speech data (6 speakers), we focus upon occurrences of a phonetic variant, previously referred to as a ‘semi-fricative’. Intermediate between a voiceless stop and a voiceless fricative, it is reported to occur in Pisan (e.g. [1], [2]) but not Florentine Italian (e.g. [3])."
2619588,21258,21089,Movie-DiC: a Movie Dialogue Corpus for Research and Development,2012,"This paper describes Movie-DiC a Movie Dialogue Corpus recently collected for research and development purposes. The collected dataset comprises 132,229 dialogues containing a total of 764,146 turns that have been extracted from 753 movies. Details on how the data collection has been created and how it is structured are provided along with its main statistics and characteristics."
112002,21258,9804,Improved Word Confidence Estimation using Long Range Features,2001,"This paper describes experiments in improving word confidence estimation using document- and task-level features of the hypothesized word sequence from a recognizer. The improved confidence estimates are shown to improve information extraction performance, specifically named entity (NE) recognition. The detected names can then be used to further improve confidence estimation in a multi-pass NE recognition framework."
545928,21258,9804,Phase-based harmonic/percussive separation.,2014,"In this paper, a method for separation of harmonic and percussive elements in music recordings is presented. The proposed method is based on a simple spectral peak detection step followed by a phase expectation analysis that discriminates between harmonic and percussive components. The proposed method was tested on a database of 10 audio tracks and has shown superior results to the reference state-of-the-art approach."
115859,21258,9804,High-quality memoryless subband coding of impulse responses at 22 bits per frame.,2005,A method for efficient quantization of speech spectral models is proposed. The impulse response of the model is coded in two spectral subbands of unequal width. The distortion measure used is a squared error between impulse responses. The quality obtained at 22 bits per frame is as good as that of Split Vector Quantization of Line Spectral Frequencies at 24 bits per frame.
891293,21258,9463,Extending Pronunciation Lexicons via Non-phonemic Respellings,2009,This paper describes work in progress towards using non-phonemic respellings as an additional source of information besides spelling in the process of extending pronunciation lexicons for speech recognition and text-to-speech systems. Preliminary experimental data indicates that the approach is likely to be successful. The major benefit of the approach is that it makes extending pronunciation lexicons accessible to average users.
113438,21258,9463,An MDL-based approach to extracting subword units for grapheme-to-phoneme conversion,2010,"We address a key problem in grapheme-to-phoneme conversion: the ambiguity in mapping grapheme units to phonemes. Rather than using single letters and phonemes as units, we propose learning chunks, or subwords, to reduce ambiguity. This can be interpreted as learning a lexicon of subwords that has minimum description length. We implement an algorithm to build such a lexicon, as well as a simple decoder that uses these subwords."
1359,21258,9804,The schwa in Albanian,2001,"In Albanian, the schwa as a phoneme is restricted to the Tosk variety, whereas it is described as a back, rounded vowel in the Gheg variety. The first two formants of schwa-vowels of both Tosk and Gheg speakers (within and outside the Republic of Albania) have been investigated. No differences could be found within the borders of the Republic of Albania, whereas speakers outside the borders displayed significant differences."
62464,21258,9804,Improved Warping-Invariant Features for Automatic Speech Recognition,2006,"In this paper, we extend a previously introduced method for the generation of vocal tract length invariant (VTLI) features. The novelty is a reduction of the number of obtained invariances to the more desired ones, which results in a significant improvement of recognition rates. In experiments on the TIMIT database, the enhanced discrimination capabilities and robustness to mismatches between training and test conditions are shown."
2273232,21258,9804,Speaker verification systems and security considerations,2003,"In speaker verification technology, the security considerations are quite dierent from performance measures that are usually studied. The security level of a system is generally expressed in the amount of eort it takes to have a successful break-in attempt. This paper discusses potential weaknesses of speaker verification systems and methods of exploiting these weaknesses, and suggests proper experiments for determining the security level of a speaker verification system."
24353,21258,9804,New MAP Estimators for Speaker Recognition,2003,"We report the results of some experiments which demonstrate that eigenvoice MAP and eigenphone MAP are at least as effective as classical MAP for discriminative speaker modeling on SWITCHBOARD data. We show how eigenvoice MAP can be modified to yield a new model-based channel compensation technique which we call eigenchannel MAP. When compared with multi-channel training, eigenchannel MAP was found to reduce speaker identification errors by 50%."
151315,21258,9804,Discarding Impossible Events from Statistical Language Models,2000,"This paper describes a method for detectingvimpossible bigrams using a vocabulary of V elements. The idea is to discard all the ungrammatical events to expect an improvement of the language model. We extract the impossible bigrams by using automatic rules. The biclass associations which are ungrammatical are detected and all the corresponding bigrams are analyzed and set as possible or impossible events, we also decided to manage for each of the retrieved rule an exception list."
1908374,21258,9804,Lexical Stress in Continuous Speech Recognition,2006,"Human listeners use lexical stress for word segmentation and disambiguation. We look into using lexical stress for largevocabulary speech recognition for the Dutch language. It appears that beside vowels, consonants should be taken into account. By introducing stressed phonemes, and features for spectral bands and the fundamental frequency, we reduce the word error rate by 2.6 %. Index Terms: speech recognition, lexical stress, Dutch."
259729,21258,9804,Prediction of Fujisaki model’s phrase commands,2003,This paper presents a model to predict the phrase commands of the Fujisaki Model for F0 contour for the Portuguese Language. Phrase commands location in text is governed by a set of weighted rules. The amplitude (Ap) and timing (T0) of the phrase commands are predicted in separate neural networks. The features for both neural networks are discussed. Finally a comparison between target and predicted values is presented.
866437,21258,9463,A Conversational In-Car Dialog System,2007,"In this demonstration we present a conversational dialog system for automobile drivers. The system provides a voice-based interface to playing music, finding restaurants, and navigating while driving. The design of the system as well as the new technologies developed will be presented. Our evaluation showed that the system is promising, achieving high task completion rate and good user satisfation."
189542,21258,9804,Mixed-lingual text analysis for polyglot TTS synthesis.,2003,"Text-to-speech (TTS) synthesis is more and more confronted with the language mixing phenomenon. An important step towards the solution of this problem and thus towards a socalled polyglot TTS system is an analysis component for mixedlingual texts. In this paper it is shown how such an analyzer can be realized for a set of languages, starting from a corresponding set of monolingual analyzers which are based on DCGs and chart parsing."
134447,21258,9804,Results from a Survey of Attendees at ASRU 1997 and 2003,2005,"In 1997 the author conducted a survey at the IEEE workshop on ‘Automatic Speech Recognition and Understanding’ (ASRU) in which attendees were offered a set of twelve putative future events to which they were asked to assign a date. Six years later at ASRU’2003, the author repeated the survey with the addition of eight additional items. This paper presents the combined results from both surveys."
2292290,21258,9804,Frequency-related representation of speech.,2003,"Cepstral features derived from power spectrum are widely used for automatic speech recognition. Very little work, if any,hasbeendoneinspeechresearchtoexplorephase-based representations. In this paper, an attempt is made to investigate the use of phase function in the analytic signal of critical-band filtered speech for deriving a representation of frequencies present in the speech signal. Results are presented which show the validity of this approach."
32876,21258,9804,Beyond the Conventional Statistical Language Models: The Variable-Length Sequences Approach,2000,"In natural language, several sequences of words are very frequent. A classical language model, like n-gram, does not adequately take into account such sequences, because it underestimates their probabilities. In this paper, we present an original method for automatically determining the most important phrases in corpora. The use of word sequences proposed by our algorithm reduces perplexity by more than 16% compared to those which are limited to single words."
62870,21258,9804,Predictive neural networks applied to phoneme recognition,1997,"In this paper a phoneme recognition system based on predictive neural networks is proposed. Neural networks are used to predict observation vectors of speech frames. The obtained prediction error is used for phoneme recognition as 1) distortion measure on the frame level and 2) as feature, which is statistically modeled by the Rayleigh distribution. Continuous speech phoneme recognition experiments are performed different settings of the system are evaluated."
463509,21258,9804,Robust speech interaction in a mobile environment through the use of multiple and different media input types,2003,"Mobile and outdoor environments have long been out of#R##N#reach for speech engines due to the performance limitations#R##N#that were associated with portable devices, and the#R##N#difficulties of processing speech in high-noise areas. This#R##N#paper outlines an architecture for attaining robust speech#R##N#recognition rates in a mobile pedestrian indoor/outdoor#R##N#navigation environment, through the use of a media fusion#R##N#knowledge component."
333036,21258,9804,Spoken Language Output: Realising the Vision,2003,"Significant progress has taken place in ‘Spoken Language Output’ (SLO) R&D, yet there is still some way to go before it becomes a ubiquitous and widely deployed technology. This paper reviews the challenges facing SLO, using ‘Technology Roadmapping’ (TRM) to identify market drivers and future product concepts. It concludes with a summary of the behaviours that will be required in future SLO systems."
453131,21258,9804,"A MAP approach, with synchronous decoding and unit-based normalization for text-dependent speaker verification",2000,"Keywords: speech Note: IDIAP-RR 00-48 Reference EPFL-CONF-82639 URL: http://publications.idiap.ch/downloads/reports/2000/rr00-48.pdf Related documents: http://publications.idiap.ch/index.php/publications/showcite/marietho:2000:rr00-48 Record created on 2006-03-10, modified on 2016-08-08"
527976,21258,9804,An Open-source State-of-the-art Toolbox for Broadcast News Diarization,2013,"This paper presents the LIUM open-source speaker diarization toolbox, mostly dedicated to broadcast news. This tool includes both Hierarchical Agglomerative Clustering using well-known measures such as BIC and CLR, and the new ILP clustering algorithm using i-vectors. Diarization systems are tested on the French evaluation data from ESTER, ETAPE and REPERE campaigns."
431336,21258,9804,The Basque Speech_Dat (II) Database: A Description and First Test Recognition Results,2003,"In this work we present a telephone speech database for Basque, compliant with the guidelines of the Speechdat project. The database contains 1060 calls from the fixed telephone network. We first describe the main aspects of the database design. We also present the recognition results using the database and a set of procedures following the language independent reference recogniser commonly named Refrec."
103890,21258,9804,Let's Go Public! Taking a Spoken Dialog System to the Real World,2005,"In this paper, we describe how a research spoken dialog system was made available to the general public. The Let’s Go Public spoken dialog system provides bus schedule information to the Pittsburgh population during off-peak times. This paper describes the changes necessary to make the system usable for the general public and presents analysis of the calls and strategies we have used to ensure high performance."
321628,21258,9804,Towards a universal speech interface.,2000,"Abstract : We discuss our ongoing attempt to design and evaluate universal human-machine speech-based interfaces. We describe one such initial design suitable for database retrieval applications, and discuss its implementation in a movie information application prototype. Initial user studies provided encouraging results regarding the usability of the design, as well as suggest some questions for further investigation."
2605284,21258,9804,The INTERSPEECH 2014 Computational Paralinguistics Challenge: Cognitive & Physical Load,2014,"The INTERSPEECH 2014 Computational Paralinguistics Challenge provides for the first time a unified test-bed for the automatic recognition of speakers’ cognitive and physical load in speech. In this paper, we describe these two Sub-Challenges, their conditions, baseline results and experimental procedures, as well as the COMPARE baseline features generated with the openSMILE toolkit and provided to the participants in the Challenge."
441271,21258,9804,Speech Interaction with an Emotional Robotic Dog,2008,"The consumer electronics of the future will be aware of the emotion of their users in order to provide more natural, engaging, entertaining and productive experiences. The paper reports on the development and evaluation of an emotionally responsive robotic dog. The dog is able to recognise emotion in its user through acoustic emotion recognition, and respond through a series of actions. In particular this paper details the technology implementation of the acoustic emotion recognition and the integration into the robotic dog."
16361,21258,9804,A robust variational method for the acoustic-to-articulatory problem,2009,"This paper presents a novel acoustic-to-articulatory inversion method based on an articulatory synthesizer and variational calculus, without the need for an initial trajectory. Validation in ideal conditions is performed to show the potential of the method, and the performances are compared to codebook based methods. We also investigate the precision of the articulatory trajectories found for various acoustic vectors dimensions. Possible extensions are discussed."
173602,21258,9804,Classification of Transition sounds with application to Automatic Speech Recognition,2001,"This paper addresses the problem of classification of speech transition sounds. A number of non parametric classifiers are compared, and it is shown that some non-parametric classifiers have considerable advantages over traditional hidden Markov models. Among the non-parametric classifiers, support vector machines were found the most suitable and the easiest to tune. Some of the reasons for the superiority of non-parametric classifiers will be discussed. The algorithm was tested on the voiced stop consonant phones extracted from the TIMIT corpus and resulted in very low error rates."
109346,21258,9463,A Hybrid Morphologically Decomposed Factored Language Models for Arabic LVCSR,2010,"In this work, we try a hybrid methodology for language modeling where both morphological decomposition and factored language modeling (FLM) are exploited to deal with the complex morphology of Arabic language. At the end, we are able to obtain from 3.5% to 7.0% relative reduction in word error rate (WER) with respect to a traditional full-words system, and from 1.0% to 2.0% relative WER reduction with respect to a non-factored decomposed system."
187252,21258,9804,Effects of tempo in radio commercials on young and elderly listeners,2009,"The aim of the present study is to investigate the effects of tempo manipulations in radio commercials, on listeners’ evaluation, cognition and persuasion. Questionnaire scores from 131 young and 130 elderly listeners show effects of tempo manipulation on listeners’ subjective evaluation, but not on their cognitive scores. Tempo effects on persuasion scores are modulated by the listeners’ general disposition towards radio and radio commercials. In sum, it seems that not age but listeners’ general disposition is of importance in evaluating tempo manipulation of radio commercials."
1911306,21258,9804,SPICE: Web-based Tools for Rapid Language Adaptation in Speech Processing Systems,2007,"In this paper we describe the design and implementation of a user interface for SPICE, a web-based toolkit for rapid prototyping of speech and language processing components. We report on the challenges and experiences gathered from testing these tools in an advanced graduate hands-on course, in which we created speech recognition, speech synthesis, and smalldomain translation components for 10 different languages within only 6 weeks."
696711,21258,9804,An audio-visual corpus for multimodal speech recognition in Dutch language,2002,This paper describes the gathering and availability of an audio-visual speech corpus for Dutch language. The corpus was prepared with the multi-modal speech recognition in mind and it is currently used in our research on lip-reading and bimodal speech recognition. It contains the prompts used also in the well-established POLYPHONE corpus and therefore captures the Dutch language characteristics with a reasonable accuracy.
2422690,21258,65,Social Attitude Towards A Conversational Character,2006,"This paper describes our experience with the design, implementation and validation of a user model for adapting health promotion dialogs with ECAs to the attitude of users toward the agent. The model was conceived in agreement with the theory of social emotions in communication. It integrates a linguistic parser with a dynamic Bayesian network and was learnt from a corpus of data collected with a Wizard of Oz study."
420949,21258,9804,Building VoiceXML-Based Applications,2002,"Abstract : The Language Technologies Institute (LTI) at Carnegie Mellon University has, for the past several years, conducted a lab course in building spoken-language dialog systems. In the most recent versions of the course, we have used (commercial) web-based development environments to build systems. This paper describes our experiences and discusses the characteristics of applications that are developed within this framework."
5087,21258,9804,Likelihood Ratio Test with Complex Laplacian Model for Voice Activity Detection,2003,"This paper proposes a voice activity detector (VAD) based on the complex Laplacian model. With the use of a goodness-of-fit (GOF) test, it is discovered that the Laplacian model is more suitable to describe noisy speech distribution than the conventional Gaussian model. The likelihood ratio (LR) based on the Laplacian model is computed and then applied to the VAD operation. According to the experimental results, we can find that the Laplacian statistical model is more suitable for the VAD algorithm compared to the Gaussian model."
1880489,21258,23735,Calibration of a multimodal head-mounted device for ecological assessment of social orienting behavior in children,2009,"In this work a multimodal head-mounted device for the assessment of social orienting behavior in children between 12 and 24 months is presented. The device is specifically designed to be used in poorly structured and uncontrolled environments such as day-care centers. Accordingly, a calibration procedure is described which fully exploits the multimodal approach and which is particularly suitable for an ecological assessment."
92716,21258,9804,Large Scale Evaluation of Corpus-based Synthesizers: Results and Lessons from the Blizzard Challenge 2005,2005,"The Blizzard Challenge 2005 was a large scale international evaluation of various corpus-based speech synthesis systems using common datasets. Six sites from around the world, both academic and industrial, participated in this evaluation, the first ever to compare voices built by different systems using the same data. Here we describe results of the evaluation and many of the observations and lessons discovered in carrying it out."
319304,21258,9804,Speech technology for language training and e-inclusion,2005,Efficient language learning is one of the keys to social inclusion. In this paper we present some work aiming at creating a virtual language tutor. The ambition is to create a tutor that can be engaged in many aspects of language learning from detailed pronunciation training to conversational practice. Some of the crucial components of such a system are described. An initial implementation of a stress/quantity training tutor for Swedish will be presented.
1844848,21258,9804,Using RASTA in task independent TANDEM feature extraction,2004,"In this work, we investigate the use of RASTA filter in the TANDEM feature extraction method when trained with a task independent data. RASTA filter removes the linear distortion introduced by the communication channel which is demonstrated in a 18% relative improvement on the Numbers 95 task. Also, studies yielded a relative improvement of 35% over the basic PLP features by combining TANDEM features and conventional PLP features."
1530575,21258,9616,Comparison of Syllable/Phone HMM Based Mandarin TTS,2010,"The performance of HMM-based text to speech (TTS) system is affected by the basic modeling units and the size of training data. This paper compares two HMM based Mandarin TTS systems using syllable and phone as basic units respectively with 1000, 3000 and 5000 sentences’ training data. Two female speakers’ corpora are used as training data for evaluation. For both corpora, the system using syllable as basic unit outperforms the system using phone as basic unit with 3000 and 5000 sentences’ training data."
2692107,21258,9463,English orthography is not close to optimal,2015,"In spite of the apparent irregularity of the English spelling system, Chomsky and Halle (1968) characterize it as “near optimal”. We investigate this assertion using computational techniques and resources. We design an algorithm to generate word spellings that maximize both phonemic transparency and morphological consistency. Experimental results demonstrate that the constructed system is much closer to optimality than the traditional English orthography."
2111915,21258,535,Generalized cyclic transformations in speaker-independent speech recognition,2009,"A feature extraction method is presented that is robust against vocal tract length changes. It uses the generalized cyclic transformations primarily used within the field of pattern recognition. In matching training and testing conditions the resulting accuracies are comparable to the ones of MFCCs. However, in mismatching training and testing conditions with respect to the mean vocal tract length the presented features significantly outperform the MFCCs."
390770,21258,9804,Application of Weighted Finite-State Transducers to Improve Recognition Accuracy for Dysarthric Speech,2008,"Standard speaker adaptation algorithms perform poorly on dysarthric speech because of the limited phonemic repertoire of dysarthric speakers. In a previous paper, we proposed the use of metamodels to correct dysarthric speech. Here, we report on an improved technique that makes use of a cascade of Weighted Finite-State Transducers (WFSTs) at the confusion-matrix, word and language levels. This approach outperforms both standard MLLR and metamodels."
621609,21258,9804,Comparison of HMM and TMDN Methods for Lip Synchronisation,2010,"This paper presents a comparison between a hidden Markov model (HMM) based method and a novel artificial neural network (ANN) based method for lip synchronisation. Both model types were trained on motion tracking data and a perceptual evaluation was carried out comparing the output of the models, both to each other and to the original tracked data. It was found that the ANN based method was judged significantly better than the HMM based method. Furthermore the original data was not judged significantly better than the output of the ANN method."
30560,21258,9804,Improving acoustic-to-articulatory inversion by using hypercube codebooks.,2000,"This paper presents an articulatory codebook construction method which gives a good space coverage with a limited number of points. For this aim, the articulatory space is decomposed into regions, represented by hypercubes, where the mapping is quasi-linear. The codebook, therefore, is a collection of hypercubes. The main quality of this codebook is that it respects the non-linear nature of the articulatory-to-acoustic mapping. We also present an inversion method using this hypercube codebook."
68319,21258,9804,Learning words and speech units through natural interactions,2010,"This work provides an ecological approach to learning words and speech units through natural interactions, without the need for preprogrammed linguistic knowledge in form of phonemes. Interactions such as imitation games and multimodal word learning create an initial set of words and speech units. These sets are then used to train statistical models in an unsupervised way. Index Terms: multimodal learning, ecological approach, motor learning, interactions"
134432,21258,9804,Auditory Visual Speech Processing,2001,"This paper provides an overview of the developments in Auditory Visual Speech Processing, a Special Interest Group within ISCA. I hope that this discussion will be informative and useful to readers in a variety of fields, including psychology, speech science, animation, psycholinguistics, human-machine interaction, hearing-impaired communication, and numerous other fields which also share in this fruitful intersection."
2472184,21258,21089,NICT-ATR Speech-to-Speech Translation System,2007,"This paper describes the latest version of speech-to-speech translation systems developed by the team of NICT-ATR for over twenty years. The system is now ready to be deployed for the travel domain. A new noise-suppression technique notably improves speech recognition performance. Corpus-based approaches of recognition, translation, and synthesis enable coverage of a wide variety of topics and portability to other languages."
2009292,21258,9804,Speechalator: two-way speech-to-speech translation on a consumer PDA,2003,"This paper describes a working two-way speech-to-speech translation system that runs in near real-time on a consumer handheld computer. It can translate from English to Arabic and Arabic to English in the domain of medical interviews. We describe the general architecture and frameworks within which we developed each of the components: HMM-based recognition, interlingua translation (both rule and statistically based), and unit selection synthesis."
1825514,21258,9463,Building Conversational Agents with Basilica,2009,Basilica is an event-driven software architecture for creating conversational agents as a collection of reusable components. Software engineers and computer scientists can use this general architecture to create increasingly sophisticated conversational agents. We have developed agents based on Basilica that have been used in various application scenarios and foresee that agents build on Basilica can cater to a wider variety of interactive situations as we continue to add functionality to our architecture.
397061,21258,9804,Resources for speech research: Present and future infrastructure needs,2009,"This paper introduces the EU-FP7 project CLARIN, a joint effort of over 150 institutions in Europe, aimed at the creation of a sustainable language resources and technology infrastructure for the humanities and social sciences research community. The paper briefly introduces the vision behind the project and how it relates to speech research with a focus on the contributions that CLARIN can and will make to research in spoken language processing."
2547480,21258,9804,The phonetic labeling on read and spontaneous discourse corpora.,2000,"Read and spontaneous discourses are two different but very significant speech styles to be investigated. So phonetic labeling on read and spontaneous discourse corpora are made one is ASCCD, a 10 hours read discourse corpus and the other is CASS, a 4 hours spontaneous discourse corpus. First the principles and conventions of transcription are presented. Then, these two speech styles are compared from phonetic and syntactic point of view, including the statistic results of different phonetic units got from the annotated corpora."
2662712,21258,9804,Diacritics restoration for Arabic dialect texts,2013,"In this paper we present a statistical approach for automatic diacritization of Algiers dialectal texts. This approach is based on statistical machine translation. We first investigate this approach on Modern Standard Arabic (MSA) texts using several data sources and extrapolated the results on available dialectal texts. For evaluation we used word and diacritization error rates and also precision and recall. Index Terms: Machine translation system, Modern Standard Arabic, automatic diacritization, Algiers’s dialect."
24582,21258,9804,The perceptual relevance of glottal-pulse parameter variations.,2001,"The perceptual relevance of changes to glottal-pulse parameters is studied. First, it is demonstrated that a distance measure based on excitation patterns can predict audibility discrimination thresholds for small changes to the R parameters of the Liljencrants-Fant (LF) model. Next, by using this measure the perceptual relevance of the LF parameters is quantified. Results are presented for a number of sets of glottal-pulse parameters that were taken from literature, representing distinct voice qualities."
607441,21258,9804,A Comparative Study of Adaptation Methods for Speaker Verification,2002,"Keywords: learning ; speech Note: IDIAP-RR 01-34 Reference EPFL-CONF-82827 URL: http://publications.idiap.ch/downloads/reports/2002/mariethoz-icslp2002.pdf Related documents: http://publications.idiap.ch/index.php/publications/showcite/mariethoz:2001:idiap-01-34 Record created on 2006-03-10, modified on 2016-08-08"
393987,21258,9804,Hands Free Audio Analysis from Home Entertainment,2010,"In this paper, we describe a system developed for hands free audio analysis for a living room environment. It comprises detection and localisation of the verbal and paralinguistic events, which can augment the behaviour of virtual director and improve the overall experience of interactions between spatially separated families and friends. The results show good performance in reverberant environments and fulfil real-time requirements."
150110,21258,9804,RAMCESS/HandSketch : A Multi-Representation Framework for Realtime and Expressive Singing Synthesis,2007,"In this paper we describe the different investigations that are part of the development of a new singing digital musical instrument, adapted to real-time performance. It concerns improvement of low-level synthesis modules, mapping strategies underlying the development of a coherent and expressive control space, and the building of a concrete bi-manual controller. Index Terms: real-time expressive singing synthesis"
3710,21258,9804,Development of the primary CRIM system for the NIST 2008 speaker recognition evaluation.,2008,"We describe how we modified the CRIM factor analysis speaker verification system to handle the new cross-channel conditi ons encountered in the 2008 NIST speaker recognition evaluation. Using the 2006 evaluation data for development, we obtained results on a broad spectrum of test conditions that are uniformly bette r than the best results that have been published in the literature. Index Terms: speaker verification, factor analysis"
1351950,21258,9463,Detecting Emotion in Speech: Experiments in Three Domains,2006,"The goal of my proposed dissertation work is to help answer two fundamental questions: (1) How is emotion communicated in speech? and (2) Does emotion modeling improve spoken dialogue applications? In this paper I describe feature extraction and emotion classification experiments I have conducted and plan to conduct on three different domains: EPSaT, HMIHY, and ITSpoke. In addition, I plan to implement emotion modeling capabilities into ITSpoke and evaluate the effectiveness of doing so."
178667,21258,9804,Data Pruning for Template-based Automatic Speech Recognition,2010,"In this paper we describe and analyze a data pruning method in combination with template-based automatic speech recognition. We demonstrate the positive effects of polishing the template database by minimizing the word error rate scores. Data pruning allowed to effectively reduce the database size, and therefore the model size, by an impressive 30%, with consequent benefits on the computation time and memory usage. Index Terms: pruning, template, automatic speech recognition"
2371230,21258,8960,Bounds on the complexity of recurrent neural network implementations of finite state machines,1994,"In this paper the efficiency of recurrent neural network implementations of m-state finite state machines will be explored. Specifically, it will be shown that the node complexity for the unrestricted case can be bounded above by O (√m). It will also be shown that the node complexity is O (√m log m) when the weights and thresholds are restricted to the set {-1, 1}, and O (m) when the fan-in is restricted to two. Matching lower bounds will be provided for each of these upper bounds assuming that the state of the FSM can be encoded in a subset of the nodes of size [log m]."
151260,21258,20332,Improving (Meta)Cognitive Tutoring by Detecting and Responding to Uncertainty.,2009,"We hypothesize that enhancing computer tutors to respond to student uncertainty over and above correctness is one method for increasing both student learning and self-monitoring abilities. We explore this hypothesis using data from an experiment with a wizarded spoken tutorial dialogue system, where tutor responses to uncertain and/or incorrect student answers were manipulated. Our results suggest that monitoring and responding to student uncertainty has the potential to improve both cognitive and metacognitive student abilities."
2410431,21258,9804,Training of Coarticulation Models using Dominance Functions and Visual Unit Selection Methods for Audio-Visual Speech Synthesis,2006,"This paper presents results of training of coarticulation models for Czech audio-visual speech synthesis. Two approaches for solution of coarticulation in audio-visual speech synthesis were used, coarticulation based on dominance functions and visual unit selection. For both approaches, coarticulation models were trained. Models for unit selection approach were trained by visualy clustered data. These data were obtained using decision tree algorithm. Outputs of audio-visual speech synthesis for both approaches were assessed and compared objectively. Index Terms: audio-visual speech synthesis, talking head, coarticulation model"
2623789,21258,344,Predicting Romanian Stress Assignment,2014,"We train and evaluate two models for Romanian stress prediction: a baseline model which employs the consonant-vowel structure of the words and a cascaded model with averaged perceptron training consisting of two sequential models ‐ one for predicting syllable boundaries and another one for predicting stress placement. We show in this paper that Romanian stress is predictable, though not deterministic, by using data-driven machine learning techniques."
2456182,21258,9804,Production Boundary between Fricative and Affricate in Japanese and Korean Speakers,2009,"A fricative [s] and an affricate [ts] pronounced by both native Japanese and Korean speakers were analyzed to clarify the effect of the mother language on speech production. It was revealed that Japanese speakers have a clear individual production boundary between [s] and [ts], and that this boundary corresponds to the production boundary of all Japanese speakers. In contrast, although Korean speakers tend to have a clear individual production boundary, the boundary dose not corresponds to that of Japanese speakers. These facts suggest that Korean speakers tend to have a stable [s]-[ts] production boundary but that differ from Japanese speakers. Index Terms: affricate, fricative, speech production, nonnative speaker, phoneme boundary"
87860,21258,9804,An objective measure for assessment of the concatenative TTS segment inventories.,2001,In the paper we present a method for assessment of the segment inventories for concatenative text-to-speech synthesis. We argue that the overall comprehensibility of the synthesized speech depends on the length of the segments --- longer segments imply more intelligible speech. The problem of minimum text cover by the given segment set is formulated in the paper as well as an algorithm finding the solution. Some improvements speeding up the algorithm are discussed in the rest of the paper.
2221206,21258,11529,Using a transcription graph for large vocabulary continuous speech recognition,1996,"A new algorithm is developed for constructing a transcription graph in large vocabulary continuous speech recognition. The transcription graph satisfies lexical constraints and encodes all high scoring phonetic transcriptions. By building a transcription graph in parallel with the word graph, we use high-level knowledge sources to constrain the paths of phonetic transcriptions such that they are created only if needed for building the word graph. This not only speeds up the search of the lexicon, but also limits the memory requirement in our system."
190595,21258,9804,Multi-Context Rules for Phonological Processing in Polyglot TTS Synthesis,2004,"Polyglot text-to-speech synthesis, i.e. the synthesis of sentences containing one or more inclusions from other languages, primarily depends on an accurate morphosyntactic analyzer for such mixed-lingual texts. From the output of this analyzer, the pronunciation can be derived by means of phonological transformations which are language-specific and depend on various contexts. In this paper a new rule formalism for such phonological transformations is presented, which complies also with the requirements of the mixed-lingual situation."
2401775,21258,9463,"Entrainment in Spoken Dialogue Systems: Adopting, Predicting and Influencing User Behavior",2013,"Entrainment is the phenomenon of the speech of conversational partners becoming more similar to each other. This thesis proposal presents a comprehensive look at entrainment in human conversations and how entrainment may be incorporated into the design of spoken dialogue systems in order to improve system performance and user satisfaction. We compare different kinds of entrainment in both classic and novel dimensions, provide experimental results on the utility of entrainment, and show that entrainment can be used to improve a system’s ASR performance and turntaking decisions."
122856,21258,9804,Fusing Fast Algorithms to Achieve Efficient Speech Detection in FM Broadcasts,2009,"This paper describes a system aimed at detecting speech segments in FM broadcasts. To achieve high processing speeds, simple but fast algorithms are used. To output robust decisions, a combination of many different algorithms has been considered. The system is fully operational in the context of Open Source Intelligence, since 2007. Index Terms: speech detection, speech segmentation, fusion, intelligence."
211671,21258,9804,Effects of familiarity with faces and voices on second-language speech processing: Components of memory traces,2006,"Familiarity with a talker’s voice and face was found to facilitate processing of second-language speech. This advantage is accentuated when visual cues are limited to either the mouth and jaw area, or eyes and upper cheek areas of a talker’s face. Findings are compatible with a multiple-trace model of bimodal speech processing. Index Terms: auditory-visual, memory traces, talker familiarity, language learners"
165695,21258,9804,Modeling sensory-to-motor mappings using neural nets and a 3d articulatory speech synthesizer.,2006,"A comprehensive neural model of speech motor control including a three dimensional articulatory speech synthesizer as a front-end device is described in detail in this paper. The training of the sensory-to-motor mappings – which can be interpreted as the prelinguistic phase of speech acquisition – is described in detail for quasi-static as well as for dynamic articulation. Index Terms: speech production, neural model, articulatory model, articulatory speech synthesis, speech acquisition"
328345,21258,9804,Rhythm measures with language-independent segmentation,2009,"We compare 15 measures of speech rhythm based on an automatic segmentation of speech into vowel­like and consonant­like regions. This allows us to apply identical segmentation criteria to all languages and to compute rhythm measures over a large corpus. It may also approximate more closely the segmentation available to pre­lexical infants, who apparently can discriminate between languages. We find that within­language variation is large and comparable to the"
304073,21258,9804,Combining Evidence from a Generative and a Discriminative Model in Phoneme Recognition,2008,"We investigate the use of the log-likelihood of the features obtained from a generative Gaussian mixture model, and the posterior probability of phonemes from a discriminative multilayered perceptron in multi-stream combination for recognition of phonemes. Multi-stream combination techniques, namely early integration and late integration are used to combine the evidence from these models. By using multi-stream combination, we obtain a phoneme recognition accuracy of 74% on the standard TIMIT database, an absolute improvement of 2.5% over the single best stream."
515640,21258,9804,Using Eye Movements for Online Evaluation of Speech Synthesis,2007,"This paper * describes an eye tracking experiment to study the processing of diphone synthesis, unit selection synthesis, and human speech taking segmental and suprasegmental speech quality into account. The results showed that both factors influenced the processing of human and synthetic speech, and confirmed that eye tracking is a promising albeit time consuming research method to evaluate synthetic speech. Index Terms: eye movements, human speech, diphone synthesis, unit selection synthesis."
2663231,21258,9804,Using generalized additive models and random forests to model prosodic prominence in German,2013,"The perception of prosodic prominence is influenced by different sources like different acoustic cues, linguistic expectations and context. We use a generalized additive model and a random forest to model the perceived prominence on a corpus of spoken German. Both models are able to explain over 80% of the variance. While the random forests give us some insights on the relative importance of the cues, the general additive model gives us insights on the interaction between different cues to prominence."
2586335,21258,21089,A Composite Kernel Approach for Dialog Topic Tracking with Structured Domain Knowledge from Wikipedia,2014,Dialog topic tracking aims at analyzing and maintaining topic transitions in ongoing dialogs. This paper proposes a composite kernel approach for dialog topic tracking to utilize various types of domain knowledge obtained from Wikipedia. Two kernels are defined based on history sequences and context trees constructed based on the extracted features. The experimental results show that our composite kernel approach can significantly improve the performances of topic tracking in mixed-initiative human-human dialogs.
1787029,21258,9804,How can you use disfluencies and still sound as a good speaker,2008,"This paper explores the results of a previous experiment concerning listeners’ ratings of different types of (dis)fluencies and extends the analysis of such phenomena to a corpus of university lectures. Results suggest that, although not all disfluency types are equally tolerated by listeners, such differences may be overridden by an adequate control of tonal scaling and pause length, at least. Index Terms: disfluencies, prosody, fluency ratings."
1812701,21258,535,Evolutionary discriminative speaker adaptation,2011,"This paper presents a new evolutionary-based approach that aims at investigating more solutions while simplifying the speaker adaptation process. In this approach, a single global transformation set of parameters is optimized by genetic algorithms using a discriminative objective function. The goal is to achieve accurate speaker adaptation whatever the amount of available adaptive data. Experiments using the ARPA-RM database demonstrate the effectiveness of the proposed method."
378750,21258,9804,Robust Methods in Automatic Speech Recognition and Understanding,2003,"This paper overviews robust architecture and modeling techniques for automatic speech recognition and understanding. The topics include robust acoustic and language modeling for spontaneous speech recognition, unsupervised adaptation of acoustic and language models, robust architecture for spoken dialogue systems, multi-modal speech recognition, and speech understanding. This paper also discusses the most important research problems to be solved in order to achieve ultimate robust speech recognition and understanding systems."
58309,21258,9804,EM TRAINING OF FINITE-STATE TRANSDUCERS AND ITS APPLICATION TO PRONUNCIATION MODELING,2002,"Recently, nite-state transducers (FSTs) have been shown to be useful for a number of applications in speech and language processing. FST operations such as composition, determinization, and minimization make manipulating FSTs very simple. In this paper, we present a method to learn weights for arbitrary FSTs using the EM algorithm. We show that this FST EM algorithm is able to learn pronunciation weights that improve the word error rate for a spontaneous speech recognition task."
1991687,21258,9463,RavenCalendar: A Multimodal Dialog System for Managing a Personal Calendar,2007,"Dialog applications for managing calendars have been developed for every generation of dialog systems research (Heidorn, 1978; Yankelovich, 1994; Constantinides and others, 1998; Horvitz and Paek, 2000; Vo and Wood, 1996; Huang and others, 2001). Today, Web-based calendar applications are widely used. A spoken dialog interface to a Web-based calendar application permits convenient use of the system on a handheld device or over the telephone."
435550,21258,9804,Sequential Noise Compensation by A Sequential Kullback Proximal Algorithm,2001,"We present a sequential noise compensation method based on the sequential Kullback proximal algorithm, which uses the Kullback-Leibler divergence as a regularization function for the maximum likelihood estimation. The method is implemented as filters. In contrast to sequential noise compensation method based on the sequential EM algorithm, the convergence rate of the method and estimation error after convergence can be adjusted by a relaxation factor , where the sequential EM algorithm corresponds to the particular case of . Through experiments on parameter estimation and speech recognition in noise, we verified the efficacy of the algorithm."
94011,21258,9804,"Evaluation of PROS-3 for the assignment of prosodic structure, compared to assignment by human experts",2001,"This paper describes the results of an evaluation of PROS-3, a system that assigns prosodic structure to text on the basis of the output of a syntactic parser. In order to evaluate the performance of PROS-3 as such and in combination with a revised algorithm for prosodic phrasing, we compare it to the prosodic structure as assigned by human experts. Also, the results of a perception experiment are presented, which show that listeners have the same preference of prosodic realization as we would expect on the basis of the comparison of the prosodic structures as assigned by PROS-3 and by human experts."
318062,21258,9804,Silence Models in Weighted Finite-State Transducers,2008,"We investigate the effects of different silence modelling strategies in Weighted Finite-State Transducers for Automatic Speech Recognition. We show that the choice of silence models, and the way they are included in the transducer, can have a significant effect on the size of the resulting transducer; we present a means to prevent particularly large silence overheads. Our conclusions include that context-free silence modelling fits well with transducer based grammars, whereas modelling silence as a monophone and a context has larger overheads."
255441,21258,9804,"Korean lenis, fortis, and aspirated stops: Effect of place of articulation on acoustic realization",2010,"Unlike most of the world's languages, Korean distinguishes three types of voiceless stops, namely lenis, fortis, and aspirated stops. All occur at three places of articulation. In previous work, acoustic measurements are mostly collapsed over the three places of articulation. This study therefore provides acoustic measurements of Korean lenis, fortis, and aspirated stops at all three places of articulation separately. Clear differences are found among the acoustic characteristics of the stops at the different places of articulation. Index Terms: speech production, stop consonants, phonetics"
518511,21258,9804,Modelling the effect of speaker familiarity and noise on infant word recognition,2010,In the present paper we show that a general-purpose word learning model can simulate several important findings from recent experiments in language acquisition. Both the addition of background noise and varying the speaker have been found to influence infants' performance during word recognition experiments. We were able to replicate this behaviour in our artificial word learning agent. We use the results to discuss both advantages and limitations of computational models of language acquisition.
2799088,21258,9804,Two-step Correction of Speech Recognition Errors Based on N-gram and Long Contextual Information,2013,"This paper presents a fully automatic word error correction on a confusion network that makes use of long contextual information. However, a problem with long contextual information is that improvement of the recognition accuracy is minimal because of the word errors surrounding words. In this paper, recognition errors are first reduced by error correction using Ngram features. After that, the long-distance context scores are applied to the correction of the residual recognition errors. Index Terms: confusion network, conditional random fields, word-error correction, long contextual information"
622678,21258,9804,Enumerating Differences Between Various Communicative Functions for Purposes of Czech Expressive Speech Synthesis in Limited Domain,2012,This paper deals with determination of a penalty matrix that should represent differences between various communicative functions. These are supposed to describe expressivity that can occur in expressive speech and were designed to fit a limited domain of conversations between seniors and a computer on a given topic. The penalty matrix is assumed to increase a rate of the expressivity perception in synthetic speech produced by unit selection method. It should reflect both acoustic differences and differences based on human perception of expressivity.
243925,21258,9804,Speech categorization context effects in seven- to nine-month-old infants.,2010,"Adults have been shown to categorize an ambiguous syllable differently depending on which sound precedes it. The present paper reports preliminary results from an on-going experiment, investigating seven- to nine-month-olds on their sensitivity to non-speech contexts when perceiving an ambiguous syllable. The results suggest that the context effect is present already in infancy. Additional data is currently collected and results will be presented in full at the conference."
1092836,21258,9463,Joint Versus Independent Phonological Feature Models within CRF Phone Recognition,2007,"We compare the effect of joint modeling of phonological features to independent feature detectors in a Conditional Random Fields framework. Joint modeling of features is achieved by deriving phonological feature posteriors from the posterior probabilities of the phonemes. We find that joint modeling provides superior performance to the independent models on the TIMIT phone recognition task. We explore the effects of varying relationships between phonological features, and suggest that in an ASR system, phonological features should be handled as correlated, rather than independent."
529959,21258,9804,A case study of Portuguese and English bilinguality,2002,"The study of the acoustic characteristics of European Portuguese and British English fricatives as produced by two bilingual subjects, consisted of time and frequency analysis of words in a carrier sentence. Time-averaged power spectra were calculated and parameterised in order to aid comparisons across speaker, across corpus, and across language, and to gain insight into the production mechanisms underlying the language-specific variations."
387744,21258,9804,An elitist approach for extracting automatically well-realized speech sounds with high confidence,2005,"This paper presents an elitist approach\ for extracting automatically well-realized speech sounds with high confidence. The elitist approach uses a speech recognition system based on Hidden Markov Models (HMM). The HMM are trained on speech sounds which are systematically well-detected in an iterative procedure. The results show that, by using the HMM models defined in the training phase, the speech recognizer detects reliably specific speech sounds with a small rate of errors."
1806798,21258,21089,Interactive ASR Error Correction for Touchscreen Devices,2008,We will demonstrate a novel graphical interface for correcting search errors in the output of a speech recognizer. This interface allows the user to visualize the word lattice by pulling apart regions of the hypothesis to reveal a cloud of words simlar to the tag clouds popular in many Web applications. This interface is potentially useful for dictation on portable touchscreen devices such as the Nokia N800 and other mobile Internet devices.
601343,21258,9804,"LinTest, A development tool for testing dialogue systems",2006,"In this paper we present a development tool for testing dialogue systems. Testing software through the specification is important for software development in general and should be as automated as possible. For dialogue systems, the corpus can be seen as one part of the specification and the dialogue system should be tested on available corpora on each new build. The testing tool is inspired from work on agile software development methods, test driven development and unit testing, and can be used in two modes and during various phases of development."
2866069,21258,21089,Joint Word Segmentation and Phonetic Category Induction,2016,We describe a model which jointly performs word segmentation and induces vowel categories from formant values. Vowel induction performance improves slightly over a baseline model which does not segment; segmentation performance decreases slightly from a baseline using entirely symbolic input. Our high joint performance in this idealized setting implies that problems in unsupervised speech recognition reflect the phonetic variability of real speech sounds in context.
282769,21258,9804,Individual on-line variance adaptation of frequency filtered parameters for robust ASR.,2006,"In this paper we address the problem of robust speech recognition. We propose a new method based on the individual variance adaptation of frequency filtered parameters to reduce the deleterious effects of additive narrow-band noise. The method can be interpreted as a spectral weighting that assigns increased importance to the most reliable spectral components, typically the spectral peaks. The experiments confirm that the suggested method results in significantly improved recognition rates for additive narrow-band noise."
54278,21258,9804,Cross-Speaker Articulatory Position Data for Phonetic Feature Prediction,2005,"Through the use of a device called an Electromagnetic Articulograph, it is possible to measure the locations of a person’s articulators during speech. As more of this data becomes available, one important question is how it can be used. In this paper, we demonstrate that it can improve performance for the recognition of some phonetic features. As articulatory position data is scarce, we also describe experiments that use articulatory position data from one speaker with another and provide results. These experiments use cross-speaker articulatory positions to predict phonetic features."
2820831,21258,9804,A Client mobile application for Chinese-Spanish statistical machine translation,2014,"This show and tell paper describes a client mobile application for Chinese-Spanish machine translation. The system combines a standard server-based statistical machine translation (SMT) system, which requires online operation, with different input modalities including text, optical character recognition (OCR) and automatic speech recognition (ASR). It also includes an index-based search engine for supporting off-line translation."
532596,21258,9804,Posteriori Probabilities and Likelihoods Combination for Speech and Speaker Recognition,2004,"This paper investigates a new approach to perform simultaneous speech and speaker recognition. The likelihood estimated by a speaker identification system is combined with the posterior probability estimated by the speech recognizer. So, the joint posterior probability of the pronounced word and the speaker identity is maximized. A comparison study with other standard techniques is carried out in three different applications, (1) closed set speech and speaker identification, (2) open set speech and speaker identification and (3) speaker quantization in speaker-independent speech recognition."
579016,21258,9804,Speech Synthesis Development Made Easy: The Bonn Open Synthesis System,2001,"This paper describes a new open source architecture for unit-selection based speech synthesis called BOSS (Bonn Open Synthesis System). It is built up modularly, with communications between modules taking place in a fixed format. This makes the addition, deletion and substitution of modules very easy. The strict separation between data and algorithms allows for the simple creation of new speech corpora for different domains and languages."
2232635,21258,9804,Use of Acoustic Prior Information for Confidence Measure in ASR Applications,2001,"In this paper, we propose a new acoustic confidence measure of ASR hypothesis and compare it to approaches proposed in the literature. This approach takes into account prior information on the acoustic model performance specific to each phoneme. The new method is tested on two types of recognition errors: the out-of-vocabulary words and the errors due to additive noise. We then propose an efficient way to interpret the raw confidence measure as a correctness prior probability."
1424319,21258,8840,Real-time decision detection in multi-party dialogue,2009,"We describe a process for automatically detecting decision-making sub-dialogues in multi-party, human-human meetings in real-time. Our basic approach to decision detection involves distinguishing between different utterance types based on the roles that they play in the formulation of a decision. In this paper, we describe how this approach can be implemented in real-time, and show that the resulting system's performance compares well with other detectors, including an off-line version."
2307455,21258,11529,A phoneme-similarity based ASR front-end,1996,"A training procedure for phoneme similarity reference models is described and two word recognition methods based on phoneme similarities for the English language are evaluated under clean, noisy and channel-distorted speech conditions. Optimization of recognition performance is examined in terms of multi-style training, cepstral normalizations, gender dependent models and length of time over which the phoneme similarities are computed. Phoneme similarities provide a compact speech representation which is relatively insensitive to the variations between speakers."
180175,21258,9804,An Automatic Dialogue System Generator from the Internet Information Contents,2001,"We propose a semi-automatic dialogue system generator from the Internet information contents. We classify the practical Web site into three classes of task: slot-filling, database search, and explanation. Using three levels of dialogue library for each task, our generator translates XML based Web site into VoiceXML, which controls a conversation between a user and a computer system. In this paper, we explain an outline of our project and report implementation examples."
363009,21258,9804,A Component by Component Listening Test Analysis of the IBM Trainable Speech Synthesis System,2001,"This paper reports on a listening test conducted to determine the impact on speech quality of each component in the IBM Trainable Speech Synthesiser. The study was originally conceived to direct future research effort to those components with the greatest potential for improvement. However, the results and conclusions regarding prosodic modification, concatenation unit length, and decision tree clustering are generally applicable and may be of wider interest."
429632,21258,20515,Scatter Difference NAP for SVM Speaker Recognition,2009,"This paper presents Scatter Difference Nuisance Attribute Projection (SD-NAP) as an enhancement to NAP for SVM-based speaker verification. While standard NAP may inadvertently remove desirable speaker variability, SD-NAP explicitly de-emphasises this variability by incorporating a weighted version of the between-class scatter into the NAP optimisation criterion. Experimental evaluation of SD-NAP with a variety of SVM systems on the 2006 and 2008 NIST SRE corpora demonstrate that SD-NAP provides improved verification performance over standard NAP in most cases, particularly at the EER operating point."
2617441,21258,9804,MODIS: an audio motif discovery software.,2013,"MODIS is a free speech and audio motif discovery software developed at IRISA Rennes. Motif discovery is the task of discovering and collecting occurrences of repeating patterns in the absence of prior knowledge, or training material. MODIS is based on a generic approach to mine repeating audio sequences, with tolerance to motif variability. The algorithm implementation allows to process large audio streams at a reasonable speed where motif discovery often requires huge amount of time."
2888762,21258,9804,Quantitative analysis of backchannels uttered by an interviewer during neuropsychological tests,2016,"This paper examines in detail the backchannels uttered by a French professional interviewer during a neuropsychological test of verbal memories. These backchannels are short utterances such as oui, d'accord, uhm, etc. They are mainly produced here to encourage subjects to retrieve a set of words after their controlled encoding. We show that the choice of lexical items, their production rates and their associated prosodic contours are influenced by the subject performance and conditioned by the protocol."
61125,21258,9804,Efficient Speech Enhancement by Diffusive Gain Factors (DGF),2001,"In this paper we propose a very simple but highly effective algorithm for single channel noise reduction of speech signals. One of the main objectives is to find a balanced tradeoff between noise reduction and speech distortion in the processed signal. This is accomplished by a system based on spectral minimum detection and diffusive gain factors. Our approach to speech enhancement is capable of distinguishing between speech and noise interference in the microphone signal, even when they are located in the same frequency band."
179036,21258,9804,Development of Advanced Dialog Systems with PATE,2006,"Current commercial dialog systems show only limited capabilities with regard to the phenomena occurring in spontaneous, natural dialog. Many research prototypes, in contrast, are already able to deal with a great number of phenomena but lack the clarity and maintainability of commercial systems. In this paper we present a framework for developing advanced multimodal dialog systems designed to bridge this gap. Index Terms: multimodal dialogue systems, commercial applications."
41102,21258,9804,"The COST278 Broadcast News Segmentation and Speaker Clustering Evaluation - Overview, Methodology, Systems, Results",2005,"This paper describes a large scale experiment in which eight research institutions have tested their audio partitioning and labeling algorithms on the same data, a multi-lingual database of news broadcasts, using the same evaluation tools and protocols. The experiments have provide more insight in the cross-lingual robustness of the methods and they have demonstrated that by further collaborating in thedomains of speaker change detection and speaker clustering it should be possible to achieve further technological progress in the near future."
33136,21258,9804,Neologos: an optimized database for the development of new speech processing algorithms,2005,"The Neologos project is a speech database creation project for the French language, resulting from a collaboration between universities and industrial companies and supported by the French Ministry of Research. The goal of Neologos is to re-think the design of the speech databases in order to enable the development of new algorithms in the field of speech processing. A general method is proposed to optimize the database contents in terms of diversity of the recorded voices, while reducing the number of recorded speakers."
2364932,21258,9804,Automatic Regularization of Cross-entropy Cost for Speaker Recognition Fusion,2013,"In this paper we study automatic regularization techniques for the fusion of automatic speaker recognition systems. Parameter regularization could dramatically reduce the fusion training time. In addition, there will not be any need for splitting the development set into different folds for cross- validation. We utilize majorization-minimization approach to automatic ridge regression learning and design a similar way to learn LASSO regularization parameter automatically. By experiments we show improvement in using automatic regularization."
40696,21258,9804,SUXES - user experience evaluation method for spoken and multimodal interaction.,2009,"Much work remains to be done with subjective evaluations of speech-based and multimodal systems. In particular, user experience is still hard to evaluate. SUXES is an evaluation method for collecting subjective metrics with user experiments. It captures both user expectations and user experiences, making it possible to analyze the state of the application and its interaction methods, and compare results. We present the SUXES method with examples of user experiments with different applications and modalities. Index Terms: evaluation, subjective metrics, user experience"
224701,21258,9804,Tonal alignment in three varieties of Hiberno-English,2009,"This pilot study investigates the tonal alignment of pre-nuclear (PN) and nuclear (N) accents in three Hiberno-English (HE) regional varieties: Dublin, Drogheda, and Donegal English. The peak alignment is investigated as a function of the number of unstressed syllables before PN and after N. Dublin and Drogheda English appear to a have fixed peak alignment in both nuclear and pre-nuclear conditions. Donegal English, however, shows a drift in peak alignment in nuclear and prenuclear conditions. Findings also show that the peak is located earlier in nuclear and later in pre-nuclear conditions across the three dialects. Index Terms: peak alignment, Hiberno-English, nuclear and pre-nuclear accents"
829695,21258,235,Parsing plans situation-dependently in dialogues,1996,"This paper describes a plan parsing method that can handle the effects and preconditions of actions and that parses plans in a manner dependent on dialogue state changes, especially on the mental state changes of dialogue participants caused by utterances. This method is based on active chart parsing and uses augmented edge structures to keep state information locally and time map management to deal with state changes. It has been implemented in Prolog and is used for plan recognition in dialogues."
407520,21258,9804,On Building a Concatenative Speech Synthesis System from the Blizzard Challenge Speech Databases,2005,"In this paper, we compare two methods of building a concatenative speech synthesis system from the relatively small, “Blizzard Challenge” speech databases. In the first method we build a system directly from the Blizzard databases using the IBM Concatenetative Speech Synthesis System originally designed for very large speech databases. In the second method, a larger database is used to build the synthesis system and the output is “morphed” to match the speakers in the Blizzard databases. The second method outperformed the first while maintaining the identity of the Blizzard target speakers."
1913841,21258,9804,Comparing American and Palestinian Perceptions of Charisma Using Acoustic-Prosodic and Lexical Analysis,2007,"Charisma, the ability to lead by virtue of personality alone, is difficult to define but relatively easy to identify. However, cultural factors clearly affect perceptions of charisma. In this paper we compare results from parallel perception studies investigating charismatic speech in Palestinian Arabic and American English. We examine acoustic/prosodic and lexical correlates of charisma ratings to determine how the two cultures differ with respect to their views of charismatic speech."
1211401,21258,9078,Automatic sign language identification,2013,"We propose a Random-Forest based sign language identification system. The system uses low-level visual features and is based on the hypothesis that sign languages have varying distributions of phonemes (hand-shapes, locations and movements). We evaluated the system on two sign languages - British SL and Greek SL, both taken from a publicly available corpus, called Dicta Sign Corpus. Achieved average F1 scores are about 95% - indicating that sign languages can be identified with high accuracy using only low-level visual features."
2619280,21258,235,A Hierarchical Domain Model-Based Multi-Domain Selection Framework for Multi-Domain Dialog Systems,2012,"We proposed a hierarchical domain model (HDM)-based multi-domain selection framework (MDSF) for multi-domain dialog systems. The HDM-based MDSF statistically detects one or more candidate domains and heuristically determines one or more final domains from among the candidate domains. The HDM is used in both the candidate domain detection and final domain determination components. Multi-domain dialog systems that employ the HDM-based MDSF provide service to one or more domains at the same time, whereas traditional multi -domain dialog systems provide service to only one domain at a time. To validate the HDM-based MDSF, we developed a multi-domain dialog system for TV program, video-on-demand, and TV device domains. The experimental results show that the HDM-based MDSF correctly selects one or more domains and enables multi-domain dialog systems to provide more accurate and rapid dialog service than traditional multi-domain dialog systems."
2579306,21258,9804,Speaking to the Crowd: Looking at Past Achievements in Using Crowdsourcing for Speech and Predicting Future Challenges.,2011,"This paper examines the literature on the use of crowdsourcing for speech-related tasks: speech acquisition, transcription and annotation as well as the assessment of speech technology. 29 papers were found, representing, 37 different experiments, which were annotated and analyzed to find trends in the field. The paper focuses on the different techniques used for quality control and the variety of sources of “crowds”. Finally, we propose several challenges for the future of crowdsourcing for speech processing."
2662718,21258,9804,Probabilistic speech F 0 contour model incorporating statistical vocabulary model of phrase-accent command sequence.,2013,"We have previously proposed a generative model of speech F0 contours, based on the discrete-time version of the Fujisaki model (a model of the mechanisim for controlling F0s through laryngeal muscles). One advantage of this model is that it allows us to apply statistical methods to estimate the Fujisakimodel parameters from speech F0 contours. This paper proposes a new generative model of speech F0 contours incorporating a vocabulary model of intonation patterns. A parameter inference algorithm for the present model is derived. We quantitatively evaluated the performance of our parameter inference algorithm."
429014,21258,9804,Keyphrase Cloud Generation of Broadcast News,2013,"This paper describes an enhanced automatic keyphrase extraction method applied to Broadcast News. The keyphrase extraction process is used to create a concept level for each news. On top of words resulting from a speech recognition system output and news indexation and it contributes to the generation of a tag/keyphrase cloud of the top news included in a Multimedia Monitoring Solution system for TV and Radio news/programs, running daily, and monitoring 12 TV channels and 4 Radios."
1506570,21258,65,Evaluation of a dialogue manager for a mobile robot,2013,"This paper presents an evaluation of the dialogue manager (DM) used on Carl, a prototype of an intelligent service robot, designed and developed having in mind hosting tasks in a building or event. The developed DM, based on the “Information State” approach, is described. In an experimental evaluation, in which 10 participants attempted to complete several interaction tasks with the robot, 81% of tasks were performed successfully. The results of an usability evaluation are also presented and discussed."
293427,21258,9804,ROBUSTNESS OF PHASE BASED FEATURES FOR SPEAKER RECOGNITION,2009,"This paper demonstrates the robustness of group-delay based features for speech processing. An analysis of group delay functions is presented which show that these features retain formant structure even in noise. Furthermore, a speaker verification task performed on the NIST 2003 database show lesser error rates, when compared with the traditional MFCC features. We also mention about using feature diversity to dynamically choose the feature for every claimed speaker."
368217,21258,11321,Piecing together the emotion jigsaw,2004,"People are emotional, and machines are not. That constrains their communication, and defines a key challenge for the information sciences. Different groups have addressed it from different angles, trying to develop methods of detecting emotion, agents that convey emotion, systems that predict behaviour in emotional circumstances, and so on. Progress has been limited. The new network of excellence HUMAINE explores the idea that progress depends on addressing the problem as a whole, not in isolated fragments."
2781864,21258,9804,A comparison between human vowel normalization strategies and acoustic vowel transformation techniques,2001,"Perceptual and acoustic representations of vowel data were compared directly to evaluate the perceptual relevance of several speaker normalization transformations. The acoustic representations consisted of raw F0 and formant data. The perceptual representations were obtained through an experimental procedure, with phonetically trained listeners as subjects. The raw acoustic data were transformed according to several normalization schemes. The perceptual and the acoustic representations were compared using regression techniques. A zscore-transformation of the raw data appeared to resemble the perceptual data."
146609,21258,9804,Rarefaction gestures and coarticulation in Mangetti Dune !Xung clicks,2009,"We provide high-speed ultrasound data on the four Mangetti DuneXung clicks. The posterior constriction is uvular for all four clicks—front uvular for ( � � ) and (� ) and back uvular for (�� ) and (�� ). (�� ) and (�� ) both involve tongue center lowering and tongue root retraction as part of the rarefaction gestures. The rarefaction gestures in (� � ) and (� ) involve tongue center lowering. Lingual cavity volume is largest for (�� ), followed by (�� ), (� ) and ( � � ). A tongue tip recoil effect is found following (�� ), but the effect is smaller than that seen in IsiXhosa in earlier studies."
2402776,21258,535,Recognition and understanding of meetings the AMI and AMIDA projects,2007,"The AMI and AMIDA projects are concerned with the recognition and interpretation of multiparty meetings. Within these projects we have: developed an infrastructure for recording meetings using multiple microphones and cameras; released a 100 hour annotated corpus of meetings; developed techniques for the recognition and interpretation of meetings based primarily on speech recognition and computer vision; and developed an evaluation framework at both component and system levels. In this paper we present an overview of these projects, with an emphasis on speech recognition and content extraction."
2788562,21258,9804,The interrelation between the stimulus range and the number of response categories in vowel categorization,2010,"We investigate the influence of the stimulus range and the number of response categories on the location of perceptual boundaries. The F1 continuum between Spanish /i/ and /e/ was presented to Peruvian listeners in three ranges. Half of the listeners could classify the tokens as /i/ and /e/, the other half chose from the five Spanish vowels. A boundary shift between /i/ and /e/ was observed as a function of the stimulus range, which was larger when listeners were given only two response categories. These results are interpreted as an effect of listeners’ category expectations on speech perception."
187037,21258,9804,Using X-grams for speech-to-speech translation,2002,"In this paper, a statistical speech-to-speech translation system, developed at TALP during the last months, is presented. By adapting well-known speech recognition techniques to the specific translation setting, th e system is able to integrate speech sign al into a fin ite state tran sdu cer th at tran slates statistically domain-constrained Span ish sen tences into En glish on es."
86028,21258,9804,Local word confidence measure using word graph and n-best list.,2005,"This paper presents some confidence measures for large vocabulary speech recognition which are based on word graph or N-Best List structures. More and more applications need fast estimation of any measures in order to stay real-time. We propose some simple and fast measures, locally computed, that can be directly used within the first decoding recognition process. We also define some other measures combining word graph and N-Best List. Experimental results on a French broadcast news corpus are also presented."
2765764,21258,9804,Phoneme resistance during speech-in-speech comprehension,2012,This study investigates masking effects occurring during speech comprehension in the presence of concurrent speech signals. We examined the differential effects of 4- to 8-talker babble (natural speech) or babble-like noise (reversed speech) on word identification. We measured phoneme identification rates. Results showed that different types of linguistic information can interfere with speech recognition and that different resistances are observed for different phonemes depending on interfering noise.
397102,21258,9804,"Nucleus position within the intonation phrase : a typological study of English, Czech and Hungarian",2010,"In this paper we examine cases of non-final nucleus (or#R##N#sentence stress) in English, Czech and Hungarian. These three languages differ substantially with respect to word order rules, prosodic plasticity (ability to signal information structure by shifting the nucleus) and the degree of grammaticalization in nucleus position. Recordings of parallel texts are studied with the aim to quantify different categories of shifts, as well as inter-speaker agreement in the position of the nucleus."
2518548,21258,21089,A Speech-based Just-in-Time Retrieval System using Semantic Search,2011,"The Automatic Content Linking Device is a just-in-time document retrieval system which monitors an ongoing conversation or a monologue and enriches it with potentially related documents, including multimedia ones, from local repositories or from the Internet. The documents are found using keyword-based search or using a semantic similarity measure between documents and the words obtained from automatic speech recognition. Results are displayed in real time to meeting participants, or to users watching a recorded lecture or conversation."
2419593,21258,21089,IRIS: a Chat-oriented Dialogue System based on the Vector Space Model,2012,"This system demonstration paper presents IRIS (Informal Response Interactive System), a chat-oriented dialogue system based on the vector space model framework. The system belongs to the class of example-based dialogue systems and builds its chat capabilities on a dual search strategy over a large collection of dialogue samples. Additional strategies allowing for system adaptation and learning implemented over the same vector model space framework are also described and discussed."
374451,21258,9804,Segmental Durations Predicted With a Neural Network,2003,"This paper presents a segmental durations’ model applied to the European Portuguese language for TTS purposes. The model is based on a feed-forward neural network, trained with a back-propagation algorithm, and has as input a set of phonological and contextual features, automatically extracted from the text. The relative importance of each feature, concerning the correlation with segmental durations and improvements in the performance of the model, is presented. Finally the model is evaluated objectively and subjectively by a perceptual test."
2627578,21258,8840,Turn-taking phenomena in incremental dialogue systems,2015,"In this paper, a turn-taking phenomenon taxonomy is introduced, organised according to the level of information conveyed. It is aimed to provide a better grasp of the behaviours used by humans while talking to each other, so that they can be methodically replicated in spoken dialogue systems. Five interesting phenomena have been implemented in a simulated environment: the system barge-in with three variants (resulting from either an unclear, an incoherent or a sufficient user message), the feedback and the user barge-in. The experiments reported in the paper illustrate that how such phenomena are implemented is a delicate choice as their impact on the system’s performance is variable."
528578,21258,9804,Exploiting models intrinsic robustness for noisy speech recognition.,2004,"We propose in this paper an original approach to build masks in the framework of missing data recognition. The proposed soft masks are estimated from the models themselves, and not from the test signal as it is usually the case. They represent the intrinsic robustness of model's log-spectral coefficients. The method is validated with cepstral models, on two synthetic and two real-life noises, at different signal-to-noise ratios. We further discuss how such masks can be combined with other signal-based masks and noise compensation techniques."
2728571,21258,9804,Improving Mandarin Prosodic Boundary Prediction with Rich Syntactic Features,2014,"Previous researches indicated that the performance of automatic prosodic boundary labeling benefited from syntactic phrase information for Mandarin. However, the influence of other syntactic features such as dependency has not been studied in-depth yet, especially on large scale corpus. This paper demonstrates the usefulness of rich syntactic features for Mandarin phrase boundary prediction. Both syntactic phrase and dependency features are considered in our methods. The experimental results show that rich syntactic features improve the performance of prosodic boundary prediction effectively. Index Terms: prosodic boundary, syntactic feature, syntactic phrase structure, dependency."
32491,21258,9804,Aspects of Pharyngealized Phonemes in Arabic Using Articulography,2008,"In this paper, we present preliminary results of an articulatory study of pharyngealized phonemes in Arabic using an electromagnetic articulograph (AG500). We compared the articulations of pharyngealized phonemes to non-pharyngealized ones. The articulation of the tongue was tracked by four sensors glued onto the tongue. A corpus of several CVCVCVs was recorded using the AG500, labeled, and analyzed. The main finding of this work is that while the main articulation of the tongue is actually moving towards a dental position, the secondary articulation of backing of the tongue can be observed."
41333,21258,9804,Aspects of Visual Speech in Arabic,2007,"In this paper, we present a study of visual speech in Arabic. More specifically, we performed a lipreading recognition experiment on Arabic, where a set of consonant-vowel stimuli were presented as visual-only speech and participants were asked to report what they recognized. The overall lipreading scores were consistent with other experiments in other languages. The resulting consonant confusion matrix shows that some of the phonemes were well discriminated, however, for others it depends on the context. Results are discussed based on the category of phonemes and the vowel context."
15344,21258,9804,Do Humans and speaker verification system use the same information to differentiate voices,2009,The aim of this paper is to analyze the pairwise comparisons of voices by a speaker verification system (ALIZE/Spk) and by human. A database of familial groups of 24 speakers was created. A single sentence was chosen for the perception test. The same sentence was used the test signal for the ALIZE/Spk trained on another part of the corpus. Results shows that the voice proximities within a familial group were well recovered in the speaker representation by ALIZE and much less returned in the representation from perception test
214502,21258,9804,Neighborhood density and neighborhood frequency effects in French spoken word recognition,2007,"According to activation-based models of spoken word recognition, words with many and high frequency neighbors are processed more slowly than words with few and low frequency neighbors. Because empirical support for inhibitory neighborhood effects comes mainly from studies conducted in English, the effects of neighborhood density and neighborhood frequency were examined in French. As typically observed in English, we found that words residing in dense neighborhoods are recognized more slowly than words residing in sparse neighborhoods. Moreover, we showed that words with higher frequency neighbors are processed more slowly than words with no higher frequency neighbors. Implications of theses results for spoken word recognition are discussed."
1786554,21258,65,Smoothing human-robot speech interaction with blinking-light expressions,2008,We propose a method to enable smooth speech interactions between a user and a robot. Our method is based on subtle expression whereby a robot blinks a small LED attached to its chest. We performed experiments in which participants played a last-and-first games and counted the number of repetitions made by the participants and analyzed their impression of the game and the robot. The experimental results suggested that the blinking-light could prevent utterance collisions between a user and a robot and could create familiar and attentive impressions about the game on users.
2350909,21258,535,A study on Hidden Structural Model and its application to labeling sequences,2009,"This paper proposes Hidden Structure Model (HSM) for statistical modeling of sequence data. The HSM generalizes our previous proposal on structural representation by introducing hidden states and probabilistic models. Compared with the previous structural representation, HSM not only can solve the problem of misalignment of events, but also can conduct structure-based decoding, which allows us to apply HSM to general speech recognition tasks. Different from HMM, HSM accounts for the probability of both locally absolute and globally contrastive features. This paper focuses on the fundamental formulation and theories of HSM. We also develop methods for the problems of state inference, probability calculation and parameter estimation of HSM. Especially, we show that the state inference of HSM can be reduced to a quadratic programming problem. We carry out two experiments to examine the performance of HSM on labeling sequences. The first experiment tests HSM by using artificially transformed sequences, and the second experiment is based on a Japanese corpus of connected vowel utterances. The experimental results demonstrate the effectiveness of HSM."
2308586,21258,535,Back-off action selection in summary space-based POMDP dialogue systems,2009,"This paper deals with the issue of invalid state-action pairs in the Partially Observable Markov Decision Process (POMDP) framework, with a focus on real-world tasks where the need for approximate solutions exacerbates this problem. In particular, when modelling dialogue as a POMDP, both the state and the action space must be reduced to smaller scale summary spaces in order to make learning tractable. However, since not all actions are valid in all states, the action proposed by the policy in summary space sometimes leads to an invalid action when mapped back to master space. Some form of back-off scheme must then be used to generate an alternative action. This paper demonstrates how the value function derived during reinforcement learning can be used to order back-off actions in an N-best list. Compared to a simple baseline back-off strategy and to a strategy that extends the summary space to minimise the occurrence of invalid actions, the proposed N-best action selection scheme is shown to be significantly more robust."
1801533,21258,235,Almost Flat Functional Semantics for Speech Translation,2008,"We introduce a novel semantic representation formalism, Almost Flat Functional semantics (AFF), which is designed as an intelligent compromise between linguistically motivated predicate/argument semantics and ad hoc engineering solutions based on flat feature/value lists; the central idea is to tag each semantic element with the functional marking which most closely surrounds it. We argue that AFF is well-suited for medium-vocabulary speech translation applications, and describe simple and general algorithms for parsing, generating and performing transfer using AFF representations. The formalism has been fully implemented within a medium-vocabulary interlingua-based Open Source speech translation system which translates between English, French, Japanese and Arabic."
1899767,21258,535,Automatic detection of contrastive elements in spontaneous speech,2007,"In natural speech people use different levels of prominence to signal which parts of an utterance are especially important. Contrastive elements are often produced with stronger than usual prominence and their presence modifies the meaning of the utterance in subtle but important ways. We use a richly annotated corpus of conversational speech to study the acoustic characteristics of contrastive elements and the differences between them and words at other levels of prominence. We report our results for automatic detection of contrastive elements based on acoustic and textual features, finding that a baseline predicting nouns and adjectives as contrastive performs on par with the best combination of features. We achieve a much better performance in a modified task of detecting contrastive elements among words that are predicted to bear pitch accent."
2655309,21258,30,The use of technology in Suicide Prevention.,2015,"Suicide is one of the leading causes of death globally, and is notably a significant cause of death amongst young people. A suicide outcome is a complex combination of personal, social, and health factors, and therefore suicide prevention is a challenge, requiring a systems approach incorporating public health strategies, screening at-risk individuals, targeted interventions, and follow-up for suicide survivors and those bereaved by suicide. Engineering practice has been implicated in the hindrance of the adoption of suicide prevention strategies, such as installing safety barriers at the Golden Gate Bridge, however technological developments offer new opportunities in suicide prevention, and the potential to reduce the number of deaths by suicide. We present an overview of current technological developments which are facilitating research in the field of suicide prevention, including multiple modes of screening such as network analysis of mobile-phone collected connectivity data, automatic detection of suicidality from social media content, and crisis detection from acoustic variability in speech patterns. The current field of mhealth apps for suicide prevention is assessed, and an innovative app for an Indigenous population is presented. From this overview, future challenges - technical and ethical - are discussed. Language: en"
2532471,21258,8228,Low Bit-Rate High-Quality Audio Encoding and Low Complexity Bandwidth Extension Technologies for ITU-T G.718/G.718-SWB,2011,"We worked toward development of a low bit-rate encoding algorithm and a low-complexity encoding algorithm for G.718/G.718-SWB, a new standard of International Telecommunication Union - Telecommunication Standardization Sector (ITU-T). Many key technologies including Band-Selective Shape-Gain Coding (BS-SGC) were developed for the standard. BS-SGC can encode the audio signal efficiently at a low bit-rate. Great improvement was confirmed by official subjective testing. Furthermore, our low-complexity technologies can reduce the computational complexity substantially with no quality degradation of the output signal. They were adopted as featured components of G.718/G.718-SWB, which were approved, respectively, in June 2008 (G.718) and in March 2010 (G.718-SWB)."
2064340,21258,9463,Automatic Generation of English Respellings,2013,"A respelling is an alternative spelling of a word in the same writing system, intended to clarify pronunciation. We introduce the task of automatic generation of a respelling from the word’s phonemic representation. Our approach combines machine learning with linguistic constraints and electronic resources. We evaluate our system both intrinsically through a human judgment experiment, and extrinsically by passing its output to a letterto-phoneme converter. The results show that the respellings generated by our system are better on average than those found on the Web, and approach the quality of respellings designed by an expert."
2672409,21258,535,Acoustic model training based on node-wise weight boundary model increasing speed of discrete neural networks,2015,"Our purpose is to realize discrete neural networks (NNs), whose some parameters are discretized, as a low-resource and fast NNs for acoustic models. Two essential problems should be tackled for its realization; 1) the reduction of discretization errors and 2) the implementation method for fast processing. We propose a new parameter training algorithm for 1) and an implementation using look-up table (LUT) on general-purpose CPUs for 2), respectively. The former can set proper boundaries of discretization at each node of NNs, resulting in the reduction of discretization error. The latter can reduce the memory usage of NNs within the cache size of CPU by encoding parameters of NNs. Experiments with 2-bit discrete NNs showed that our algorithm maintained almost the same word accuracy as 8-bit discrete NNs and achieved a 40% increase in speed of the NN's forward calculation."
2619305,21258,235,Explorations in the Speakers' Interaction Experience and Self-Assessments,2012,"The paper focuses on the interlocutors' self-evaluation in Finnish and Estonian first encounter dialogues. It studies affective and emotive impressions of the participants after they have met the partner for the first time, and presents comparison of the evaluation along the gender, age and education parameters. The results bring forward some statistically significant differences between the two groups, and point to different, culturally determined evaluation scales. The paper discusses the impact of the findings on the complex issues related to the evaluation of automatic interactive systems, and carries over to such applications as intelligent training and tutoring systems, and interactions with robots, encouraging further studies on the interlocutors' engagement in interaction and their evaluation of the success of the interaction."
1990052,21258,9463,Atypical Prosodic Structure as an Indicator of Reading Level and Text Difficulty,2013,"Automatic assessment of reading ability builds on applying speech recognition tools to oral reading, measuring words correct per minute. This work looks at more fine-grained analysis that accounts for effects of prosodic context using a large corpus of read speech from a literacy study. Experiments show that lower-level readers tend to produce relatively more lengthening on words that are not likely to be final in a prosodic phrase, i.e. in less appropriate locations. The results have implications for automatic assessment of text difficulty in that locations of atypical prosodic lengthening are indicative of difficult lexical items and syntactic constructions."
2390778,21258,535,Acoustic emotion recognition: A benchmark comparison of performances,2009,"In the light of the first challenge on emotion recognition from speech we provide the largest-to-date benchmark comparison under equal conditions on nine standard corpora in the field using the two pre-dominant paradigms: modeling on a frame-level by means of Hidden Markov Models and supra-segmental modeling by systematic feature brute-forcing. Investigated corpora are the ABC, AVIC, DES, EMO-DB, eNTERFACE, SAL, SmartKom, SUSAS, and VAM databases. To provide better comparability among sets, we additionally cluster each database's emotions into binary valence and arousal discrimination tasks. In the result large differences are found among corpora that mostly stem from naturalistic emotions and spontaneous speech vs. more prototypical events. Further, supra-segmental modeling proves significantly beneficial on average when several classes are addressed at a time."
22739,21258,9804,Direct Acoustic Feature Using Iterative EM Algorithm and Spectral Energy for Classifying Suicidal Speech,2007,"Abstract –Research has shown that the voice itself contains important information about immediate psychological state and certain vocal parameters are capable of distinguishing speaking patterns of speech signal affected by emotional disturbances (i.e., clinical depression). In this study, the GMM based feature of the vocal tract system response and spectral energy have been studied and found to be a primary acoustic feature set for separating two groups of female patients carrying a diagnosis of depression and suicidal risk. Index Terms : suicidal speech, depression, vocal tract, energy 1. Introduction Suicide is a common outcome in persons with serious mental disorders. However, it remains a phenomenon that is underresearched and poorly understood. Moreover, methods to help to identify persons who are at an elevated risk are sorely needed in clinical practice. This study represents an attempt to identify characteristic vocal patterns in persons with imminent suicidal potential which could lead to the development of new technology to aid in the assessment of suicidal potential. This project is to study vocal acoustic properties in suicidal states. Two study groups will be contrasted in this work: near–term suicidal and depressed. In the early 1980’s the Silvermans began to collect and analyze recorded suicide notes and interviews made shortly before suicide attempts. Their results suggested that voice can provide important information about immediate psychological state. They have described that the depressed patients have the same vocal speech as suicidal patients but the tonal quality of speech changes significantly when patients become suicidal. As reported in [1], [2], [3], the emotional arousal produces changes in the speech production scheme by affecting the respiratory, phonatory, and articulatory processes that in turn are encoded in the acoustic signal. The emotional content of the voice can be associated with acoustical variables such as thelevel, range, contour, and perturbation of the fundamental frequency, the distribution of energy in frequency spectrum, the location, bandwidth and intensity of formant frequencies, and a variety of temporal measures. The measurable change in vocal parameters affected by emotional disturbances is able to be evaluated by utilizing an appropriate speech processing approach associated with certain acoustic features. Researches have shown that depression has a major effect on the acoustic characteristics of voice when compared to the normal controls. Certain changes in acoustic properties of the affective speech are possibly specific to the near–term suicidal states in persons. In the published pilot studies [4], [6], analytical techniques have been developed to determine if subjects were in one of three mental states: healthy control, non–suicidal depressed, or high–risk suicidal. Several studies have used the vocal tract (VT) measures (i.e., formants) and prosody to classify the emotional disorders. France et. al [4] found the formants and percentages of total energy in frequency spectrum over a frequency range of 0–2,000 Hz to be the most distinguishing acoustic feature set for classifying groups of control, major depressed, and suicidal subjects. These features were recently re–investigated and extracted from a new speech database recorded in a better controlled environment. The experimental results have shown that the investigated feature set was still found as powerful acoustic discriminators in distinguishing suicidal, depressed and remitted patients [1]. Ozdas et. al [6] used a set of low order mel–cepstral coefficients to identify speakers who were diagnosed to be major depressed, suicidal, and normal by a psychiatrist. Her comparative result of classification performance as a measure of group separation was significantly high. Moore et al. compared the results of speaking pattern recognition by employing the prosody, formant and glottal ratio/spectrum in classifying normal controls and depressed patients. The optimal classifiers designated by the glottal ratio/spectrum and formant performed most effectively to separate two individual groups [7]. In this work, the characterization of the vocal tract system and distribution of energy in frequency spectrum of speech signal are focused. The speech processing algorithm to solve a specific problem of extracting the vocal features representing the characteristics of the VT system response is implemented and proposed. The estimate of smoothed magnitude spectrum is determined via the cepstrum analysis and the spectral structure contained in that magnitude spectrum is modeled by a mixture of Gaussian density components whose model parameters are estimated via a well–known “Expectation–Maximization” (EM) algorithm.This paper is organized as follows: Section 2 provides the descriptions of database, feature extraction, primary feature selection, and performance evaluation. Section 3 presents the results. Finally, section 4 concludes all findings from this work."
235165,21258,9804,Preservation of speech spectral dynamics enhances intelligibility.,2013,"Speech is the most important communication modality for human interaction. Automatic speech recognition and speech synthesis have extended further the relevance of speech to man-machine interaction. Environment noise and various distortions, such as reverberation and speech processing artifacts, reduce the mutual information between the message modulated inthe clean speech and the message decoded from the observed signal. This degrades intelligibility and perceived quality, which are the two attributes associated with quality of service. An estimate of the state of these attributes provides important diagnostic information about the communication equipment and the environment. When the adverse effects occur at the presentation side, an objective measure of intelligibility facilitates speech signal modification for improved communication.The contributions of this thesis come from non-intrusive quality assessment and intelligibility-enhancing modification of speech. On the part of quality, the focus is on predictor design for limited training data. Paper A proposes a quality assessment model for bounded-support ratings that learns efficiently from a limited amount of training data, scales easily with the sampling frequency, and provides a platform for modeling variations in the individual subjective ratings. The predictive performance of the model for the mean of the subjective quality ratings compares favorably to the state-of-art in the field. Patterns in the spread of the individual ratings are captured in the feature space of the training data.Paper B focuses on enhancing predictive performance for the mean of the quality variable when the signal feature space is sparsely sampled by the training data. Using a Gaussian Processes framework, the deterministic signal-based feature set is augmented with a stochastic feature that is hypothesized to be jointly distributed with the target quality rating. An uncertainty propagation mechanism ensures that the variance of this feature is reflected in the prediction. The proposed architecture can take advantage of i) data that cannot be pooled due to subjective test protocol incompatibility and ii) models trained on data that are no longer available.With respect to intelligibility enhancement, a hierarchical perspective of the speech communication process, extended from foundational work in the field, is used in paper C to create a unified framework for method analysis and comparison. A high-level intelligibility measure related to the probability for correct recognition is derived using a hit-or-miss distortion criterion in the transcription domain. The measure is used to optimize two speech modifications at different levels of the message encoding hierarchy leading to significantly enhanced intelligibility in noise. The conceptual novelty of the method comes at the cost of higher complexity and the requirement for additional information including message transcription, sound segmentation, and a model of speech.Mapping the high-level measure to a lower level takes away the need for additional information and preserves asymptotically high-level optimality. Two methods are proposed to reduce degradation in the accuracy of the spectral dynamics due to additive noise. The focus of paper D is dynamics preservation in a range that is lower-bounded by an optimal band-power threshold. The performance of the method is competitive but allows for improvement in power efficiency. This issue is addressed in paper E which proposes and optimizes a distortion measure for spectral dynamics leading to a significant increase in intelligibility. Use of functional optimization techniques allows for families of solutions, among which are dynamic range compressors adaptive to the statistics of the speech and the noise."
247850,21258,9804,Determining Optimal Features for Emotion Recognition from Speech by applying an Evolutionary Algorithm,2010,"Abstract The automated recognition of emotions from speech is a chal-lenging issue. In order to build an emotion recognizer well de-ﬁned features and optimized parameter sets are essential. Thispaper will show how an optimal parameter set for HMM-basedrecognizerscanbefoundbyapplyinganevolutionaryalgorithmon standard features in automated speech recognition. For this,we compared different signal features, as well as several archi-tectures of HMMs. The system was evaluated on a non-acteddatabase and its performance was compared to a baseline sys-tem. We present an optimal feature set for the public part of theSmartKom database.Index Terms: Emotion Recognition, Evolutionary Algorithms,Feature Optimization, Hidden-Markov Models 1. Introduction The interaction between men and machines using language isnowadays becoming more and more self-evident, but machinesstill lack of many human abilities which would considerablysimplify communication and would also help to increase theacceptance of such systems. For some time, research activitiesalso focus stronger on the emotional aspect of speech. Exploit-ing information about the emotional state of a user, machinescan be enabled to adapt their dialog strategy online, depend-ing on the user’s emotions and hence react in a more appro-priate and empathic manner. As emotion recognition in manyapplications goes hand in hand with automated speech recog-nition (ASR) it would be favorable to make use of the samefeatures or even a subset thereof. Especially small devices likesmart phones or PDAs, which do not provide huge computa-tionalpowerwouldbeneﬁtfromsuchasparsefeatureapproach.Parallel research of other groups bases on pooling together(high level) features including the application of brute forcemethods in order to fully exploit the feature space (compare[1]). This paper however describes an evolutionary strategy(ES) of ﬁnding an optimal sparse feature set, given not morethan the common acoustic features used in ASR. Applying anES has the advantage of self adaptation of its parameters andis able to ﬁnd optimal parameter constellations in high dimen-sional search spaces and further gives insights into the rele-vance of each parameter in terms of the model’s accuracy. Es-pecially in case of many parameters with unknown relation-ships ES quickly avoids wasting time on generating and test-ingunsuitableparametercombinationsastheevolutionaryforceminimizes the probability of the evolvement of such combina-tions effectively. In ASR Mel-Frequency-Cepstral-Coefﬁcients(MFCCs) have established as a basic feature in order to trainphoneme based recognizers. As we do not want additional pa-rameters to be extracted from the speech signal we concentrateonly on MFCCs, which have also proven to perform well inemotion recognition during the Emotion Challenge within In-terspeech 2009 (compare [2]).Thispaperisstructuredasfollows: InSection2wedescribethespontaneous database and the emotions we want to recognize.Section 3 describes, which parameters we investigated and inwhich range they were allowed to change during the evolutionprocess. Section4introducestheevolutionaryalgorithm,showshow ﬁtness is measured and the population evolves over time.TheresultsarepresentedinSection5andcomparedtoourbase-line recognizer. Finally Section 6 summarizes our ﬁndings andgives an outlook."
122833,21258,9804,MMSE-Based Channel Error Mitigation for Distributed Speech Recognition,2001,"Abstract Recently, the ﬁrst version of an ETSI standard for DistributedSpeech Recognition has been proposed. The main beneﬁt ofthis approach is the possibility of maintaining a high recogni-tion performance when accessing remote information systems.The use of a digital channel for transmission of the encodedspeech parameters implies the introduction of several channeldistortions. Our paper deals with the mitigation of such dis-tortions. We study the application of MMSE estimation to thisproblem and propose a new MMSE procedure that obtains theprobabilities needed for MMSE from a forward-backward al-gorithm. We show that MMSE estimation obtains better per-formance than the mitigation algorithm described in the ETSIstandard under different channel conditions. 1. Introduction Very recently, the problem of recognizing speech transmittedover digital channels has been addressed and an ETSI standardhas been elaborated (ETSI-ES-201-108 [1]). The AURORAworking group was the responsible for developing this ﬁrst s-tandard and a Distributed Speech Recognition (DSR) approach,that is, a local front-end and a remote back-end, was adopted.There are clear advantages in this approach: voice features arenot affected by the speech coder, more robustness against chan-nel errors, and access from different networks with a guaranteedperformance.An important issue being currently addressed is robustnessagainst adverse environments (in which the front-end of a D-SR system must operate). Also, robustness against transmissionchannel errors must be taken into account. This is not exclusive-ly a channel coding problem. During the last years, several er-ror mitigation (or concealment) techniques, that provide an im-proved decoding, have been studied for speech or image coding[2] [3]. These techniques usually exploit some kind of knowl-edge about the encoded parameters which is embedded in a softdecoding scheme. In the case of DSR, we ﬁnd that the encodedparameters (MFCCs in the current version of the standard) dif-fer from those normally utilized in speech coding. Moreover,the goal of DSR is completely different from subjective visionor hearing, since at the back-end we ﬁnd an automatic speechrecognition system. Therefore, the development of speciﬁc mit-igation algorithms for DSR is clearly justiﬁed. The ETSI DSRstandard already includes a basic mitigation algorithm that hasbeen shown quite effective for medium and good quality chan-nels on TETRA and GSM environments [4]. Error mitigationcan be also interesting not only for DSR, but also for other ap-plications such as speech reconstruction from the transmittedDSR speech features.In this paper, we address the problem of mitigating channelerrors, studying the performance of mitigation algorithms basedon an MMSE (Minimum Mean Square Error) philosophy. Inparticular, we propose a new MMSE mitigation algorithm thatutilizes correct frames received before and after the frame be-ing estimated. The different proposed techniques are develope-d using the AURORA ETSI standard front-end, although theycould be straightforwardly extended to other encoding schemes.The proposed mitigation algorithms affect only to the decodingstage of the ETSI standard. For the sake of simplicity, we willassume a BPSK modulation and test two different data chan-nels (AWGN and bursty). The recognition experiments are per-formed on the Aurora-2 speech database.The paper is organized as follows. First, we brieﬂy summa-rize the ETSI DSR standard and its error mitigation algorithm.Sections 3 and 4 are devoted to the study of several mitigationtechniques over AWGN and bursty channels, respectively. Fi-nally, the conclusions of this work are summarized."
176059,21258,9804,Bayes Risk-based Optimization of Dialogue Management for Document Retrieval System with Speech Interface,2007,"Abstract We propose an efﬁcient dialogue management for an informa-tionnavigationsystembasedonadocument knowledge base. Itis expected that incorporation of appropriate N-best candidatesofASRandcontextualinformationwillimprovethesystemper-formance. The system also has several choices in generatingresponses or conﬁrmations. In this paper, this selection is opti-mizedasminimizationofBayesriskbasedonrewardforcorrectinformation presentation and penalty for redundant turns. Wehave evaluated this strategy with our spoken dialogue system“Dialogue Navigator for Kyoto City”, which also has question-answering capability. Effectiveness of the proposed frameworkwas conﬁrmed in the success rate of retrieval and the averagenumber of turns for information access. Index Terms : spoken dialogue system, dialogue management,Bayes risk 1. Introduction The target of spoken dialogue systems is being extended fromsimple databases such as ﬂight information to general docu-ments including manuals and newspaper articles[1]. In suchsystems, the automatic speech recognition (ASR) result of theuser utterance is matched against a set of target documents us-ing the vector space model, and documents with high matchingscores are presented to the user. These kinds of applicationsare expected to be useful especially when retrieving informa-tion with keyboardless devices such as a PDA, a tablet PC anda car navigation system. We have developed “Dialogue Navi-gator for Kyoto City”, which can make interactive guidance byincorporating the question-answering (QA) function as well asgeneral document retrieval.In these systems, making conﬁrmation is needed to elim-inate misunderstandings caused by ASR errors, but so manyredundant conﬁrmations are bothersome to users. There havebeenmanystudiesthatdealwithefﬁcientdialoguemanagementto make conﬁrmation[2, 3, 4]. However, most of them are de-signed for relational database (RDB) query tasks, which havea deﬁnite set of keywords, and they are not directly applica-ble to document retrieval tasks, in which every word is used inmatching. In document retrieval tasks, therefore, it is more rea-sonable to make conﬁrmation considering the conﬁdence scoreof retrieval.In addition, our system classiﬁes user utterances to queriesand questions, and generates appropriate responses for respec-tive inputs. Unlike conventional QA tasks, such as TREC QATrack[5], it is not obvious whether the utterance is a query orU1: Please explain the Golden Pavilion.S1: The Golden Pavilion is one of the buildings in theRokuon-ji in Kyoto, and is the main attraction of thetemple sites. The entire pavilion except the basementﬂoor is covered with pure gold leaf.U2: When was it built?S2: The Golden Pavilion was originally built in 1397to serve as a retirement villa for Shogun AshikagaYoshimitsu.U3: Then, please tell me its history.......Figure 1: Example dialogue of Dialogue Navigator for KyotoCitya question. In addition, an exact answer for a question doesnot necessarily exist in the document sets. Therefore, it is notalways optimal to respond the question with its answer alone.In this paper, we address the extension of conventional op-timization methods of dialogue management, to be applicableto general document retrieval tasks with QA function. Specif-ically, we propose a dialogue management that optimizes thechoicesinresponsegenerationbyminimizingBayesrisk,basedon reward for correct information presentation and penalty forredundant turns, which are deﬁned by the score of documentretrieval and answer extraction."
398569,21258,9804,Characteristics of a Low Reject Mode Speaker Verification System,2002,"In this work, speaker characteristic modeling has been applied in the fields of automatic speech recognition (ASR) and automatic speaker verification (ASV). In ASR, a key problem is that acoustic mismatch between training and test conditions degrade classification per- formance. In this work, a child exemplifies a speaker not represented in training data and methods to reduce the spectral mismatch are devised and evaluated. To reduce the acoustic mismatch, predictive modeling based on spectral speech transformation is applied. Follow- ing this approach, a model suitable for a target speaker, not well represented in the training data, is estimated and synthesized by applying vocal tract predictive modeling (VTPM). In this thesis, the traditional static modeling on the utterance level is extended to dynamic modeling. This is accomplished by operating also on sub-utterance units, such as phonemes, phone-realizations, sub-phone realizations and sound frames. Initial experiments shows that adaptation of an acoustic model trained on adult speech significantly reduced the word error rate of ASR for children, but not to the level of a model trained on children’s speech. Multi-speaker-group training provided an acoustic model that performed recognition for both adults and children within the same model at almost the same accuracy as speaker-group dedicated models, with no added model complexity. In the analysis of the cause of errors, body height of the child was shown to be correlated to word error rate. A further result is that the computationally demanding iterative recognition process in standard VTLN can be replaced by synthetically extending the vocal tract length distribution in the training data. A multi-warp model is trained on the extended data and recognition is performed in a single pass. The accuracy is similar to that of the standard technique. A concluding experiment in ASR shows that the word error rate can be reduced by ex- tending a static vocal tract length compensation parameter into a temporal parameter track. A key component to reach this improvement was provided by a novel joint two-level opti- mization process. In the process, the track was determined as a composition of a static and a dynamic component, which were simultaneously optimized on the utterance and sub- utterance level respectively. This had the principal advantage of limiting the modulation am- plitude of the track to what is realistic for an individual speaker. The recognition error rate was reduced by 10% relative compared with that of a standard utterance-specific estimation technique. The techniques devised and evaluated can also be applied to other speaker characteristic properties, which exhibit a dynamic nature. An excursion into ASV led to the proposal of a statistical speaker population model. The model represents an alternative approach for determining the reject/accept threshold in an ASV system instead of the commonly used direct estimation on a set of client and impos- tor utterances. This is especially valuable in applications where a low false reject or false ac- cept rate is required. In these cases, the number of errors is often too few to estimate a reli- able threshold using the direct method. The results are encouraging but need to be verified on a larger database."
211176,21258,9804,Some Practical Considerations in the Deployment of a Wireless-Communication Interactive Voice Response System,2001,"Abstract In this paper, we describe the design procedure for a wirelesscommunication interactive voice response system. The applica-tion must work in a very noisy environment which has imposedmany design constraints. Wewilladdress the sensible aspects ofthree components of the application: the voice activity detector(VAD),the automatic speech recognition (ASR)system, and theconﬁdence measure (CM) determination. In order to get a sat-isfactory product, it has been necessary to reduce the importantmismatch between available linguistic and acoustic resourcesand the operational environment. Adaptation techniques for theacoustic models of the speech recognition system have provento be effective to speed up the application deployment time. 1. Introduction It is widely accepted that speech technology is nowadays ma-ture enough for the deployment of practical, commercial ap-plications in the real-world. In the last years, many servicesand products have shown up specially in the telecommunicationarena[1]. Also, many resources as speech and text databases,recognition engines, text-to-speech engines are available to thedevelopers for building up applications and products [2]. Nev-ertheless, the concourse of highly specialized and well-trainedpersonnel is required in order to get a quality product. Even af-ter a careful design of the project speciﬁcations and an adequateselection of linguistic and software components, an error-and-trial strategy must be conducted in order to tune and debug allthe components. This is even more delicate when there is a sig-niﬁcantmismatch between theavailable linguisticresources andthe actual acoustic and linguistic environment. This is the caseof the project we will describe in this paper. The applicationitself, a command-control system, is simple in nature, but theacoustic environment (a very noisy lumberyard), the transmis-sion channel (a half-duplex radio channel), and the two possiblelanguages (Galician and Spanish) are factors that make very dif-ﬁcult the transfer of technology from off-the-shelf componentsto a complete product.A state-of-the-art system may perform poorly when the testdata are collected under a totally different environmental con-dition. Regarding to the possible mismatches, both linguisticand acoustic mismatches might occur. A linguistic mismatchis mainly caused by incomplete task speciﬁcation, inadequateknowledge representations, and insufﬁcient training data. Onthe other hand, an acoustic mismatch between training and op-erational conditions arises, being in this case changes in trans-ducers, channel, speaker environment, background noise, etc.We will address how some techniques must be applied inorder to improve the performance and ergonomy of the appli-cation. We have selected three subsystems that required of spe-cial attention: the voice activity detector (VAD), the automaticspeech recognition (ASR) system, and the conﬁdence measure(CM) determination. Before describing the most relevant as-pect of each of them, we will show an overview of the wholeapplication. Some conclusions will be drawn at the end of thepaper."
181016,21258,9804,Sentence Boundary Detection of Spontaneous Japanese using Statistical Language Model and Support Vector Machines,2006,"Abstract This paper presents two different approaches utilizing sta-tisticallanguagemodel(SLM)andsupportvectormachines(SVM) for sentence boundary detection of spontaneousJapanese. In the SLM-based approach, linguistic likeli-hoods and occurrence of pause are used to determine sen-tence boundaries. To suppress false alarms, heuristic pat-terns of end-of-sentence expressions are also incorporated.On the other hand, SVM is adopted to realize robust classi-ﬁcation against a wide variety of expressions and speechrecognition errors. Detection is performed by an SVM-based text chunker using lexical and pause information asfeatures. We evaluated these approaches on manual and au-tomatic transcription of spontaneous lectures and speeches,and achieved F-measures of 0.85 and 0.78, respectively. Index Terms : sentence boundary detection, spontaneousspeech, statistical language model, support vector ma-chines. 1. Introduction Recent advance of automatic speech recognition (ASR)technology, especially for spontaneous speech, enables var-ious applications such as spoken document archiving andretrieval, speech summarization and speech translation. Toorganizeaspokendocumentinastructuredformandtogiveuseful indices, transcriptions should be segmented into ap-propriate units like sentences. Moreover, these applicationsare usually built by combining an ASR system with naturallanguage processing (NLP) systems such as a parser and amachine translator, which often assume that input text is asentence. However,sentencesinspontaneousspeechareill-formed, and sentence boundaries are indistinct. Output textby ASR systems is just a sequence of words and has no ex-plicitsentenceboundaries, sothefurtherstepofsegmentingthe ASR output is required for these applications.Automatic boundary detection of spoken sentences hasbeen explored mainly on broadcast news (BN)tasks[1,2,3]and conversational telephone speech (CTS) tasks[3, 4, 5] inEnglish. As features for detection, pause, prosodic and lin-guistic information is often used. Most popular approach isa combination of prosodic and linguistic information[2, 3],which realizes high performance on BN and CTS tasks.Prosody-based approaches[1, 5] have also been investi-gated. Meanwhile, linguistic information is not used by it-self, since most of these works were performed on Englishdata, where cue words or expressions of sentence bound-aries are not easily deﬁned.In Japanese, cue expressions are typically observed atthe end of sentences and expected to be useful for bound-ary detection. However, variety of such expressions is solarge in spoken Japanese, that it is hard to collect sufﬁcientamountofdatafortrainingstatisticalmodelssuchasamax-imum entropy (ME) model. Moreover, many of cue expres-sions consist of particles, which are apparently difﬁcult tobedetectedinASR.Thus,robustnessforASRerrorsshouldalso be investigated.In this paper, we address two approaches of sen-tence boundary detection for spontaneous Japanese. Asframeworks of detection, we adopt and compare statisti-cal language model (SLM) and support vector machines(SVM). The proposed approaches are evaluated with reallectures and speeches included in the Corpus of Sponta-neous Japanese (CSJ)."
70552,21258,9804,Investigation of the Relationship between Turn-taking and Prosodic Features in Spontaneous Dialogue,2005,"Abstract In this study, we investigated the relationship between turn-taking and prosody. We considered that to interact smoothlyin real-time communication, speakers must show presignalsto turn-taking as prosodic features before turn edges. We at-tempted to discriminate the turn change by the decision treemethod using only prosodic features in turn-ﬁnal accentualphrases that include earlier positions compared with turn-ﬁnalmora. In the discrimination experiment, we used the corpus ofJapanese spontaneous dialogue, and deﬁned prosodic parame-ters such as F0 contour, power contour and duration. We com-pared the two parameter conditions for using parameters withand without the ﬁnal mora of turns. From the results, the ac-curacy under the conditions of not using the parameters of theﬁnal mora is 80%, which is not signiﬁcantly worse than the re-sult of 83% when using all parameters. Taking into accountonly prosody was used, we consider this result to be reasonablygood. 1. Introduction In real-time communication, we can interact very smoothly us-ing speech. There has been a growing appreciation of the im-portant role of prosody in human-human, and also in human-machine communication [1, 2]. Prosody has functions that en-able listeners to achieve real-time and easy understanding, andto control dialogue smoothly. Making effective use of prosodicinformation leads us to the expectations of improvements inthetechnologiesofspeechunderstanding,speechsynthesis,andspoken dialogue systems.In this study, we focus on the dialogue management func-tions of prosody with respect to turn-taking. There have beenmany previous studies on turn-taking and prosodic informa-tion, in various research ﬁelds. For example, intonation pat-terns at sentence boundaries are relevant to modality and dis-course functions [3, 4, 5, 6, 7, 8]. From another point of view,for practical applications such as human-machine dialogue sys-tems, prosodic features are used to detect suitable timing forturn-taking or backchannel [9, 10, 11, 12]. Most of these stud-ieshaveshownthatparticularcombinationsoflexical,syntacticand prosodic information in turn-ﬁnal can function as cues forsignalling that a speaker wants to keep the ﬂoor or wants to endthe turn.In order to judge whether it is possible to take the turn ornot, however, the hearer does not necessarily have to perceivethe speaker’s utterance to the last phoneme. We observed thatturn-taking proceeded very smoothly with minimal delay be-tween consecutive speaking turns. In some cases, there werealsosuccessfulturntransitionswithshortoverlap,called“latch-ing”. Therefore, when taking into account these phenomena, inaddition to the above cues at the edges of turns, it is consideredthat more global cues to turn-taking exist. The speakers mightshow presignals as prosodic features before the turn edge, sothat the listeners clearlyknow whether aspeaker wantsto ﬁnisha speaking turn at an earlier time before a possible transitionpoint. Moreover, in our previous study, we evaluated the effec-tiveness of prosodic features at earlier positions for estimatingthe syntactic structure [13, 14].Thus, in the present study, we aim to treat prosodic func-tions more positively with respect to turn-taking. We considerthat prosodic information might have some potential to morestrongly express a speaker’s attitude regarding turn-taking.From this point of view, we attempt to judge whether the turnchanged or not using only prosodic features. We focused onthe ﬁnal phrases of utterances which include not only turn-ﬁnalmora but also earlier positions of turn-ﬁnal, and also attempteddiscrimination under the condition of using all features exceptthe ﬁnal mora. We used the contours, heights and peaks of F0and the power, duration and speaking rate as the prosodic pa-rameters."
168419,21258,9804,DIARCA: A Component Approach to Voice Recognition,2001,"Abstract Current voice recognition systems tend to be implemented asa PC desktop facility. This model is not suitable for thegrowing complexities of present and future developments: Itis single-user, it is non portable, and it assumes theworkstation model, where all the CPU resources are supposedto be locally available. This work researches how a highperformance speech recognition system can be redesigned andimplemented as a time-critical network service shared throughordinary data transmission media with three main designgoals: Scalability, predictability and POSIX portability. Thewhole idea has been tested by rebuilding IVORY, a wellknown robust desktop voice recognition methodology, as adistributed component. 1. Introduction While Speech Processing and Recognition is a fieldexperiencing a rapid and promising expansion, the operating-system environments for the desktop PC still typically lack oftrue real-time support. To overcome this limitation, currentspeech recognition systems are confident on the workstationprinciple: all the CPU resources are always available to theapplication where they are embedded. This approach shows amain limitation: Its growing computational complexity.IVORY ([1], [7]), a stand-alone speech recognition system ofisolated words, gives figures of computational complexityaround 21 Mflop/s. Though this load is easily assumed bycurrent CPU's, continuous speech can raise the computingpower demand one order of magnitude. Noise cancellationdemands up to five or six times the power of the recognitionitself. Furthermore, new applications of speech processingdemand much more computing power. For instance, tracking asingle speaker by the Microphone Arrays technique shows acomputational complexity near 166 Mflop/s ([6]). Thoughtoday's PC microprocessors claim peak execution ratesexceeding 1 Gflop/s, regular DSP algorithms rarely result insuch a high performance. In our view, desktop speechprocessing is -and will always be- strongly limited by itscomputational complexity, nowadays constrained to thecomputing power of the average personal computer. This work was founded by CICYT and Junta de Extremaduraunder the TIC99-0609 (DIARCA) and CICYTEX IPRR98A039 projects respectively.Distributed computing should change this scenery.Ongoing developments on component based softwareengineering makes possible to envision a remote service ofDSP computing power for speech processing. It would allowto bring both to the current desktop PC and to the futureinternet appliances the more advanced developments on thefield. This work investigates the distribution of speechrecognition in the context of DIARCA, a research projectwhose aim is two-fold. Firstly, to distribute IVORY with threedesign goals: Scalability, predictability and POSIX portability.Secondly, to extend the results in order to support microphonearray developments. This work is about the first goal.Figure 1. IVORY: a Robust Voice Recognition system"
2676882,21258,9804,Contributions of F1 and F2 (F2') to the perception of plosive consonants,2011,"Abstract This study examined the contribution of F1 and F2 alone on the perception of plosive consonants in a CV context. Applying a 3-Bark spectral integration the F2 frequency was corrected for effects of proximity either to F1 or to F3, i.e., was replaced by F2’. Subjects used a two-dimensional Method of Adjustment to select the F1 and F2 consonant onset frequencies that led to a subjectively optimal percept of a predefined target CV. Results indicate that place prototypes are guided by F2 and are largely independent of F1. Nevertheless, while F2 alone is sufficient for segregating place prototypes for some consonants and vocalic contexts, it is insufficient for explaining the perception of place. Index Terms : speech perception, plosives, F2’, 3-Bark integration 1. Introduction Various perception experiments have shown that the third formant is essential for differentiating the place of articulation of consonants like /d/ and /g/ and that the slopes of the first and second formant are insufficient to convey this information. Harris et al. [1] have noted that although, in synthesized /CV/ experiments, the consonant /d/ cannot be obtained in the vocalic context /i/ without the third formant, it can be perceived in the /ae/ context. Similarly, Godfrey et al. [2] reported that the rate and/or the direction of third formant transition was necessary for perceiving the distinction between /d/ and /g/, whereas for the /b/-/d/ contrast the transition of the second formant was sufficient. In duplex perception, Mann et al. [3] also used F3 for the /d/-/g/ distinction. In contrast, Delattre et al. [4] were able to produce synthetic CV perceived as /bV/, /dV/, or /gV/ by changing the slopes of the first two formants alone. However, for /gu/, /go/ and /g/, they observed a different acoustic behavior compared to /gi/, /ge/, /g/ and /ga/ [5]. In production, Ohman [6] represented the [Co] transitions in the F2/F3 plane with different preceding vowels and identified three main regions, each corresponding to one of the three consonants /b, d, g/. Lindblom [7] displayed the F2 and F3 onsets of the plosives as a function of the second formant of the final vowel taken from Ohman’s data and observed that, indeed, there is a boundary between /b/ and /d, g/ in the F2-onset/F2-vowel plane and another boundary between /g/ and /b, d/ in the F3-onset/F2-vowel plane. The different behavior observed by Delattre et al. for the perception of /g/ was also observable in production as two different locus equations for back and front vowels [8]. However, following another deductive approach [9], the closed-open Distinctive Region Model (DRM, see Figure 1) can predict the vocal tract regions corresponding to place of articulation of /b/ (region R8), /d/ (region R6) and /g/ (region R5). F3 is necessary to make the /d/-/g/ distinction based on a simple, increasing or decreasing F-pattern shape. While the VCV tokens examined by Ohman [6] were produced using the model via superimposition of a consonantal gesture on vocalic gestures [10], the role of the first three formants is defined in a more direct fashion by the DRM. In the case of /Cu/, (and /Co/, /C/), however, the DRM configuration becomes closed-closed symmetric, with the consequence that the regions deduced will be different from those of the closed-open variety illustrated in Figure 1. But what are the consequences of these two configurations to the production of CV syllables? Are F3 transitions an absolute necessity allowing perceptual distinction of CVs in the F1-F2 plane alone? The objective of the research reported in this paper was an attempt to answer this question. Figure 1:"
2757717,21258,9804,Resistance is futile - The intonation between continuation rise and calling contour in German,2013,"Abstract German knows two plateau-based phrase-final intonation con-tours: the high level plateau of the continuation rise and the descending plateau sequence of the calling contour. They oc-cur within a narrow scaling range of only a few semitones. The paper presents production and perception evidence for a third plateau-based phrase-final intonation contour inside this narrow scaling range. The new plateau contour shows a F0 de-crease of between 1-3 st (in the form of a slightly declining plateau or a descending plateau sequence), involves additional lengthening of the vowels underneath the plateau, and occurs when resistance is futile, i.e. when speakers signal that they finally, but reluctantly, give in to a demand of the dialogue partner. Phonological implications are briefly outlined. Index Terms : intonation, plateau, rise, German, stylization. 1. Introduction The end of a phrase is intonationally particularly rich. The Kiel Intonation Model (KIM) for German [1] and its labelling system ProLab [2], for instance, distinguish five different phrase-final intonation movements: a terminal fall, a non-ter-minal fall, a high level plateau, a small and shallow rise, and a large and steep rise until the upper limit of the speaker’s mo-dal voice range. Additionally, there is the stylized intonation [3], i.e. a stepped drop in pitch from a high to a mid-high pla-teau, which is also referred to as chanted call contour, vocative chant, or calling contour [4,5] (the term calling contour will be used henceforth). If we then also include the concave vs. con-vex contrast in phrase-final rises [6], complex contours like fall-rise sequences, and so-called pseudo-terminal contours [7], we quickly end up with more than a dozen formally and functionally different phrase-final intonations in German. The autosegmental-metrical (AM) phonology is not as diverse as that of the KIM. The most prominent AM model for German, GToBI [8], only distinguishes between five different phrase-final intonations that are represented by the boundary tones L-%, H-%, H-^H%, L-H%, and !H-%. The latter bound-ary tone denotes the calling contour. Irrespective of the differently sized inventories of phrase-final intonation categories, KIM and GToBI agree in postula-ting two different phrase-final intonations that are built from plateaux. They will briefly be characterised in the following. First, there is the high level plateau or H-%. It is preceded by a rising pitch accent. The sequence of rise and high plateau is frequently referred to as continuation rise (Fig.4). In fact, it was found by [9] in a label-based analysis of the Kiel Corpus of Spontaneous Speech that 92-94% of all naturally produced continuation rises occurred at turn-internal phrase boundaries. So, the term ‘continuation rise’ seems to be fairly adequate, even though it is presumably not the ultimate characterisation of the communicative function of this first plateau contour. The second type of plateau-based phrase-final intonation is the calling contour or !H-%. The formal and functional aspects of the calling contour have been subject of intense de-bate for a long time. However, hardly any experimental study has been conducted so far. The tenor of the mostly descriptive analyses may be summarized as follows: Stylization is not an isolated phenomenon, but a process that can be superimposed on different intonation patterns and hence generates many sub-types of stylized contours (including stylized rises) [3,10,11]. Yet, the calling contour in the form of “a ‘stepping down’ from one […] level pitch to another” [3] is clearly the proto-typical type of stylization (Fig.4). How the step down in be-tween the two plateaux of the calling contour is aligned with the segmental string is influenced by the underlying syllable structure [5]. The scaling of the step can, but need not be the often assumed interval of a minor third, i.e. 3 semitones. Ra-ther, the scaling can vary substantially as a function of speaker and distance to the dialogue partner [12]. However, it mainly ranges between 2-6 semitones. Each plateau may start with a lengthening of the initial syllable [13]. With regard to com-municative function, many studies agree that calling contours or stylizations in general are attention-seeking or convey a kind of contact signals. That is, they are used to establish or consolidate the communication channel between speaker and hearer, which, however, need not involve a large physical distance between them. Independent of the physical distance, calling contours bridge a personal distance between speaker and hearer, which is why they frequently occur in connection with routine matters and shared conventions between speaker and hearer [3,11,12,14,15]. The aim of the present paper is to provide initial pro-duction and perception evidence for a third plateau-based phrase-final intonation that is phonetically intermediate be-tween the continuation rise and the calling contour. Although the study was conducted for German, it is very likely that the same phrase-final intonation occurs with the same meaning also in many other languages, at least in Western Germanic languages like English and Dutch."
16763,21258,9804,The Blizzard Challenge - 2005: Evaluating corpus-based speech synthesis on common datasets,2005,"In order to better understand different speech synthesis techniques on a common dataset, we devised a challenge that will help us better compare research techniques in building corpusbased speech synthesizers. In 2004, we released the first two 1200-utterance single-speaker databases from the CMU ARCTIC speech databases, and challenged current groups working in speech synthesis around the world to build their best voices from these databases. In January of 2005, we released two further databases and a set of 50 utterance texts from each of five genres and asked the participants to synthesize these utterances. Their resulting synthesized utterances were then presented to three groups of listeners: speech experts, volunteers, and US English-speaking undergraduates. This paper summarizes the purpose, design, and whole process of the challenge. 1. Background With a view to allowing closer comparison of corpus-based techniques, from labeling, pruning, join costs, signal processing techniques, and others, we devised a challenge for participants to use the same databases to synthesize utterances from a small number of genres. An organized evaluation, based on listening tests, was then carried out to try to rank the systems and help identify the effectiveness of the techniques. The sister field of speech recognition has clearly benefited from the availability of common datasets in order to provide valid comparisons between systems [1]. These evaluations concentrate efforts in the speech recognition fields, particularly through the 1990s with DARPA workshops where NIST (and others) devised standardized tests for speech recognition. It is clear that these standardized tests and widely available datasets allowed speech recognition results to be more easily compared and more importantly cause the core technology to improve. Although today many may criticize a naive word error metric as a sole accuracy measure for speech recognition systems, few would complain that it has not contributed to drastic improvement in the utility of speech recognition as a viable technology. Speech synthesis has not been as lucky in having a welldefined evaluation metric, nor has it had a well-funded centralized community that could be targeted to the same task. With the rise of general corpus-based speech synthesis over the last ten years, we have moved from a domain where new synthetic voices could only be built with many man-years of effort from highly skilled researchers. Such systems were tuned to the particular data sets being used, thus comparisons of techniques such as labeling and signal processing could only be done within the research group that originally developed the dataset. Such tying of databases to particular systems made it hard to genuinely compare techniques since the quality of the original recorded voice itself contributed greatly to the resulting synthetic voice quality."
13386,21258,9804,A robust isolated word recognizer for highly non-stationary environments. recognition results.,1999,"ABSTRACT Through the present paper, the evaluation results of aSpeaker Independent Robust-to-Noise Isolated WordRecognizer are presented. The system, which is in partthe results achieved by the project IVORY (ESPRITproject No. 20277) [6], is intended for working in highlynon-stationary environments. The system comprises twomain modules: the Noise Cancellator and the SpeechRecognizer itself. System robustness is achieved in thenoise cancellation module. This module incorporates anAdaptive Filter [7] [13], operating in the time domain,and a subsequent Spectral Subtraction step [2] operatingin the frequency domain with the enhanced signalprovided by the previous stage. Recognition results fordifferent noise cancellation configurations and forseveral Parameter Extraction Front-Ends, includingLPC [7], FFT Cepstrum [3] and PLP based methods [8]are presented.Keywords: robust speech recognition, non-stationaryenvironments, recognition results. 1. INTRODUCTION Environmental noise is one of the major problems to befaced by speech recognition systems working under realconditions, as most systems performance degradesubstantially with low S/N ratios [1]. This is particularlyimportant in non-stationary environments as in cars,plane cockpits, workshops with heavy machinery,crowded public places, etc. These scenarios arecharacterized by high noise levels (of about 95 dB withSNR´s about 0- 10 dB) produced by different sourcesand where noise contributions are especially harsh, asmany different noise sources may be activesimultaneously, varying dramatically with time. To facewith the treatment of noise, Robust Speech Recognitionhas become an objective in itself [4].Along the years, several independent techniques havebeen proposed. Most of them are based in two basicapproaches: Adaptive Time-Domain Filtering (ATDF)[14], and Frequency-Domain Spectral Subtraction(FDSS) [2]. Both methods have been used independentlywith different degree of success, depending on theproblem being solved. Generally, ATDF has been usedwith array microphones, ranging from two-microphoneto multiple-microphone systems. On the other hand,FDSS has been preferred for systems where a singlemicrophone is a requirement (e.g. Telephone speechrecognition). This technique has found its niche inapplications where noise is more or less a quasi-stationary and evenly distributed process. ATDF has theadvantage of being highly suitable to deal with non-stationary signals as speech and most kinds of noise,although at a higher cost (both in instrumentation and incomputational power). Although both methods can beintegrated in the same system, most of the times theyhave been used separately [5]. However, the co-operation of both procedures allows important gains inthe S/N ratio to be achieved, improving the overallrecognition results, as well."
325094,21258,9804,Auditory and Dynamic Modeling Paradigms to Detect L2 Mispronunciations.,2012,"This doctoral thesis is the result of a research effort performed in two fields of speech technology, i.e., speech recognition and mispronunciation detection. Although the two areas are clearly distinguishable, the proposed approaches share a common hypothesis based on psychoacoustic processing of speech signals. The conjecture implies that the human auditory periphery provides a relatively good separation of different sound classes. Hence, it is possible to use recent findings from psychoacoustic perception together with mathematical and computational tools to model the auditory sensitivities to small speech signal changes.The performance of an automatic speech recognition system strongly depends on the representation used for the front-end. If the extracted features do not include all relevant information, the performance of the classification stage is inherently suboptimal. The work described in Papers A, B and C is motivated by the fact that humans perform better at speech recognition than machines, particularly for noisy environments. The goal is to make use of knowledge of human perception in the selection and optimization of speech features for speech recognition. These papers show that maximizing the similarity of the Euclidean geometry of the features to the geometry of the perceptual domain is a powerful tool to select or optimize features. Experiments with a practical speech recognizer confirm the validity of the principle. It is also shown an approach to improve mel frequency cepstrum coefficients (MFCCs) through offline optimization. The method has three advantages: i) it is computationally inexpensive, ii) it does not use the auditory model directly, thus avoiding its computational cost, and iii) importantly, it provides better recognition performance than traditional MFCCs for both clean and noisy conditions.The second task concerns automatic pronunciation error detection. The research, described in Papers D, E and F, is motivated by the observation that almost all native speakers perceive, relatively easily, the acoustic characteristics of their own language when it is produced by speakers of the language. Small variations within a phoneme category, sometimes different for various phonemes, do not change significantly the perception of the language’s own sounds. Several methods are introduced based on similarity measures of the Euclidean space spanned by the acoustic representations of the speech signal and the Euclidean space spanned by an auditory model output, to identify the problematic phonemes for a given speaker. The methods are tested for groups of speakers from different languages and evaluated according to a theoretical linguistic study showing that they can capture many of the problematic phonemes that speakers from each language mispronounce. Finally, a listening test on the same dataset verifies the validity of these methods."
507802,21258,9804,Auditory model based optimization of MFCCs improves automatic speech recognition performance,2009,"This doctoral thesis is the result of a research effort performed in two fields of speech technology, i.e., speech recognition and mispronunciation detection. Although the two areas are clearly distinguishable, the proposed approaches share a common hypothesis based on psychoacoustic processing of speech signals. The conjecture implies that the human auditory periphery provides a relatively good separation of different sound classes. Hence, it is possible to use recent findings from psychoacoustic perception together with mathematical and computational tools to model the auditory sensitivities to small speech signal changes.The performance of an automatic speech recognition system strongly depends on the representation used for the front-end. If the extracted features do not include all relevant information, the performance of the classification stage is inherently suboptimal. The work described in Papers A, B and C is motivated by the fact that humans perform better at speech recognition than machines, particularly for noisy environments. The goal is to make use of knowledge of human perception in the selection and optimization of speech features for speech recognition. These papers show that maximizing the similarity of the Euclidean geometry of the features to the geometry of the perceptual domain is a powerful tool to select or optimize features. Experiments with a practical speech recognizer confirm the validity of the principle. It is also shown an approach to improve mel frequency cepstrum coefficients (MFCCs) through offline optimization. The method has three advantages: i) it is computationally inexpensive, ii) it does not use the auditory model directly, thus avoiding its computational cost, and iii) importantly, it provides better recognition performance than traditional MFCCs for both clean and noisy conditions.The second task concerns automatic pronunciation error detection. The research, described in Papers D, E and F, is motivated by the observation that almost all native speakers perceive, relatively easily, the acoustic characteristics of their own language when it is produced by speakers of the language. Small variations within a phoneme category, sometimes different for various phonemes, do not change significantly the perception of the language’s own sounds. Several methods are introduced based on similarity measures of the Euclidean space spanned by the acoustic representations of the speech signal and the Euclidean space spanned by an auditory model output, to identify the problematic phonemes for a given speaker. The methods are tested for groups of speakers from different languages and evaluated according to a theoretical linguistic study showing that they can capture many of the problematic phonemes that speakers from each language mispronounce. Finally, a listening test on the same dataset verifies the validity of these methods."
2821823,21258,9804,Building an ASR System for Noisy Environments: SRI's 2001 SPINE Evaluation System,2002,"We describe SRI’s recognition system as used in the 2001 DARPA Speech in Noisy Environments (SPINE) evaluation. The SPINE task involves recognition of speech in simulated military environments. The task had some unique challenges, including segmentation of foreground speech from noisy background, the need for robust acoustic models to handle noisy speech, and development of language models from limited training data. In developing the SRI evaluation system for this task, we addressed each of these challenges using a combination of state-of-the-art techniques, including several types of feature normalization, model adaptation, class-based language modeling, multi-pass segmentation and recognition, and word posterior-based decoding and system combination We describe SRI’s evaluation system for the October 2001 Speech in Noisy Environments (SPINE) task. The main aim of the paper is to present the key algorithms and components of a state-of-art speech recognition system and how they were combined into a system for optimal performance. We have organized the paper as follows. First, we provide a brief introduction to the SPINE task and its challenges. We then present the key components and algorithms used in our system and how the task features guided the design of the system. We then present the results on two test sets, the dry run and the evaluation sets of the 2001 SPINE evaluation. We conclude with a discussion of the results. SPINE is a relatively new task developed by the Naval Research Laboratory (NRL) to test the state of the art of speech recognition in military noise environments. The primary challenge of the task is recognition of speech with significant amounts of background noise as found in various military environments, such as fighter jet cockpits and aircraft carrier flightdecks. The data consists of dialogs between two participants playing a battleship-like game, with recorded military noises played back into the recording environment. The players use realistic microphones and headgear (e.g., fighter pilot helmets) as appropriate for the different scenarios. The language used comprises a mix of commands, status reports, and confirmations specific to this limited domain, involving an active vocabulary of about 2000 words. More details are available at [1]. Due to its focus on noisy environments, SPINE posed some unique challenges. One of the difffculties was to segment foreground speech from the noisy background, which, in some environments, included background speech. Another challenge was to develop robust acoustic features, models, and techniques capable of recognizing the noise-degraded speech. Yet another challenge was the limited amount of training data, particluarly for training the language model. Thus, the task posed challenges not only for research, but also for system development. To solve these issues of segmentation and robust acoustic and language modeling, we drew on a number of stateof-the-art algorithms as the building blocks of our system. In the following, we describe these components and how they were integrated into a system that achieved the lowest word error rate (WER) in the 2001 SPINE evaluation."
11885,21258,9804,Time Adjustable Mixture Weights for Speaking Rate Fluctuation,2003,"Abstract Oneof themostseriousproblemsin spontaneousspeechrecog-nition is the degradation of recognition accuracy due to thespeaking rate ﬂuctuation in an utterance. This paper proposesa method for adjusting mixture weights of an HMM frame byframe depending on the local speaking rate. The proposedmethodisimplementedusingtheBayesiannetworkframework.A hidden variable representing the variation of the “mode” ofthespeakingrateisintroducedanditsvaluecontrolsthemixtureweights of Gaussian mixtures. Model training and maximumprobability assignment of the variablesare conducted usingtheEM/GEMandinferencealgorithmsforBayesiannetworks. TheBayesian network is used to rescore the acoustic likelihood ofthe hypotheses in N-best lists. Experimental results show thatthe proposed method improves word accuracy by 1.6% for theabsolute value on meeting speech given the speaking rate in-formation, whereas improvement by a regression HMM is lesssigniﬁcant. 1. Introduction Conventional HMM-based recognizers suffer a lower recogni-tion rate for spontaneous speech. One of the signiﬁcant factorsreducing the recognition rate is the variable nature of sponta-neousutterances[1]. For example,thespeakingratecanchangeeven within one utterance. A possible strategy to manage thisproblem is ﬁrst estimating the speaking rate and then adjustingarecognizer basedonthe speakingrate. Thispaperinvestigateshow to use the speaking rate information to control probabilitydensity functions in the HMM.Since estimating the speaking rate is itself a difﬁcult prob-lem and assuming an accurate detection is unrealistic, themethod of adjusting the decoder accordingto the speaking rateshould be probabilistic. This paper proposes a method of ad-justing mixture weights of the HMM at every frame based onthe speaking rate. A hidden variable that represents a “mode”ofthespeakingratecontrolsthemixtureweights. Theproposedmethod can model complex changes of acoustic characteristicswith only asmall increaseof the numberof parameters.The proposed method is implemented using the Bayesiannetwork framework to realize detailed control of HMM param-eters[2]. WeuseGMTK[3]for modelparametertraining usingthe EM/GEM algorithms and for decoding. The network ac-ceptsthespeakingrateinadditiontotheusualacousticfeatures.The proposed method is used to rescore the acoustic likelihoodof N-best hypotheses.This paper is organized as follows. In Section 2, the pro-posed method is formulated as a Bayesian network. In Sec-tion 3, an experimental set up is described. Training processesof the Bayesian network are described in Section 4, and ob-tained parameters related to the speaking rate are analyzed inSection 5. The proposed method is applied to meeting speechrecognition in Section 6 and the results are discussed in Sec-tion 7. It is shown that the proposed method is more effectivein improving the recognition ratethan aregressionHMM givenspeaking rate information. Finally, in Section 8, the paper isconcluded."
15165,21258,9804,Japanese Pitch Conversion for Voice Morphing Based on Differential Modeling,2009,"Abstract In this paper, we convert the pitch contours predicted by a TTSsystem that models a source speaker to resemble the pitch con-toursofatargetspeaker. Whenthespeakingstylesofthespeak-ers are very different, complex conversions such as adding ordeletingpitchpeaksmayberequired. Ourmethoddoesthecon-versions by modeling the direct pitch features and differentialpitch features at the same time based on linguistic features. Thedifferential pitch features are calculated from matched pairs ofsource and target pitch values. We show experimental results inwhich the target speaker’s characteristics are successfully mod-eledbasedonaverylimitedtrainingcorpus. TheproposedpitchconversionmethodstretchesthepossibilitiesofTTScustomiza-tion for various speaking styles. Index Terms : pitch conversion, voice conversion, voice mor-phing, speech synthesis, differential modeling. 1. Introduction Voice conversion changes the characteristics of the voice of anSPS (SPeaker Source) to those of an SPT (SPeaker Target) forvarious applications. One important application is to build cus-tomized text-to-speech (TTS) systems for different companies,soaTTSsystemwitheachcompany’sfavoritevoicecanbecre-ated quickly and inexpensively by modifying the speech corpusof some original speaker.Spectra and prosody are the two major characteristics ofvoice. For spectral conversion, recent work such as [1, 2]has achieved signiﬁcant improvements in the naturalness andsimilarity of the voices converted using only a limited amountof training data. However, not much research has been doneinto prosody conversion. Most spectral conversion researchuses simple linear transformations for the prosody. It is truethat the detailed prosody difference is sometimes difﬁcult forlisteners to distinguish [3], especially when the speakers aremonotonously reading for TTS corpus recording. However, togenerate TTS voices with a lively colloquial speaking style, re-production of the detailed prosody characteristics is important.Our objective is to reproduce the SPT’s speaking style ofpitch contours based on limited training data. We assume 100sentences as training data is a reasonable amount to require forthe SPT’s speech corpus. A speech corpus with that size caneasily be recorded in a thirty-minute recording session. We donot assume the existence of a parallel corpus, which can be adifﬁcult condition to satisfy. We focus on the pitch changesaround the syllable level, because the important pitch changesin Japanese are mainly at the syllable level.Figure 1 illustrates examples of pitch contour pairs that theproposed pitch conversion method can handle. They are (a)asymmetrical slope changes, (b) adding or deleting peaks, and(c) adding or deleting phrase-ﬁnal rises."
2488990,21258,9804,Detection of instants of glottal closure using characteristics of excitation source,2007,"In this paper, we propose a method for detection of glottal closure instants (GCI) in the voiced regions of speech signals. The method is based on periodicity of significant excitations of the vocal tract system. The key idea is the computation of coherent covariance sequence, which overcomes the effect of dynamic range of the excitation source signal, while preserving the locations of significant excitations. The Hilbert envelope of linear prediction residual is used as an estimate of the source of excitation of the vocal tract system. Performance of the proposed method is evaluated in terms of the deviation between true GCIs and hypothesized GCIs, using clean speech and degraded speech signals. The signal-to-noise ratio (SNR) of speech signals in the vicinity of GCIs has significant bearing on the performance of the proposed method. The proposed method is accurate and robust for detection of GCIs, even in the presence of degradations. Index Terms: glottal closure instants, excitation source, periodicity, coherent covariance sequence Voiced speech is produced by exciting a time-varying vocal tract system with a sequence of impulse-like excitations. The impulse-like excitation is due to the closure of glottis during the vibration of vocal folds. The time instant at which the closure is achieved (called glottal closure instant or GCI) is an important feature for analysis of speech signals. Detection of GCIs enables the identification of region of closed glottis within a pitch period. Analysis of short segments of speech signals over such regions helps in accurate estimation of vocal tract parameters such as formants [1], and also in the extraction of characteristics of voice source. In text-to-speech synthesis, accurate detection of GCIs is necessary for prosodic manipulation of speech sounds [2]. Moreover, speech signal in the vicinity of GCIs has relatively high signal-to-noise ratio (SNR), due to impulselike excitation and damped sinusoid-like impulse response of the vocal tract system. These regions of high SNR are likely to preserve features specific to sound and speaker, even under the influence of degradations. Hence, methods for robust detection of GCIs in speech signals are necessary. Some of the methods proposed for the detection of GCI assume a linear source-system model for the production of speech signal. These methods identify GCI with the time instant of strongest excitation which will be around the region with least predictability [3, 4, 5]. Normally, linear prediction (LP) residual is used as an estimate of source of excitation [5]. Another class of methods fordetection of GCIs is based on the properties of minimum phase signals and group delay functions. In [6], the average slope of the unwrapped phase spectrum of speech signal is computed as a function of time, and the positive zero crossings of the phase slope function are hypothesized as the instants of glottal closure. In [7], the phase spectrum is computed from the LP residual instead of speech signal to reduce the effects of truncation. Robustness of the group delay based methods against noise and distortion is studied in [8]. In [9], properties of the phase slope function are used to hypothesize candidates for GCI, which are validated using a dynamic programming approach. Energy-weighted group delay is proposed as a measure for the detection of GCIs in [10]. In this paper, we propose a method for detection of instants of glottal closure using the periodicity of significant excitations in speech signals. In Section 2, we describe the representation of excitation source in terms of the Hilbert envelope of the linear prediction residual. The section describes the proposed method for detection of GCI, and also the issues involved in the choice of parameters used in the method. Section 3 discusses the experiments conducted for evaluating the performance of the proposed method, and the results of these studies. Conclusions are given in Section 4."
205957,21258,9804,Stability and composition of functional synergies for speech movements in children and adults,2009,"Abstract The consistency and composition of functional synergies forspeech movements were investigated in 7 year-old children andadults in a reiterated speech task using electromagnetic articu-lography (EMA). Results showed higher variability in childrenfor tongue tip and jaw, but not for lower lip movement trajecto-ries. Furthermore, therelativecontributiontotheoral closureoflower lip was smaller in children compared to adults, whereasin this respect no difference was found for tongue tip. Theseresults support and extend ﬁndings of non-linearity in speechmotor development and illustrate the importance of a multi-measures approach in studying speech motor development. Index Terms : speech motor control, speech motor develop-ment, coordinative structures, speech movement patterns 1. Introduction Speaking can be considered (one of) the most complex skillshumans perform, and the development of speech motor controlis a popular and well explored ﬁeld of research. One way tostudy speech motor development is to focus on the consistencyand stability of movement patterns. The development of speechmotorskillsessentiallyequatestothedevelopment offunctionalsynergies of muscle activations or coordinative structures. Inthis way, the degrees of freedom are reduced, which makesthe control task simpler. Consequently, as the speech produc-tion system matures the dynamic coordination among orofacialstructures becomes more consistent [1, 2]. The progression ofspeech motor development - i.e. the degree of functional syn-ergy that reﬂects it - thus can be measured by assessing coor-dination and movement variability in speech production, a con-cept that has been successfully exploited in a fair number ofstudies.Movementvariabilitytosomeextentisexpressedbytheco-efﬁcients of variation of kinematic movement parameters (e.g.amplitude, duration and peak velocity). In addition, Smith andcolleagues developed the spatiotemporal variability index (STI)as a method to examine the entire movement trajectory overtime[3,4,5]. TheSTIcapturesthevariabilityofmultiplemove-ment sequences whose trajectories are time and amplitude nor-malized. A lower STI value indicates a smaller deviation fromthe target movement template, and thus less variability.Utilizing the STI, a recent longitudinal study has shown in-fants’ articulator movements (jaw, upper lip, and lower lip) tobecome more stable over time and during linguistic/phonemicdevelopment [6]. Earlier group-studies yielded similar results,albeit not unambiguously. Measuring the words"
120367,21258,9804,Developing an Automatic Functional Annotation System for British English Intonation,2009,"Abstract One of the fundamental aims of prosodic analysis is to provide a reliable means of extracting functional information (what prosody contributes to meaning) directly from prosodic form (i.e. what prosody is – in this case intonation). This paper addresses the development of an automatic functional annotation system for British English. It is based on the study of a large corpus of British English and a procedure of analysis by synthesis, enabling to test and enrich different models of English intonation on the one hand and work towards an automatic version of the annotation process on the other. Index terms: phonetic and phonology , speech analysis and representation, prosody modeling and generation 1. Introduction The question of how to annotate prosodic phenomena is a vital one when one wants to investigate or analyse intonation. Prosodic annotation systems were developed and widely used to research intonation in English (see ToBI [1] and the Tonetic Stress Marks (TSMs) [2]). However, recent evaluations of these systems [3], show that they fail to answer the growing needs to cover bigger corpora at lower costs in terms of time and money. Hirst [4] has argued that one of the main obstacles to the development of these systems has been the lack of separation between the annotation of prosodic functions and prosodic forms. While prosodic forms can be accurately modelled by algorithms today, prosodic functions remain within the field of human interpretation. “Machines are good at labelling forms, humans at interpreting the message” [4]. Hence, our drive to develop an automatic functional annotation system for British English. Such an attempt was successfully achieved recently, in Finnish (see [5]). It was suggested that the manual functional annotation of a small corpus could be used as a bootstrap for such an attempt, and as a starting point for high quality multilingual speech synthesis. In an earlier study [6], we also developed a procedure of analysis by synthesis generating formal representation from a minimal functional representation. The representation of prosodic forms was optimized starting from these functional labels using the INTSINT coding system. The sequences of INTSINT tones were then converted into phonetic representations by means of the Momel algorithm so that the output could be compared directly to the original recordings. The quality of fit of the model was measured by linear correlation with hand corrected modelled fundamental frequency curves, intonation unit by intonation unit. The parameters of this model were optimized on an extract of the Eurom1 corpus for which the functional annotation was carried out manually using the IF annotation system in a subsequent study [7]. This paper presents the developments of this approach based on a large corpus of expressive speech which already contains a prosodic transcription in TSMs. The functional annotation was provided automatically through a conversion of the TSMs into the Intonation Function (IF) annotation system on the Marsec corpus which will be presented in the next section. This will be followed by the presentation of the automatic procedure of optimization of the representation of prosodic forms, starting from the functional annotation. Finally, our first attempts at making the functional annotation process automatic, will be discussed."
2870109,21258,20411,Report on the 1st International Workshop on Recent Trends in News Information Retrieval (NewsIR16),2016,"The news industry has gone through seismic shifts in the past decade with digital content and social media completely redefining how people consume news. Readers check for accurate fresh news from multiple sources throughout the day using dedicated apps or social media on their smartphones and tablets. At the same time, news publishers rely more and more on social networks and citizen journalism as a frontline to breaking news. In this new era of fastflowing instant news delivery and consumption, publishers and aggregators have to overcomea great number of challenges. These include the verification or assessment of a source's reliability; the integration of news with other sources of information; real-time processing of both news content and social streams in multiple languages, in different formats and in high volumes; deduplication; entity detection and disambiguation; automatic summarization; and news recommendation. Although Information Retrieval (IR) applied to news has been a popular research area for decades, fresh approaches are needed due to the changing type and volume of media content available and the way people consume this content. Hence, the first international workshop on recent trends in News Information Retrieval (NewsIR) was held in conjunction with ECIR 2016. As part of the workshop, we released a new dataset consisting of one million news articles to the research community. The workshop was very well attended with around 70 registered participants. We received a healthy number of 19 submissions in total of which 12 were accepted for presentation. In addition to that, we were pleased to have two keynote talks by well-known experts in the field - on with an industry background (Jochen Leidner) and one from academia (Julio Gonzalo). The workshop also included a breakout session to discuss ideas for a future data challenge in news IR and closed with a focused panel discussion to reflect on the day. Throughout the day the workshop stimulated discussions around new and powerful uses of IR applied to news sources and the intersection of multiple IR tasks to solve real user problems. In particular, several ideas were presented on solving complex information needs for media monitoring, event detection and summarisation. Moreover, and going forward, the workshop concluded with a long list of suggestions for shared tasks, and dataset requirements."
63819,21258,9804,Automatic Lips Reading for Audio-Visual Speech Processing and Recognition,2004,"This contribution is about the method for automatic lips reading from the video picture. The results of this automatic method are used for the next audio-visual speech processing and recognition. The simple image processing method for finding of the human face in the video picture is presented here. The lips are found from the marked human face in the region of interest, where the lips are, with the help of the mathematical gradient method. This gradient method is based on the image histogram. The histogram is computed from the colour value of the region of interest. The first results for visual speech recognition of isolated words are presented in conclusion. The method described here was used for face and lips detection to help speech recognition. Today’s computer technology makes possible to use also the visual part of the data used for speech processing and recognition. The visual part of the speech doesn’t have such quantity of information about what has been said as the audio part of the speech signal. This visual part can improve, in the combination with the audio part of the signal, the recognition rate in noisy conditions (in the noisy factory, in the street, in the railway station, and so on). We want the recognition response to be very short. It means that speech recognition has to be done in real time. Image processing and recognition is very time consuming compared to audio signal speech processing and recognition. Therefore we need some fast and simple methods for this. These methods for image processing and recognition must be sufficiently robust. The parameters obtained from the found outline of the lips are used in the visual part of speech most frequently. The finding of the lips is done in two steps: The first step is to find the human face in the video picture. In this step we want to know if some human face is in the video picture or not, and if somebody speaks to the camera. The camera is connected to the computer on which runs the program for audio-visual speech recognition. The second step is to separate the region of interest containing the human lips from the detected face. This is very difficult task because the skin colour of the face can be the same like the colour of the lips. The methods based on colour or shape segmentation of the image with the human lips (face) fail in certain specific cases. We concentrate on normal cases (with normal press and temperature for people, when the person isn’t sick, and so on) for lips finding and reading for the present. The static and dynamic (from the video stream) visual parameters from the detected outline of the lips in the combination with the parameters from the audio speech signal are used for audio-visual speech recognition than. 2. Finding of the Face of the Human Subject in video picture The face detection is the first task for the quality lips reading. A lot of methods of image processing exist that solve this problem [1]. They include the methods for face detection that work with the skin colour model, shape of the face, and so on. The methods based on skin colour model utilize simple threshold segmentation or segmentation using Gaussian mixture models of the pixels from the skin. Two-mixture models for the people with the “bright” or “dark” colour of the facial skin are used most frequently [2]. We have used the method of segmentation based on one-mixture Gaussian model of the human facial skin. Our method is used only for people with the “bright” colour of facial skin at present. We would like to create a two-mixture Gaussian model for all people in the near future. This method combined with the shape segmentation is fast and satisfactory for robust face detection. The Cr, Cb colour transformation (1) of the colour elements RGB from the original colour picture was used for the creation of the one-mixture Gaussian model of the human facial skin."
237220,21258,9804,Multi-Microphone Periodicity Function for Robust F0 Estimation in Real Noisy and Reverberant Environments,2006,"Abstract This paper outlines a new method to extract F0 from distant-talking speech signals acquired by a microphone network, whichexploits the redundancy across the signals proceeding from eachmicrophone, by jointly processing the different contributes. Tothis purpose, a multi-microphone periodicity function is derivedfrom the magnitude spectrum computed on each microphone sig-nal. This function allows to estimate F0 reliably, even under re-verberant conditions, without the need of any post-processing orsmoothing technique. Experiments, conducted on real lectures,showed that the proposed frequency-domain algorithm is moresuitable than other time-domain based ones. IndexTerms : speechanalysis,fundamentalfrequencyestimation,multi-microphone processing, distant-talking interaction. 1. Introduction In the CHIL project, various signal processing techniques are be-ing investigated that aim to address challenging problems amongwhich acoustic event classiﬁcation, speakerlocalization and track-ing, distant-talking speech recognition, speech activity detection,speaker identiﬁcation and veriﬁcation [1].One way to pursue all these objectives is that of deriving amodel of the source (e.g. the speaker) from the given multi-microphone data. To this purpose, a Distributed Microphone Net-work (DMN) is used, which consists in a generic set of micro-phones localized in space without any speciﬁc geometry.In this work we address the problem of deriving a robust es-timation of the fundamental frequency F0 from the variety of sig-nals recorded through the microphone network. Speech signalsrecorded by microphones placed far from a talker are severely de-graded by both background noise and reverberation, which de-pends on spatial relationships among the microphones and thetalker, as well as on the scenario acoustic characteristics.Estimating F0 independently for each microphone signal andapplying then majority vote or other fusion based methods mayrepresent a possible approach. Another way to perform F0 es-timation is to extend to the multi-microphone case a paradigmthat works for a single microphone close-talking case. A time-domain F0 extraction algorithm based on Weighted Autocorre-lation (WAUTOC) [2] was experimented in the past [3], whichshowed good performance on a real multi-microphone databaseof distant-talking speech sequences reproduced in an ofﬁce envi-ronment. In particular, the resulting multi-microphone WAUTOCtechnique offers the advantage of obtaining better performance"
191780,21258,9804,Margin-space integration of MPE loss via differencing of MMI functionals for generalized error-weighted discriminative training.,2009,"Abstract Using the central observation that margin-based weightedclassiﬁcation error (modeled using Minimum Phone Error(MPE)) corresponds to the derivative with respect to the mar-gin term of margin-based hinge loss (modeled using MaximumMutual Information (MMI)), this article subsumes and extendsmargin-based MPE and MMI within a broader framework inwhich the objective function is an integral of MPE loss over arange of margin values. Applying the Fundamental Theorem ofCalculus,thisintegraliseasilyevaluatedusingﬁnitedifferencesof MMI functionals; lattice-based training using the new crite-rion can then be carried out using differences of MMI gradi-ents. Experimental results comparing the new framework withmargin-based MMI, MCE and MPE on the Corpus of Sponta-neous Japanese and the MIT OpenCourseWare/MIT-World cor-pus are presented. 1. Introduction The ﬁeld of discriminative training for speech recognition haswitnessed considerable activity in recent years. The appeal ofminimizingphoneorworderrorratherthanstringerrorhasmo-tivated a transition from well-known string-level methods suchas MMI and MCE [1][2] to error-weighted approaches, such asMPE [3][4]. More recently, there has been a surge in proposalsfor“largemargin”approachestohiddenMarkovmodel(HMM)design, such as the “large-margin HMM” [5], “soft margin es-timation” [6], and incrementally shifted MCE loss [7]. Sha andSaul [8] made the important proposal that a ﬁne-grained er-ror measure, such as the Hamming distance between candidaterecognition strings, be itself directly incorporated into the mar-gin term for HMM-based learning. It turns out that introducinga margin term that multiplies ﬁne-grained error can easily bebrought to MMI, MCE and MPE based HMM training as well,simply by adding margin-scaled local frame/phone/word errorto lattice arc log-likelihoods during Forward-Backward com-putation [9][10][11]. This approach links the original use ofmargin in the context of machine learning (e.g. Support VectorMachines (SVMs)) with margin in the context of “tried-and-tested” frameworks for large-scale discriminative training withwell-understood methods for HMM optimization on large-scaleASR tasks. Beneﬁts to performance for large-scale tasks havebeen reported for the use of margin in MMI and MPE, thoughit appears the relative gains are larger for MMI than for MPE[10][11].Aiming at leveraging the beneﬁts of margin use within thecontextofMPE-styleerror-weightedHMMtraining,thisarticlepresents a uniﬁcation of margin-based MMI and MPE trainingbased on a novel concept:"
247528,21258,9804,Speech Emotion Recognition Using Hidden Markov Models,2001,"This paper introduces a first approach to emotion recognition using RAMSES, the UPC’s speech recognition system. The approach is based on standard speech recognition technology using hidden semi-continuous Markov models. Both the selection of low level features and the design of the recognition system are addressed. Results are given on speaker dependent emotion recognition using the Spanish corpus of INTERFACE Emotional Speech Synthesis Database. The accuracy recognising seven different emotions—the six ones defined in MPEG-4 plus neutral style—exceeds 80% using the best combination of low level features and HMM structure. This result is very similar to that obtained with the same database in subjective evaluation by human judges. Dealing with the speaker’s emotion is one of the latest challenges in speech technologies. Three different aspects can be easily identified: speech recognition in the presence of emotional speech, synthesis of emotional speech, and emotion recognition. In this last case, the objective is to determine the emotional state of the speaker out of the speech samples. Possible applications include from help to psychiatric diagnosis to intelligent toys, and is a subject of recent but rapidly growing interest [1]. This paper describes the TALP researchers first approach to emotion recognition. The work is inserted in the scope of the INTERFACE project [2]. The objective of this European Commission sponsored project is “to define new models and implement advanced tools for audio-video analysis, synthesis and representation in order to provide essential technologies for the implementation of large-scale virtual and augmented environments. The work is oriented to make man-machine interaction as natural as possible, based on everyday human communication by speech, facial expressions and body gestures.” In the field of emotion recognition out of speech, the main goal of the INTERFACE project will be the construction of a real-time multi-lingual speaker independent emotion recogniser. For this purpose, large speech databases with recordings from many speakers and languages are needed. As these resources are not available yet, a reduced problem will be addressed first: emotion recognition in multi-speaker language dependent conditions. Namely, this paper deals with the recognition of emotion for two Spanish speakers using standard hidden Markov models technology."
58860,21258,9804,Using Graphical Models for Mixed-Initiative Dialog Management Systems with Realtime Policies,2009,"Abstract In this paper, we present a novel approach for dialog modeling, whichextends the idea underlying the partially observable Markov DecisionProcesses (POMDPs), i.e. it allows for calculating the dialog policy inreal-time and thereby increases the system ﬂexibility. The use of statis-tical dialog models is particularly advantageous to react adequately tocommon errors of speech recognition systems. Comparing our resultsto the reference system (POMDP), we achieve a relative reduction of31:6% of the average dialog length. Furthermore, the proposed systemshows a relative enhancement of 64:4% of the sensitivity rate in theerror recognition capabilities using the same speciﬁty rate in both sys-tems. The achieved results are based on the Air Travelling InformationSystem with 21650 user utterances in 1585 natural spoken dialogs.Index Terms: dialog modeling, dialog strategy, spoken language un-derstanding 1. Introduction In modern dialog management systems Markov Models, e.g. partiallyobservable Markov Decision Processes (POMDPs), have proven theirpower for modeling sequences of naturally spoken dialogs [1, 2]. Thisis, because speech recognition technology remains imperfect: recog-nition errors are common and inﬂuence dialog management systems.Motivated by the advantages of POMDPs in comparison to conventionalsystems [3], we design a novel approach which is similar to the POMDPmodel, and additionally, allows for calculating the dialog policy in real-time. Thus, the ﬂexibility of the system is increased: dialog strategiescan be adjusted during the runtime, e.g. to the user behavior. In theproposed model, we use semantic slots, as introduced in [4]. These rep-resent a cluster of sequences of words with a speciﬁc meaning, whichare determined during the runtime.Several semantic slots at a time deﬁne a dialog state whose transi-tions are called semantic pairs. Temporally consecutive semantic pairsform the trellis through a dialog. The user is led to a deﬁned (or learned)user goal by well-directed questions asked by the system. The param-eters of the dialog model are learned automatically from a annotatedtraining set, which is introduced in Sec. 2. In Sec. 3, we describe theconcept of semantic slots. In Sec. 4, the dialog model based on a trellisis presented. In Sec. 5, we introduce the POMDP reference model. Fi-nally in Sec. 7, we compare the performance of our model to the one ofthe reference model, before we conclude in Sec. 8."
96574,21258,9804,Perceptual Postfilter Estimation for Low Bit Rate Speech Coders Using Gaussian Mixture Models,2005,"A novel perceptual postfilter is introduced. For each frame, the filter gains, z, are estimated given a vector, y, of the quantized LSFs and the long-term prediction gain of the corresponding frame. The proposed perceptual postfilter is derived from an optimal MMSE estimator, i.e. the estimated gain vector isz = E{z|y}. The MMSE estimator is based on the conditional pdf of z given y, which is computed from the joint pdf modelled by a GMM. The proposed perceptual postfilter improves the speech naturalness comparing with the conventional adaptive postfilter, while maintaining the property of being an add-on postfilter without modification to the current encoder. Adaptive postfilters (1) have been widely applied in current Linear Prediction Analysis-by-Synthesis (LPAS) speech coders. Conventional postfiltering improves the decoded speech quality using the information available at the decoder, and is empiri- cally designed based on aspects of human perception. As re- search furthers in modelling of the human auditory system, bet- ter psychoacoustic models (2, 3) have been proposed and ap- plied in speech and audio processing, especially in audio cod- ing. However, only a few improvements (for instance, (4, 5)) have been made to adaptive postfilters despite our better under- standing of the human auditory system. A speech codec usually operates on a frame-by-frame basis. When we have access to the clean speech and its decoded ver- sion from a speech codec, a perceptual postfilter can be con- structed based on perceptual properties. The perceptual filter gains can be derived from each processing frame and applied to the decoded speech to improve the speech quality. However, in practice we do not have the information about the percep- tual postfilter gains at the decoder if they are not sent as side information. In this paper, we focus on the estimation of the perceptual postfilter gains without additional side information. Assume a given speech frame is coded by a LPAS speech coder, the decoder retrieves the quantized linear prediction (LP) coefficients. The LP coefficients represent the envelope of the short-time power spectrum which is very important for both the quality and intelligibility of coded speech. The perceptual post- filter gains are calculated for the corresponding frame. Since the open-loop prediction gain of the long-term prediction (LTP) in speech signals indicates the degree of voicing of the speech, we also calculate the LTP gain of this frame. We take the LP coefficients and the LTP gain as an input vector, and the per- ceptual postfilter gains as a target vector. A feature vector is constructed from input and target vectors. In order to find a Minimum Mean Square Error (MMSE) estimate of the target vector, a priori information of the joint probability density func-"
103294,21258,9804,Analysis of impostor tests with high scores in NIST-SRE context,2008,"Abstract Inspeakerrecognition,performanceofasystemisusuallyestimated globally on a large set of tests, even if it is wellknown that some subsets of tests could show a very dif-ferent behavior from the complete set. In fact, a smallsubset of tests could represent the main part of the re-ported errors. In this work, we highlight a such subset oftests, for which impostors obtain very high recognitionscores. We evaluate if the problem comes from the invol-ved speakers, from the voice excerpts or from the clientmodel estimation technique. We also propose a strategyin order to minimize the eﬀects of the observed pheno-mena on the overall performance of the system. 1. Introduction During the past years, performance of text inde-pendent speaker recognition systems increases signiﬁ-cantly as illustrated by NIST-SRE results 1 . EERs ob-tained with a single system are lower than 5%. This pro-gress is mainly due to inter-session variability reductiontechniques like Latent Factor Analysis (LFA) [5]. In thiscontext, it becomes interesting to analyze more deeplysystemperformance,usuallydescribedbytheglobalEERor DCF, since a large diﬀerence between two EERs couldcome from a small number of trials or from a small num-ber of speakers.This paper proposes an analysis of scores issued from astate-of-the-art system applied on the NIST-SRE 05 and06 corpora. It focuses on impostor trials for two mainreasons. Firstly, there is about ten time more impostortrials than target trials in a classical NIST-SRE proto-col, helping a statistical analysis. Secondly, we note thata non negligible number of impostor trials obtains a veryhigh score, sometimes higher than a good target trial.Section 2 presents the experimental context of this work(system, protocols and data). Section 3 is dedicated tothe core of this paper:adetailedanalysis of impostorscores. Section 4 proposes to take advantage of the pre-vioussectioninordertoimprovetherobustnessofaspea-ker recognition system. Finally, the last section presentssome conclusions and future works."
300186,21258,9804,Estimation of semantic confidences on lattice hierarchies.,2004,"Abstract Inspired by the well-known method for conﬁdence measure cal-culation via estimation of word posterior probabilities on theword graph, we devised a technique to estimate conﬁdences onall levels of the hierarchically structured output of our one-stagedecoder for interpretation of natural speech (ODINS). By con-structing a nested lattice hierarchy, the generalized counterpartof the word graph, we estimate posterior probabilities for allnodes in the decoded semantic tree, namely for all containedsemantic units and words. The obtained experimental resultsshow that the tree node conﬁdence measure performs signiﬁ-cantly better than the conﬁdence error base line, no matter ifthe evaluation is carried out on tree nodes representing seman-tic concepts, word classes, or words. Furthermore, the paperproposes possible applications of the tree node conﬁdences toimprove the grounding strategy of spoken dialog systems. 1. Introduction Regarding a robust recognition of application-speciﬁc informa-tion, a spoken dialogue system can beneﬁt a great deal fromconﬁdence measures delivered by the underlying speech recog-nition engine. On word level, there are efﬁcient methods forcomputing conﬁdence measures [1]. However, the speech in-terpreting component of the dialogue system usually derives ahierarchically structured semantic representation of the user’sutterance, that comprises more complex units than words, e.g.semantic concepts or word classes. Thus, in addition to wordconﬁdences, higher-level conﬁdences related to these semanticunits are needed by the dialogue system to safeguard the rec-ognized structured content and to generate feedback in an ade-quate way.Recent publications [2, 3] suggested to incorporate wordconﬁdences together with various other features extracted dur-ing the speech recognition and interpretation process into a clas-siﬁer to assign conﬁdences to each recognized semantic unit.The used classiﬁers (multi-layer perceptrons in [2] and decisiontrees in [3]) need explicit training before their application. Adifferent approach is proposed by [4] which exclusively usesthe primary knowledge sources of speech recognition and inter-pretation for conﬁdence estimation. Here, the common methodfor word posterior probability calculation on the word graph [1]was extended to estimate concept posterior probabilities on aso-called concept graph, which is generated from an intermedi-ateword graph by semanticparsing using stochastic context freegrammars. However, the determined concept posteriors havebeen applied to enhance word conﬁdences and haven’t beenevaluated as semantic conﬁdences."
522266,21258,9804,Modelling Speech Line Spectral Frequencies with Dirichlet Mixture Models,2010,"Statistical modeling plays an important role in various research areas. It provides away to connect the data with the statistics. Based on the statistical properties of theobserved data, an appropriate model can be chosen that leads to a promising practicalperformance. The Gaussian distribution is the most popular and dominant probabilitydistribution used in statistics, since it has an analytically tractable Probability DensityFunction (PDF) and analysis based on it can be derived in an explicit form. However,various data in real applications have bounded support or semi-bounded support. As the support of the Gaussian distribution is unbounded, such type of data is obviously notGaussian distributed. Thus we can apply some non-Gaussian distributions, e.g., the betadistribution, the Dirichlet distribution, to model the distribution of this type of data.The choice of a suitable distribution is favorable for modeling efficiency. Furthermore,the practical performance based on the statistical model can also be improved by a bettermodeling. An essential part in statistical modeling is to estimate the values of the parametersin the distribution or to estimate the distribution of the parameters, if we consider themas random variables. Unlike the Gaussian distribution or the corresponding GaussianMixture Model (GMM), a non-Gaussian distribution or a mixture of non-Gaussian dis-tributions does not have an analytically tractable solution, in general. In this dissertation,we study several estimation methods for the non-Gaussian distributions. For the Maxi-mum Likelihood (ML) estimation, a numerical method is utilized to search for the optimalsolution in the estimation of Dirichlet Mixture Model (DMM). For the Bayesian analysis,we utilize some approximations to derive an analytically tractable solution to approxi-mate the distribution of the parameters. The Variational Inference (VI) framework basedmethod has been shown to be efficient for approximating the parameter distribution byseveral researchers. Under this framework, we adapt the conventional Factorized Approx-imation (FA) method to the Extended Factorized Approximation (EFA) method and useit to approximate the parameter distribution in the beta distribution. Also, the LocalVariational Inference (LVI) method is applied to approximate the predictive distributionof the beta distribution. Finally, by assigning a beta distribution to each element in thematrix, we proposed a variational Bayesian Nonnegative Matrix Factorization (NMF) forbounded support data. The performances of the proposed non-Gaussian model based methods are evaluatedby several experiments. The beta distribution and the Dirichlet distribution are appliedto model the Line Spectral Frequency (LSF) representation of the Linear Prediction (LP)model for statistical model based speech coding. For some image processing applications,the beta distribution is also applied. The proposed beta distribution based variationalBayesian NMF is applied for image restoration and collaborative filtering. Comparedto some conventional statistical model based methods, the non-Gaussian model basedmethods show a promising improvement."
128031,21258,9804,Speaker Adaptation of Trajectory HMMs Using Feature-Space MLLR,2006,"Abstract Recently, a trajectory model, derived from the hiddenMarkov model (HMM) by imposing explicit relationshipsbetween static and dynamic features, has been proposed.The derived model, named trajectory HMM , can alleviatetwo limitations of the HMM: constant statistics within astate and conditional independence assumption of state out-put probabilities. In the present paper, a speaker adapta-tion algorithm for the trajectory HMM based on feature-space Maximum Likelihood Linear Regression (fMLLR)is derived and evaluated. Results of a simple continu-ous speech recognition experiment shows that adapting tra-jectory HMMs using the derived adaptation algorithm im-proves the speech recognition performance. Index Terms : trajectory HMM, adaptation, fMLLR. 1. Introduction Speech recognition technologies have achieved signiﬁcantprogress with the introduction of hidden Markov models(HMMs). Their tractability and eﬃcient implementationsare achieved by a number of assumptions, such as constantstatistics within an HMM state, conditional independenceof state output probabilities. Although these assumptionsmake the HMM practically useful, they are not realistic formodeling sequences of speech spectra, especially in spon-taneous speech. To overcome these shortcomings of theHMM, a variety of alternative models have been proposed,e.g., [1–3]. Although these models can improve the speechrecognition performance, they generally require an increaseinthenumberofmodelparametersandcomputationalcom-plexity. Alternatively, the use of dynamic features (deltaand delta-delta features) [4] also improves the performanceof HMM-based speech recognizers. It can be viewed as asimple mechanism to capture time dependencies. However,it has been thought of as an ad hoc rather than an essentialsolution. Generally, dynamic features are calculated as re-gression coeﬃcients from their neighboring static features.Therefore, relationshipsbetweenstaticanddynamicfeaturevector sequences are deterministic. However, usually theserelationships are ignored and the static and dynamic fea-tures are modeled as independent random variables. Ignor-ing these dependencies allows inconsistency between thestatic and dynamic features when the HMM is used as agenerative model in the obvious way.Recently, a trajectory model, derived from the HMM byimposing the explicit relationships between static and dy-namic features, has been proposed [5]. The derived model,named"
44479,21258,9804,Automatic discriminative measurement of voice onset time,2010,"Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S., and Singer, Y. (2006). Online passive-aggressivealgorithms. The Journal of Machine Learning Research, 7:551–585.Das, S. and Hansen, J. (2004). Detection of Voice Onset Time (VOT) for unvoiced stops (/p/,/t/,/k/) using the TeagerEnergy Operator (TEO) for automatic detection of accented English. In Proc. 6th NORSIG, pp. 344–347.Fischer, E. and Goberman, A. (2010). Voice onset time in Parkinson disease. J. Comm. Disorders, 43:21–34.Kazemzadeh, A., Tepperman, J., Silva, J., You, H., Lee, S., Alwan, A., and Narayanan, S. (2006). Automaticdetection of voice onset time contrasts for use in pronunciation assessment. In Proc. INTERSPEECH,pp. 721–724.Keshet, J., Shalev-Shwartz, S., Singer, Y., and Chazan, D. (2005). Phoneme alignment based on discriminativelearning. In Proc. INTERSPEECH, pp. 2961–2964.Keshet, J., Shalev-Shwartz, S., Singer, Y., and Chazan, D. (2007). A large margin algorithm for speech-to-phonemeand music-to-score alignment. IEEE Trans. Audio, Speech, Language Process., 15(8):2373–2382.Kuhl, P. and Miller, J. (1978). Speech perception by the chinchilla: Identiﬁcation functions for synthetic VOT stimuli.J. Acoust. Soc. America, 63(3):905–917.Morris, R., McCrea, C., and Herring, K. (2008b). Voice onset time differences between adult males and females:Isolated syllables. J. Phonetics, 36(2):308–317.Stouten, V. and van Hamme, H. (2009). Automatic voice onset time estimation from reassignment spectra. SpeechCommunication, 51(12):1194–1205.Taskar, B., Guestrin, C., and Koller, D. (2004). Max-margin Markov networks. Advances in Neural InformationProcessing Systems, 16.Theodore, R., Miller, J., and DeSteno, D. (2009b). Individual talker differences in voice-onset-time: Contextualinﬂuences. J. Acoust. Soc. America, 125:3974–3982.Tsochantaridis, I., Hofmann, T., Joachims, T., and Altun, Y. (2004). Support vector machine learning forinterdependent and structured output spaces. In Proc. 21st ICML.Yao, Y. (2007). Closure duration and VOT of word-initial voiceless plosives in English in spontaneous connectedspeech. UC Berkeley Phonology Lab Annual Report, pp. 183–225."
336287,21258,9804,How to Judge Reusability of Existing Speech Corpora for Target Task by Utilizing Statistical Multidimensional Scaling,2007,"Abstract In order to develop a target speech recognition system with less cost of time and money, reusability of existing speech corpora is becoming one of the most important issues. This paper proposes a new technique to judge the reusability of existing speech corpora for a target task by utilizing a statistical multidimensional scaling method. In an experiment using twelve tasks in five speech corpora, our proposed method could show high correlation to the cross task recognition performance and judge the reusability of existing speech corpora correctly for the target task with lower cost. Index Terms : statistical MDS, reusability, acoustic model, task dependency 1. Introduction Recognition accuracy is still extremely sensitive to environmental conditions such as the speaker characteristic, the speaking style, the background noise and the task domain. These issues are called a task dependency. The task dependency has strong impact on a recognition performance of the Automatic Speech Recognition (ASR) in embedded appliances such as car-navigation systems, personal digital assistants and robots. In these appliances, processing power and available memory size are generally restricted at a cost-conscious point of view, as not only the ASR but also other applications are operating on a same platform. In such a case, a number of parameters contained in an acoustic model should be reduced. That's why the acoustic model cannot demonstrate enough performance even if it is trained from a huge speech corpus covering various tasks. So, an acoustic modeling optimized for a target task is expected. In recent research [4], a task dependency and reusability of four speech corpora have been investigated by a cross task recognition experiment. We can select speech data, which has closer acoustic characteristics, through a speech recognition experiment with a few target task speech data (development data). However, no one can judge whether the selected speech data is enough and has high reusability for the target task. If there is a technique of a judgment of the reusability for the target task, we can judge what we should invest the ASR system in. For instance, a collecting target speech data, an acoustic modeling, a language modeling, an evaluation and a system maintenance. In this paper, how to judge reusability of existing speech corpora for a target task is described. In an experiment, 12 tasks contained in 5 Japanese speech corpora are evaluated by a statistical multidimensional scaling (MDS) method called as COSMOS (COmprehensive Space Map of Objective Signal) method [5] that visualizes aggregate of speech data within two or three dimensional space. The visualization is acknowledged as an effective technique to grasp the multidimensional space that humans cannot understand easily. It is expected that to comprehend the relationship between a target task speech data and existing speech corpora by using the visualization of their acoustic space is effective in order to analyze reusability of existing speech corpora. In the next section, our proposed method is described. In Section 3, an overview of speech corpora is described. In Section 4, our proposed method is described and the effective ness is investigated trough a cross task recognition experiment. Finally, a summary and an outlook on a future work are given in Section 5."
478949,21258,9804,Analysis of Dialectal Influence in Pan-Arabic ASR,2011,"In this paper, we analyze the impact of five Arabic dialects on the front-end and pronunciation dictionary components of an Automatic Speech Recognition (ASR) system. We use ASR's phonetic decision tree as a diagnostic tool to compare the robustness of MFCC and MLP front-ends to dialectal variations in the speech data and found that MLP Bottle-Neck features are less robust to such variations. We also perform a rule-based analysis of the pronunciation dictionary, which enables us to identify dialectal words in the vocabulary and automatically generate pronunciations for unseen words. We show that our technique produces pronunciations with an average phone error rate 9.2%. Arabic language is characterized by its multitude of dialects. Although Modern Standard Arabic (MSA) is used in writing, TV/radio broadcasts and for formal communication, all informal communication is typically carried out in one of the regional dialects of Arabic. Dialectal variations influence the pronunciation dictionary, acoustic and language models in an ASR. Previous works on dialectal Arabic ASR include cross- dialectal data sharing (1), improved pronunciation and language modeling (2, 3), etc. In this paper, we describe our experiments on a dialectal Arabic speech database, where we focus on analyzing the behavior of different front-ends and pronunciation dictionary due to dialectal variations between speakers. We evaluate Mel-Frequency Cepstral Coefficients (MFCC) and Multi-Layer Perceptrons (MLP), on their ability to handle these variations that arise due to different dialects. Extending our previous work on gender normalization (4), we use phonetic decision trees as a diagnostic tool to analyze the influence of dialect in the clustered models. We introduce questions pertaining to dialect in addition to context in the building of the decision tree. We then build the tree to cluster the contexts and calculate the number of leaves that belong to branches with dialectal questions. The ratio of such 'dialectal' models to the total model size is used as a measure for dialect normalization. The higher the ratio, the more models are affected by the dialect, hence less normalization and vice versa. We further extend our analysis to the pronunciation dictionary, where we investigate ways to generate rule-based pronunciations for unseen words in a dialect with minimum manual effort. Our setup features a 'Pan-Arabic' dictionary, which contains pronunciations typically found in five Arabic dialects. We analyze the pronunciation variants in our common dictionary using acoustic model alignments to derive the dialect-specific pronunciations for each word. This forms the source of our rule-learning algorithm which maps word pronunciations from one dialect to another. These rules are then used to generate pronunciations for unseen words and the accuracy is estimated."
142583,21258,9804,Articulatory Modeling Based on Semi-polar Coordinates and Guided PCA Technique,2009,"Abstract Research on 2-dimensional static articulatory modeling has been performed by using the semi-polar system and the guided PCA analysis of lateral X-ray images of vocal tract. The density of the grid lines in the semi-polar system has been increased to have a better descriptive precision. New parameters have been introduced to describe the movements of tongue apex. An extra feature, the tongue root, has been extracted as one of the elementary factors in order to improve the precision of tongue model. New methods still remain to be developed for describing the movements of tongue apex. Index Terms : articulatory modeling, semi-polar coordinate system, guided PCA 1. Introduction Articulatory models are usually used to transform articulatory parameters to an estimated geometric shape of the vocal tract from which the cross-sectional area function of the vocal tract can be specified and acoustic characteristics can be determined [1]. In creating such models, the concern is mainly geometric accuracy. Various articulatory models have been developed which can be classified into static and dynamic categories [2]. Also, the models can be implemented in either 2- dimensional or 3-dimensional space. In this paper, we present a research on 2-dimensional static articulatory modeling based on statistical analysis of lateral X-ray images of vocal tract. The methods for measuring articulatory parameters have been researched, as well as the methods for both feature extraction and articulatory modeling. A technique for articulatory modeling based on statistical analysis of midsagittal vocal tract X-ray images was originally developed by S. Maeda [3-5] and was extended by others. Maeda proposed a semi-polar coordinate system for measuring the midsagittal outlines of vocal tract. Based on it, an articulatory model of tongue which was composed of linear factors was determined by a statistical analysis of tongue shapes and jaw-opening measured on the images. The combination of these linear factors could adequately describe the tongue shapes which had been observed during the utterances of 12 French vowels in continuous speech sentences and in certain disyllables. We try to extend Maeda’s techniques for articulatory modeling. A modified semi-polar coordinate system has been developed in order to increase the precision of the models. Two parameters have been introduced to represent the position of the tongue apex. A software toolbox, XArticulators 2.0, has been developed, aiming at measuring the shapes of vocal tract automatically. The guided PCA technique has been used to extract features and to build linear articulatory models. Furthermore, non-linear articulatory models have been built by using ANNs and the precision of the linear models has been compared with that of the non-linear models. The X-ray image sequence is described in Sec. 2. In Sec. 3, the design of the semi-polar system is reviewed. The proposed improvements to articulatory measurement are presented in detail. The design of the software toolbox is also described. Sec. 4 presents the guided PCA technique for extracting features and for building linear articulatory models. In Sec. 5, the performance of the linear articulatory models are evaluated and compared with the ANN non-linear models in terms of modeling precision. It is concluded in Sec. 6 that though the proposed improvements are effective to represent the movements of vocal tract, some new methods still remain to be developed for describing the movements of tongue apex."
96421,21258,9804,Speech Technology for e-Inclusion of People with Physical Disabilities and Disordered Speech,2005,"Abstract Speech technology is potentially of enormous benefit to people with physical disabilities. Applications of speech technology to e-inclusion are reviewed and described in the areas of access, control, communication and rehabilitation/therapy, with particular reference to speech technology developments for people with disordered speech. To be successful, applications should effectively take into account the needs of user groups and have the ability to adapt to the needs of individuals. This is a challenging area but effective progress can be made through multi-disciplinary research and development. 1. Introduction People with physical disabilities take advantage of a variety of methods to gain access to information technology and to electronic assistive technology for communication, mobility and daily living tasks. Many of these access methods are slow and can lead to frustration, a prime example being the use of switch-activated menu scanning (hereafter referred to as switch-scanning). Automatic Speech Recognition is potentially of enormous benefit to people with severe physical disabilities. The tremendous richness of human speech communication gives the user many degrees of freedom for control and input. The speed of speech recognition also gives it a potential advantage over other input methods commonly employed by physically disabled people. People with neurological conditions causing disability often have associated dysarthria, which is the most common acquired speech disorder affecting 170 per 100,000 population [1]. This may be developmental dysarthria such as that associated with cerebral palsy or acquired dysarthria associated with progressive neurological disease (e.g. Parkinson's disease, motor neurone disease, and multiple sclerosis) or sudden onset conditions such as/or head injury. In its severest form, dysarthric speech is unintelligible to others and may take the form of producing vocal utterances, rather than words recognisable to unfamiliar communication partners. The combination of speech and general physical disability can make it particularly problematic for people to interact in their environment and can severely limit independence and inclusion. This paper examines some of the areas for e-inclusion of people with disabilities that can benefit from the use of speech technology. These areas include access, control, communication, rehabilitation and therapy. In all areas, the use of speech technology for people with dysarthric speech is particularly examined and discussed."
82300,21258,9804,Excitation Modeling Based on Waveform Interpolation for HMM-based Speech Synthesis,2010,"It is generally known that a well-designed excitation produces high quality signals in hidden Markov model (HMM)-based speech synthesis systems. This paper proposes a novel tech- niques for generating excitation based on the waveform inter- polation (WI). For modeling WI parameters, we implemented statistical method like principal component analysis (PCA). The parameters of the proposed excitation modeling techniques can be easily combined with the conventional speech synthesis sys- tem under the HMM framework. From a number of experi- ments, the proposed method has been found to generate more naturally sounding speech. Index Terms: HMM-based speech synthesis, Waveform Inter- polation, Principal Component Analysis In this paper, we propose a novel approach to excitation modeling under the waveform interpolation (WI) framework. For parameterizing the excitation generation model, a charac- teristic waveform (CW) is extracted from each frame of LP residual signals. To derive a compact representation of each CW, we apply principal component analysis (PCA) to a collec- tion of the extracted CW's. Once PCA is done, each CW can be compactly approximated as a linear combination of a few PCA basis vectors. The statistical distribution of the linear com- bination coefficients and their dynamics can be efficiently de- scribed by means of HMM's for which the relevant parameters are estimated by following the conventional HMM training pro- cedure. Given a sentence we want to synthesize, the sequence of CW's can be generated from the trained HMM's according to the maximum likelihood (ML) criterion. The WI algorithm enables a smooth transition between adjacent CW's resulting in a more natural excitation signal. The major advantages of the proposed technique are twofold. First, instead of using a fixed set of waveforms such as the impulse train and the ran- dom noise, the proposed method finds CWs which represents the excitation waveforms from the various kinds of modeling in frequency domain. Second, the WI approach lets the excita- tion signal evolve smoothly, which may reduce the audible arti- facts of the synthesized speech. From a number of experiments on speech synthesis, it has been demonstrated that the propose technique enhances the quality of the synthesized speech."
419947,21258,9804,Automatic N-gram Language Model Creation from Web Resources,2001,"Abstract This paper describes an automatic building of N-gram languagemodels from Web texts for large vocabulary continuous speechrecognition. Although a huge amount of well-formed texts areneeded to train a model, collecting and organizing such text cor-pus for every task by hand needs a great labor. We need the lan-guage model to update frequently to cover the current topics.To deal with this problem, we propose an automatic languagemodel creation method by collecting Web texts via keyword-based Web search engines. We can build a task-dependent lan-guage model by selecting suitable keywords for the task. A textﬁltering algorithm based on character perplexity is developedto extract proper Japanese texts from Web texts. A languagemodel for a medical consulting task created by the proposedmethod shows the higher word recognition rate by 11.4% thanthat of a conventional newspaper language model. 1. Introduction Recently, the performance of a large vocabulary continuousspeech recognition has accomplished improvements in Japan.The progress of the Japanese databases[1, 2], such as a text cor-pus and speech data, is a big factor of it. In the present largevocabulary continuous speech recognition, it uses two statisti-cal models, an N-gram language model and an HMM (HiddenMarkov Model) acoustic model. A maintenance of a large-scaleJapanese database for highly precise training is indispensablebecause of statistical learning for model construction.It is desirable to make a language model suitable for everytask, since it inherits highly the properties of the training data.However the quantity of the training data is not still enough. Forexample, the system with the language model from the newspa-per article corpus can recognize only a sentence like a newspa-per. For spoken language recognition, a collection of spokenlanguage texts is needed. Offering the language model for alltasks is not realistic because of a great labor for the maintenanceby hand.To solve this problem, we propose an automatic creationsystem for task dependent language models using Web re-sources."
125422,21258,9804,Automatic Evaluation of Characteristic Speech Disorders in Children with Cleft Lip and Palate,2008,"Abstract This paper discusses the automatic evaluation of speech of chil-dren with cleft lip and palate (CLP). CLP speech shows specialcharacteristics such as hypernasality, backing, and weakeningof plosives. In total ve criteria were subjectively assessed byan experienced speech expert on the phone level. This subjec-tive evaluation was used as a gold standard to train a classi-cation system. The automatic system achieves recognition re-sults on frame, phone, and word level of up to 75.8% CL. Onspeaker level signicant and high correlations between the sub-jective evaluation and the automatic system of up to 0.89 areobtained. Index Terms : pathologic speech, speech assessment, pronun-ciation scoring, children’s speech 1. Introduction Cleft Lip and Palate (CLP) is the most common malformationof the head. It constitutes almost two-thirds of the major facialdefects and almost 80% of all orofacial clefts [1]. Its prevalencediffers in different populations from 1 in 400 to 500 newborns inAsians to 1 in 1500 to 2000 in African Americans. The preva-lence in Caucasians is 1 in 750 to 900 births [2, 3].In clinical practice, articulation disorders are mainly eval-uated by subjective tools. The simplest method is the audi-tive perception, mostly performed by a speech therapist. Pre-vious studies have shown that experience is an important fac-tor that inuences the subjective estimation of speech disorderswhich leads to inaccurate evaluation by persons with only fewyears of experience as speech therapist [4]. Until now, objectivemeans exist only for quantitative measurements of nasal emis-sions [5, 6, 7] and for the detection of secondary voice disorders[8]. But other specic articulation disorders in CLP cannot besufciently quantied.In this paper, we present a new technical procedure for themeasurement and evaluation of specic speech disorders andcompare the results obtained with subjective ratings of an expe-rienced speech therapist."
201660,21258,9804,Robust automatic speech recognition in low-SNR car environments by the application of a connectionist subspace-based approach to the melbased cepstral coefficients.,2001,"In this paper, the problem of robust large-vocabulary continuous- speech recognition (CSR) in the presence of highly interfering car noise has been considered. Our approach is based on the noise reduction of the parameters that we use for recognition, that is, the Mel-based cepstral coefficients. This is achieved by the use of a Multilayer Perceptron (MLP) network for noise re- duction in the cepstral domain in order to get less-variant pa- rameters. Then, the obtained enhanced features are refined via the Karhunen-Loeve Transform (KLT) implemented using the Principal Component Analysis (PCA). Experiments show that the use of the enhanced parameters using such an approach in- creases the recognition rate of the CSR process in highly inter- fering car noise environments. The HTK Hidden Markov Model Toolkit was used throughout our experiments. Results show that the proposed hybrid technique when included in the front-end of an HTK-based CSR system, outperforms that of the conven- tional recognition process based on either a KLT- or an MLP- based preprocessing recognition in severe interfering car noise environments for a wide range of SNRs varying from 16 dB to -4 dB using a noisy version of the TIMIT database. In this paper, we propose a novel robust CSR system to be used in car noisy environments. Our approach for noise reduction is applied in the cepstral domain. It is based on the application of a combination of the Karhunen-Loeve Transform (KLT) and a Connectionist approach. Each of these two approaches has been successfully used in both speech enhancement and recognition processes. We show in this paper through experiments on highly noisy data that a cepstral noise reduction can be obtained us- ing such an approach and consequently an improvement of the recognition performance. This paper will be organized into the following sections. In sec- tion 2 we describe the basis of the MLP network and the PCA approaches that will be used to describe our proposed hybrid PCA-MLP approach. Then, we proceed in section 3 with the de- scription of the database, the platform used in our experiments and the evaluation of the proposed MLP-PCA-based recognizer in a noisy car environment and the comparison of such a recog- nizer to both the MLP- and the PCA-based recognizers in order to evaluate its performance. Finally, in section 5 we conclude and discuss our results."
2197477,21258,9804,Noise robust pitch tracking by subband autocorrelation classification,2012,"Speech pitch tracking is one of the elementary tasks of the Computational Auditory Scene Analysis (CASA). While a human can easily listen to the voiced pitch in highly noisy recordings, the performance of automatic speech pitch tracking degrades in unknown noisy audio conditions. Traditional pitch trackers use either autocorrelation or the Fourier transform to calculate periodicity, which works well for clean recordings. For noisy recordings, however, the accuracy of these pitch trackers degrades in general. For example, the information in parts of the frequency spectrum may be lost due to analog radio band transmission and/or contain additive noise of various kinds. #R##N#Instead of explicitly using the most obvious features of autocorrelation, we propose a trained classier-based approach, which we call Subband Autocorrelation Classification (SAcC). A multi-layer perceptron (MLP) classier is trained on the principal components of the autocorrelations of subbands from an auditory filterbank. The output of the MLP classifier is temporally smoothed to produce the pitch track by finding the Viterbi path of a Hidden Markov Model (HMM). Training on various types of noisy speech recordings leads to a great increase in performance over state-of-the-art algorithms, according to both the traditional Gross Pitch Error (GPE) measure, and a proposed novel Pitch Tracking Error (PTE) which more fully reflects the accuracy of both pitch estimation/extraction and voicing detection in a single measure. #R##N#To verify the generalization and specificity of SAcC, we test SAcC on a real world problem that has a large-scale noisy speech corpus. The data is from the DARPA Robust Automatic Transcription of Speech (RATS) program. The experiments on the performance evaluation of SAcC pitch tracking confirm the generalization power of SAcC across various unknown noise conditions and distinct speech corpora. We also report the use of SAcC output adds a significant improvement to a Speaker Identification (SID) system for RATS as well, suggesting the potential contribution of SAcC pitch tracking in the higher-level tasks."
570108,21258,9804,An Information Theoretic Approach to Predict Speech Intelligibility for Listeners with Normal and Impaired Hearing,2007,"Hearing loss afflicts as many as 10\% of our population.Fortunately, technologies designed to alleviate the effects ofhearing loss are improving rapidly, including cochlear implantsand the increasing computing power of digital hearing aids. Thisthesis focuses on theoretically sound methods for improvinghearing aid technology. The main contributions are documented inthree research articles, which treat two separate topics:modelling of human speech recognition (Papers A and B) andoptimization of diagnostic methods for hearing loss (Paper C).Papers A and B present a hidden Markov model-based framework forsimulating speech recognition in noisy conditions using auditorymodels and signal detection theory. In Paper A, a model of normaland impaired hearing is employed, in which a subject's pure-tonehearing thresholds are used to adapt the model to the individual.In Paper B, the framework is modified to simulate hearing with acochlear implant (CI). Two models of hearing with CI arepresented: a simple, functional model and a biologically inspiredmodel. The models are adapted to the individual CI user bysimulating a spectral discrimination test. The framework canestimate speech recognition ability for a given hearing impairmentor cochlear implant user. This estimate could potentially be usedto optimize hearing aid settings.Paper C presents a novel method for sequentially choosing thesound level and frequency for pure-tone audiometry. A Gaussianmixture model (GMM) is used to represent the probabilitydistribution of hearing thresholds at 8 frequencies. The GMM isfitted to over 100,000 hearing thresholds from a clinicaldatabase. After each response, the GMM is updated using Bayesianinference. The sound level and frequency are chosen so as tomaximize a predefined objective function, such as the entropy ofthe probability distribution. It is found through simulation thatan average of 48 tone presentations are needed to achieve the sameaccuracy as the standard method, which requires an average of 135presentations."
60541,21258,9804,Using Word-level Pitch Features to Better Predict Student Emotions during Spoken Tutoring Dialogues,2005,"Abstract In this paper, we advocate for the usage of word-level pitch features for detecting user emotional states during spoken tutoring dialogues. Prior research has primarily focused on the use of turn-level features as predictors. We compute pitchfeatures at the word level and resolve the problem of combining multiple features per turn using a word-levelemotion model . Even under a very simple word-level emotionmodel, our results show an improvement in prediction using word-level features over using turn-level features. We find that the advantage of word-level features lies in a betterprediction of longer turns. 1. Introduction We investigate the utility of using pitch features applied at the word level for the task of predicting student emotions in twocorpora of spoken tutoring dialogues. Motivation for thiswork comes from the performance gap between human tutorsand current machine tutors; typically students tutored byhuman tutors learn more than students tutored by computer tutors. One of the methods currently being explored as a wayof closing this gap is to incorporate affective reasoning intocurrent computer tutoring systems, including dialogue-based tutoring systems, e.g. [1, 2].Previous spoken dialogue research in other domains hasshown that turn-level prosodic, lexical, dialogue, and otherfeatures can be used to predict user emotional states [3-5]. Tobetter approximate the prosodic information [6] uses word-level features and successfully applies them to a different emotion detection task. To our knowledge, there is noprevious work that directly compares the impact of usingfeatures at the sub-turn rather than the turn level for emotionprediction. In this paper we are performing a first comparisonof the two levels for the task of detecting student emotional states.There are many choices for sub-turn units (breath groups, intonational phrases, syntactic chunks, words, syllables). Wewill use words as our sub-turn units because it is straightforward to do the segmentation and because theseunits have been used successfully by other researchers for similar tasks [6]. Moreover, in a real-time dialogue system,the segmentation is available as a byproduct of the automatic speech recognition. To simplify our word versus turn-level featurecomparison, we will focus"
393719,21258,9804,Modelling human speech recognition using automatic speech recognition paradigms in SpeM,2003,"We have recently developed a new model of human speech recognition, based on automatic speech recognition techniques [1]. The present paper has two goals. First, we show that the new model performs well in the recognition of lexically ambiguous input. These demonstrations suggest that the model is able to operate in the same optimal way as human listeners. Second, we discuss how to relate the behaviour of a recogniser, designed to discover the optimum path through a word lattice, to data from human listening experiments. We argue that this requires a metric that combines both path-based and word-based measures of recognition performance. The combined metric varies continuously as the input speech signal unfolds over time. The SPEech-based Model of human speech recognition (SpeM [1]) is based on procedures and techniques used in automatic speech recognition (ASR), but attempts to account for the performance of human listeners. SpeM therefore implements the same core theoretical assumptions about human speech recognition (HSR) as are implemented in the HSR model Shortlist [2,3]. SpeM is an advance on Shortlist in at least two ways (see [1] for further details). First, SpeM can take real speech as input, while the input of Shortlist consists of an error-free string of discrete phonemes. Second, SpeM can deal with the pronunciation variants in real speech caused by processes such as insertion and deletion. The lexical search process in Shortlist is unable to deal with a mismatch between the number of phones in the input and the number of phones stored in the canonical pronunciations stored in the Shortlist lexicon. In the present paper, we show that SpeM is able to account for key aspects of human listening ability. We compare its performance to that of the Shortlist model, and show that SpeM, like Shortlist before it, can recognise the words in stretches of speech that are lexically ambiguous. Most data on human spoken word recognition involves measures of how quickly or accurately words can be identified. A central requirement of any model of human speech recognition is therefore that it should be able to provide a continuous measure (usually referred to as ‘activation’ in the psychological literature) of how easy each word will be for listeners to identify. We address the problem of relating the performance of a path-based model of continuous speech recognition to word-based data from psycholinguistic experiments."
134918,21258,9804,AN ARTICULATORY STUDY OF EMOTIONAL SPEECH PRODUCTION,2005,"Few studies exist on the topic of emotion encoding in speech in the articulatory domain. In this report, we analyze articulatory data collected during simulated emotional speech production and investigate differences in speech articulation among four emotion types; neutral, anger, sadness and happiness. The movement data of the tongue tip, the jaw and the lower lip, along with speech, were obtained from a subject using an Electromagnetic articulography (EMA) system. The effectiveness of the articulatory parameters in emotion classification was also investigated. A general articulatory behavior observed was that emotionally elaborated speech production exhibits more peripheral articulations when compared to neutral speech. The tongue tip, jaw and lip positioning become more advanced when emotionally charged. This tendency was especially prominent for the tongue tip and jaw movements associated with sad speech. Angry speech was characterized by greater ranges of displacement and velocity, while it was opposite for sad speech. Happy speech was comparable in articulation to the neutral speech, but showed the widest range of pitch variation. It, however, remains to be seen if there is a trade-off between articulatory activity and voicing activity in emotional speech production. Multiple discriminant analysis showed that emotion is better classified in the articulatory domain. One probable reason is that the independency in the manipulation of each articulator may provide more degrees of freedom and less overlap in the articulatory parameter space. Analysis also showed distinct emotion effects for different phonemes: the high front vowel /IY/ was found to be less discriminated in both articulatory and acoustic domains than other peripheral vowels such as /AA/ and /UW/. It is likely that the physical boundary effect in the /IY/ articulation may leave less room to vary the tongue positioning and/or the lip configuration when compared to other vowels, resulting in less acoustic contrast among emotion types."
14336,21258,9804,Perception experiment combining a parametric loudspeaker and a synthetic talking head,2005,"In this thesis, different aspects concerning how to make synthetic talking faces more expressive have been studied. How can we collect data for the studies, how is the lip articulation affected by expressive speech, can the recorded data be used interchangeably in different face models, can we use eye movements in the agent for communicative purposes? The work of this thesis includes studies of these questions and also an experiment using a talking head as a complement to a targeted audio device, in order to increase the intelligibility of the speech. The data collection described in the first paper resulted in two multimodal speech corpora. In the following analysis of the recorded data it could be stated that expressive modes strongly affect the speech articulation, although further studies are needed in order to acquire more quantitative results and to cover more phonemes and expressions as well as to be able to generalise the results to more than one individual. When switching the files containing facial animation parameters (FAPs) between different face models (as well as research sites), some problematic issues were encountered despite the fact that both face models were created according to the MPEG-4 standard. The evaluation test of the implemented emotional expressions showed that best recognition results were obtained when the face model and FAP-file originated from the same site. The perception experiment where a synthetic talking head was combined with a targeted audio, parametric loudspeaker showed that the virtual face augmented the intelligibility of speech, especially when the sound beam was directed slightly to the side of the listener i. e. at lower sound intesities. In the experiment with eye gaze in a virtual talking head, the possibility of achieving mutual gaze with the observer was assessed. The results indicated that it is possible, but also pointed at some design features in the face model that need to be altered in order to achieve a better control of the perceived gaze direction."
2759224,21258,9804,Validating a second language perception model for classroom context. A longitudinal study within the Perceptual Assimilation Model,2011,"Abstract The present study verified whether adult listeners retain the ability to improve non-native speech perception and if it can be significantly enhanced in the formal context, a very impoverished context with respect to the natural one. We tested (i) whether perceptual learning is possible for adults in a classroom context during focused phonetic lessons, and (ii) whether it follows the pattern predicted for natural acquisition by the PAM-L2 [1]. The results showed that adult listeners are still able to improve foreign sound perception and this ability seems to occur also in formal contexts in line with the PAM-L2 predictions. Index Terms : non-native phone perception, foreign language acquisition, PAM. 1. Introduction The dominant theories in second language (L2) phoneme acquisition argue that perceptual similarity/dissimilarity between the sounds of L2 and native language (L1) governs their assimilation or non-assimilation and dictates their learnability in adulthood. According to the Perceptual Assimilation Model (PAM) [2], subjects with learning an L2 in classroom context (FLA), most of them limited L2 instruction, especially that typified by classroom-only education with instructors with a strong L1 accent, can be considered as naive listeners, i.e., functional monolinguals not actively learning or using an L2. The PAM assumes that the way naive listeners assimilate non -native phones to native phonemes is determined by the detection of commonalities between them [3]. The PAM predicts that the non-native phones can be Categorized or not consistently categorised (i.e., Uncategorized) as exemplars of native phonemes, falling between two or more L1 phonemes. Finally, non-native phones cannot be categorised at all as speech sounds. If two Categorized non-native phones are perceived as acceptable exemplars of two different native phonemes, a very good/excellent discrimination is predicted (Two Category assimilation, TC). Conversely, poor discrimination is expected for Single Category (SC) assimilation, where two non-native phones are perceived as equally good or poor exemplars of a single native phoneme. If two non-native phones are both perceived as a single native phoneme but differ in rating, intermediate degree of discrimination is predicted (Category Goodness assimilation; CG). For the Uncategorized phones, if one non-native phone is perceived as a native phoneme and the other is perceived as an uncategorised speech sound, the predicted degree of discrimination is good (Uncategorised-Categorised assimilation, UC). Two non-native phones assimilated to partially-similar native phonemes will be discriminated from poor to moderate (Uncategorised-Uncategorised assimilation; UU). Finally, the predicted discrimination is good/excellent for the Non assimilable typology (NA) since two non-native phones are not perceived as any speech sound and are easily distinguishable from each other. A recent extension of this model, i.e., the PAM-L2 [1], refers to adult L2 learners in an L2 immersion context, for which a common L1-L2 system (i.e., an interlanguage) is developing. The L2 perceptual learning seems to be determined by the increase in L2 vocabulary size that causes the learners to"
77171,21258,9804,An Automatic Singing Skill Evaluation Method for Unknown Melodies Using Pitch Interval Accuracy and Vibrato Features,2006,"This paper presents a method of evaluating singing skills that does not require score information of the sung melody. This requires an approach that is different from existing systems, such as those currently used for Karaoke systems. Previous research on singing evaluation has focused on analyzing the characteristics of singing voice, but were not aimed at developing an automatic evaluation method. The approach presented in this study uses pitch inter- val accuracy and vibrato as acoustic features which are indepen- dent from specific characteristics of the singer or melody. The ap- proach was tested by a 2-class (good/poor) classification test with 600 song sequences, and achieved an average classification rate of 83.5%. The aim of this study is to explore a method of automatic evalua- tion of singing skills without score information. Our interest lies in identifying the criteria that human subjects use in judging the quality of singing for unknown melodies, using acoustic features which are independent from specific characteristics of the singer or melody. Such evaluation systems can be useful tools for im- proving singing skills, and also can be applied to broadening the scope of music information retrieval and singing voice synthesis. Previous work related to singing skills include those based on a control model of fundamental frequency (F0) trajectory (1), gen- eral characteristics (2, 3), as well as work on automatic discrimi- nation of singing and speaking voices (4), and acoustic differences between trained and untrained singers' voices (5, 6, 7). None of these work have gone as far as presenting an automatic evaluation method. This paper presents a singing skill evaluation scheme based on pitch interval accuracy and vibrato, which are regarded as features that function independently from the individual characteristics of singer or melody. To test the validity of these features, an experi- ment of automatic evaluation of singing performance by a 2-class classification (good/poor) was conducted. The following sections describe our approach and the exper- imental results of its evaluation. Section 2 presents discussion of features. Section 3 describes the classification experiment and its evaluation. Section 4 concludes the paper, with discussion on di- rections for future work."
580711,21258,9804,Investigating Fine Temporal Dynamics of Prosodic and Lexical Accommodation,2013,"Abstract Conversationalinteractionisadynamicactivityinwhichpartic-ipantsengageintheconstructionofmeaningandinestablishingand maintaining social relationships. Lexical and prosodic ac-commodation have been observed in many studies as contribut-ing importantly to these dimensions of social interaction. How-ever, while previous works have considered accommodationmechanisms at global levels (for whole conversations, halvesand thirds of conversations), this work investigates their evo-lution through repeated analysis at time intervals of increasinggranularity to analyze the dynamics of alignment in a spokenlanguage corpus. Results show that the levels of both prosodicand lexical accommodation ﬂuctuate several times over thecourse of a conversation.Index Terms: prosodic accommodation, lexical alignment, dy-namics, task-based interactions 1. Introduction In spoken interaction, participants have been observed to adapttheir speech production to that of their interlocutor. Inter-speakeradaptationhasbeentermedaccommodation,alignment,convergence, priming as well as synchrony, and has been re-ported in terms of pronunciation [1, 2], prosody [3, 4, 5, 6],lexicon [7, 8, 9] and syntax [10, 11]. Herein, we will use theterm accommodation when the features of a speaker’s produc-tionchangeasafunctionoftheirpartners’. Theoriesaccountingfor some of these phenomena include Garrod and Pickering’sInteractive Alignment Theory [8] as well as Giles and Coup-land’s CAT theory [12].Accommodation mechanisms are a particularly importantaspectofspokeninteractionastheyfacilitate,throughthealign-ment of cognitive representations, comprehension and under-standing between interlocutors. They correlate with the com-municative success of the interaction, by decreasing misunder-standings and attaining goals faster [13, 8, 14]. In addition, ac-commodationcontributestothesocialsuccessoftheinteractionby building rapport (i.e. harmonious relationships and mutualattention) and afﬁliation [15, 16, 17, 18].In terms of prosody, accommodation in pitch, (measuredas meanf"
137400,21258,9804,Speaker Adaptation Based on Two-Step Active Learning,2009,"We propose a two-step active learning method for supervised speaker adaptation. In the first step, the initial adaptation data is collected to obtain a phone error distribution. In the second step, those sentences whose phone distributions are close to the error distribution are selected, and their utterances are collected as the additional adaptation data. We evaluated the method us- ing a Japanese speech database and maximum likelihood linear regression (MLLR) as the speaker adaptation algorithm. We confirmed that our method had a significant improvement over a method using randomly chosen sentences for adaptation. Index Terms: speech recognition ,speaker adaptation, active learning nition accuracies in comparison with the conventional adapta- tion frameworks when the initial speaker-independent acoustic model has already been tuned to the given recognition task. In this paper, we propose a two-step active learning method for supervised speaker adaptation. In the first step, our method collects a small amount of utterances from a user to obtain his/her tendency in speech recognition errors. In the second step, it selects those sentences rich in phonetic units in the er- rors from a sentence pool, and it collects their utterances as ad- ditional data for adaptation. Since our method directly aims at decreasing recognition errors, it is expected to be highly dis- criminative. Our method has two critical issues: One is how to relate the recognition errors to the selection criterion of adap- tation sentences. The other is how to set the size of the initial adaptation data in the first step; it should be as small as possi- ble in order to decrease the user's effort, but it must be sufficient enough to estimatethe tendency of errors precisely. Wedescribe our approach to resolving these issues in this paper. This paper is organized as follows. Section 2 explains our method, and Section 3 briefly explains the MLLR speaker adap- tation method. Section 4 reports our evaluation experiments us- ing a Japanese speech database, and Section 5 concludes the paper."
72568,21258,9804,"Bilingual aligned corpora for speech to speech translation for Spanish, English and Catalan.",2005,"Abstract In the framework of the EU-funded Project LC-STAR, a set of Language Resources (LR) for all the Speech to SpeechTranslation components (Speech recognition, MachineTranslation and Speech Synthesis) was developed. This paperdeals with the development of bilingual corpora in Spanish,US English and Catalan. The corpora were obtained fromspontaneous dialogues in one of these three languages whichwere translated to the other two languages. The paperdescribes the translation methodology, specific problems oftranslating spontaneous dialogues to be used for MT training,formats and the validation criteria. 1. Introduction The EU-funded project LC-STAR 1 aims to create lexica andcorpora (LR) needed for transferring Speech-to-SpeechTranslation (SST) components, i.e. flexible vocabulary speech recognition, high quality text-to-speech synthesis and speech-centred translation into the selected languages. LC-STARconcentrates on large lexica with phonetic, prosodic andmorphosyntactic content and on bilingual aligned text corpora.Different approaches to speech translation show that it ispossible to develop robust speech-to-speech translationsystems for small to medium-sized domains usingsophisticated speech recognition and machine translationtechnology. The major problems in the field of speech-to-speech translation are: • Acquisition of mono- or bilingual, domain-specifictraining data.• Robust behaviour of the MT component for speechrecognition errors and spoken language phenomena.• Development of efficient recognition and translationcomponents, and a high quality text-to-speech synthesissystem.Currently, the most promising approach for speech-to-speechtranslation systems is to use statistical machine translation(SMT). This system is able to learn from example translations, shows some robustness against speech recognition errors andoutperforms other speech-to-speech-translation systems (cf.Verbmobil)One of the most powerful resources for statistical machinetranslation are multilingual corpora. For Speech-to-SpeechTranslation applications, LR need to cope with spontaneous"
2714909,21258,9804,A unified approach for underdetermined blind signal separation and source activity detection by multichannel factorial hidden Markov models,2014,"This paper proposes to introduce a new model called “the multichannel factorial hidden Markov Model (MFHMM)” for underdetermined blind signal separation (BSS). For monaural source separation, one successful approach involves applying nonnegative matrix factorization (NMF) to the magnitude spectrogram of a mixture signal, interpreted as a non-negative matrix. Up to now, multichannel extensions of NMF, which allow for the use of spatial information as an additional clue for source separation, have been proposed by several authors and proven to be an effective approach for underdetermined BSS. This approach is based on the assumption that an observed signal is a mixture of a limited number of source signals each of which has a static power spectral density scaled by a time-varying amplitude. However, many source signals in real world are nonstationary in nature and the variations of the spectral densities are much richer in time. Moreover, many sources including speech tend to stay inactive for some while until they switch to an active mode, implying that the total power of a source may depend on its underlying state. To reasonably characterize such a non-stationary nature of source signals, this paper proposes to extend the multichannel NMF model by modeling the transition of the set consisting of the spectral densities and the total power of each source using a hidden Markov model (HMM). By letting each HMM contain states corresponding to active and inactive modes, we will show that voice activity detection and source separation can be solved simultaneously through parameter inference of the present model. The experiment showed that the proposed algorithm provided a 7.65 dB improvement compared with the conventional multichannel NMF in terms of the signal-to-distortion ratio. Index Terms: blind signal separation, source activity detection, a hidden Markov model, non-negative matrix factorization"
55731,21258,9804,Never-ending learning with dynamic hidden Markov network.,2007,"Current automatic speech recognition systems have two distinctive modes of operation: training and recognition. After the training, system parameters are fixed, and if a mismatch between training and testing conditions occurs, an adaptation procedure is commonly applied. However, the adaptation methods change the system parameters in such a way that previously learned knowledge is irrecoverably destroyed. In searching for a solution to this problem and motivated by the results of recent neuro-biological studies, we have developed a network of hidden Markov states that is capable of unsupervised on-line adaptive learning while preserving the previously acquired knowledge. Speech patterns are represented by state sequences or paths through the network. The network can detect previously unseen patterns, and if such a new pattern is encountered, it is learned by adding new states and transitions to the network. Paths and states corresponding to spurious events or ”noises” and, therefore, rarely visited, are gradually removed. Thus, the network can grow and shrink when needed, i.e. it dynamically changes its structure. The learning process continues as long as the network lasts, i.e. theoretically forever, so it is called neverending learning. The output of the network is the best state sequence and the decoding is done concurrently with the learning. Thus the network always operates in a single learning/decoding mode. Initial experiments with a small database of isolated spelled letters showed that the Dynamic Hidden Markov network is indeed capable of never-ending learning and can perfectly recognize previously learned speech patterns. Index Terms: never-ending learning, life-long learning, dynamic hidden markov network, self-organization, topology representation."
342651,21258,9804,Low-Latency Incremental Speech Transcription in the Synface Project,2003,"This thesis presents work in the area of automatic speech recognition (ASR). The thesis focuses on methods for increasing the efficiency of speech recognition systems and on techniques for efficient representation of different types of knowledge in the decoding process. In this work, several decoding algorithms and recognition systems have been developed, aimed at various recognition tasks. The thesis presents the KTH large vocabulary speech recognition system. The system was developed for online (live) recognition with large vocabularies and complex language models. The system utilizes weighted transducer theory for efficient representation of different knowledge sources, with the purpose of optimizing the recognition process. A search algorithm for efficient processing of hidden Markov models (HMMs) is presented. The algorithm is an alternative to the classical Viterbi algorithm for fast computation of shortest paths in HMMs. It is part of a larger decoding strategy aimed at reducing the overall computational complexity in ASR. In this approach, all HMM computations are completely decoupled from the rest of the decoding process. This enables the use of larger vocabularies and more complex language models without an increase of HMM-related computations. Ace is another speech recognition system developed within this work. It is a platform aimed at facilitating the development of speech recognizers and new decoding methods. A real-time system for low-latency online speech transcription is also presented. The system was developed within a project with the goal of improving the possibilities for hard-of-hearing people to use conventional telephony by providing speech-synchronized multimodal feedback. This work addresses several additional requirements implied by this special recognition task."
334836,21258,9804,Transducer Optimizations for Tight-Coupled Decoding,2001,"This thesis presents work in the area of automatic speech recognition (ASR). The thesis focuses on methods for increasing the efficiency of speech recognition systems and on techniques for efficient representation of different types of knowledge in the decoding process. In this work, several decoding algorithms and recognition systems have been developed, aimed at various recognition tasks. The thesis presents the KTH large vocabulary speech recognition system. The system was developed for online (live) recognition with large vocabularies and complex language models. The system utilizes weighted transducer theory for efficient representation of different knowledge sources, with the purpose of optimizing the recognition process. A search algorithm for efficient processing of hidden Markov models (HMMs) is presented. The algorithm is an alternative to the classical Viterbi algorithm for fast computation of shortest paths in HMMs. It is part of a larger decoding strategy aimed at reducing the overall computational complexity in ASR. In this approach, all HMM computations are completely decoupled from the rest of the decoding process. This enables the use of larger vocabularies and more complex language models without an increase of HMM-related computations. Ace is another speech recognition system developed within this work. It is a platform aimed at facilitating the development of speech recognizers and new decoding methods. A real-time system for low-latency online speech transcription is also presented. The system was developed within a project with the goal of improving the possibilities for hard-of-hearing people to use conventional telephony by providing speech-synchronized multimodal feedback. This work addresses several additional requirements implied by this special recognition task."
2229981,21258,9804,Syntactic Complexity induces Explicit Grounding in the MapTask corpus,2008,"This paper provides evidence for theories of grounding and di- alogue management in human conversation. For each utterance in a corpus of task-oriented dialogues, we calculated integ ration costs, which are based on syntactic sentence complexity. We compared the integration costs and grounding behavior under two conditions, namely face-to-face and a no-eye-contact con- dition. The results show that integration costs were signifi cantly higher for explicitly grounded utterances in the no-eye-co ntact condition, but not in the face-to-face condition. Index Terms: dialogue, syntactic complexity, grounding each of the eye-contact/no-eye-contact conditions. For th e syn- tactic analysis, MINIPAR (7) was used to produce the necessary input to the algorithm that computes the integration costs. SPLT assigns costs to each word depending on its syntactic class and context. We computed the costs for each utterance containing a CSA and checked whether it was followed by a GSA. We used two metrics to calculate the cost for an utterance from its wo rds. One metric sums the costs of all words in the utterance, and the other uses the maximum of the costs associated with the words in the utterance. 3. Results All tests were performed using the maximum metric 1 . Com- paring the utterance costs for each condition, we found no si g- nificant cost difference between the grounded and ungrounde d utterances in the eye-contact condition (t(889) = 0.82, p > 0.6), but there is a significant difference in the no-eye contact condition (t(779) = 2.69, p < 0.01)). To test the effect of both grounding and eye-contact, we ran a 2 × 2 ANOVA. The re- sults of the ANOVA showed clear effects of both groundedness (p < 0.05) and condition (p < 0.01) as well as an interaction between the two (p < 0.01). 4. Conclusions In this study we showed that increased sentence complexity in- creases the likelihood of explicit grounding when the parti ci- pants do not have eye-contact. This result is relevant for th e design of spoken dialogue systems, for example, sentence com- plexity of system utterances could be reduced in the face of fre- quent misunderstandings. The lack of a difference in the eye- contact condition is an indication of the effectiveness eye gaze and facial expressions."
2830875,21258,9804,Some Issues affecting the Transcription of Hungarian Broadcast Audio,2013,"This paper reports on a speech-to-text (STT) transcription system for Hungarian broadcast audio developed for the 2012 Quaero evaluations. For this evaluation, no manually transcribed audio data were provided for model training, however a small amount of development data were provided to assess system performance. As a consequence, the acoustic models were developed in an unsupervised manner, with the only supervision provided indirectly by the language model. The language models were trained on texts downloaded from various websites, also without any speech transcripts. This contrasts with other STT systems for Hungarian broadcast audio which use at least 10 to 50 hours of manually transcribed data for acoustic training, and typically include speech transcripts in the language models. Based on mixed results previously reported applying morph-based approaches to agglutinative languages such as Hungarian, word-based language models were used. The initial Word Error Rate (WER) of the system using contextindependent seed models from other languages of 59.8% on the 3h development corpus was reduced to 25.0% after successive training iterations and system refinement. The same system obtained a WER of 23.3% on the independent Quaero 2012 evaluation corpus (a mix of broadcast news and broadcast conversation data). These results compare well with previously reported systems on similar data. Various issues affecting system performance are discussed, such as amount of training data, the acoustic features and choice of text sources for language model training. Index Terms: Large vocabulary continuous speech recognition (LVCSR), broadcast news transcription, Hungarian language, unsupervised training, agglutinative languages, Bottleneck MLP features"
127876,21258,9804,A Two-Microphone Diversity System and its Application for Hands-Free Car Kits,2005,"Abstract In this paper we consider a two-channel diversity technique thatcombines the processed signals of two separate microphones.Forin-car applications, this enables a better compromise forthemicrophone positions. The advantage of the proposed systemis its insensitivity with respect to varying speaker sizes or lo-cal noise sources. To achieve this we choose the microphoneposition in that way that one microphone is optimum for a tallspeaker, and the second one is suitable for a small speaker. Forlocal noise sources we may apply a similar design to choosethe microphone position in accordance with the location of thenoise sources. A corresponding signal combiner has to tasks:compensation of phase shifts and weighting proportional to thesignal strength. We propose solutions for both problems anddemonstrate the effectiveness of diversity combining. 1. INTRODUCTION Due to the obvious dangers of holding a telephone in one hand,and steering a car with the other, many countries either stronglyrecommended, or legally enforced hands-free telephone oper-ation in all moving vehicles. Thus for safety and comfort rea-sons,ahands-freetelephonesystemthatprovidesthesamequal-ity of speech as conventional ﬁxed telephones is desirable. Anatural bottleneck for the speech quality of a hands-free car kitis the position of the microphone. Obviously, speech has tobe picked up as close to the mouth as possible. The importantquestion, where to place the microphone inside the car, is how-ever difﬁcultto answer. The"
14671,21258,9804,Reduced Complexity Equalization of Lombard Effect for Speech Recognition in Noisy Adverse Environments,2009,"In real-world adverse environments, speech signal corruption by background noise, microphone channel variations, and speech production adjustments introduced by speakers in an effort to communicate efficiently over noise (Lombard effect) severely impact automatic speech recognition (ASR) performance. Recently, a set of unsupervised techniques reducing ASR sensitivity to these sources of distortion have been presented, with the main focus on equalization of Lombard effect (LE). The algorithms performing maximum-likelihood spectral transformation, cepstral dynamics normalization, and decoding with a codebook of noisy speech models have been shown to outperform conventional methods, however, at a cost of considerable increase in computational complexity due to required numerous decoding passes through the ASR models. In this study, a scheme utilizing a set of speech-in-noise Gaussian mixture models and a neutral/LE classifier is shown to substantially decrease the computational load (from 14 to 2‐4 ASR decoding passes) while preserving overall system performance. In addition, an extended codebook capturing multiple environmental noises is introduced and shown to improve ASR in changing environments (8.2‐49.2% absolute WER improvement). The evaluation is performed on the Czech Lombard Speech Database (CLSD‘05). The task is to recognize neutral/LE connected digit strings presented in different levels of background car noise and Aurora 2 noises. Index Terms: Lombard effect, speech recognition, codebook decoding, frequency transformation, cepstral normalization"
2318725,21258,535,Never-ending learning system for on-line speaker diarization,2007,"In this paper, we describe new high-performance on-line speaker diarization system which works faster than real-time and has very low latency. It consists of several modules including voice activity detection, novel speaker detection, speaker gender and speaker identity classification. All modules share a set of Gaussian mixture models (GMM) representing pause, male and female speakers, and each individual speaker. Initially, there are only three GMMs for pause and two speaker genders, trained in advance from some data. During the speaker diarization process, for each speech segment it is decided whether it comes from a new speaker or from already known speaker. In case of a new speaker, his/her gender is identified, and then, from the corresponding gender GMM, a new GMM is spawned by copying its parameters. This GMM is learned on-line using the speech segment data and from this point it is used to represent the new speaker. All individual speaker models are produced in this way. In the case of an old speaker, s/he is identified and the corresponding GMM is again learned on-line. In order to prevent an unlimited grow of the speaker model number, those models that have not been selected as winners for a long period of time are deleted from the system. This allows the system to be able to perform its task indefinitely in addition to being capable of self-organization, i.e. unsupervised adaptive learning, and preservation of the learned knowledge, i.e. speakers. Such functionalities are attributed to the so called  Never-Ending Learning systems . For evaluation, we used part of the TC-STAR database consisting of European Parliament Plenary speeches. The results show that this system achieves a speaker diarization error rate of 4.6% with latency of at most 3 seconds."
1074883,21258,535,Roles of high-fidelity acoustic modeling in robust speech recognition,2007,"In this paper I argue that high-fidelity acoustic models have important roles to play in robust speech recognition in face of a multitude of variability ailing many current systems. The discussion of high-fidelity acoustic modeling is posited in the context of general statistical pattern recognition, in which the probabilistic-modeling component that embeds partial, imperfect knowledge is the fundamental building block enabling all other components including recognition error measure, decision rule, and training criterion. Within the session’s theme of acoustic modeling and robust speech recognition, I advance my argument using two concrete examples. First, an acoustic-modeling framework which embeds the knowledge of articulatory-like constraints is shown to be better able to account for the speech variability arising from varying speaking behavior (e.g., speaking rate and style) than without the use of the constraints. This higher-fidelity acoustic model is implemented in a multi-layer dynamic Bayesian network and computer simulation results are presented. Second, the variability in the acoustically distorted speech under adverse environments can be more precisely represented and more effectively handled using the information about phase asynchrony between the un-distorted speech and the mixing noise than without using such information. This high-fidelity, phase-sensitive acoustic distortion model is integrated into the same multi-layer Bayesian network but at separate, causally related layers from those representing the speaking-behavior variability. Related experimental results in the literature are reviewed, providing empirical support to the significant roles that the phase-sensitive model plays in environment-robust speech recognition."
2322043,21258,9804,Robust Speech Recognition in Noise: An Evaluation using the SPINE Corpus †,2001,"In this paper, methodologies for effective sp eech recognition are considered along with evaluations of an NRL speech in noise corpus entitled SPINE. When speech is produced in adverse conditions that include high levels of noise, workload task stress, and Lombard effect, new challenges arise concerning how to best improve recognition performance. Here, we consider tradeoffs in (i) robust features, (ii) front-end noise suppression, (iii) model adaptation, and (iv) training and testing in the same conditions. The type of noise and recording conditions can significantly impact the type of signal processing and speech modeling methods that would be most effective in achieving robust sp eech recognition. We considered alternative frequency scales (M-MFCC, ExpoLog), feature processing (CMN, VCMN, LP-vs-FFT MFCCs), model adaptation (PMC), and combinations of gender dependent with gender independent models. For the purposes of achieving effective speech recognition performance, computational speed and availability of adaptation data greatly impacts final recognition performance. In particular, while reliable algorithm formulations for addressing specific types of distortion can improve recognition rates, these algorithms cannot reach their full potential wit hout proper front-end algorithm data processing to direct compensation. While parallel banks of speech recognizers can improve recognition performance, their significant computational requirements can render the recognizer useless in actual speech applications."
225689,21258,9804,Consonant Context Effects on Vowel Sensorimotor Adaptation,2014,"Speech sensorimotor adaptation is the short-term learning of modified articulator movements evoked through sensory­ feedback perturbations. A common experimental method manipulates acoustic parameters, such as formant frequencies, using real time resynthesis of the participant's speech to perturb auditory feedback. While some studies have examin ed phrases comprised of vowels, diphthongs, and semivowels, the bulk of research on auditory feedback-driven sensorimotor adaptation has focused on vowels in neutral co nt exts (/hVd/). The current study investigates coarticulatory influen ces of adjacent consonants on sensorimotor adaptation. The purpose is to evaluate differences in the adaptation effects for vowels in consonant environments that vary by place and manner of articu lation. In ,p'articu lar, we addressed the hypothesis that contexts with greater intra-articulator coarticulation and more static articulatory postures (a lveolars and fricatives) offer greater resistance to vowel adaptation than contexts with primarily inter-articulator coarticulation and more dynamic articu latory pattems (bilabials and stops). Participants completed formant perturbation-driven vowel ada ptation experiments for varying CVCs. Results from discrete formant measures at the vowel midpoint were generally consistent with the hypothesis. Analyses of more complete formant trajectories suggest that adaptation can also (or alt ernative ly) influence formant onsets, offsets, and transitions, resulting in complex formant pattern changes that may reflect modifications to consonant articu lation."
2354539,21258,23735,Real-Time Robot Audition System That Recognizes Simultaneous Speech in The Real World,2006,"This paper presents a robot audition system that recognizes simultaneous speech in the real world by using robot-embedded microphones. We have previously reported missing feature theory (MFT) based integration of sound source separation (SSS) and automatic speech recognition (ASR) for building robust robot audition. We demonstrated that a MFT-based prototype system drastically improved the performance of speech recognition even when three speakers talked to a robot simultaneously. However, the prototype system had three problems; being offline, hand-tuning of system parameters, and failure in voice activity detection (VAD). To attain online processing, we introduced FlowDesigner-based architecture to integrate sound source localization (SSL), SSS and ASR. This architecture brings fast processing and easy implementation because it provides a simple framework of shared-object-based integration. To optimize the parameters, we developed genetic algorithm (GA) based parameter optimization, because it is difficult to build an analytical optimization model for mutually dependent system parameters. To improve VAD, we integrated new VAD based on a power spectrum and location of a sound source into the system, since conventional VAD relying only on power often fails due to low signal-to-noise ratio of simultaneous speech. We, then, constructed a robot audition system for Honda ASIMO. As a result, we showed that the system worked online and fast, and had a better performance in robustness and accuracy through experiments on recognition of simultaneous speech in a noisy and echoic environment"
107356,21258,9804,Towards the Automatic Generation of Mixed-Initiative Dialogue Systems from Web Content,2003,"Through efforts over the past fifteen years, we have acquired a great deal of experience in designing spoken dialogue systems that provide access to large corpora of data in a variety of different knowledge domains, such as flights, hotels, restaurants, weather, etc. In our recent research, we have begun to shift our focus towards developing tools that enable the rapid development of new applications. This paper addresses a novel approach that drives system design from the on-line knowledge resource. We were motivated by a desire to minimize the need for a pre-determined dialogue flow. In our approach, decisions on dialogue flow are made dynamically based on analyses of data, either prior to user interaction or during the dialogue itself. Automated methods, used to organize numeric and symbolic data, can be applied at every turn, as user constraints are being specified. This helps the user mine through large data sets to a few choices by allowing the system to synthesize intelligent summaries of the data, created on-the-fly at every turn. Moreover automatic methods are ultimately more robust against the frequent changes to on-line content. Simulations generating hundreds of dialogues have produced log files that allow us to assess and improve system behavior, including system responses and interactions with the dialogue flow. Together, these techniques are aimed towards the goal of instantiating new domains with little or no input from a human developer."
68734,21258,9804,Speech-Based Automated Cognitive Status Assessment,2010,"Verbal interviews performed by trained clinicians are a common form of assessments to measure cognitive decline. The aim in this paper is to study the usability of automated methods for evaluating verbal cognitive status assessment tests for the elderly. If reliable, such methods for cognitive assessment can be used for frequent, non-intrusive, low-cost screenings and provide objective and longitudinal cognitive status monitoring data that can complement regular clinical visits and would be useful for early detection of conditions associated with language and communication impairments. This study focuses on two types of tests: a story-recall test, used for memory and language functioning assessment, and a picture description test, used to assess the information content in speech. A data collection was designed for this study involving recordings of about 100 people, mostly over 70 years old, performing these tests. The speech samples were manually transcribed and annotated with semantic units in order to obtain manual evaluation scores. We explore the use of automatic speech recognition and language processing methods to derive objective, automatically extracted metrics of cognitive status that are highly correlated with the manual scores. We use recall and precision based metrics based on semantic content units associated with the tests. Our experiments show high correlation between manually obtained scores and the automatic metrics obtained using either manual or automatic speech transcriptions. Index Terms: speech recognition, language processing, automated cognitive status assessment, elderly speech"
2715909,21258,9804,A Comparative Analytic Study on the Gaussian Mixture and Context Dependent Deep Neural Network Hidden Markov Models,2014,"We conducted a comparative analytic study on the contextdependent Gaussian mixture hidden Markov model (CD-GMMHMM) and deep neural network hidden Markov model (CDDNN-HMM) with respect to the phone discrimination and the robustness performance. We found that the DNN can significantly improve the phone recognition performance for every phoneme with 15.6% to 39.8% relative phone error rate reduction (PERR). It is particularly good at discriminating certain consonants, which are found to be “hard” in the GMM. On the robustness side, the DNN outperforms the GMM at all SNR levels, across different devices, and under all speaking rate with nearly uniform improvement. The performance gap with respect to different SNR levels, distinct channels, and varied speaking rate remains large. For example, in CD-DNNHMM, we observed 1∼2% performance degradation per 1dB SNR drop; 20∼25% performance gap between the best and least well performed devices; 15∼30% relative word error rate increase when the speaking rate speeds up or slows down by 30% from the “sweet” spot. Therefore, we conclude the robustness remains to be a major challenge in the deep learning acoustic model. Speech enhancement, channel normalization, and speaking rate compensation are important research areas in order to further improve the DNN model accuracy. Index Terms: GMM-HMM, CD-DNN-HMM, noise robustness, channel compensation, speaking rate normalization"
2755292,21258,9804,QAT 2 - The QCRI Advanced Transcription and Translation System,2015,"Abstract QAT 2 is a multimedia content translation web service devel-oped by QCRI to help content provider to reach audiences andviewers speaking different languages. It is built with estab-lishing open source technologies such as KALDI, Moses andMaryTTS, to provide a complete translation experience for webusers. It translates text content in its original format, and pro-duce translated videos with speech-to-speech translation. Theresult is a complete native language experience for end userson foreign language websites. The system currently supportstranslation from Arabic to English.Index Terms: speech recognition, speech-to-speech transla-tion, machine translation, text-to-speech synthesis 1. Introduction Every day, new online multimedia content is generated acrossthe globe. Unfortunately, language barriers pose a limitationfor the dissemination of such content. In recent years, languagetechnologies have made important progress, and have becomeviable tools that enable users to consume this information. On-line translators (e.g. Google Translate, Bing Translate) are goodexamples of how translation technologies can help reduce thelanguage gap. However, most of the current technology is onlyfocused on text, and it is not prepared to deal with the growingtrend of multimedia content. For instance, many news articlesand blog posts come accompanied by complementary videos.QCRI has developed the QAT"
511358,21258,9804,Voice Transformations For Improving Children's Speech Recognition In A Publicly Available Dialogue System,2002,"This thesis presents work done during the last ten years on developing five multimodal spoken dialogue systems, and the empirical user studies that have been conducted with them. The dialogue systems have been multimodal, giving information both verbally with animated talking characters and graphically on maps and in text tables. To be able to study a wider rage of user behaviour each new system has been in a new domain and with a new set of interactional abilities. The five system presented in this thesis are: The Waxholm system where users could ask about the boat traffic in the Stockholm archipelago; the Gulan system where people could retrieve information from the Yellow pages of Stockholm; the August system which was a publicly available system where people could get information about the author Strindberg, KTH and Stockholm; the AdAptsystem that allowed users to browse apartments for sale in Stockholm and the Pixie system where users could help ananimated agent to fix things in a visionary apartment publicly available at the Telecom museum in Stockholm. Some of the dialogue systems have been used in controlled experiments in laboratory environments, while others have been placed inpublic environments where members of the general public have interacted with them. All spoken human-computer interactions have been transcribed and analyzed to increase our understanding of how people interact verbally with computers, and to obtain knowledge on how spoken dialogue systems canutilize the regularities found in these interactions. This thesis summarizes the experiences from building these five dialogue systems and presents some of the findings from the analyses of the collected dialogue corpora."
2811395,21258,9804,An Unsupervised Visual-only Voice Activity Detection Approach Using Temporal Orofacial Features,2015,"Detecting the presence or absence of speech is an important step toward building robust speech-based interfaces. While previous studies have made progress on voice activity detection (VAD), the performance of these systems significantly degrades when subjects employ challenging speech modes that deviate from normal acoustic patterns (e.g., whisper speech), or in noisy/adverse conditions. An appealing approach under these conditions is visual voice activity detection (VVAD), which detects speech using features characterizing the orofacial activity. This study proposes an unsupervised approach that relies only on visual features, and, therefore, is insensitive to vocal style or time-varying background noise. This study proposes an unsupervised approach that relies on visual features. We estimate optical flow variance and geometrical features around lips, extracting the short-time zero crossing rates, short-time variances, and delta features over a small temporal window. These variables are fused using principal component analysis (PCA) to obtain a “combo” feature, which displays a bimodal distributions (speech versus silence). A threshold is automatically determine using the expectation-maximization (EM) algorithm. The approach can be easily transformed into a supervised VVAD, if needed. We evaluate the system in neutral and whisper speech. While speech based VADs generally fail to detect speech activity in whisper speech, given its important acoustic differences, the proposed VVAD achieves near 80% accuracy in both neutral and whisper speech, highlighting the benefits of the system. Index Terms: Visual voice activity detection, whisper speech"
117663,21258,9804,Analysis of voice fundamental frequency contours of continuing and terminating prosodic phrases in four swiss German dialects.,2009,"Abstract In the present study, the F0 contours of continuing and terminating prosodic phrases of 4 Swiss German dialects are analyzed by means of the command-response model. In every model parameter, the two prosodic phrase types show significant differences: continuing prosodic phrases indicate higher phrase command magnitude and shorter durations. Locally, they demonstrate more distinct accent command amplitudes as well as durations. In addition, continuing prosodic phrases have later rises relative to segment onset than terminating prosodic phrases. In the same context, fine phonetic differences between the dialects are highlighted. Index Terms : Command-response model, Swiss German dialects, talk-in-interaction, spontaneous speech 1. Introduction Fujisaki [1] distinguishes between linguistic, paralinguistic, and nonlinguistic information carried by speech. Linguistic information is concerned with lexical, syntactic and semantic contents of the message and is discrete in nature. Paralinguistic information is concerned with the intention and attitude of the speaker and is used to modify or supplement linguistic information, while nonlinguistic information is concerned with physical and mental states of the speaker over which the speaker usually has no direct control. The information on conversational structuring such as turn-taking is thus paralinguistic, and is primarily expressed by intonation. This function of intonation has provided fertile grounds for research in the branch of conversation analysis, but the results have so far been mainly qualitative. The present study examines features of intonation contours quantitatively within the scope of day-to-day communication. Two concepts which are central are the terms"
2652276,21258,9804,Robust Articulatory Speech Synthesis using Deep Neural Networks for BCI Applications,2014,"Brain-Computer Interfaces (BCIs) usually propose typing strategies to restore communication for paralyzed and aphasic people. A more natural way would be to use speech BCI directly controlling a speech synthesizer. Toward this goal, a prerequisite is the development a synthesizer that should i) produce intelligible speech, ii) run in real time, iii) depend on as few parameters as possible, and iv) be robust to error fluctuations on the control parameters. In this context, we describe here an articulatory-to-acoustic mapping approach based on deep neural network (DNN) trained on electromagnetic articulography (EMA) data recorded synchronously with produced speech sounds. On this corpus, the DNN-based model provided a speech synthesis quality (as assessed by automatic speech recognition and behavioral testing) comparable to a state-of-the-art Gaussian mixture model (GMM), yet showing higher robustness when noise was added to the EMA coordinates. Moreover, to envision BCI applications, this robustness was also assessed when the space covered by the 12 original articulatory parameters was reduced to 7 parameters using deep auto-encoders (DAE). Given that this method can be implemented in real time, DNN-based articulatory speech synthesis seems a good candidate for speech BCI applications. Index Terms: articulatory speech synthesis, brain computer interface (BCI), deep neural networks, deep auto-encoder, EMA, noise robustness, dimensionality reduction"
559713,21258,9804,Acoustic and Prosodic Correlates of Social Behavior,2011,"We describe acoustic/prosodic and lexical correlates of social variables annotated on a large corpus of task-oriented sponta- neous speech. We employ Amazon Mechanical Turk to label the corpus with a large number of social behaviors, examining results of three of these here. We find significant differences between male and female speakers for perceptions of attempts to be liked, likeability, speech planning, that also differ depending upon the gender of their conversational partners. There has been much work in the speech community on the acoustic-prosodic and lexical indicators of classic emotions. Similar approaches have also been used to identify other related types of speaker state, including uncertainty, confi- dence, and deception, as well as less clearly 'emotional' states as charisma, sarcasm, personality, and medical conditions such as depression. More recently researchers have begun to explore the acoustic and prosodic cues that may be correlated with the production and perception of social behavior in conversation, including flirtation, agreeableness and awk- wardness. In this paper we examine the perception of three types of social behavior in conversation: likeability, the attempt to be liked, and conversational planning. These behaviors represent part of a larger ongoing study of social behavior in task-oriented conversation in the Columbia Games Corpus. Section 2 describes previous research in this area. In Section 3 we describe the corpus. Section 4 discusses the annotation of social behavior we elicited using Amazon Mechanical Turk. Our current experiments are described in Section 5 and we discuss our conclusions and future research in Section 6."
1300571,21258,11491,Audio-Based Multimedia Event Detection with DNNs and Sparse Sampling,2015,"This paper presents advances in analyzing audio content information to detect events in videos, such as a parade or a birthday party. We developed a set of tools for audio processing within the predominantly vision-focused deep neural network (DNN) framework Caffe. Using these tools, we show, for the first time, the potential of using only a DNN for audio-based multimedia event detection. Training DNNs for event detection using the entire audio track from each video causes a computational bottleneck. Here, we address this problem by developing a sparse audio frame-sampling method that improves event-detection speed and accuracy. We achieved a 10 percentage-point improvement in event classification accuracy, with a 200x reduction in the number of training input examples as compared to using the entire track. This reduction in input feature volume led to a 16x reduction in the size of the DNN architecture and a 300x reduction in training time. We applied our method using the recently released YLI-MED dataset and compared our results with a state-of-the-art system and with results reported in the literature for TRECVIDMED. Our results show much higher MAP scores compared to a baseline i-vector system - at a significantly reduced computational cost. The speed improvement is relevant for processing videos on a large scale, and could enable more effective deployment in mobile systems."
2865383,21258,9804,Sparsely Connected and Disjointly Trained Deep Neural Networks for Low Resource Behavioral Annotation: Acoustic Classification in Couples' Therapy,2016,"Observational studies are based on accurate assessment of human state. A behavior recognition system that models interlocutors' state in real-time can significantly aid the mental health domain. However, behavior recognition from speech remains a challenging task since it is difficult to find generalizable and representative features because of noisy and high-dimensional data, especially when data is limited and annotated coarsely and subjectively. Deep Neural Networks (DNN) have shown promise in a wide range of machine learning tasks, but for Behavioral Signal Processing (BSP) tasks their application has been constrained due to limited quantity of data. We propose a Sparsely-Connected and Disjointly-Trained DNN (SD-DNN) framework to deal with limited data. First, we break the acoustic feature set into subsets and train multiple distinct classifiers. Then, the hidden layers of these classifiers become parts of a deeper network that integrates all feature streams. The overall system allows for full connectivity while limiting the number of parameters trained at any time and allows convergence possible with even limited data. We present results on multiple behavior codes in the couples' therapy domain and demonstrate the benefits in behavior classification accuracy. We also show the viability of this system towards live behavior annotations."
41575,21258,9804,Can you read tongue movements?,2008,"Abstract Lip reading relies on visible articulators to ease audiovisual speech understanding. However, lips and face alone provide very incomplete phonetic information: the tongue, t hat is generally not entirely seen, carries an important p art of the articulatory information not accessible through lip reading . The question was thus whether the direct and full v ision of the tongue allows tongue reading . We have therefore generated a set of audiovisual VCV stimuli by contro lling an audiovisual talking head that can display all speec h articulators, including tongue, in an augmented speech mode, from articulators movements tracked on a speaker. T hese stimuli have been played to subjects in a series of audiovisual perception tests in various presentation conditions (audio signal alone, audiovisual signal with profile cutaw ay display with or without tongue, complete face), at various Signal-to-Noise Ratios. The results show a given implicit effe ct of tongue reading learning, a preference for the more ecological rendering of the complete face in comparison with t he cutaway presentation, a predominance of lip reading over tongue reading, but the capability of tongue readin g to take over when the audio signal is strongly degraded or absent. We conclude that these tongue reading capabilities cou ld be used for applications in the domain of speech therapy fo r speech retarded children, perception and production rehabi litation of hearing impaired children, and pronunciation traini ng for second language learners. Index Terms : Lip reading, tongue reading, audiovisual speech perception, audiovisual talking head, hearin g losses, augmented speech."
2574732,21258,9804,Prosodic phrasing modeling for Vietnamese TTS using syntactic information,2014,"This research aims at modeling prosodic phrasing for improving the naturalness of Vietnamese (a tonal language) speech synthesis. The proposed phrasing model includes hypotheses on: (i) prosodic structure based on syntactic rules (ii) final lengthening linked to syllabic structures and tone types. Audio files in the analysis corpus are manually transcribed at the syllable level and perceived pauses. Text files are parsed and represented with annotated-syntax trees. Statistical treatment brings out a correlation between syntactic element boundaries and pause duration. Major breaks may appear at the end of a clause or between predicates or head elements. Other rules between grammatical phrases/words or shorter clauses may trigger minor breaks. Break levels (including ones predicted by syntactic rules) and relative positions of syllables are used to train VTed, an HMM-based Text-To-Speech (TTS) system for Vietnamese. In the synthesis phase, break levels are explicitly inserted while lengthening is applied for last syllables of prosodic phrases. Perceptive testing shows an increase of 0.34 on a 5 point MOS scale, for the new prosodic informed system (3.95/5) compared to the previous TTS system (3.61/5). In the pairwise comparison test, about 70% of the synthetic voice with the proposed model is preferred to the previous version."
1258139,21258,535,Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription,2013,"YouTube is a highly visited video sharing website where over one billion people watch six billion hours of video every month. Improving accessibility to these videos for the hearing impaired and for search and indexing purposes is an excellent application of automatic speech recognition. However, YouTube videos are extremely challenging for automatic speech recognition systems. Standard adapted Gaussian Mixture Model (GMM) based acoustic models can have word error rates above 50%, making this one of the most difficult reported tasks. Since 2009, YouTube has provided automatic generation of closed captions for videos detected to have English speech; the service now supports ten different languages. This paper describes recent improvements to the original system, in particular the use of owner-uploaded video transcripts to generate additional semi-supervised training data and deep neural networks acoustic models with large state inventories. Applying an “island of confidence” filtering heuristic to select useful training segments, and increasing the model size by using 44,526 context dependent states with a low-rank final layer weight matrix approximation, improved performance by about 13% relative compared to previously reported sequence trained DNN results for this task."
2772553,21258,9804,Strategies for High Accuracy Keyword Detection in Noisy Channels,2013,"We present design strategies for a keyword spotting (KWS) system that operates in highly degraded channel conditions with very low signal-to-noise ratio levels. We employ a system combination approach by combining the outputs of multiple large vocabulary automatic speech recognition (LVCSR) systems, each of which employs a different system design approach targeting three different levels of information: front-end signal processing features (standard cepstra-based, noise-robust modulation and multi layer perceptron features), statistical acoustic models (gaussian mixtures models (GMM) and subspace GMMs) and keyword search strategies (word-based and phonebased). We also use keyword-aware capabilities in the system at two levels: in the LVCSR language models by assigning higher weights to n-grams with keywords in them and in LVCSR search by using a relaxed pruning threshold for keywords. The LVCSR system outputs are represented as latticebased unigram indices whose scores are fused by a logisticregression based classifier to produce the final system combination output. We present the performance of our system in the phase II evaluations of DARPA’s Robust Automatic Transcription of Speech (RATS) program for both Levantine Arabic and Farsi conversational speech corpora. Index Terms: noise-robust keyword detection, automatic speech recognition, system combination, noise robustness"
2818514,21258,9804,Detecting Out-Of-Domain Utterances Addressed to a Virtual Personal Assistant,2014,"Conversational understanding systems, especially virtual personal assistants (VPAs), perform “targeted” natural language understanding, assuming their users stay within the walled gardens of covered domains, and back-off to generic web search otherwise. However, users usually do not know the concept of domains and sometimes simply do not distinguish the system from simple voice search. Hence it becomes an important problem to identify these rejected out-of-domain utterances which are actually intended for the VPA. This paper presents a study tackling this new task, showing that how one utters a request is more important for this task than what is uttered, resembling addressee detection or dialog act tagging. To this end, syntactic and semantic parse “structure” features are extracted in addition to lexical features to train a binary SVM classifier using a large number of random web search queries and VPA utterances from multiple domains. We present controlled experiments leaving one domain out and check the precision of the model when combined with unseen queries. Our results indicate that such structured features result in higher precision especially when the test domain bears little resemblance to the existing domains. Index Terms: conversational understanding, semantic parsing, keyword search, out-of-domain detection, machine learning, virtual personal assistants"
240480,21258,9804,Regularizing linear discriminant analysis for speech recognition,2005,"Feature extraction is an essential first step in speech recognition applications. In addition to static features extracted from each frame of speech data, it is beneficial to use dynamic features (called Δ and ΔΔ coefficients) that use information from neighboring frames. Linear Discriminant Analysis (LDA) followed by a diagonalizing maximum likelihood linear transform (MLLT) applied to spliced static MFCC features yields important performance gains as compared to MFCC+Δ+ΔΔ features in most tasks. However, since LDA is obtained using statistical averages trained on limited data, it is reasonable to regularize LDA transform computation by using prior information and experience. In this paper, we regularize LDA and heteroschedastic LDA transforms using two methods: (1) Using statistical priors for the transform in a MAP formulation (2) Using structural constraints on the transform. As prior, we use a transform that computes static+Δ+ΔΔ coefficients. Our structural constraint is in the form of a block structured LDA transform where each block acts on the same cepstral parameters across frames. The second approach suggests using new coefficients for static, first difference and second difference operators as compared to the standard ones to improve performance. We test the new algorithms on two different tasks, namely TIMIT phone recognition and AURORA2 digit sequence recognition in noise. We obtain consistent improvement in our experiments as compared to MFCC features. In addition, we obtain encouraging results in some AURORA2 tests as compared to LDA+MLLT features."
2770175,21258,9804,Towards a Neural Measure of Perceptual Distance—Classification of Electroencephalographic Responses to Synthetic Vowels,2014,"How vowels are organized cortically has previously been studied using auditory evoked potentials (AEPs), one focus of which is to determine whether perceptual distance could be inferred using AEP components. The present study extends this line of research by adopting a machine-learning framework to classify evoked responses to four synthetic mid-vowels differing only in second formant frequency (F2 = 840, 1200, 1680, and 2280 Hz). 6 subjects attended 4 EEG sessions each on separate days. Classifiers were trained using time-domain data in successive timewindows of various sizes. Results were the most accurate when a window of about 80 ms was used. By integrating the scores from individual classifiers, the maximum mean binary classification rates improved to 70% (10 trials) and 77% (20 trials). To assess how well perceptual distances among the vowels were reflected in our results, discriminability indices (d � ) were computed using both the behavioral results in a screening test and the classification results. It was found that the two set of indices were significantly correlated. The pair that was the most (least) discriminable behaviorally was also the most (least) classifiable neurally. Our results support the use of classification methodology for developing a neural measure of perceptual distance. Index Terms: vowel perception, electroencephalography, perceptual distance, classification, machine learning"
165064,21258,9804,Predicting Speaker Changes and Listener Responses with and without Eye-Contact.,2011,"Parallel with the orthographic streams of words in conversation are multiple layered epiphenomena, short in duration and with a communicativepurpose. These paralinguistic events regulate the interaction flow via gaze,gestures and intonation. This thesis focus on how to compute, model, discoverand analyze prosody and it’s applications for spoken dialog systems.Specifically it addresses automatic classification and analysis of conversationalcues related to turn-taking, brief feedback, affective expressions, their crossrelationshipsas well as their cognitive and neurological basis. Techniques areproposed for instantaneous and suprasegmental parameterization of scalarand vector valued representations of fundamental frequency, but also intensity and voice quality. Examples are given for how to engineer supervised learned automata’s for off-line processing of conversational corpora as well as for incremental on-line processing with low-latency constraints suitable as detector modules in a responsive social interface. Specific attention is given to the communicative functions of vocal feedback like mhm, okay and yeah, that’s right as postulated by the theories of grounding, emotion and a survey on laymen opinions. The potential functions and their prosodic cues are investigated via automatic decoding, data-mining, exploratory visualization and descriptive measurements."
436196,21258,9804,The prosody of Swedish conversational grunts.,2010,"Parallel with the orthographic streams of words in conversation are multiple layered epiphenomena, short in duration and with a communicativepurpose. These paralinguistic events regulate the interaction flow via gaze,gestures and intonation. This thesis focus on how to compute, model, discoverand analyze prosody and it’s applications for spoken dialog systems.Specifically it addresses automatic classification and analysis of conversationalcues related to turn-taking, brief feedback, affective expressions, their crossrelationshipsas well as their cognitive and neurological basis. Techniques areproposed for instantaneous and suprasegmental parameterization of scalarand vector valued representations of fundamental frequency, but also intensity and voice quality. Examples are given for how to engineer supervised learned automata’s for off-line processing of conversational corpora as well as for incremental on-line processing with low-latency constraints suitable as detector modules in a responsive social interface. Specific attention is given to the communicative functions of vocal feedback like mhm, okay and yeah, that’s right as postulated by the theories of grounding, emotion and a survey on laymen opinions. The potential functions and their prosodic cues are investigated via automatic decoding, data-mining, exploratory visualization and descriptive measurements."
2553801,21258,9804,Semi-Supervised GMM and DNN Acoustic Model Training with Multi-system Combination and Confidence Re-calibration,2013,"We present our study on semi-supervised Gaussian mixture model (GMM) hidden Markov model (HMM) and deep neural network (DNN) HMM acoustic model training. We analyze the impact of transcription quality and data sampling approach on the performance of the resulting model, and propose a multisystem combination and confidence re-calibration approach to improve the transcription inference and data selection. Compared to using a single system recognition result and confidence score, our proposed approach reduces the phone error rate of the inferred transcription by 23.8% relatively when top 60% of data are selected. Experiments were conducted on the mobile short message dictation (SMD) task. For the GMM-HMM model, we achieved 7.2% relative word error rate reduction (WERR) against a well-trained narrow-band fMPE+bMMI system by adding 2100 hours of untranscribed data, and 28.2% relative WERR over a wide-band MLE model trained from transcribed out-of-domain voice search data after adding 10K hours of untranscribed SMD data. For the CD-DNN-HMM model, 11.7% and 15.0% relative WERRs are achieved after adding 1K hours of untranscribed data using random and importance sampling, respectively. We also found using large amount of untranscribed data for pretraining does not help. Index Terms: semi-supervised acoustic model training, system combination, confidence re-calibration, importance sampling"
1574338,21258,9099,A Multimodal Predictive Model of Successful Debaters or How I Learned to Sway Votes,2015,"Interpersonal skills such as public speaking are essential assets for a large variety of professions and in everyday life. The ability to communicate in social environments often greatly influences a person's career development, can help resolve conflict, gain the upper hand in negotiations, or sway the public opinion. We focus our investigations on a special form of public speaking, namely public debates of socioeconomic issues that affect us all. In particular, we analyze performances of expert debaters recorded through the Intelligence Squared U.S. (IQ2US) organization. IQ2US collects high-quality audiovisual recordings of these debates and publishes them online free of charge. We extract audiovisual nonverbal behavior descriptors, including facial expressions, voice quality characteristics, and surface level linguistic characteristics. Within our experiments we investigate if it is possible to automatically predict if a debater or his/her team are going to sway the most votes after the debate using multimodal machine learning and fusion approaches. We identify unimodal nonverbal behaviors that characterize successful debaters and our investigations reveal that multimodal machine learning approaches can reliably predict which individual (~75% accuracy) or team (85% accuracy) is going to win the most votes in the debate. We created a database consisting of over 30 debates with four speakers per debate suitable for public speaking skill analysis and plan to make this database publicly available for the research community."
2825966,21258,9804,A Speaker Diarization System for Studying Peer-Led Team Learning Groups,2016,"Peer-led team learning (PLTL) is a model for teaching STEM courses where small student groups meet periodically to collaboratively discuss coursework. Automatic analysis of PLTL sessions would help education researchers to get insight into how learning outcomes are impacted by individual participation, group behavior, team dynamics, etc.. Towards this, speech and language technology can help, and speaker diarization technology will lay the foundation for analysis. In this study, a new corpus is established called CRSS-PLTL, that contains speech data from 5 PLTL teams over a semester (10 sessions per team with 5-to-8 participants in each team). In CRSS-PLTL, every participant wears a LENA device (portable audio recorder) that provides multiple audio recordings of the event. Our proposed solution is unsupervised and contains a new online speaker change detection algorithm, termed G 3 algorithm in conjunction with Hausdorff-distance based clustering to provide improved detection accuracy. Additionally, we also exploit cross channel information to refine our diarization hypothesis. The proposed system provides good improvements in diarization error rate (DER) over the baseline LIUM system. We also present higher level analysis such as the number of conversational turns taken in a session, and speaking-time duration (participation) for each speaker."
1941445,21258,8960,ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition,2002,"A challenging, unsolved problem in the speech recognition community is recognizing speech signals that are corrupted by loud, highly nonstationary noise. One approach to noisy speech recognition is to automatically remove the noise from the cepstrum sequence before feeding it in to a clean speech recognizer. In previous work published in Eurospeech, we showed how a probability model trained on clean speech and a separate probability model trained on noise could be combined for the purpose of estimating the noise-free speech from the noisy speech. We showed how an iterative 2nd order vector Taylor series approximation could be used for probabilistic inference in this model. In many circumstances, it is not possible to obtain examples of noise without speech. Noise statistics may change significantly during an utterance, so that speech-free frames are not sufficient for estimating the noise model. In this paper, we show how the noise model can be learned even when the data contains speech. In particular, the noise model can be learned from the test utterance and then used to denoise the test utterance. The approximate inference technique is used as an approximate E step in a generalized EM algorithm that learns the parameters of the noise model from a test utterance. For both Wall Street Journal data with added noise samples and the Aurora benchmark, we show that the new noise adaptive technique performs as well as or significantly better than the non-adaptive algorithm, without the need for a separate training set of noise examples."
1746160,21258,11166,Parameter-Free Audio Motif Discovery in Large Data Archives,2013,"The discovery of repeated structure, i.e. motifs/near-duplicates, is often the first step in exploratory data mining. As such, the last decade has seen extensive research efforts in motif discovery algorithms for text, DNA, time series, protein sequences, graphs, images, and video. Surprisingly, there has been less attention devoted to finding repeated patterns in audio sequences, in spite of their ubiquity in science and entertainment. While there is significant work for the special case of motifs in music, virtually all this work makes many assumptions about data (often to the point of being genre specific) and thus these algorithms do not generalize to audio sequences containing animal vocalizations, industrial processes, or a host of other domains that we may wish to explore. In this work we introduce a novel technique for finding audio motifs. Our method does not require any domain-specific tuning and is essentially parameter-free. We demonstrate our algorithm on very diverse domains, finding audio motifs in laboratory mice vocalizations, wild animal sounds, music, and human speech. Our experiments demonstrate that our ideas are effective in discovering objectively correct or subjectively plausible motifs. Moreover, we show our novel probabilistic early abandoning approach is efficient, being two to three orders of magnitude faster than brute-force search, and thus faster than real-time for most problems."
2552707,21258,9804,Context-dependent Pronunciation Error Pattern Discovery with Limited Annotations,2014,"A Computer-Assisted Pronunciation Training (CAPT) system can provide greater benefit to language learners if it provides not only scoring but also corrective feedback. However, the process of deriving pronunciation error patterns usually requires linguistic knowledge, or large quantities of expensive, annotated, corpora from nonnative speakers. In this paper we explore the possibility of deriving context-dependent error patterns with limited human annotations. A two-stage labeling mechanism is proposed, which first selects a set of templates for human annotation, and then propagates the labels. To deal with the imbalanced number of correct and incorrect phone-level pronunciations in nonnative speech, pronunciation patterns on an individual learner-level are first summarized, and then corpuslevel clustering is done for template selection. The concept of contextual similarity based on a phonemic broad class definition is also proposed for label propagation. For evaluation, we view the task as an information retrieval task, and take advantage of metrics that consider both the importance and the ranking of an error type. Experimental results on a Chinese University of Hong Kong (CUHK) nonnative corpus show that the proposed framework can effectively discover prominent error patterns. Index Terms: Computer-Assisted Language Learning, unsupervised clustering, graph-based label propagation"
2613822,21258,9804,Rapidly Building Domain-Specific Entity-Centric Language Models Using Semantic Web Knowledge Sources,2014,"For domain-specific speech recognition tasks, it is best if the statistical language model component is trained with text data that is content-wise and style-wise similar to the targeted domain for which the application is built. For state-of-the-art language modeling techniques that can be used in real-time within speech recognition engines during first-pass decoding (e.g., N-gram models), the above constraints have to be fulfilled in the training data. However collecting such data, even through crowd sourcing, is expensive and time consuming, and can still be not representative of how a much larger user population would interact with the recognition system. In this paper, we address this problem by employing several semantic web sources that already contain the domain-specific knowledge, such as query click logs and knowledge graphs. We build statistical language models that meet the requirements listed above for domain-specific recognition tasks where natural language is used and the user queries are about name entities in a specific domain. As a case study, in the movies domain where users’ voice queries are movie related, compared to a generic web language model, a language model trained with the above resources not only yields significant perplexity and word-errorrate improvements, but also presents an approach where such language models can be rapidly developed for other domains. Index Terms: speech recognition, language modeling, knowledge graphs, query click graphs, name entities, semantic web"
1155990,21258,23735,Dereverberation robust to speaker's azimuthal orientation in multi-channel human-robot communication,2013,"The acoustical dynamics of reverberation in an enclosed environment poses a problem to human-robot communication. Any change in the azimuthal orientation of the speaker contributes to unpredictable acoustical activity resulting in a degradation in the performance of the automatic speech recognition (ASR) system. Thus, dereverberation techniques need to address this issue prior to ASR. Dereverberation in multi-channel applications primarily evolves in the adoption of a suitable reverberant model that results to a computationally feasible solution and at the same time yields an accurate estimate of the harmful reflections (i.e., late reflection) for effective suppression. In this paper we address this problem by introducing a hybrid method based on multi-channel processing on a singlechannel reverberant model platform. The proposed method is capable of accurate signal estimation, a property inherent to a multi-channel system, and at the same time bears the computational efficiency derived from single-channel reverberant model approach. The proposed method is summarized as follows; First, multi-channel sound-source processing is employed to obtain the full reverberant and the late reflection signal estimates. Then, equalization is employed to update the late reflection estimate reflective of the change in azimuth prior to dereverberation. The equalization parameters for azimuthal change are obtained through an offline optimization procedure. Experimental evaluation in an actual human-robot communication environment shows that the proposed method outperforms existing methods in terms of robustness in the ASR performance."
2662150,21258,535,A CHiME-3 challenge system: Long-term acoustic features for noise robust automatic speech recognition,2015,"The paper describes an automatic speech recognition (ASR) system for the 3rd CHiME challenge that addresses noisy acoustic scenes within public environments. The proposed system includes a multi-channel speech enhancement front-end including a microphone channel failure detection method that is based on cross-comparing the modulation spectra of speech to detect erroneous microphone recordings. The main focus of the submission is the investigation of the amplitude modulation filter bank (AMFB) as a method to extract long-term acoustic cues prior to a Gaussian mixture model (GMM) or deep neural network (DNN) based ASR classifier. It is shown that AMFB features outperform the commonly used frame splicing technique of filter bank features even on a performance optimized ASR challenge system. I.e., temporal analysis of speech by hand-crafted and auditory motivated AMFBs is shown to be more robust compared to a data-driven method based on extracting temporal dynamics with a DNN. Our final ASR system, which additionally includes adaptation of acoustic features to speaker characteristics, achieves an absolute word error rate reduction of approx. 21.53 % relative to the best CHiME-3 baseline system on the real test condition."
105725,21258,9804,High Front Vowels in Czech: a Contrast in Quantity or Quality?,2009,"Abstract We investigate the perception and production of Czech / / and / /, a contrast traditionally described as quantitative. First, we show that the spectral difference between the vowels is for many Czechs as strong a cue as (or even stronger than) duration. Second, we test the hypothesis that this shift towards vowel quality as a perceptual cue for this contrast resulted in weakening of the durational differentiation in production. Our measurements confirm this: members of the / /-/ / pair differed in duration much less than those of other short-long pairs. We interpret these findings in terms of Lindblom’s HH the authors concluded that quantity in Hungarian is not as important as it had been considered. Pind [3] found that in Icelandic, for the [ ]-[ ] vowel pair differing in quality (despite the transcription), this quality difference can work as a stronger perceptual cue than duration. The Czech language has five vowel pairs distinguished by phonological length. Traditionally, the long vowels had been described to be approximately twice as long as the short vowels [4]. However, our informal observations suggested that this may not be the case in the / /-/ / pair. This is perhaps because there is also a difference in quality between its members. While this qualitative difference, and therefore an exceptional status of / /-/ / among Czech short-long vowel pairs, has been recognized in literature [5: 33] (even though quite reluctantly by prescriptive linguists) its potential contribution to the contrast has not been meaningfully considered. Recall the finding by Pind about Icelandic cited above. In a subsequent paper [6], Pind tested the hypothesis, based on Lindblom’s"
1817394,21258,9804,Consonant Discrimination in Elicited and Spontaneous Speech: A Case for Signal-adaptive Front Ends in ASR,2000,"The constant frame length in typical ASR front ends is too long to capture transient phenomena in speech, such as stop bursts. However, current HMM systems have consistently outperformed systems based solely on non-uniform units. This work investigates an approach to “add back” such transient information to a speech recognizer, without losing the robustness of the standard acoustic models. We demonstrate a set of phonetically-motivated acoustic features that discriminate a preliminary test set of highly ambiguous voiceless stops in CV contexts. The features are automatically computed from data that had been hand-marked for consonant burst location and voicing onset (extension to automatic marking is also proposed). Two corpora are processed using a parallel set of features: conversational speech over the telephone (Switchboard), and a corpus of carefully elicited speech. The latter provides an upper bound on discrimination, and allows for comparison of feature usage across speaking style. We explore data-driven approaches to obtaining variable-length time-localized features compatible with an HMM statistical framework. We also suggest techniques for extension to automatic annotation of burst location, for computation of features at such points, and for augmentation of an HMM system with the added information."
2850326,21258,9804,Sinusoidal modelling for ecoacoustics,2016,"Biodiversity assessment is a central and urgent task, necessary to monitoring the changes to ecological systems and under- standing the factors which drive these changes. Technological advances are providing new approaches to monitoring, which are particularly useful in remote regions. Situated within the framework of the emerging field of ecoacoustics, there is grow- ing interest in the possibility of extracting ecological informa- tion from digital recordings of the acoustic environment. Rather than focusing on identification of individual species, an increas- ing number of automated indices attempt to summarise acoustic activity at the community level, in order to provide a proxy for biodiversity. Originally designed for speech processing, sinu- soidal modelling has previously been used as a bioacoustic tool, for example to detect particular bird species. In this paper, we demonstrate the use of sinusoidal modelling as a proxy for bird abundance. Using data from acoustic surveys made during the breeding season in UK woodland, the number of extracted sinusoidal tracks is shown to correlate with estimates of bird abundance made by expert ornithologists listening to the recordings. We also report ongoing work exploring a new approach to investigate the composition of calls in spectro-temporal space that constitutes a promising new method for Ecoaoustic biodiversity assessment."
2384820,21258,23735,Variable frame rate hierarchical analysis for robust speech recognition,2011,"A new bio-inspired speech analysis system that extracts acoustical speech events is proposed and used in the design of a variable frame rate (VFR) speech recognizer. The same speech recognizer (Hidden Markov Model -HMM- and Mel Frequency Cepstrum Coefficients -MFCC-) has been used with the proposed VFR analysis and conventional fixed frame rate (FFR) approach. In comparison with other VFR recognizers, the hierarchical features in the proposed system have the potential to serve as classification parameters of a complete bio-inspired speech recognition system. Also, no voice activity detection is required and there are no hard decisions to be taken by the system. Events are used to label and identify the moments at which the acoustical properties of speech are stable or changing. These events are markers on which an analysis window can be positioned to perform the recognition. Inspired by our knowledge of the auditory and visual systems, hierarchical complex features like transients and energy orientation are used. Training has been done on clean speech and recognition on noisy (from 20dB to −10dB Signal to Noise Ratios -SNR) or reverberated speech by using the TI 46-word database corrupted with 4 noises taken from the Aurora 2 data. In comparison with a FFR recognizer, our VFR system yields more than 50% increase in recognition rates for a speaker independent isolated word recognition task when SNRs are between 0 and 20 dB."
2740231,21258,9804,Developing STT and KWS systems using limited language resources,2014,"This paper presents recent progress in developing speech-totext (STT) and keyword spotting (KWS) systems for the 2014 IARPA-Babel evaluation. Systems have been developed for the limited language pack condition for four of the five development languages in this program phase: Assamese, Bengali, Haitian Creole and Zulu. The systems have several novel characteristics that support rapid development of KWS systems. On the STT side different acoustic units are explored based on phonemic or graphemic representations, and system combination is used to improve STT performance. The acoustic models are trained on only 10 hours of speech data with manual transcriptions, completed with unsupervised training on additional untranscribed data. Both word and subword units (morphologically decomposed, syllables, phonemes) are used for KWS. The KWS systems are based on the multi-hypotheses produced by a consensus network decoding or searching word lattices. The word error rates of the individual STT systems are on the order of 50-60%, and the KWS systems obtain Maximum Term Weighted Values ranging from 30-45% for all keywords (invocabulary and out-of-vocabulary (OOV)). Sub-word units are shown to be successful at locating some of the OOV keywords, and system combination improves system performance. Index Terms: STT, KWS, semi-supervised training, lattice, consensus network, sub-word lexical units, Morfessor"
1800903,21258,9804,Pitch Accent versus Lexical Stress: Quantifying Acoustic Measures Related to the Voice Source,2007,"In this paper, we explore acoustic correlates of pitch accent and main lexical stress in American English, and the interaction of these cues with other factors that affect prosody. In a controlled study, we varied presence or absence and type of pitch accent (L ∗ vs H ∗ ), boundary-related tone sequence (L-L% vs. HH%) and gender of the talker, for the sentence “Dagada gave Bobby doodads”. The measures were duration, F0 (fundamental frequency), H ∗ 1 −H ∗ 2 (related to open quotient), and H ∗ 1 −A ∗ (related to spectral tilt). Contour approximations were used to analyze time-course movements of these measures. For “Dagada” we found that, consistent with earlier literature, a) H ∗ and L ∗ pitch accents showed different F0 contours, b) pitchaccented syllables were longer than unaccented ones, c) stressed “ga” syllables had lower H ∗ 1 − H ∗ 2 values than surrounding unstressed syllables, and for male talkers, lower H ∗ 1 − A ∗ values, indicating lesser spectral tilt. Unexpectedly, F0 maxima associated with an H ∗ accent occurred most of the time later in the accented syllable than F0 minima associated with L ∗ . The cues to lexical stress were consistent with or without pitch accent (e.g. lower H ∗ − H ∗ 2 ), but they sometimes interacted with gender and/or boundary tones: for example, lower H ∗ − A ∗ in stressed “ga” syllables was only found for female talkers in unaccented cases, and some cues of both accent and stress were less pronounced in the final word “doodads”, which also carried boundary-related tones. Index Terms: voice source, prosody, voice quality"
2857169,21258,9804,The Sheffield Wargame Corpus - day two and day three,2016,"Improving the performance of distant speech recognition is of considerable current interest, driven by a desire to bring speech recognition into people’s homes. Standard approaches to this task aim to enhance the signal prior to recognition, typically using beamforming techniques on multiple channels. Only few real-world recordings are available that allow experimentation with such techniques. This has become even more pertinent with recent works with deep neural networks aiming to learn beamforming from data. Such approaches require large multi-channel training sets, ideally with location annotation for moving speakers, which is scarce in existing corpora. This paper presents a freely available and new extended corpus of English speech recordings in a natural setting, with moving speakers. The data is recorded with diverse microphone arrays, and uniquely, with ground truth location tracking. It extends the 8.0 hour Sheffield Wargames Corpus released in Interspeech 2013, with a further 16.6 hours of fully annotated data, including 6.1 hours of female speech to improve gender bias. Additional blog-based language model data is provided alongside, as well as a Kaldi baseline system. Results are reported with a standard Kaldi configuration, and a baseline meeting recognition system."
243641,21258,9804,Classifying Subject Ratings of Emotional Speech Using Acoustic Features.,2003,"This paper presents results from a study examining emotional speech using acoustic features and their use in automatic machine learning classification. In addition, we propose a classification scheme for the labeling of emotions on continuous scales. Our findings support those of previous research as well as indicate possible future directions utilizing spectral tilt and pitch contour to distinguish emotions in the valence dimension. Speech is a rich source of information, not only about what a speaker says, but also about what the speaker’s attitude is toward the listener and toward the topic under discussion — as well as the speaker’s own current state of mind. Until recently, most research on spoken language systems has focused on propositional content: what words is the speaker producing? Currently there is considerable interest in going beyond mere words to discover the semantic content of utterances. However, we believe it is important to go beyond semantic content as well, in order to fully interpret what human listeners infer from listening to other humans. In this paper we present results from some recent and ongoing experiments in the study of emotional speech, designed to elicit subjective judgments of tokens of emotional speech and to identify acoustic and prosodic correlates of such speech based on these classifications. We discuss previous research as well as show results from correlation and machine learning experiments, and conclude with the implications of this study 1 ."
2641819,21258,535,Deep bi-directional recurrent networks over spectral windows,2015,"Long short-term memory (LSTM) acoustic models have recently achieved state-of-the-art results on speech recognition tasks. As a type of recurrent neural network, LSTMs potentially have the ability to model long-span phenomena relating the spectral input to linguistic units. However, it has not been clear whether their observed performance is actually due to this capability, or instead if it is due to a better modeling of short term dynamics through the recurrence. In this paper. we answer this question by applying a windowed (truncated) LSTM to conversational speech transcription, and find that a limited context is adequate, and that it is not necessaary to scan the entire utterance. The sliding window approach allows not only incremental (online) recognition with a bidirectional model, but also frame-wise randomization (as opposed to utterance randomization), which results in faster convergence. On the SWBD/Fisher corpus, applying bidirectional LSTM RNNs to spectral windows of about 0.5s improves WER on the Hub5'00 benchmark set by 16% relative compared to our best sequence-trained DNN. On an extended 3850h training set that that also includes lectures, the relative gain becomes 28% (Hub5'00 WER 9.2%). In-house conversational data improves by 12 to 17% relative."
2741899,21258,9804,Speaker Dependent Bottleneck Layer Training for Speaker Adaptation in Automatic Speech Recognition,2014,"Speaker adaptation of deep neural networks (DNN) is difficult, and most commonly performed by changes to the input of the DNNs. Here we propose to learn discriminative feature transformations to obtain speaker normalised bottleneck (BN) features. This is achieved by interpreting the final two hidden layers as speaker specific matrix transformations. The hidden layer weights are updated with data from a specific speaker to learn speaker-dependent discriminative feature transformations. Such simple implementation lends itself to rapid adaptation and flexibility to be used in Speaker Adaptive Training (SAT) frameworks. The performance of this approach is evaluated on a meeting recognition task, using the official NIST RT’07 and RT’09 evaluation test sets. Supervised adaptation of the BN layer shows similar performance to the application of supervised CMLLR as a global transformation, and the combination of these appears to be additive. In unsupervised mode, CMLLR adaptation only yields 3.4% and 2.5% relative word error rate (WER) improvement, on the RT’07 and RT’09 respectively, where the baselines include speaker based cepstral mean and variance normalisation. The combined CMLLR and BN layer speaker adaptation yields a relative WER gain of 4.5% and 4.2% respectively. SAT style BN layer adaptation is attempted and combined with conventional CMLLR SAT, to show that it provides a relative gain of 1.43% and 2.02% on the RT’07 and RT’09 data sets respectively when compared with CMLLR SAT. While the overall gain from BN layer adaptation is small, the results are found to be statistically significant on both the test sets. Index Terms: Deep neural networks, bottleneck features, speaker adaptation, automatic speech recognition."
238813,21258,9804,The Goodness of Pronunciation algorithm applied to disordered speech,2014,"In this paper, we report on a study with the aim of automatically detecting phoneme-level mispronunciations in 32 French speakers suffering from unilateral facial palsy at four different clinical severity grades. We sought to determine if the Goodness of Pronunciation (GOP) algorithm, which is commonly used in Computer-Assisted Language Learning systems to detect learners’ individual errors, could also detect segmental deviances in disordered speech. For this purpose, speech read by the 32 speakers was aligned and GOP scores were computed for each phone realization. The highest scores, which indicate large dissimilarities with standard phone realizations, were obtained for the most severely impaired speakers. The corresponding speech subset was manually transcribed at phone-level. 8.3% of the phones differed from standard pronunciations extracted from our lexicon. The GOP technique allowed to detect 70.2% of mispronunciations with an equal rate of about 30% of false rejections and false acceptances. The phone substitutions detected by the algorithm confirmed that some of the speakers have difficulties to produce bilabial plosives, and showed that other sounds such as sibilants are prone to mispronunciation. Another interesting finding was the fact that speakers diagnosed with a same pathology grade do not necessarily share the same pronunciation issues. Index Terms: pronunciation automatic assessment, Goodness of Pronunciation, disordered speech"
2672554,21258,535,An i-Vector PLDA based gender identification approach for severely distorted and multilingual DARPA RATS data,2015,"This study proposes an i-Vector based approach to gender identification. Gender-labeled utterances from the Fisher English (FE) corpus are used to formulate an i-Vector extraction framework, and a Probabilistic Linear Discriminant Analysis (PLDA) back-end is employed to compute the scores for gender identification. A novel duration mismatch compensation strategy is also presented that offers very little degradation in identification accuracy even with a large reduction in the duration of the test-segment. The proposed method is shown to consistently outperform a GMM-UBM based gender-identification scheme on several test-sets created from a held-out portion of the FE corpus, and is able to achieve an identification accuracy of up to 97.63%. On the severely distorted and multilingual DARPA-RATS (Robust Automatic Transcription of Speech) corpora, the proposed approach achieves an identification accuracy of 76.48% using only the FE data in training. Next, a novel unsupervised domain adaptation strategy is also presented that utilizes only unlabeled RATS data to adapt the out-of-domain PLDA parameters derived from the FE training data. The strategy is able to offer a 6.8% relative improvement in identification accuracy, and a 14.75% relative reduction in Equal Error Rate (EER) compared to using the out-of-domain PLDA model on the RATS test-utterances. These improvements are significant since: 1) RATS test-utterances are severely distorted, 2) No labeled data of any kind is used for 4 of the 5 languages present in the test-utterances."
181108,21258,9804,Addressing database mismatch in forensic speaker recognition with Ahumada III: a public real-casework database in Spanish,2008,"This paper presents and describes Ahumada III, a speech database in Spanish collected from real forensic cases. In i ts current release, the database presents male speakers recorded using the systems and procedures followed by Spanish Guardia Civil police force. The paper also explores the usefulness of such a corpus for facing the important problem of database mismatch in speaker recognition, understood as the difference between the database used for tuning a speaker recognition system and the data which the system will handle in operational conditions. This problem is typical in forensics, where var iability in speech conditions may be extreme and difficult to model . Therefore, this work also presents a study evaluating the impact of such problem, for which a corpus quoted as NIST4M (NIST MultiMic MisMatch) has been constructed from NIST SRE 2006 data. NIST4M presents microphone data both in the enrolled models and in the test segments, allowing the generation of trials in a variety of strongly mismatching conditio ns. Database mismatch is simulated by eliminating some microphone channels of interest from the background data, and computing scores with speech from such microphones in unknown testing conditions as usually happens in forensic speaker r ecognition. Finally, we show how the incorporation of Ahumada III as background data is useful to face database mismatch in real-world forensic conditions."
2364572,21258,9804,Two-Stage System for Robust Neutral/Lombard Speech Recognition,2007,"Performance of current speech recognition systems is significantly deteriorated when exposed to strongly noisy environment. It can be attributed to background noise and Lombard effect (LE). Attempts for LE-robust systems often display a tradeoff between LE-specific improvements and the portability to neutral speech. Therefore, towards LE-robust recognition, it seems effective to use a set of conditionsdedicated subsystems driven by a condition classifier, rather than attempting for one universal recognizer. Presented paper focuses on a design of a two-stage recognition system (TSR) comprising talking style classifier (neutral/LE) followed by two style-dedicated recognizers differing in input features. First, the binary neutral/LE classifier is built, with a particular interest in developing suitable features for the classification. Second, performance of common speech features (MFCC, PLP), LE-robust features (Expolog) and newly proposed features is compared in neutral/LE digit recognition tasks. In addition, robustness to the changes of average speech pitch and various noise backgrounds is evaluated. Third, the TSR is built, employing two recognizers, each using style-specific features. Comparison of the proposed system with either neutralspecific or LE-specific recognizer on a joint neutral/LE speech shows an improvement 6.5o4.2 % WER on neutral and 48.1o28.4 % WER on LE Czech utterances."
53938,21258,9804,Combining multiple-sized sub-word units in a speech recognition system using baseform selection,2006,"Abstract A Longer-sized sub-word unit is known to be a better candi-date in the development of a continuous speech recognition sys-tem. However, the basic problem with such units is the data spar-sity. To overcome this problem, researchers have tried to com-bine longer-sized sub-word unit models with phoneme models. Inthis paper, we have considered only frequently occurring syllablesand VC (Vowel + Consonant) units, and phone-sized units (mono-phones and triphones) for the development of a continuous speechrecognition system. Insuch a case, even for a single pronunciationof a word, there can be multiple representational baseforms in thelexicon, each with different-sized units. We show that a consid-erable improvement in recognition performance can be achievedif the baseforms are selected properly. Out of all possible base-forms for a given word in the lexicon, the baseform that maxi-mizes the acoustic likelihood, for possible sub-word unit concate-nations to make a word, alone is considered. In the baseline sys-tems’ word-lexicon, like pure monophone or triphone-based sys-tems, since only the acoustically weaker baseforms are replacedby baseforms with longer-sized units, the resultant performance isguaranteed to be better than that of baseline systems. The prelim-inary experiments carried out on the TIMIT speech corpus showa considerable improvement in the recognition performance overa pure monophone/triphone-based systems when the larger-sizedunits are combined using proper selection of baseforms."
2563645,21258,9804,Speech activity detection for NASA apollo space missions: Challenges and solutions,2014,"Speech Activity Detection(SAD) is a well researched problem for communication, command and control applications, where audio segments are short duration and solution proposed for noisy as well as clean environments. In this study, we investigate the SAD problem using NASA’s Apollo space mission data [1]. Unlike traditional speech corpora, the audio recordings in Apollo are extensive from a longitudinal perspective (i.e., 612 days each). From SAD perspective, the data offers many challenges: (i) noise distortion with variable SNR, (ii) channel distortion, and (iii) extended periods of non-speech activity. Here, we use the recently proposed Combo-SAD, which has performed remarkably well in DARPA RATS evaluations, as our baseline system [2]. Our analysis reveals that the ComboSAD performs well when speech-pause durations are balanced in the audio segment, but deteriorates significantly when speech is sparse or absent. In order to mitigate this problem, we propose a simple yet efficient technique which builds an alternative model of speech using data from a separate corpora, and embeds this new information within the Combo-SAD framework. Our experiments show that the proposed approach has a major impact on SAD performance (i.e., +30% absolute), especially in audio segments that contain sparse or no speech information. Index Terms: Speech Activity Detection, Long Audio Recordings, NASA, Apollo, Noise Robustness"
322106,21258,9804,Prosodic Analysis of Foreign-Accented English,2009,"Abstract This study compares utterances by Vietnamese learners of Australian English with those of native subjects. In a previous study the utterances had been rated for foreign accent and intelligibility. We aim to find measurable prosodic differences accounting for the perceptual results. Our outcomes indicate, inter alia, that unaccented syllables are relatively longer compared with accented ones in the Vietnamese corpus than those in the Australian English corpus. Furthermore, the correlations of syllabic durations in utterances of one and the same sentence are much higher for Australian English subjects than for Vietnamese learners of English. Vietnamese speakers use a larger range of f0 and produce more pitch-accents than Australian speakers. Index Terms: foreign accent, prosodic analysis 1. Introduction Although foreign accent is most readily associated with segmental deviations from the native norm, prosodic differences certainly account for many difficulties in understanding accented speech (see, for instance, [1][2]). In the current study we examine speech collected from Vietnamese learners of Australian English. In previous work the data have been assessed by native listeners for intelligibility and strength of foreign accent on a scale from 1 to 5 [3]. We now attempt to perform a prosodic analysis of the recordings and compare them with corresponding utterances by native Australian subjects in order to establish objective parameters that best reflect foreign accent, as well as are correlated with the subjective measures of foreign accent and intelligibility. Whereas English is often classified as a stress-timed language, Vietnamese is a syllable-timed tone language, a contrast which obviously poses a number of prosodic problems for learners of the other language."
149970,21258,9804,Towards a Comprehensive Investigation of Factors Relevant to Peak Alignment Using a Unit Selection Corpus,2006,"This paper aims to demonstrate the use of a unit selection corpus, the IMS German Festival synthesis system [1], in carrying out a comprehensive investigation of factors influencing specific aspects of the phonetic realization of tonal categories. The study restricts itself to the alignment of peaks in H*L pitch accents in German. First results confirm not only well-known effects of syllable structure, e.g., peaks occurring relatively early when there is a sonorant onset or relatively late when there is a sonorant in the coda, but also attest to the special status of the nuclear pitch accent vs. accents occurring earlier in the intonation phrase. Furthermore, instances of H*L in syllables directly at the phrase boundaries (initial or final) are shown to behave significantly differently from those that are located farther away. A similar effect is observed when another pitch accent follows the H*L peak in the very next syllable as opposed to a distance of two or more syllables. In these cases it also matters whether a low or high target is following (the peaks occur relatively later when followed by a L target). The results should have the benefit of both describing the specific characteristics of the voice providing the corpus (allowing a more detailed phonetic realization of tonal categories during the synthesis process) and offering general insights into which factors are relevant to the alignment of H*L peaks in German. Index Terms: intonation synthesis, peak alignment, German"
1701235,21258,23735,Pitch extraction in Human-Robot interaction,2010,"We present a system for real-time fundamental frequency, i. e. pitch, extraction on a humanoid robot. The system extracts pitch using an 8 channel microphone array mounted on the Honda humanoid robot in a realistic Human-Robot interaction scenario. The main building blocks of the system are a multi-channel signal enhancement followed by robust pitch extraction and tracking. The signal enhancement is based on 8 channel Geometric Source Separation. For the pitch extraction the signal is first transformed with a Gammatone filter bank into the frequency domain. Next a histogram of zero crossing distances is calculated from all filter bank signals. During the calculation of the histogram spurious side peaks resulting from harmonics and sub-harmonics of the true fundamental frequency are inhibited. The resulting histogram then serves as input to a grid based Bayesian tracker which deploys Bayesian filtering in a forward step and Bayesian smoothing in a backward step on a 100ms time window. We demonstrate the performance of the system in a scenario where male and female speakers utter different phrases while standing at a normal interaction distance to the robot. For the evaluation we compare the pitch tracking results once obtained from a clean headset signal and once from the signals obtained from the robot. The results show that the tracking performance only degrades to a small extent in the realistic interaction scenario compared to the headset recordings."
2607392,21258,9804,"Data Augmentation, Feature Combination, and Multilingual Neural Networks to Improve ASR and KWS Performance for Low-resource Languages",2014,"This paper presents the progress of acoustic models for lowresourced languages (Assamese, Bengali, Haitian Creole, Lao, Zulu) developed within the second evaluation campaign of the IARPA Babel project. This year, the main focus of the project is put on training high-performing automatic speech recognition (ASR) and keyword search (KWS) systems from language resources limited to about 10 hours of transcribed speech data. Optimizing the structure of Multilayer Perceptron (MLP) based feature extraction and switching from the sigmoid activation function to rectified linear units results in about 5% relative improvement over baseline MLP features. Further improvements are obtained when the MLPs are trained on multiple feature streams and by exploiting label preserving data augmentation techniques like vocal tract length perturbation. Systematic application of these methods allows to improve the unilingual systems by 4-6% absolute in WER and 0.064-0.105 absolute in MTWV. Transfer and adaptation of multilingually trained MLPs lead to additional gains, clearly exceeding the project goal of 0.3 MTWV even when only the limited language pack of the target language is used. Index Terms: ASR, KWS, MTWV, MLP, rectified linear units, multilingual, low-resource"
2867496,21258,9804,Log-linear system combination using structured support vector machines,2016,"Copyright © 2016 ISCA.Building high accuracy speech recognition systems with limited language resources is a highly challenging task. Although the use of multi-language data for acoustic models yields improvements, performance is often unsatisfactory with highly limited acoustic training data. In these situations, it is possible to consider using multiple well trained acoustic models and combine the system outputs together. Unfortunately, the computational cost associated with these approaches is high as multiple decoding runs are required. To address this problem, this paper examines schemes based on log-linear score combination. This has a number of advantages over standard combination schemes. Even with limited acoustic training data, it is possible to train, for example, phone-specific combination weights, allowing detailed relationships between the available well trained models to be obtained. To ensure robust parameter estimation, this paper casts log-linear score combination into a structured support vector machine (SSVM) learning task. This yields a method to train model parameters with good generalisation properties. Here the SSVM feature space is a set of scores from well-trained individual systems. The SSVM approach is compared to lattice rescoring and confusion network combination using language packs released within the IARPA Babel program."
2662633,21258,9804,Speaker Identification for Whispered Speech Using A Training Feature Transformation From Neutral To Whisper,2011,"A number of research studies in speaker recognition have recently focused on robustness due to microphone and channel mismatch(e.g., NIST SRE). However, changes in vocal effort, especially whispered speech, present significant challenges in maintaining system performance. Due to the mismatch spectral structure resulting from the different production mechanisms, performance of speaker identification systems trained with neutral speech degrades significantly when tested with whispered speech. This study considers a feature transformation method in the training phase that leads to a more robust speaker model for speaker ID with whispered speech. In the proposed system, a Speech Mode Independent (SMI) Universal Background Model (UBM) is built using collected real neutral features and pseudo whispered features generated with Vector Taylor Series (VTS), or via Constrained Maximum Likelihood Linear Regression (CMLLR) model adaptation. Text-independent closed set speaker ID results using the UT-VocalEffort II corpus show an accuracy of 88.87% using the proposed method, which represents a relative improvement of 46.26% compared with the 79.29% accuracy of the baseline system. This result confirms a viable approach to improving speaker ID performance for neutral and whispered speech mismatched conditions. Index Terms: whispered speech, speech identification"
2638044,21258,9804,Recurrent Models for Auditory Attention in Multi-Microphone Distance Speech Recognition,2016,"Integration of multiple microphone data is one of the key ways to achieve robust speech recognition in noisy environments or when the speaker is located at some distance from the input device. Signal processing techniques such as beamforming are widely used to extract a speech signal of interest from background noise. These techniques, however, are highly dependent on prior spatial information about the microphones and the environment in which the system is being used. In this work, we present a neural attention network that directly combines multi-channel audio to generate phonetic states without requiring any prior knowledge of the microphone layout or any explicit signal preprocessing for speech enhancement. We embed an attention mechanism within a Recurrent Neural Network (RNN) based acoustic model to automatically tune its attention to a more reliable input source. Unlike traditional multi-channel preprocessing, our system can be optimized towards the desired output in one step. Although attention-based models have recently achieved impressive results on sequence-to-sequence learning, no attention mechanisms have previously been applied to learn potentially asynchronous and non-stationary multiple inputs. We evaluate our neural attention model on the CHiME-3 challenge task, and show that the model achieves comparable performance to beamforming using a purely data-driven method."
2619160,21258,535,Improving robustness against reverberation for automatic speech recognition,2015,"Reverberation is a phenomenon observed in almost all enclosed environments. Human listeners rarely experience problems in comprehending speech in reverberant environments, but automatic speech recognition (ASR) systems often suffer increased error rates under such conditions. In this work, we explore the role of robust acoustic features motivated by human speech perception studies, for building ASR systems robust to reverberation effects. Using the dataset distributed for the Automatic Speech Recognition In Reverberant Environments (ASpIRE-2015) challenge organized by IARPA, we explore Gaussian mixture models (GMMs), deep neural nets (DNNs) and convolutional deep neural networks (CDNN) as candidate acoustic models for recognizing continuous speech in reverberant environments. We demonstrate that DNN-based systems trained with robust features offer significant reduction in word error rates (WERs) compared to systems trained with baseline mel-filterbank features. We present a novel time-frequency convolution neural net (TFCNN) framework that performs convolution on the feature space across both the time and frequency scales, which we found to consistently outperform the CDNN systems for all feature sets across all testing conditions. Finally, we show that further WER reduction is achievable through system fusion of n-best lists from multiple systems."
2671489,21258,9804,Multilingual Hierarchical MRASTA Features for ASR,2013,"Recently, a multilingual Multi Layer Perceptron (MLP) training method was introduced without having to explicitly map the phonetic units of multiple languages to a common set. This paper further investigates this method using bottleneck (BN) tandem connectionist acoustic modeling for four high-resourced languages — English, French, German, and Polish. Aiming at the improvement of already existing high performing automatic speech recognition (ASR) systems, the multilingual training of the BN-MLP is extended from short-term to hierarchical longterm (multi-resolutional RASTA) feature extraction. Furthermore, deeper structures and context-dependent target labels are also examined. We experimentally demonstrate that a single state-of-the-art BN feature set can be trained for multiple languages, which is superior to the monolingual feature set, and results in significant gains in all the four languages. Studying the scalability of the multilingual BN features, a similar gain is observed in small (50 hours) and in larger scale (300 hours) ASR experiments regardless of the distribution of the data amount between the languages. Using deeper structures, context-dependent targets, and speaker adaptation, the multilingual BN reduces the word error rates by 3‐7% relative over the target language BN features and 25‐30% over the conventional MFCC system. Index Terms: deep MLP, bottleneck, multilingual, hierarchical, MRASTA, LVCSR"
2488543,21258,20348,RESONATE: reverberation environment simulation for improved classification of speech models,2014,"Home monitoring systems currently gather information about peoples activities of daily living and information regarding emergencies, however they currently lack the ability to track speech. Practical speech analysis solutions are needed to help monitor ongoing conditions such as depression, as the amount of social interaction and vocal affect is important for assessing mood and well-being. Although there are existing solutions that classify the identity and the mood of a speaker, when the acoustic signals are captured in reverberant environments they perform poorly. In this paper, we present a practical reverberation compensation method called RESONATE, which uses simulated room impulse responses to adapt a training corpus for use in multiple real reverberant rooms. We demonstrate that the system creates robust classifiers that perform within 5 -- 10% of baseline accuracy of non-reverberant environments. We demonstrate and evaluate the performance of this matched condition strategy using a public dataset, and also in controlled experiments with six rooms, and two long-term and uncontrolled real deployments. We offer a practical implementation that performs collection, feature extraction, and classification on-node, and training and simulation of training sets on a base station or cloud service."
2665194,21258,9804,Acoustic Factor Analysis based Universal Background Model for Robust Speaker Verification in Noise,2013,"The Universal Background Model (UBM) is known as a speaker independent Gaussian Mixture Model (GMM) trained on a large speech corpus containing many speakers’ recordings in various conditions. When noisy test data is involved, UBM trained on clean data is generally not optimal. Using noisy data for UBM training, however, creates a bias towards the specific development noise samples resulting in degraded speaker recognition performance in unseen noise types. In this study, we utilize an Acoustic Factor Analysis (AFA) based UBM that iteratively learns the dominant feature sub-spaces in each mixture component, resulting in a more robust model. We explore two variants of the model: one with an isotropic and the other with a diagonal residual noise. The Maximum-Likelihood (ML) training formulations of the models are provided. The latent variables of the model, termed acoustic factors, are used as features to train the second stage of factor analysis parameters, i.e., the traditional i-vector extractor. Experiments performed on the 2012 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE) indicate the effectiveness of the proposed strategy in both clean and noisy conditions. Index Terms: speaker verification, NIST SRE 2012, noisy data, acoustic factor analysis"
2783998,21258,9804,The IBM Speaker Recognition System: Recent Advances and Error Analysis,2016,"We present the recent advances along with an error analysis of the IBM speaker recognition system for conversational speech. Some of the key advancements that contribute to our system include: a nearest-neighbor discriminant analysis (NDA) approach (as opposed to LDA) for intersession variability compensation in the i-vector space, the application of speaker and channel-adapted features derived from an automatic speech recognition (ASR) system for speaker recognition, and the use of a DNN acoustic model with a very large number of output units (~10k senones) to compute the frame-level soft alignments required in the i-vector estimation process. We evaluate these techniques on the NIST 2010 SRE extended core conditions (C1-C9), as well as the 10sec-10sec condition. To our knowledge, results achieved by our system represent the best performances published to date on these conditions. For example, on the extended tel-tel condition (C5) the system achieves an EER of 0.59%. To garner further understanding of the remaining errors (on C5), we examine the recordings associated with the low scoring target trials, where various issues are identified for the problematic recordings/trials. Interestingly, it is observed that correcting the pathological recordings not only improves the scores for the target trials but also for the nontarget trials."
1855856,21258,9804,MIPAD: A NEXT GENERATION PDA PROTOTYPE,2000,"MiPad is one of the application prototypes in a project codenamed Dr Who. As a wireless Personal Digital Assistant (PDA), MiPad fully integrates continuous speech recognition (CSR) and spoken language understanding (SLU) to enable users to accomplish many common tasks using a multimodal interface and wireless technologies. It tries to solve the problem of pecking with tiny styluses or typing on minuscule keyboards in today’s PDAs or smart phones. It also avoids the problem of being a cellular telephone that depends on speech-only interaction. MiPad incorporates a built-in microphone that activates whenever a field is selected. As a user taps the screen or uses a built-in roller to navigate, the tapping action narrows the number of possible instructions for spoken language processing. MiPad currently runs on a Windows CE Pocket PC with a Windows 2000 Server where speech recognition is performed. The Dr Who CSR engine has a 64k word vocabulary with a unified context-free grammar and ngram language model. The Dr Who SLU engine is based on a robust chart parser and a plan-based dialog manager. This paper discusses MiPad’s design, implementation work in progress, and preliminary user study in comparison to the existing pen-based PDA interface."
2450031,21258,11470,Heading toward to the natural way of human-machine interaction: the nimitek project,2009,"Spoken human-machine interaction supported by state-ofthe- art dialog systems is becoming a standard technology. A lot of effort was invested for this kind of artificial communication interface. But still the spoken dialog systems (SDS) are not able to provide for the user a natural way of communication. Because, existing automated dialog system do not dedicate enough attention to problems in the interaction related to affected user behavior. This paper addresses some aspects of design and implementation of user behavior models in dialog systems aimed to provide naturalness of human-machine interaction. We discuss a viable integration technique of speech based emotion classification in SDS for robust affected automatic speech recognition and user emotion correlated dialog strategy. First of all, we describe existing methods of emotion recognition within speech and affected speech adapted ASR methods. Second, we introduce an approach to achieve emotion adaptive dialog management in human-machine interaction. A multimodal human-machine interaction system with integrated user behavior model is created within the project “Neurobiologically Inspired, Multimodal Intention Recognition for Technical Communication Systems” (NIMITEK). Currently NIMITEK provides a technical demonstrator to study these principles in a dedicated prototypical task, namely solving the game Towers of Hanoi. In this paper, we will describe the general approach NIMITEK takes to emotional man-machine interactions."
72535,21258,9804,Age and Gender Recognition Based on Multiple Systems - Early vs. Late Fusion,2010,"This paper focuses on the automatic recognition of a person’s age and gender based only on his or her voice. Up to five different systems are compared and combined in different configurations: three systems model the speaker’s characteristics in different feature spaces, i.e., MFCC, PLP, TRAPS, by Gaussian mixture models. The features of these systems are the concatenated mean vectors. System number 4 uses a physical two-mass vocal model and estimates in a data-driven optimization procedure 9 glottal features from voiced speech sections. For each utterance the minimum, maximum and mean vectors form a 27-dimensional feature vector. The last system calculates a 219-dimensional prosodic feature set for each utterance based on voice and unvoiced speech segments. We compare two different ways to fuse the different systems: First, we concatenate the system on feature level. The second way of combination is performed on score level by multi-class logistic regression. Despite there are just minor differences between the two approaches, late fusion is slightly superior. On the development set of the Interspeech Agender challenge we achieved an unweighted recall of 46.1% with early fusion and 47.8% with late fusion. Index Terms: acoustic analysis, classification, Gaussian mixture models"
2650373,21258,9804,Improving Robustness to Compressed Speech in Speaker Recognition,2013,"The goal of this paper is to analyze the impact of codecdegraded speech on a state-of-the-art speaker recognition system and propose mitigation techniques. Several acoustic features are analyzed, including the standard Mel filterbank cepstral coefficients (MFCC), as well as the noise-robust medium duration modulation cepstrum (MDMC) and power normalized cepstral coefficients (PNCC), to determine whether robustness to noise generalizes to audio compression. Using a speaker recognition system based on i-vectors and probabilistic linear discriminant analysis (PLDA), we compared four PLDA training scenarios. The first involves training PLDA on clean data, the second included additional noisy and reverberant speech, a third introduces transcoded data matched to the evaluation conditions and the fourth, using codec-degraded speech mismatched to the evaluation conditions. We found that robustness to compressed speech was marginally improved by exposing PLDA to noisy and reverberant speech, with little improvement using trancoded speech in PLDA based on codecs mismatched to the evaluation conditions. Noise-robust features offered a degree of robustness to compressed speech while more significant improvements occurred when PLDA had observed the codec matching the evaluation conditions. Finally, we tested i-vector fusion from the different features, which increased overall system performance but did not improve robustness to codec-degraded speech. Index Terms: speaker recognition, speech coding, codec degradation, speaker verification."
1583616,21258,535,Detection-based accented speech recognition using articulatory features,2011,"We propose an attribute-based approach to accented speech recognition based on automatic speech attribute transcription with high efficiency detection of articulatory features. In order to utilize appropriate and extensible phonetic and linguistic knowledge, conditional random field (CRF) is designed to take frame-level inputs with binary feature functions. The use of CRF with merely the state features to generate probabilistic phone lattices is then utilized to solve the phone under-generation problem. Finally an attribute discrimination module is incorporated to handle a diversity of accent changes without retraining any model, leading to flexible “plug ‘n’ play” modular design. The effectiveness of the proposed approach is evaluated on three typical Chinese accents, namely Guanhua, Yue and Wu. Our method yields a significant absolute phone recognition accuracy improvement 5.04%, 4.68% and 6.06% for the corresponding three accent types over a conventional monophone HMM system. Compared to a context-dependent triphone HMM system, we achieve comparable phone accuracies at only less than 20% of the computation cost. In addition, our proposed method is equally applicable to speaker-independent systems handling multiple accents."
2676936,21258,9804,Exploring How Deep Neural Networks Form Phonemic Categories,2015,"Deep neural networks (DNNs) have become the dominant technique for acoustic-phonetic modeling due to their markedly improved performance over other models. Despite this, little is understood about the computation they implement in creating phonemic categories from highly variable acoustic signals. In this paper, we analyzed a DNN trained for phoneme recognition and characterized its representational properties, both at the single node and population level in each layer. At the single node level, we found strong selectivity to distinct phonetic features in all layers. Node selectivity to specific manners and places of articulation appeared from the first hidden layer and became more explicit in deeper layers. Furthermore, we found that nodes with similar phonetic feature selectivity were differentially activated to different exemplars of these features. Thus, each node becomes tuned to a particular acoustic manifestation of the same feature, providing an effective representational basis for the formation of invariant phonemic categories. This study reveals that phonetic features organize the activations in different layers of a DNN, a result that mirrors the recent findings of feature encoding in the human auditory system. These insights may provide better understanding of the limitations of current models, leading to new strategies to improve their performance. Index Terms: Deep neural networks, deep learning, automatic speech recognition."
2662169,21258,535,A comparative study of neural network models for lexical intent classification,2015,"Domain and intent classification are critical pre-processing steps for many speech understanding and dialog systems, as it allows for certain types of utterances to be routed to particular subsystems. In previous work, we explored many types of neural network (NN) architectures — some feedforward and some recurrent — for lexical intent classification and found that they improved upon more traditional statistical baselines. In this paper we carry out a more comprehensive comparison of NN models including the recently proposed gated recurrent unit network, for two domain/intent classification tasks. Furthermore, whereas the previous work was confined to relatively small and controlled datasets, we now include experiments based on a large set obtained from the Cortana personal assistant application. We compare feedforward, recurrent, and gated — such as LSTM and GRU — networks against each other. On both the ATIS intent task and the much larger Cortana domain classification tasks, gated networks outperform recurrent models, which in turn outperform feedforward networks. Also, we compared standard word vector models against a representation which encodes words as sets of character n-grams to mitigate the out-of-vocabulary problem. We find that in nearly all cases, the standard word vectors outperform character-based word representations. Best results are obtained by linearly combining scores from NN models with log likelihood ratios obtained from N-gram language models."
149632,21258,9804,Automatic Prosody Generation - a Model for Hungarian,2001,"Prosody generation relates to the composition of the detailed time structure of continuous speech and the realisation of the fundamental frequency and intensity structure, embedded in it. In our model a complex function set is described for the three prosody components for read speech. Each of the three prosody components is modelled separately by a three-step procedure. It was found that the correct modelling of the time structure (sound durations and breaks) is the most important component of prosody, because the time structure gives the framework of the melody and intensity structure. A new method, based on indirect determination of specific sound durations was developed. Final duration values are calculated from the specific durations in two further steps. The procedure starts on the lowest level (1st step on segmental level) and gradually goes towards higher levels, ending at sentence level. This duration model describes the behaviour of sound durations in continuous read sp eech. Modelling of fundamental frequency changes is also described by three levels, starting with rules on sentence level, followed by rules for word and syllable level. Finally micro intonation on sound level is applied. Similarly a three level model serves to describe the intensity structure, i.e. rules applied on sounds, words and on the complete sentence. The models of the three prosody components have influence on each other during prosody generation. Cross effects among them are also mentioned. The model can be applied in speech research and in applications (synthesis and recognition). It was tested for Hungarian."
112992,21258,9804,Multi-flow block interleaving applied to distributed speech recognition over IP networks.,2006,"Abstract Interleaving has shown to be a useful technique to provide robustdistributed speech recognition over IP networks. This is due toits ability to disperse consecutive losses. However, this ability isrelated to the delay introduced by the interleaver. In this work,we propose a novel multi-ﬂow block interleaver which exploitsthe presence of several streams and allows to reduce the involveddelay. Experimental results have shown that this interleaver ap-proximates the performance of end-to-end interleavers but with afraction of their delay. As disadvantage, this interleaver must beplaced in a common node where more than one ﬂow are available. Index Terms : distributed speech recognition, IP networks, inter-leaving, active networks. 1. Introduction Since its beginning, Internet has been growing in size, incorporat-ing many new networks, as well as in functionality, adding newservices. As many other features have been integrated into In-ternet, such as mailing, instant messaging, telephony and so on,speech enabled services (SES) are also being incorporated. Theseservices provide ubiquitous speech recognition, allowing multipleusers to remotely access and share high performance recognitionengines.A very attractive approach to speech recognition over IP net-works is the distributed speech recognition (DSR) solution [1]. Asmany other services over Internet, it is based on a client-serverarchitecture. On one hand, a simple and low power client ("
439112,21258,11321,Robust speech activity detection in interactive smart-room environments,2006,"In perceptive interface technologies used in smart-room environments, the determination of speech activity is one of key objectives. Due to the presence of environmental noises and reverberation, a robust Speech Activity Detection (SAD) system is required. In a previous work, a SAD system, which used Linear Discriminant Analysis-extracted features and a Decision Tree classifier, was successfully contrasted with other previously reported techniques in a set of room environment tests with the SPEECON database. In this work, the same SAD system has been tested in even more realistic conditions involving meetings, and it has been modified to significantly improve its performance. Actually, we have trained the SAD system with a subset of SPEECON data, and without any further tuning we have used it to carry out tests with the meeting databases from the NIST RT05 evaluation. In order to improve the SAD performance, we consider two additional features which are measures of energy dynamics at low and high frequencies, respectively. Besides that, two alternative classifiers have been tested, which are based on Support Vector Machines and Gaussian Mixture Models, respectively. With the latter classifier, and using both the LDA features and the low-frequency energy dynamics feature, a large improvement in speech detection performance has been observed, e.g. the NIST error rate was reduced from 20.69% to 8.47% for the RT05 evaluation data. In addition, we report the results obtained with a slightly modified version of the SAD system in the NIST RT06 evaluation."
2729536,21258,9804,Recognizing words across regional accents: The role of perceptual assimilation in lexical competition,2013,"Unfamiliar regional accents disrupt spoken word recognition by L2 and L1 learners and L1 adults, and confuse ASR and smart systems. Little is known, however, about which aspects of non-native accents hinder word recognition, or what processes are involved. We assessed how Australian English (AusE) listeners’ recognition of words in unfamiliar accents is affected by two types of cross-accent perceptual assimilation: 1) other-accent phones that constitute ‘deviant’ versions of the matching AusE phonemes (Category Goodness assimilation: CG); 2) phones that cross a native phonological boundary, i.e., assimilate to mismatching AusE phonemes (Category Shift: CS). Eyetracking (“visual world”) revealed the timecourse of lexical competition during online identification of words spoken in Jamaican (JaME: vowel differences from AusE) and Cockney English (CknE: consonant differences), while choosing among four printed choice words: target, onset and offset competitors, unrelated distracter. Recognition was slower, and both competitor types were considered more and longer for JaME and CknE than AusE pronunciations; these effects were stronger for CS than CG differences. We conclude that: 1) perceptual assimilation plays a key role in cross-accent word recognition; 2) lexical competition involves not only onsets but also later aspects of words; 3) vowel and consonant variations affect lexical competition similarly."
516219,21258,11321,Speech activity detection on multichannels of meeting recordings,2005,"The Purdue SAD system was originally designed to identify speech regions in multichannel meeting recordings with the goal of focusing transcription effort on regions containing speech. In the NIST RT-05S evaluation, this system was evaluated in the ihm condition of the speech activity detection task. The goal for this task condition is to separate the voice of the speaker on each channel from silence and crosstalk. Our system consists of several steps and does not require a training set. It starts with a simple silence detection algorithm that utilizes pitch and energy to roughly separate silence from speech and crosstalk. A global Bayesian Information Criterion (BIC) is integrated with a Viterbi segmentation algorithm that divides the concatenated stream of local speech and crosstalk into homogeneous portions, which allows an energy based clustering process to then separate local speech and crosstalk. The second step makes use of the obtained segment information to iteratively train a Gaussian mixture model for each speech activity category and decode the whole sequence over an ergodic network to refine the segmentation. The final step first uses a cross-correlation analysis to eliminate crosstalk, and then applies a batch of post-processing operations to adjust the segments to the evaluation scenario. In this paper, we describe our system and discuss various issues related to its evaluation."
2661902,21258,535,Combining spectral feature mapping and multi-channel model-based source separation for noise-robust automatic speech recognition,2015,"Automatic Speech Recognition systems suffer from severe performance degradation in the presence of myriad complicating factors such as noise, reverberation, multiple speech sources, multiple recording devices, etc. Previous challenges have sparked much innovation when it comes to designing systems capable of handling these complications. In this spirit, the CHiME-3 challenge presents system builders with the task of recognizing speech in a real-world noisy setting wherein speakers talk to an array of 6 microphones in a tablet. In order to address these issues, we explore the effectiveness of first applying a model-based source separation mask to the output of a beamformer that combines the source signals recorded by each microphone, followed by a DNN-based front end spectral mapper that predicts clean filterbank features. The source separation algorithm MESSL (Model-based EM Source Separation and Localization) has been extended from two channels to multiple channels in order to meet the demands of the challenge. We report on interactions between the two systems, cross-cut by the use of a robust beamforming algorithm called BeamformIt. Evaluations of different system settings reveal that combining MESSL and the spectral mapper together on the baseline beamformer algorithm boosts the performance substantially."
2787227,21258,9804,"Drink and Speak: On the Automatic Classification of Alcohol Intoxication by Acoustic, Prosodic and Text-Based Features.",2011,"This paper focuses on the automatic detection of a person’s blood level alcohol based on automatic speech processing approaches. We compare 5 different feature types with different ways of modeling. Experiments are based on the ALC corpus of IS2011 Speaker State Challenge. The classification task is restricted to the detection of a blood alcohol level above 0:5 ‰. Three feature sets are based on spectral observations: MFCCs, PLPs, TRAPS. These are modeled by GMMs. Classification is either done by a Gaussian classifier or by SVMs. In the later case classification is based on GMM-based supervectors, i.e. concatenation of GMM mean vectors. A prosodic system extracts a 292-dimensional feature vector based on a voicedunvoiced decision. A transcription-based system makes use of text transcriptions related to phoneme durations and textual structure. We compare the stand-alone performances of these systems and combine them on score level by logistic regression. The best stand-alone performance is the transcriptionbased system which outperforms the baseline by 4.8 % on the development set. A Combination on score level gave a huge boost when the spectral-based systems were added (73.6 %). This is a relative improvement of 12.7 % to the baseline. On the test-set we achieved an UA of 68.6 % which is a significant improvement of 4.1 % to the baseline system. Index Terms: GMM, alcohol intoxication, system fusion"
3245197,21258,9616,Online speaker emotion tracking with a dynamic state transition model,2016,"Although emotional state recognition from voice has been extensively studied, there is not much effort focusing on the online emotion recognition. Since duration and intensity of emotional experiences change over time it is hard to employ existing static transition models while monitoring emotional states especially in an online setting. To overcome this difficulty we introduce a method which incorporates particle filter tracking for switching observation models with emotional state classification. Adopting the Active Field State Space (AFSS) used in modeling human social interactions, a dynamic state transition model is formulated in the continuous arousal-valence-stance space. Under the assumption that the target posterior of each emotional state is a GMM with unknown number of mixture components, the observation model is constructed throughout a training scheme where DPM models of the emotional states are learned via SMC sampling. Online speaker emotional state labeling performance of the proposed method has been evaluated on long speech sequences containing emotional drift and transitions. Test sequences are simulated from EmoDB based on the AFSS interaction model. It is shown that formulating the emotional state classification as an online tracking problem provides a considerable improvement over standard maximum likelihood classification approach. Test results demonstrate that the introduced method achieves 83% accuracy in an online setting which is comparable to the performance of existing offline methods."
2636080,21258,9804,Brain activations in speech recovery process after intra-oral surgery: an fMRI study,2013,"This study aims at describing cortical and subcortical activation patterns associated with functional recovery of speech production after reconstructive mouth surgery. Our ultimate goal is the understanding of how the brain deals with altered relationships between motor commands and auditory/orosensory feedback, and establishes new inter-articulatory coordination to preserve speech communication abilities. A longitudinal sparse-sampling fMRI study involving orofacial, vowel and syllable production tasks on 9 patients and in three different sessions (one week before, one month and three months after surgery) was conducted. Healthy subjects were recorded in parallel. Results show that for patients in the pre-surgery session, activation patterns are in good agreement with the classical speech production network. Crucially, lower activity in sensorimotor control brain areas during orofacial and speech production movements is observed for patients in all sessions. One month after surgery, the superior parietal lobule is more activated for simple vowel production suggesting a strong involvement of a multimodal integration process to compensate for loss of tongue motor control. Altogether, these results indicate both altered and adaptive sensorimotor control mechanisms in these patients. Index Terms: Neurophonetics, fMRI, speech recovery, motor control, glossectomy, whole-brain analysis, sparse-sampling."
40805,21258,9804,On the calibration and fusion of heterogeneous spoken term detection systems.,2013,"The combination of several heterogeneous systems is known to provide remarkable performance improvements in verification and detection tasks. In Spoken Term Detection (STD), two important issues arise: (1) how to define a common set of detected candidates, and (2) how to combine system scores to produce a single score per candidate. In this paper, a discriminative calibration/fusion approach commonly applied in speaker and language recognition is adopted for STD. Under this approach, we first propose several heuristics to hypothesize scores for systems that do not detect a given candidate. In this way, the original problem of several unaligned detection candidates is converted into a verification task. As for other verification tasks, system weights and offsets are then estimated through linear logistic regression. As a result, the combined scores are well calibrated, and the detection threshold is automatically given by application parameters (priors and costs). The proposed method not only offers an elegant solution for the problem of fusion and calibration of multiple detectors, but also provides consistent improvements over a baseline approach based on majority voting, according to experiments on the MediaEval 2012 Spoken Web Search (SWS) task involving 8 heterogeneous systems developed at two different laboratories. Index Terms: Spoken Term Detection, Majority Voting, Discriminative Calibration and Fusion, MediaEval 2012 SWS."
2578573,21258,9804,Leveraging Knowledge Graphs for Web-Scale Unsupervised Semantic Parsing,2013,"The past decade has seen the emergence of web-scale structured and linked semantic knowledge resources (e.g., Freebase, DBPedia). These semantic knowledge graphs provide a scalable “schema for the web”, representing a significant opportunity for the spoken language understanding (SLU) research community. This paper leverages these resources to bootstrap a web-scale semantic parser with no requirement for semantic schema design, no data collection, and no manual annotations. Our approach is based on an iterative graph crawl algorithm. From an initial seed node (entity-type), the method learns the related entity-types from the graph structure, and automatically annotates documents that can be linked to the node (e.g., Wikipedia articles, web search documents). Following the branches, the graph is crawled and the procedure is repeated. The resulting collection of annotated documents is used to bootstrap webscale conditional random field (CRF) semantic parsers. Finally, we use a maximum-a-posteriori (MAP) unsupervised adaptation technique on sample data from a specific domain to refine the parsers. The scale of the unsupervised parsers is on the order of thousands of domains and entity-types, millions of entities, and hundreds of millions of relations. The precision-recall of the semantic parsers trained with our unsupervised method approaches those trained with supervised annotations. Index Terms: semantic parsing, semantic web, semantic search, dialog, natural language understanding"
1932294,21258,9804,Automated transcription and topic segmentation of large spoken archives.,2003,"Digital archives have emerged as the pre-eminent method for capturing the human experience. Before such archives can be used efficiently, their contents must be described. The scale of such archives along with the associated content mark up cost make it impractical to provide access via purely manual means, but automatic technologies for search in spoken materials still have relatively limited capabilities. The NSF-funded MALACH project will use the world’s largest digital archive of video oral histories, collected by the Survivors of the Shoah Visual History Foundation (VHF) to make a quantum leap in the ability to access such archives by advancing the state-of-the-art in Automated Speech Recognition (ASR), Natural Language Processing (NLP) and related technologies [1, 2]. This corpus consists of over 115,000 hours of unconstrained, natural speech from 52,000 speakers in 32 different languages, filled with disfluencies, heavy accents, age-related coarticulations, and un-cued speaker and language switching. Thispaper discusses some of theASR and NLPtools and technologies that we have been building for the English speech in the MALACH corpus. We also discuss this new test bed while emphasizing the unique characteristics of this corpus."
2690889,21258,9804,Development of CRIM System for the Automatic Speaker Verification Spoofing and Countermeasures Challenge 2015,2015,"The automatic speaker verification spoofing and countermeasures challenge 2015 provides a common framework for the evaluation of spoofing countermeasures or anti-spoofing techniques in the presence of various seen and unseen spoofing attacks. This contribution proposes a system consisting of amplitude, phase, linear prediction residual, and combined amplitude - phase-based countermeasures for the detection of spoofing attacks. In this task we use following features: Mel-frequency cepstral coefficients (MFCC), product spectrum-based cepstral coefficients, modified group delay cepstral coefficients, weighted linear prediction group delay cepstral coefficients, linear prediction residual cepstral coefficients, cosine normalized phase-based cepstral features (CNPCC), and a combination of MFCC-CNPCC. The product spectrum-based features are influenced by both the amplitude and phase spectra. The Gaussian Mixture Model (GMM) classifier is used for the discrimination of the human and spoofed speech signals. Our primary submitted system is a linear fusion of the sub-systems based on the features mentioned above with fusion weights trained on the development dataset. Experimental results on the challenge evaluation data provided an average EER (equal error rate) of 0.041%, 5.347%, and 2.69% on the known, unknown and all (known + unknown) spoofing attacks, respectively. Among all the systems product spectrum-based cepstral coefficients- and conventional MFCC (without any feature normalization)based systems performed the best in terms of EER measure. On the known, unknown and all conditions the EER obtained by the MFCC and product spectrum-based features are 0.78% & 0.65%, 5.39% & 5.37% and 3.09% & 3.01%, respectively."
2711000,21258,9804,Phone classification by a hierarchy of invariant representation layers.,2014,"We propose a multi-layer feature extraction framework for speech, capable of providing invariant representations. A set of templates is generated by sampling the result of applying smooth, identity-preserving transformations (such as vocal tract length and tempo variations) to arbitrarily-selected speech signals. Templates are then stored as the weights of “neurons”. We use a cascade of such computational modules to factor out different types of transformation variability in a hierarchy, and show that it improves phone classification over baseline features. In addition, we describe empirical comparisons of a) different transformations which may be responsible for the variability in speech signals and of b) different ways of assembling template sets for training. The proposed layered system is an effort towards explaining the performance of recent deep learning networks and the principles by which the human auditory cortex might reduce the sample complexity of learning in speech recognition. Our theory and experiments suggest that invariant representations are crucial in learning from complex, real-world data like natural speech. Our model is built on basic computational primitives of cortical neurons, thus making an argument about how representations might be learned in the human auditory cortex. Index Terms: Invariance, Auditory Cortex, Phonetic Classification, Convolutional Network"
2404268,21258,9804,Real-life emotions detection with lexical and paralinguistic cues on Human-Human call center dialogs,2006,"The emotion detection work reported here is part of a larger study aiming to model user behavior in real interactions. We already studied emotions in a real-life corpus with human-human dialogs on a financial task. We now make use of another corpus of real agent-caller spoken dialogs from a medical emergency call center in which emotion manifestations are much more complex, and extreme emotions are common. Our global aims are to define appropriate verbal labels for annotating real-life emotions, to annotate the dialogs, to validate the presence of emotions via perceptual tests and to find robust cues for emotion detection. Annotations have been done by two experts with twenty verbal classes organized in eight macro-classes. We retained for experiments in this paper four macro classes: Relief, Anger, Fear and Sadness. The relevant cues for detecting natural emotions are paralinguistic and linguistic. Two studies are reported in this paper: the first investigates automatic emotion detection using linguistic information, whereas the second investigates emotion detection with paralinguistic cues. On the medical corpus, preliminary experiments using lexical cues detect about 78% of the four labels showing very good detection for Relief (about 90%) and Fear (about 86%) emotions. Experiments using paralinguistic cues show about 60% of good detection, Fear being best detected. Index Terms: emotion detection, real-life emotion, lexical and paralinguistic cues"
2764976,21258,9804,Analysing rhythm in ritual discourse in Yucatec Maya using automatic speech alignment,2015,"Over the years, research in ethno-linguistics contributed to gather corpora in a wide range of languages, cultures and topics. In the present work, we are investigating ritual speech in Yu-catec Maya. The ritual discourse tends to have a cyclic structure with repetitive patterns and various types of parallelisms between speech sections. Previous studies have revealed an intricate connexion between a speech's structure and vocal productions , in particular through temporal aspects including rhythm, pauses and durations of different speech sections. To further investigate our findings by relying more strongly on the acoustic recordings, automatic speech recognition tools may become of great help, in particular to test various linguistic and ethno-linguistic hypotheses. Unfortunately, Yucatec Maya, with less than one million native speakers, is an under-resourced language with respect to digital resources. As a total, 24 minutes of ritual speech from three performances were manually transcribed by expert linguists in Yucatec and a basic pronunciation dictionary for Yucatec was created accordingly. The transcribed acoustic recordings were then automatically time-aligned on a phonetic and lexical basis. Automatic segmentations were used to measure tempo changes, durations of breath units as well as to examine their link with the structure of the ritual text."
2825088,21258,9804,Acoustic and Kinematic Characteristics of Vowel Production Through a Virtual Vocal Tract in Dysarthria,2014,"Broadening our understanding of the components and processes of speech sensorimotor learning is crucial to furthering methods of speech neurorehabilitation. Recent research in limb sensorimotor control has used virtual environments to study learning in novel sensorimotor working spaces. Comparable experimental paradigms have yet to be undertaken to study speech learning. We present acoustic and kinematic data obtained from participants producing vowels in unfamiliar articulatory-acoustic working spaces using a virtual vocal tract. Talkers with dysarthria and healthy controls were asked to produce vowels using an electromagnetic articulograph-driven speech synthesizer for participantcontrolled auditory feedback. The aim of the work was to characterize performance within and between groups to generate hypotheses regarding experimental manipulations that may bolster our understanding of speech sensorimotor learning. Results indicate that dysarthric talkers displayed relatively reduced acoustic working spaces and somewhat more variable acoustic targets compared to controls. Kinematic measures of articulatory dynamics, particularly peak speed and movement jerk-cost, were idiosyncratic and did not dissociate talker groups. These findings suggest that individuals with dysarthria and healthy talkers may use idiosyncratic movement strategies in learning to control a virtual vocal tract, but that dysarthric talkers may nonetheless exhibit acoustic limitations that parallel deficits in speech intelligibility."
1815924,21258,9804,Robust feature extraction and acoustic modeling at Multitel: experiments on the Aurora databases,2003,"This paper intends to summarize some of the robust feature extraction and acoustic modeling technologies used at Multitel, together with their assessment on some of the ETSI Aurora reference tasks. Ongoing work and directions for further research are also presented. For feature extraction (FE), we are using PLP coefficients. Additive and convolutional noise are addressed using a cascade of spectral subtraction and temporal trajectory filtering. For acoustic modeling (AM), artificial neural networks (ANNs) are used for estimating the HMM state probabilities. At the junction of FE and AM, the multi-band structure provides a way to address the needs of robustness by targeting both processing levels. Robust features within sub-bands can be extracted using a form of discriminant analysis. In this work, this is obtained using sub-band ANN acoustic models. Therobust sub-band features are then used for the estimation of state probabilities. These systems are evaluated on the Aurora tasks in comparison to the existing ETSI features. Our baseline system has similar performance than the ETSI advanced features coupled with the HTK back-end. On the Aurora 3 tasks, the multi-band system outperforms the best ETSI results with an average reduction of the word error rate of about 62% with respect to the baseline ETSI system and of about 18% with respect to the advanced ETSI system. This confirm previous positive experience with the multi-band architecture on other databases."
2080574,21258,9804,Vocal Emotion Recognition with Cochlear Implants,2006,"Besides conveying linguistic information, spoken language can also transmit important cues regarding the emotion of a talker. These prosodic cues are most strongly coded by changes in amplitude, pitch, speech rate, voice quality and articulation. The present study investigated the ability of cochlear implant (CI) users to recognize vocal emotions, as well as the relative contributions of spectral and temporal cues to vocal emotion recognition. An English sentence database was recorded for the experiment; each test sentence was produced according to five target emotions. Vocal emotion recognition was tested in 6 CI and 6 normal-hearing (NH) subjects. With unprocessed speech, NH listeners’ mean vocal emotion recognition performance was 90 % correct, while CI users’ mean performance was only 45 % correct. Vocal emotion recognition was also measured in NH subjects while listening to acoustic, sine-wave vocoder CI simulations. To test the contribution of spectral cues to vocal emotion recognition, 1-, 2-, 4-, 8and 16-channel CI processors were simulated; to test the contribution of temporal cues, the temporal envelope filter cutoff frequency in each channel was either 50 or 500 Hz. Results showed that both spectral and temporal cues significantly contributed to performance. With the 50-Hz envelope filter, performance generally improved as the number of spectral channels was increased. With the 500-Hz envelope filter, performance significantly improved only when the spectral resolution was increased from 1 to 2, and then from 2 to 16 channels. For all but the 16-channel simulations, increasing the envelope filter cutoff frequency from 50 Hz to 500 Hz significantly improved performance. CI users’ vocal emotion recognition performance was statistically similar to that of NH subjects listening to 1 - 8 spectral channels with the 50-Hz envelope filter, and to 1 channel with the 500-Hz envelope filter. The results suggest that, while spectral cues may contribute more strongly to recognition of linguistic information, temporal cues may contribute more strongly to recognition of emotional content coded in spoken language. Index Terms: vocal emotion recognition, cochlear implant."
2703467,21258,9804,Word-level invariant representations from acoustic waveforms.,2014,"Extracting discriminant, transformation-invariant features from raw audio signals remains a serious challenge for speech recognition. The issue of speaker variability is central to this problem, as changes in accent, dialect, gender, and age alter the sound waveform of speech units at multiple levels (phonemes, words, or phrases). Approaches for dealing with this variability have typically focused on analyzing the spectral properties of speech at the level of frames, on par with frame-level acoustic modeling usually applied to speech recognition systems. In this paper, we propose a framework for representing speech at the word level and extracting features from the acoustic, temporal domain, without the need for spectral encoding or preprocessing. Leveraging recent work on unsupervised learning of invariant sensory representations, we extract a signature for a word by first projecting its raw waveform onto a set of templates and their transformations, and then forming empirical estimates of the resulting one-dimensional distributions via histograms. The representation and relevant parameters are evaluated for word classification on a series of datasets with increasing speakermismatch difficulty, and the results are compared to those of an MFCC-based representation. Index Terms: invariance, acoustic features, speech representation, word classification"
2838537,21258,9804,Glimpse-based metrics for predicting speech intelligibility in additive noise conditions,2016,"The glimpsing model of speech perception in noise operates by recognising those speech-dominant spectro-temporal regions, or glimpses, that survive energetic masking; hence, a speech recognition component is an integral part of the model. The current study evaluates whether a simpler family of metrics based solely on quantifying the amount of supra-threshold target speech available after energetic masking can account for subjective intelligibility. The predictive power of glimpse-based metrics is compared for natural, processed and synthetic speech in the presence of stationary and fluctuating maskers. These metrics are raw glimpse proportion, extended glimpse proportion, and two further refinements: one, FMGP, incorporates a component simulating the effect of forward masking; the other, HEGP, selects speech-dominant spectro-temporal regions with above-average energy on the noisy speech. The metrics are compared alongside a state-of-the-art non-glimpsing metric, using three large datasets of listener scores. Both FMGP and HEGP equal or improve upon the predictive power of the raw and extended metrics, with across-masker correlations ranging from 0.81--0.92; both metrics equal or exceed the state-of-the-art metric in all conditions. These outcomes suggests that easily-computed measures of unmasked, supra-threshold speech can serve as robust proxies for intelligibility across a range of speech styles and additive masking conditions."
456658,21258,9804,Tree-Based Estimation of Speaker Characteristics for Speech Recognition,2009,"Speaker adaptation by means of adjustment of speaker characteristic properties, such as vocal tract length, has the important advantage compared to conventional adaptation techniques that the adapted models are guaranteed to be realistic if the description of the properties are. One problem with this approach is that the search procedure to estimate them is computationally heavy. We address the problem by using a multi-dimensional, hierarchical tree of acoustic model sets. The leaf sets are created by transforming a conventionally trained model set using leaf-specific speaker profile vectors. The model sets of non-leaf nodes are formed by merging the models of their child nodes, using a computationally efficient algorithm. During recognition, a maximum likelihood criterion is followed to traverse the tree. Studies of one- (VTLN) and four-dimensional speaker profile vectors (VTLN, two spectral slope parameters and model variance scaling) exhibit a reduction of the computational load to a fraction compared to that of an exhaustive grid search. In recognition experiments on children's connected digits using adult and male models, the one-dimensional tree search performed as well as the exhaustive search. Further reduction was achieved with four dimensions. The best recognition results are 0.93% and 10.2% WER in TIDIGITS and PF-Star-Sw, respectively, using adult models."
51493,21258,9804,Speech-overlapped Acoustic Event Detection for Automotive Applications,2008,"We present two approaches on acoustic event detection for speech-enabled car applications: a generative GMM-UBM approach and a discriminative GMM-SVM supervector approach. The systems detect whether or not a certain acoustic event occurred while the built-in microphone of the car was active to record a spoken command, either before, while, or after the driver was speaking. These events can be music playing, phone ringing, a passenger different from the driver is talking, laughing, or coughing. The task is formally defined as a detection task along the lines of well established detection tasks such as speaker recognition or language recognition. Similarly, the evaluation procedure has been designed to resemble the respective official evaluation series performed by NIST (i.e. it was a blind ’one-shot’ evaluation on a separately provided dataset). The performance of the system was calculated in terms of detection miss and false alarm probabilities (C Mi ss = C FA =1 , and P Ta rget =0 .5). The performance of the superior GMMSVM system was 0.0345 for known test speakers and 0.1955 for novel test speakers. Frequency-filtered band energy coefficients (FFBE) outperformed MFCCS on that task. The results are promising and suggest further experiments on more data."
1087,21258,9804,Computational Auditory Scene Analysis by using statistics of high-dimensional speech dynamics and sound source direction,2003,"A main task for computational auditory scene analysis (CASA) is to separate several concurrent speech sources. From psychoacoustics it is known that common onsets, common amplitude modulation and sound source direction are among the important cues which allow the separation for the human auditory system. A new algorithm is presented here, that performs statistical estimation of different sources by a state-space approach which integrates temporal and frequency-specific features of speech. It is based on a Sequential Monte Carlo (SMC) scheme and tracks magnitude spectra and direction on a frame-by-frame basis. First results for estimating sound source direction and separating the envelopes of two voices are shown. The results indicate that the algorithm is able to localize two superimposed sound sources in a time scale of 50 ms. This is achieved by integrating measured high-dimensional statistics of speech. Also, the algorithm is able to track the short-time envelope and the short-time magnitude spectra of both voices on a time scale of 10 -4 0 ms. The algorithm presented in this paper is developed for but not restricted to use in binaural hearing aid applications, as it is based on two head-mounted microphone signals as input. It is conceptionally able to separate more than two voices and integrate additional cues."
2759940,21258,9804,Rhythm influences the tonal realisation of focus,2015,"Several studies suggest that rhythm affects different aspects in speech production and perception. For example, in German, discourse structure is normally marked by pitch accent placement and pitch accent type, however, there is variation that cannot be explained by purely semantic or syntactic factors. Prosody-inherent factors, like rhythm, can contribute to this variation. This becomes evident in prosodically more complex environments: while the prosody of utterances containing one focused constituent is well investigated and rather clear-cut, the prosodic organisation of multiple contrastive foci is less clear. In double-focus constructions, for example, two focused constituents demand prominence, possibly resulting in the realisation of two pitch accents. If these pitch accents are required on adjacent syllables they conflict with rhythmic preferences. We present a sentence reading experiment investigating the tonal realisation of two focused constituents and how their contours affect each other in different rhythmic environments. Specifically, we tested whether a potential pitch accent clash in a sentence with two corrective foci influences the pitch excursion and the absolute peak height of the accented syllables. The results demonstrate that rhythmic constraints affect the organisation of the tonal marking of corrective focus. Index Terms: rhythm, prosody, contrastive focus, melodic effects, F0 parametrisation"
391317,21258,9804,Pruning Transitions in a Hidden Markov Model with Optimal Brain Surgeon,2003,"This paper concerns about reducing the topology of a hidden Markov model (HMM) for a given task. The purpose is two-fold: (1) to select a good model topology with improved generalization capability; and/or (2) to reduce the model complexity so as to save memory and computation costs. The rst goal falls into the active research area of model selection. From the model-theoretic research community, various measures such as Bayesian information criterion, minimum description length, minimum message length have been proposed and used with some success. In this paper, we are considering another approach in which a well-performed HMM, though perhaps oversized, is optimally pruned so that the loss in the model training cost function is minimal. The method is known as Optimal Brain Surgeon (OBS) that has been used in the neural network (NN) community. The application of OBS to NN is a constrained optimization problem; its application to HMM is more involved and it becomes a quadratic programming problem with both equality and inequality constraints. The detailed formulation is presented, and the algorithm is shown effective by an example in which HMM state transitions are pruned. The reduced model also results in better generalization performance on unseen test data."
2839675,21258,11491,Recurrent Support Vector Machines for Audio-Based Multimedia Event Detection,2016,"Multimedia event detection (MED) is the task of detecting given events (e.g. parade, birthday party) in a large collection of video clips. While the most useful information comes from visual features and speech recognition, a lot can also be inferred from the non-speech audio content, either alone or in conjunction with visual and speech cues. This paper studies MED with non-speech audio information only. MED is usually performed in two stages. The first stage generates a representation for each clip in the form of either a single vector or a sequence of vectors, often by aggregating frame-level features; the second stage performs binary or multi-class classification to decide whether each target event occurs in each clip. Common classifiers used for the second stage include support vector machines (SVMs), feed-forward deep neural networks (DNNs), and recurrent neural networks (RNNs).   In this paper, we propose to classify clips for events using recurrent SVMs. These models combine the kernel mapping and the large-margin optimization criterion of SVMs, and the ability to process sequences of variable lengths of RNNs. Reinforced with data augmentation, recurrent SVMs have achieved higher mean average precision (MAP) on the TRECVID 2011 MED task than both SVMs and RNNs."
1888251,21258,9804,Word Structure and Tone Perception in Mandarin,2006,"This paper presents results concerning the relationship between word structure in terms of number of syllables and tonal realization in Mandarin. It examines whether the fact that a word (in our context a prosodic word) is more complex implies certain tonal reductions. Our hypothesis is that a monosyllabic word will be uttered more carefully than a polysyllabic word due to the potentially larger number of possibly confusable words. We also examine whether the total number of syllables in a word has an effect, creating more tonal reductions in longer than in shorter words. A database of Mandarin originally designed for concatenative speech synthesis and segmented into prosodic words was statistically analyzed regarding the occurrences of syllable/tone combinations in prosodic words of varying length. 10 sets of syllables were selected comprising all four tones of Mandarin and occurring as monosyllabic words as well as in varying positions in two- to five-syllable prosodic words. The target syllables were then extracted from their original context and presented to native speakers of Mandarin who had to decide which tone they perceived. The results of the perception test indicate, inter alia, that perception of syllables taken from polysyllables indeed is more error prone than that of monosyllabic words. The number of syllables in a word, however, has only a weak influence. Furthermore, reductions mostly appear for syllables in certain locations in a word and are related with underlying syllables' durations. Index Terms: Speech production and perception, tone languages"
2735919,21258,9804,Multilingual Features Based Keyword Search for Very Low-Resource Languages,2015,"In this paper we describe RWTH Aachen’s system for keyword search (KWS) with very limited amount of transcribed audio data available in the target language. This setting has become this year’s primary condition within the Babel project [1], seeking to minimize the amount of human effort while retaining a reasonable KWS performance. Thus the highlights presented in this paper include graphemic acoustic modeling; multilingual features trained on language data from the previous project periods; comparison of tandem and hybrid DNN-HMM acoustic models; processing of large amounts of text data available on the web and the morphological KWS based on automatically derived word fragments. The evaluation is performed using two training sets for each of the six current project period’s languages ‐ full language pack (FLP), consisting of 30 hours and very limited language pack (VLLP), comprising less than 3 hours of transcribed audio data. We put our focus on the latter of the two, which is clearly more challenging. The methods described in this work allowed us to exceed 0.3 MTWV on five out of six languages using development queries. Index Terms: acoustic modeling, keyword search, graphemic, multilingual, neural networks, semi-supervised learning"
2765737,21258,9804,On the Use of Bhattacharyya based GMM Distance and Neural Net Features for Identification of Cognitive Load Levels,2014,"This paper presents a method for detecting cognitive load levels from speech. When speech is modulated by different levels of cognitive load, acoustic characteristics of speech change. In this paper, we measure acoustic distance of a stressed utterance from the baseline stress free speech using GMM-SVM kernel with Bhattacharyya based GMM distance. In addition, it is believed that airflow structure of speech production is nonlinear. This motivates us to investigate better techniques to capture nonlinear characteristic of stress information in acoustic features. Inspired by the recent success of neural networks for representation learning, we employ a single hidden layer feed forward network with non-linear activation to extract the feature vectors. Furthermore, people have different reactions to a particular task load. This inter-speaker difference in stress responses presents a major challenge for stress level detection. We use a bootstrapped training process to learn the stress response of a particular speaker. We perform experiments using data sets from Cognitive Load with Speech and EGG (CLSE) provided for the Cognitive Load Sub-Challenge of the INTERSPEECH 2014 Computational Paralinguistics Challenge. The results show that the system with our proposed strategies performs well on validation and test sets. Index Terms: cognitive load, GMM-supervector, neural net features"
2730659,21258,9804,High-Level Speech Event Analysis for Cognitive Load Classification,2014,"The Cognitive Load (CL) refers to the load imposed on an individual’s cognitive system when performing a given task, and is usually associated with the limitations of the human working memory. Stress, fatigue, lower ability to make decisions and perceptual narrowing are induced by cognitive overload which occurs when too much information has to be processed. As many physiological measures and for a nonintrusive measurement, speech features have been investigated in order to find reliable indicators of CL levels. In this paper, we have investigated high-level speech events automatically detected using the CMU-Sphinx toolkit for speech recognition. Temporal events (speech onset latency, event starting timecodes, pause and phone segments) were extracted from the speech transcriptions (phoneme, word, silent pause, filled pause, breathing). Seven audio feature sets related to the speech events were designed and assessed. Three-class SVM classifiers (Low, Medium and High level) were developed and assessed on the CSLE (Cognitive-Load with Speech and EGG) databases provided for the Interspeech'2014 Cognitive Load Sub-Challenge. These experiments have shown an improvement of 1.5 % on the Test set compared to the official baseline Unweighted Average Recall (UAR)."
2030833,21258,9804,Speaker Independent Emotion Recognition by Early Fusion of Acoustic and Linguistic Features within Ensembles,2005,"Herein we present a comparison of novel concepts for a robust fusion of prosodic and verbal cues in speech emotion recognition. Thereby 276 acoustic features are extracted out of a spoken phrase. For linguistic content analysis we use the Bag-of-Words text representation. This allows for integration of acoustic and linguistic features within one vector prior to a final classification. Extensive feature selection by filter- and wrapper based methods is fulfilled. Likewise optimal sets via SVM-SFFS and single feature relevance by information gain ratio calculation are presented. Overall classification is realised by diverse ensemble approaches. Among base classifiers Kernel Machines, Decision Trees, Bayesian classifiers, and memory-based learners are found. Acoustics only tests ran on a database comprising 39 speakers for speaker independent accuracy analysis. Additionally the public Berlin Emotional Speech database is used. A further database of 4,221 movie related phrases forms the basis of acoustic and linguistic information analysis evaluation. Overall remarkable performance in the discrimination of seven discrete emotions could be observed."
2682324,21258,9804,"Acoustic-Prosodic, Turn-taking, and Language Cues in Child-Psychologist Interactions for Varying Social Demand",2013,"Impaired social communication and social reciprocity are the primary phenotypic distinctions between autism spectrum disorders (ASD) and other developmental disorders. We investigate quantitative conversational cues in child-psychologist interactions using acoustic-prosodic, turn-taking, and language features. Results indicate the conversational quality degraded for children with higher ASD severity, as the child exhibited difficulties conversing and the psychologist varied her speech and language strategies to engage the child. When interacting with children with increasing ASD severity, the psychologist exhibited higher prosodic variability, increased pausing, more speech, atypical voice quality, and less use of conventional conversational cue such as assents and non-fluencies. Children with increasing ASD severity spoke less, spoke slower, responded later, had more variable prosody, and used personal pronouns, affect language, and fillers less often. We also investigated the predictive power of features from interaction subtasks with varying social demands placed on the child. We found that acoustic prosodic and turn-taking features were more predictive during higher social demand tasks, and that the most predictive features vary with context of interaction. We also observed that psychologist language features may be robust to the amount of speech in a subtask, showing significance even when the child is participating in minimal-speech, low social-demand tasks. Index Terms: autism spectrum disorders, atypical prosody, social reciprocity, turn-taking, language cues"
1182411,21258,535,Acoustic modelling for speech recognition: Hidden Markov models and beyond?,2009,"Hidden Markov models (HMMs) are still the dominant form of acoustic model used in automatic speech recognition (ASR) systems. However over the years the form, and training, of the HMM for ASR have been extended and modified, so that the current forms used in state-of-the-art speech recognition systems are very different to those originally proposed thirty years ago. This talk will review two of the more important extensions that have been proposed over the years: discriminative training; and speaker and environment adaptation. The use of discriminative training is now common with forms based on minimum Bayes' training and minimum classification error being applied to systems trained on many hundreds of hours of speech data. The talk will describe these current approaches, as well as discussing the current trends towards schemes based on large-margin training approaches. Linear transform based speaker adaptation is the dominant form for speaker adaptation. Current approaches, including extensions to linear transforms and model-based noise robustness techniques, and trends will also be described. Details of the various forms of the adaptation/noise transformation, training criterion and approaches for adaptive training will be given. The final part of the talk will discuss research beyond the current HMM framework. Schemes based on both discriminative models and functions, as well as non-parametric approaches will be described."
2379654,21258,11321,On the adequacy of baseform pronunciations and pronunciation variants,2004,"This paper presents an approach to automatically extract and evaluate the “stability” of pronunciation variants (i.e., adequacy of the model to accommodate this variability), based on multiple pronunciations of each lexicon words and the knowledge of a reference baseform pronunciation. Most approaches toward modelling pronunciation variability in speech recognition are based on the inference (through an ergodic HMM model) of a pronunciation graph (including all pronunciation variants), usually followed by a smoothing (e.g., Bayesian) of the resulting graph. Compared to these approaches, the approach presented here di.ers by (1) the way the models are inferred and (2) the way the smoothing (i.e., keeping the best ones) is done. In our case, indeed, inference of the pronunciation variants is obtained by slowly “relaxing” a (usually left-to-right) baseform model towards a fully ergodic model. In this case, the more stable the model is, the less the inferred model will diverge from it. Hence, for each pronunciation model so generated, we evaluate their adequacy by calculating the Levenshtein distance of the the new model with respect to the baseform, as well as their con.- dence measure (based on some posterior estimation), and models with the lowest Levenshtein distance and highest con.dence are preserved. On a large telephone speech database (Phonebook), we show the relationship between this “stability” measure and recognition performance, and we .nally show that automatically adding a few pronunciation variants to the less stable words is enough to significantly improve recognition rates."
2526513,21258,9804,Generating segmental foreign accent,2014,"For most of us, speaking in a non-native language involves deviating to some extent from native pronunciation norms. However, the detailed basis for foreign accent (FA) remains elusive, in part due to methodological challenges in isolating segmental from suprasegmental factors. The current study examines the role of segmental features in conveying FA through the use of a generative approach in which accent is localised to single consonantal segments. Three techniques are evaluated: the first requires a highly-proficiency bilingual to produce words with isolated accented segments; the second uses cross-splicing of context-dependent consonants from the non-native language into native words; the third employs hidden Markov model synthesis to blend voice models for both languages. Using English and Spanish as the native/non-native languages respectively, listener cohorts from both languages identified words and rated their degree of FA. All techniques were capable of generating accented words, but to differing degrees. Naturally-produced speech led to the strongest FA ratings and synthetic speech the weakest, which we interpret as the outcome of over-smoothing. Nevertheless, the flexibility offered by synthesising localised accent encourages further development of the method. Index Terms: Foreign accent, speech synthesis, splicing"
2822412,21258,9804,Inferring phonemic classes from CNN activation maps using clustering techniques,2016,"Today's state-of-art in speech recognition involves deep neu-ral networks (DNN). These last years, a certain research effort has been invested in characterizing the feature representations learned by DNNs. In this paper, we focus on convolutional neu-ral networks (CNN) trained for phoneme recognition in French. We report clustering experiments performed on activation maps extracted from the different layers of a CNN comprised of two convolution and sub-sampling layers followed by three dense layers. Our goal was to get insights into phone separability and phonemic categories inferred by the network, and how they vary according to the successive layers. Two directions were explored with both linear and non-linear clustering techniques. First, we imposed a number of 33 classes equal to the number of context-independent phone models for French, in order to assess the phoneme separability power of the different layers. As expected, we observed that this power increases with the layer depth in the network: from 34% to 74% in F-measure from the first convolution to the last dense layers, when using spectral clustering. Second, optimal numbers of classes were automatically inferred through inter-and intra-cluster measure criteria. We analyze these classes in terms of standard French phonological features."
2778531,21258,9804,Dynamic noise aware training for speech enhancement based on deep neural networks.,2014,"We propose three algorithms to address the mismatch problem in deep neural network (DNN) based speech enhancement. First, we investigate noise aware training by incorporating noise informationin the testutterance with anideal binary maskbased dynamic noise estimation approach to improve DNN’s speech separation ability from the noisy signal. Next, a set of more than 100 noise types is adopted to enrich the generalization capabilities of the DNN to unseen and non-stationary noise conditions. Finally, the quality of the enhanced speech can further be improved by global variance equalization. Empirical results show that each of the three proposed techniques contributes to the performance improvement. Compared to the conventional logarithmic minimum mean squared error speech enhancement method, our DNN system achieves 0.32 PESQ (perceptual evaluation of speech quality) improvement across six signal-tonoise ratio levels ranging from -5dB to 20dB on a test set with unknown noise types. We also observe that the combined strategies can well suppress highly non-stationary noise better than all the competing state-of-the-art techniques we have evaluated. Index Terms: Speech enhancement, deep neural networks, noise aware training, ideal binary mask, non-stationary noise"
2824894,21258,9804,Improving generalisation to new speakers in spoken dialogue state tracking,2016,"Users with disabilities can greatly benefit from personalised voice-enabled environmental-control interfaces, but for users with speech impairments (e.g. dysarthria) poor ASR performance poses a challenge to successful dialogue. Statistical dialogue management has shown resilience against high ASR error rates, hence making it useful to improve the performance of these interfaces. However, little research was devoted to dialogue management personalisation to specific users so far. Recently, data driven discriminative models have been shown to yield the best performance in dialogue state tracking (the inference of the user goal from the dialogue history). However, due to the unique characteristics of each speaker, training a system for a new user when user specific data is not available can be challenging due to the mismatch between training and working conditions. This work investigates two methods to improve the performance with new speakers of a LSTM-based personalised state tracker: The use of speaker specific acoustic and ASRrelated features; and dropout regularisation. It is shown that in an environmental control system for dysarthric speakers, the combination of both techniques yields improvements of 3.5% absolute in state tracking accuracy. Further analysis explores the effect of using different amounts of speaker specific data to train the tracking system."
211903,21258,9804,Integrated Feature Normalization and Enhancement for robust Speaker Recognition using Acoustic Factor Analysis.,2012,"Abstract : State-of-the-art factor analysis based channel compensation methods for speaker recognition are based on the assumption that speaker/utterance dependent Gaussian Mixture Model (GMM) mean super-vectors can be constrained to lie in a lower dimensional subspace, which does not consider the fact that conventional acoustic features may also be constrained in a similar way in the feature space. In this study, motivated by the low-rank covariance structure of cepstral features, we propose a factor analysis model in the acoustic feature space instead of the super-vector domain and derive a mixture of dependent feature transformation. We demonstrate that, the proposed Acoustic Factor Analysis (AFA) transformation performs feature dimensionality reduction, decorrelation, variance normalization and enhancement at the same time. The transform applies a square-root Wiener gain on the acoustic feature eigenvector directions, and is similar to the signal sub-space based speech enhancement schemes. We also propose several methods of adaptively selecting the AFA parameter for each mixture. The proposed feature transformation is applied using a probabilistic mixture alignment, and is integrated with a conventional i-Vector system. Experimental results on the telephone trials of the NIST SRE 2010 demonstrate the effectiveness of the proposed scheme."
212073,21258,9804,A Novel Framework of Text-independent Speaker Verification based on Utterance Transform and Iterative Cohort Modeling,2006,"A novel framework for text-independent speaker verification is proposed. The framework is based on a new interpretation of Universal Background Model. The UBM in our framework actually defines a transform which maps the variable length observation into a fixed dimensional supervector(supervector space). Each speech utterance is then mapped into a point in this supervector space. The similarity measure in this vector space is progressively refined via an iterative cohort modeling scheme. The experiments on NIST 2002 corpus show the effectiveness of this new framework. Overall the EER drops from the baseline system(with TNorm) 9.21% to final improved system(without T-Norm) 8.07%. The new framework can effectively reduce the data dependence in the final output score which is clearly indicated in the second sets of experiments. The EER after T-Norm of final system marginally increases by relatively 1.73% compared to the EER of baseline system drops 16.12% relatively after T-Norm. Also, the relative improvement of DCF after T-Norm is marginal for the final improved system (2.47%) compared to 33.68% in baseline system. It clear shows that the iterative cohort modeling effectively reduce the data dependence of the final scores, so that T-Norm will not further improve the system performance. Also, the performance of novel frame clearly increases as the iteration grows which suggest that the framework progressively refine the similarity measure on the supervector space with the iterative cohort modeling. Index Terms: speaker verification, utterance transform, iterative cohort modeling."
2641657,21258,535,CRIM and LIUM approaches for multi-genre broadcast media transcription,2015,"The Multi-Genre Broadcast Challenge at ASRU 2015 is a controlled evaluation of speech recognition, speaker diarization, and lightly supervised alignment using BBC TV recordings. CRIM and LIUM teams participated in the speech recognition part of the challenge with a joint submission. This paper presents the CRIM and LIUM's contributions. Each team made different choices to develop its ASR system. By the way, it was expected to compare and to evaluate different approaches to diarization and acoustic modeling, and to get complementary ASR systems for effective merging. CRIM's main contributions are the use of a training scenario similar to multi-lingual training to estimate the deep neural net (DNN) acoustic models with most of the data, the use of a pruned trigram model for search, in addition to the use of a genre-dependent quadgram language model for rescoring the lattice from the search. For LIUM, the focus was on fast decoding with high accuracy. The final word error rates (WER) after merging show that it is possible to get reasonable WER with automatically aligned files. The final global WER of 25.1% corresponds to a WER reduction of about 20% absolute in comparison to the ASR baseline system provided by the organizers."
2447126,21258,9804,Robust Digit Recognition in Noise: An Evaluation Using the AURORA Corpus †,2001,"In this paper, a variety of techniques for robust digit recognition in noise are considered using the AURORA 2.0 corpus. Current recognizers perform as well as humans in small vocabulary tasks but computer recognition performance degrades substantially when noise is introduced into the speech, while human performance is much less sensitive. To make the recognizer robust, several methodologies are employed. These include, feature processing, enhancement before recognition and model adaptation. We considered a number of processing and adaptation scenarios depending on noise type. The best performance, as expected, was obtained in matched training conditions which in general has limited applicability in real world problems. As a feature processing step, using RCCs (Root Cepstrum Coeff.) instead of MFCCs gave substantial improvement. MFCC with front-end enhancement increased performance considerably, but results were far from that obtained with matched training. When we combine the RCC with enhancement, however, we get the best results. In the next step, we employed model adaptation techniques which outperformed MFCC+enhancement and gave much closer results to the matched condition limits. However, MFCC adaptation could not outperform RCC parameterization with front-end enhancement, which we show is much more computationally efficient than model adaptation."
1389406,21258,11321,Modified MMI/MPE: a direct evaluation of the margin in speech recognition,2008,"In this paper we show how common speech recognition training criteria such as the Minimum Phone Error criterion or the Maximum Mutual Information criterion can be extended to incorporate a margin term. Different margin-based training algorithms have been proposed to refine existing training algorithms for general machine learning problems. However, for speech recognition, some special problems have to be addressed and all approaches proposed either lack practical applicability or the inclusion of a margin term enforces significant changes to the underlying model, e.g. the optimization algorithm, the loss function, or the parameterization of the model. In our approach, the conventional training criteria are modified to incorporate a margin term. This allows us to do large-margin training in speech recognition using the same efficient algorithms for accumulation and optimization and to use the same software as for conventional discriminative training. We show that the proposed criteria are equivalent to Support Vector Machines with suitable smooth loss functions, approximating the non-smooth hinge loss function or the hard error (e.g. phone error). Experimental results are given for two different tasks: the rather simple digit string recognition task Sietill which severely suffers from overfitting and the large vocabulary European Parliament Plenary Sessions English task which is supposed to be dominated by the risk and the generalization does not seem to be such an issue."
2596519,21258,9804,Unsupervised Domain Discovery Using Latent Dirichlet Allocation for Acoustic Modelling in Speech Recognition,2015,"Speech recognition systems are often highly domain dependent, a fact widely reported in the literature. However the concept of domain is complex and not bound to clear criteria. Hence it is often not evident if data should be considered to be out-of-domain. While both acoustic and language models can be domain specific, work in this paper concentrates on acoustic modelling. We present a novel method to perform unsupervised discovery of domains using Latent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains is assumed to exist in the data, whereby each audio segment can be considered to be a weighted mixture of domain properties. The classification of audio segments into domains allows the creation of domain specific acoustic models for automatic speech recognition. Experiments are conducted on a dataset of diverse speech data covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech, with a joint training set of 60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation to LDA based domains was shown to yield relative Word Error Rate (WER) improvements of up to 16% relative, compared to pooled training, and up to 10%, compared with models adapted with human-labelled prior domain knowledge."
2899580,21258,9804,L1-L2 Interference: The case of final devoicing of French voiced fricatives in final position by German learners,2016,"This work is dealing with a case of L1-L2 interference in language learning. The Germans learning French as a second language frequently produce unvoiced fricatives in word-final position instead of the expected voiced fricatives. We investigated the production of French fricatives for 16 non-native (8 beginner-and 8 advanced-learners) and 8 native speakers, and designed auditory feedback to help them realize the right voicing feature. The productions of all speakers were categorized either as voiced or unvoiced by experts. The same fricatives were also evaluated by non-experts in a perception experiment targeting VCs. We compare the ratings by experts and non-experts with the feature-based analysis. The ratio of locally unvoiced frames in the consonantal segment and also the ratio between consonantal duration and V1 duration were measured. The acoustic cues of neighboring sounds and pitch-based features play a significant role in the voicing judgment. As expected, we found that beginners face more difficulties to produce voiced fricatives than advanced learners. Also, the production becomes easier for the learners, especially for the beginners, if they practice repetition after a native speaker. We use these findings to design and develop feedback via speech analysis/synthesis technique TD-PSOLA using the learner's own voice."
1617679,21258,535,Model-based parametric features for emotion recognition from speech,2011,"Automatic emotion recognition from speech is desirable in many applications relying on spoken language processing. Telephone-based customer service systems, psychological healthcare initiatives, and virtual training modules are examples of real-world applications that would significantly benefit from such capability. Traditional utterance-level emotion recognition relies on a global feature set obtained by computing various statistics from raw segmental and supra-segmental measurements, including fundamental frequency (F0), energy, and MFCCs. In this paper, we propose a novel, model-based parametric feature set that better discriminates between the competing emotion classes. Our approach relaxes modeling assumptions associated with using global statistics (e.g. mean, standard deviation, etc.) of traditional segment-level features for classification, and results in significant improvements over the state-of-the-art in 7-way emotion classification accuracy on the standard, freely-available Berlin Emotional Speech Corpus. These improvements are consistent even in a reduced feature space obtained by Fisher's Multiple Linear Discriminant Analysis, demonstrating the signficantly higher discriminative power of the proposed feature set."
338342,21258,11321,Juicer: a weighted finite-state transducer speech decoder,2006,"A major component in the development of any speech recognition system is the decoder. As task complexities and, consequently, system complexities have continued to increase the decoding problem has become an increasingly significant component in the overall speech recognition system development effort, with efficient decoder design contributing to significantly improve the trade-off between decoding time and search errors. In this paper we present the “Juicer” (from transducer) large vocabulary continuous speech recognition (LVCSR) decoder based on weighted finite-State transducer (WFST). We begin with a discussion of the need for open source, state-of-the-art decoding software in LVCSR research and how this lead to the development of Juicer, followed by a brief overview of decoding techniques and major issues in decoder design. We present Juicer and its major features, emphasising its potential not only as a critical component in the development of LVCSR systems, but also as an important research tool in itself, being based around the flexible WFST paradigm. We also provide results of benchmarking tests that have been carried out to date, demonstrating that in many respects Juicer, while still in its early development, is already achieving state-of-the-art. These benchmarking tests serve to not only demonstrate the utility of Juicer in its present state, but are also being used to guide future development, hence, we conclude with a brief discussion of some of the extensions that are currently under way or being considered for Juicer."
1311479,21258,535,Linear versus mel frequency cepstral coefficients for speaker recognition,2011,"Mel-frequency cepstral coefficients (MFCC) have been dominantly used in speaker recognition as well as in speech recognition. However, based on theories in speech production, some speaker characteristics associated with the structure of the vocal tract, particularly the vocal tract length, are reflected more in the high frequency range of speech. This insight suggests that a linear scale in frequency may provide some advantages in speaker recognition over the mel scale. Based on two state-of-the-art speaker recognition back-end systems (one Joint Factor Analysis system and one Probabilistic Linear Discriminant Analysis system), this study compares the performances between MFCC and LFCC (Linear frequency cepstral coefficients) in the NIST SRE (Speaker Recognition Evaluation) 2010 extended-core task. Our results in SRE10 show that, while they are complementary to each other, LFCC consistently outperforms MFCC, mainly due to its better performance in the female trials. This can be explained by the relatively shorter vocal tract in females and the resulting higher formant frequencies in speech. LFCC benefits more in female speech by better capturing the spectral characteristics in the high frequency region. In addition, our results show some advantage of LFCC over MFCC in reverberant speech. LFCC is as robust as MFCC in the babble noise, but not in the white noise. It is concluded that LFCC should be more widely used, at least for the female trials, by the mainstream of the speaker recognition community."
99673,21258,9804,Speech Generation from Hand Gestures Based on Space Mapping,2009,"Individuals with speaking disabilities, particularly people suffering from dysarthria, often use a TTS synthesizer for speech communication. Since users always have to type sound symbols and the synthesizer reads them out in a monotonous style, the use of the current synthesizers usually renders real-time operation and lively communication difficult. This is why dysarthric users often fail to control the flow of conversation. In this paper, we propose a novel speech generation framework which makes use of hand gestures as input. People usually use tongue gesture transitions for speech generation but we develop a special glove, by wearing which, speech sounds are generated from hand gesture transitions. For development, GMM-based voice conversion techniques (mapping techniques) are applied to estimate a mapping function between a space of hand gestures and another space of speech sounds. In this paper, as an initial trial, a mapping between hand gestures and Japanese vowel sounds is estimated so that topological features of the selected gestures in a feature space and those of the five Japanese vowels in a cepstrum space are equalized. Experiments show that the special glove can generate good Japanese vowel transitions with voluntary control of duration and articulation. Index Terms: Dysarthria, speech production, hand motions, media conversion, arrangement of gestures and vowels"
93425,21258,9804,Combined front-end signal processing for in-vehicle speech systems.,2001,"In this paper, we investigate the integration of two processing methods to improve speech quality for invehicle speech systems: multi-sensor beamforming and constrained iterative (Auto-LSP) speech enhancement. The intent is to establish an intelligent microphone array processing scheme in high noise environments by considering the effectiveness of a multi-sensor beamformer method and the Auto-LSP single channel speech enhancement method. The goal therefore is to design a system where the strengths of one method help compensate any potential weaknesses of the other. The noise cancellation method is an acoustic beamformer designed and constructed using a linear microphone array. The speech enhancement method is the constrained iterative Auto-LSP approach, previously considered for single channel enhancement. After establishing the combined processing scheme, evaluations are performed using speech and acoustic noise data collected in vehicles. Noise suppression levels by the beamformer is established for different road noise conditions. Quality improvement from the enhancement scheme is assessed using objective speech quality measures over a test speech corpus using TIMIT data. The results show that while beamforming alone can suppress background noise levels, the combination of beamforming and constrained enhancement can provide as much as a 63% improvement in objective quality, suggesting a potential single comprehensive solution for in-vehicle speech systems."
176431,21258,9804,Combining Multi-Party Speech and Text Exchanges over the Internet,2001,"Bilateral or group text chatting over the Internet has become a favoured pastime for many people across the world. Yet it would seem that, in general, text chat is a severely impoverished mode of on-line communication compared to, e.g., fully situated human-human spoken conversation, video conferencing, or even speaking over the telephone. This paper explores what happens when on-line multi-speaker conversation over the Internet is added to text chat, creating what may become a widespread mode of communication in the near future. The system used is called the Magic Lounge. Magic Lounge offers a multimodal combination of text chat and spoken conversation for meetings and other encounters among ubiquitous users who may join the communication from workstations, PDAs and WAP phones. In addition, the system has a series of meeting history tools which provide various forms of structure to the spoken and text chat records of the meeting as it unfolds and after the meeting. The paper presents rather clear-cut results on the respective communicative roles of speech and text chat from a series of user tests with the system in which different groups of users performed scenarios designed to explore the combined use of text chat and speech. The results reported may generalise to a wide range of applications which combine text and spoken information representation."
2809359,21258,9804,Uncertainty training and decoding methods of deep neural networks based on stochastic representation of enhanced features,2015,"Speech enhancement is an important front-end technique to improve automatic speech recognition (ASR) in noisy environments. However, the wrong noise suppression of speech enhancement often causes additional distortions in speech signals, which degrades the ASR performance. To compensate the distortions, ASR needs to consider the uncertainty of enhanced features, which can be achieved by using the expectation of ASR decoding/training process with respect to the probabilistic representation of input features. However, unlike the Gaussian mixture model, it is difficult for Deep Neural Network (DNN) to deal with this expectation analytically due to the nonlinear activations. This paper proposes efficient Monte-Carlo approximation methods for this expectation calculation to realize DNN based uncertainty decoding and training. It first models the uncertainty of input features with linear interpolation between original and enhanced feature vectors with a random interpolation coefficient. By sampling input features based on this stochastic process in training, DNN can learn to generalize the variations of enhanced features. Our method also samples input features in decoding, and integrates multiple recognition hypotheses obtained from the samples. Experiments on the reverberated noisy speech recognition tasks (the second CHiME and REVERB challenges) show the effectiveness of our techniques."
457248,21258,9804,Title Generation for Spoken Broadcast News using a Training Corpus,2000,"The problem of title generation involves finding the essence of a document and expressing it in only a few words. The results of a query to the Informedia Digital Video Library are summarized through an automatically generated title for each retrieved news story. When the document is errorful, as with speech-recognized broadcast news stories, the title creation challenge becomes even greater. We implemented a set of title word selection strategies and evaluated them on an independent test corpus of 579 broadcast news documents, comparing manual transcription results to automatically recognized speech using the CMU Sphinx speech recognition system with a 64000-word broadcast news language model. Using a training collection of 21190 transcribed broadcast news stories, we trained several systems to produce appropriate title words, i.e. Naive Bayesian approach with full vocabulary, Naive Bayesian approach with limited vocabulary, nearest neighbor approach and extractive approach. The F1 results shows that the nearest neighbor approach is a quick and easy way of generating good titles for speech recognized documents (F1 = 15.2%), while a Nave Bayesian approach with limited vocabulary also does well on our F1 measure (F1 = 21.6%), which ignores word order in the titles. Overall, the results show that title generation for speech recognized news documents is possible at a level approaching the accuracy of titles generated for perfect text transcriptions. One surprising phenomenon is that extractive approach performances slightly better for speech recognized documents than for manual transcripts."
264140,21258,9804,Integer Linear Programming for Speaker Diarization and Cross-Modal Identification in TV Broadcast,2013,"Most state-of-the-art approaches address speaker diariza- tion as a hierarchical agglomerative clustering problem in the audio domain. In this paper, we propose to revisit one of them: speech turns clustering based on the Bayesian Information Cri- terion (a.k.a. BIC clustering). First, we show how to model it as an integer linear programming (ILP) problem. Its resolu- tion leads to the same overall diarization error rate as standard BIC clustering but generates significantly purer speaker clus- ters. Then, we describe how this approach can easily be ex- tended to the audiovisual domain and TV broadcast in particu- lar. The straightforward integration of detected overlaid names (used to introduce guests or journalists, and obtained via video OCR) into a multimodal ILP problem yields significantly better speaker diarization results. Finally, we explain how this novel paradigm can incidentally be used for unsupervised speaker identification (i.e. not relying on any prior acoustic speaker models). Experiments on the REPERE TV broadcast corpus show that it achieves performance close to that of an oracle ca- pable of identifying any speaker as long as their name appears on screen at least once in the video."
2328426,21258,9804,Integration of supra-lexical linguistic models with speech recognition using shallow parsing and finite state transducers.,2002,"This paper proposes a layered Finite State Transducer (FST) framework integrating hierarchical supra-lexical linguistic knowledge into speech recognition based on shallow parsing. The shallow parsing grammar is derived directly from the full fledged grammar for natural language understanding, and augmented with top-level n-gram probabilities and phrase-level context-dependent probabilities, which is beyond the standard context-free grammar (CFG) formalism. Such a shallow parsing approach can help balance sufficient grammar coverage and tight structure constraints. The context-dependent probabilistic shallow parsing model is represented by layered FSTs, which can be integrated with speech recognition seamlessly to impose early phrase-level structural constraints consistent with natural language understanding. It is shown that in the JUPITER [1] weather information domain, the shallow parsing model achieves lower recognition word error rates, compared to a regular class n-gram model with the same order. However, we find that, with a higher order top-level n-gram model, pre-composition and optimization of the FSTs are highly restricted by the computational resources available. Given the potential of such models, it may be worth pursing an incremental approximation strategy [2], which includes part of the linguistic model FST in early optimization, while introducing the complete model through dynamic composition."
2714374,21258,9804,A Comparison of Features for Synthetic Speech Detection,2015,"The performance of biometric systems based on automatic speaker recognition technology is severely degraded due to spoofing attacks with synthetic speech generated using diff erent voice conversion (VC) and speech synthesis (SS) techniques. Various countermeasures are proposed to detect this type of attack, and in this context, choosing an appropriate feature extraction technique for capturing relevant information from speech is an important issue. This paper presents a concise experimental review of different features for synthetic speech detection task. A wide variety of features considered in this stud y include previously investigated features as well as some other potentially useful features for characterizing real and sy nthetic speech. The experiments are conducted on recently released ASVspoof 2015 corpus containing speech data from a large number of VC and SS technique. Comparative results using two different classifiers indicate that features representing spectral information in high-frequency region, dynamic information of speech, and detailed information related to subband characteristics are considerably more useful in detecting synthetic sp eech. Index Terms: anti-spoofing, ASVspoof 2015, feature extraction, countermeasures"
2908867,21258,339,VoiceLive: A Phoneme Localization based Liveness Detection for Voice Authentication on Smartphones,2016,"Voice authentication is drawing increasing attention and becomes an attractive alternative to passwords for mobile authentication. Recent advances in mobile technology further accelerate the adoption of voice biometrics in an array of diverse mobile applications. However, recent studies show that voice authentication is vulnerable to replay attacks, where an adversary can spoof a voice authentication system using a pre-recorded voice sample collected from the victim. In this paper, we propose VoiceLive, a practical liveness detection system for voice authentication on smartphones. VoiceLive detects a live user by leveraging the user's unique vocal system and the stereo recording of smartphones. In particular, with the phone closely placed to a user's mouth, it captures time-difference-of-arrival (TDoA) changes in a sequence of phoneme sounds to the two microphones of the phone, and uses such unique TDoA dynamic which doesn't exist under replay attacks for liveness detection. VoiceLive is practical as it doesn't require additional hardware but two-channel stereo recording that is supported by virtually all smartphones. Our experimental evaluation with 12 participants and different types of phones shows that VoiceLive achieves over 99% detection accuracy at around 1% Equal Error Rate (EER). Results also show that VoiceLive is robust to different phone placements and is compatible to different sampling rates and phone models."
2665159,21258,9804,On Efficient Training of Word Classes and Their Application to Recurrent Neural Network Language Models,2015,"In this paper, we investigated various word clustering methods, by studying two clustering algorithms: Brown clustering and exchange algorithm, and three objective functions derived from different class-based language models (CBLM): two-sided, predictive and conditional models. In particular, we focused on the implementation of the exchange algorithm with improved speed. In total, we compared six clustering methods in terms of runtime and perplexity (PP) of the CBLM on a French corpus, and show that our accelerated implementation of exchange algorithm is up to 114 times faster than the original and around 6 times faster than the best implementation of Brown clustering we could find, while performing about the same (slightly better) in PP. In addition, we conducted a keyword search experiment on the Babel Lithuanian task (IARPA-babel304b-v1.0b), which showed that CBLM improves the word error rate (WER) but not the keyword search performance. Furthermore, we used these clustering techniques for the output layer of a recurrent neural network (RNN) language model (LM) and we show that in terms of PP of the RNN LM, word classes trained under the predictive model perform slightly better than those trained under other criteria we considered. Index Terms: word clustering, language modeling, neural network based language model, recurrent neural network, long short-term memory"
2786254,21258,9804,Microphone array post-filtering using supervised machine learning for speech enhancement.,2014,"High level of noise reduces the perceptual quality and intelligibility of speech. Therefore, enhancing the captured speech signal is important in everyday applications such as telephony and teleconferencing. Microphone arrays are typically placed at a distance from a speaker and require processing to enhance the captured signal. Beamforming provides directional gai nt owards the source of interest and attenuation of interference. It is often followed by a single channel post-filter to further enhance the signal. Non-linear spatial post-filters are capable of providing high noise suppression but can produce unwanted musical noise that lowers the perceptual quality of the output. This work proposes an artificial neural network (ANN) to learn the structure of naturally occurring post-filters to enhance speech from interfering noise. The ANN uses phase-based features obtained from a multichannel array as an input. Simulations are used to train the ANN in a supervised manner. The performance is measured with objective scores from speech recorded in an office environment. The post-filters predicted by the ANN are found to improve the perceptual quality over delay-and-sum beamforming while maintaining high suppression of noise characteristic to spatial post-filters. Index Terms:Speech enhancement, Microphone arrays, Array signal processing, Artificial neural networks, Psychoacoustics."
2044093,21258,9616,Toward A Speaker-Independent Real-Time Affect Detection System,2006,"The ability to detect the human affective states is rapidly gaining interests among researchers and industrial developers since it has a broad range of applications. This paper reports the advances of human affect detection from acoustic signals in Motorola Labs. We focus on two parts of affect detection: emotion detection and conversational engagement detection. The emotion detection part is the major component of our system. The system is based only on acoustic information, that is to say, there is no recognizer and no linguistic or semantic information available. Given the truth that speech is a short-time stationary signal, we employ the Hidden Markov Model (HMM) to capture the variation and trend of acoustic signal structures caused by affective states. The affect-sensitive segmental features such as pitch, energy, zero crossing rate and energy slope are extracted to capture the finer structures of acoustic signals. Each state of the HMM is modeled by a Gaussian Mixture Model (GMM), which captures the range, mean, median and variability of above affect-sensitive measures. Besides testing the algorithm in the LDC databases, we implement a real-time conversation monitor, which can recognize and express the eight basic human emotions and can detect the conversational engagement level."
190858,21258,9804,Open Source Multi-Language Audio Database for Spoken Language Processing Applications.,2011,"Abstract : This report gives a detailed summary of research work completed under Air Force Research Laboratory (AFRL) grant 53925, over the time period (April 12, 2010 April 10, 2012). There are two main aspects of the work completed. First was the collection and annotation of a large open source data base of speech passages from web sites such as You Tube. 300 passages were collected in each of three languages English, Mandarin, and Russian. Approximately 30 hours of speech were collected for each language. Each passage has been carefully transcribed at the phrasal level by human listeners. Each passage was originally transcribed and then checked and the transcription edited as needed by at least two additional native language listeners. The English and Mandarin were then forced aligned and labeled at the phonetic level using a combination of manual and automatic methods. The Russian passages have not yet been marked at the phonetic level. Another phase of the work was to explore several algorithmic methods for improving automatic speech recognition (ASR) for this intelligible but challenging data base. Note that the body of the report has four main sections plus appendices which introduce, describe, and summarize a portion of the work."
2662183,21258,535,Naturalness and rapport in a pitch adaptive learning companion,2015,"Observed frequently in human-human interactions, entrainment is a social phenomenon in which speakers become more like each other over the course of a conversation. Acoustic-prosodic entrainment occurs when individuals adapt their acoustic-prosodic speech features, such as pitch and intensity. Correlated with communicative success, naturalness, and conversational flow as well as social variables such as rapport, a dialogue system which automatically entrains has the potential to improve verbal interactions by increasing rapport, naturalness, and conversational flow. In an application like the learning companion, such a socially responsive dialogue system may improve learning and motivation. However, it is not clear how to produce entrainment in an automatic dialogue system in ways that produce the effects seen in human-human dialogue. In this paper, we take the first steps towards implementing a spoken dialogue system which can entrain. We propose three methods of pitch adaptation based on analysis of human entrainment, and design and implement a system which can manipulate the pitch of text-to-speech output adaptively. We find a clear relationship between perceptions of rapport and different forms of pitch adaptations. Certain adaptations are perceived as significantly more natural and rapport-like. Ultimately, adapting by shifting the pitch contour of the text-to-speech output by the mean pitch of the user results in the highest reported measures of rapport and naturalness."
235360,21258,9804,Feature Switching in the i-vector Framework for Speaker Verification,2014,"Feature fusion is a paradigm that has found success in a number of speech related tasks. The primary objective in applying fusion is to leverage the complementary information present in the features. Conventionally, either early or late fusion is employed. Early fusion leads to large dimensional feature vectors. Further, the range of feature values for different streams require appropriate normalisation. Late fusion is carried out at score level, where the contribution from each type of feature is determined from the set of weights used. Feature switching is yet another paradigm that attempts to capture the diversity in the feature types used. Feature switching gains significance particularly in the context of speaker verification, where the feature type that best discriminates a speaker is used to verify the claims corresponding to that speaker. Earlier, feature switching was attempted in the conventional UBM-GMM framework. In this paper, the idea is extended to the Total Variability Space (TVS) framework. Two different feature types namely Modified Group Delay (MGD) and Mel-Frequency Cepstral Coefficients (MFCC) are explored in the proposed framework. Results are presented on NIST 2010 male database for the speaker verification task. Index Terms: speaker verification, shared nearest neighbour, feature-switching"
433043,21258,9804,An Energy Search Approach to Variable Frame Rate Front-End Processing for Robust ASR,2005,"Extensive research has been devoted to robustness in the presence of various types and degrees of environmental noise over the past several years, however this remains one of the main problems facing automatic speech recognition systems. This paper describes a new variable frame rate analysis technique, based upon searching a predefined lookahead interval for the next frame position that maximizes the firstorder difference of the log energy (ΔE) between the consecutive frames. The application of this novel technique to noise-robust ASR front-end processing is also reported. In comparison with existing variable frame rate methods in the literature, the proposed energy search approach is simpler and achieves similar recognition accuracy improvements at lower complexity. Experimental work on the Aurora II connected digits database reveals that the proposed front-end, together with cumulative distribution mapping, achieves average digit recognition accuracies of 78.32% for a model set trained from clean data and 89.95% for a model set trained from data with multiple noise conditions, representing 6.1% and 2.3% reductions in word error rates respectively over a cumulative distribution mapping baseline."
341163,21258,9804,Empowering end users to personalize dialogue systems through spoken interaction.,2003,"This paper describes recent advances we have made towards the goal of empowering end users to automatically expand the knowledge base of a dialogue system through spoken interaction, in order to personalize it to their individual needs. We describe techniques used to incrementally reconfigure a preloaded trained natural language grammar, as well as the lexicon and language models for the speech recognition system. We also report on advances in the technology to integrate a spoken pronunciation with a spoken spelling, in order to improve spelling accuracy. While the original algorithm was designed for a “speak and spell” input mode, we have shown here that the same methods can be applied to separately uttered spoken and spelled forms of the word. By concatenating the two waveforms, we can take advantage of the mutual constraints realized in an integrated composite FST. Using an OGI corpus of separately spoken and spelled names, we have demonstrated letter error rates of under 6% for in-vocabulary words and under 11% for words not contained in the training lexicon, a 44% reduction in error rate over that achieved without use of the spoken form. We anticipate applying this technique to unknown words embedded in a larger context, followed by solicited spellings."
903300,21258,535,Investigation of multilingual deep neural networks for spoken term detection,2013,"The development of high-performance speech processing systems for low-resource languages is a challenging area. One approach to address the lack of resources is to make use of data from multiple languages. A popular direction in recent years is to use bottleneck features, or hybrid systems, trained on multilingual data for speech-to-text (STT) systems. This paper presents an investigation into the application of these multilingual approaches to spoken term detection. Experiments were run using the IARPA Babel limited language pack corpora (~10 hours/language) with 4 languages for initial multilingual system development and an additional held-out target language. STT gains achieved through using multilingual bottleneck features in a Tandem configuration are shown to also apply to keyword search (KWS). Further improvements in both STT and KWS were observed by incorporating language questions into the Tandem GMM-HMM decision trees for the training set languages. Adapted hybrid systems performed slightly worse on average than the adapted Tandem systems. A language independent acoustic model test on the target language showed that retraining or adapting of the acoustic models to the target language is currently minimally needed to achieve reasonable performance."
2647129,21258,535,An iterative deep learning framework for unsupervised discovery of speech features and linguistic units with applications on spoken term detection,2015,"In this work we aim to discover high quality speech features and Linguistic units directly from unlabeled speech data in a zero resource scenario. The results are evaluated using the metrics and corpora proposed in the Zero Resource Speech Challenge organized at Interspeech 2015. A Multi-layered Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics fof the given corpus and the language behind, thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. We call this iterative deep learning framework the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which generates both high quality speech features for the Track 1 of the Challenge and acoustic tokens for the Track 2 of the Challenge. In addition, we performed extra experiments on the same corpora on the application of query-by-example spoken term detection. The experimental results showed the iterative deep learning framework of MAT-DNN improved the detection performance due to better underlying speech features and acoustic tokens."
2684315,21258,9804,Pitch-Gesture Modeling Using Subband Autocorrelation Change Detection,2013,"Calculating speaker pitch (or f0) is typically the first computational step in modeling tone and intonation for spoken language understanding. Usually pitch is treated as a fixed, single-valued quantity. The inherent ambiguity judging the octave of pitch, as well as spurious values, leads to errors in modeling pitch gestures that propagate in a computational pipeline. We present an alternative that instead measures changes in the harmonic structure using a subband autocorrelation change detector (SACD). This approach builds upon new machine-learning ideas for how to integrate autocorrelation information across subbands. Importantly however, for modeling gestures, we preserve multiple hypotheses and integrate information from all harmonics over time. The benefits of SACD over standard pitch approaches include robustness to noise and amount of voicing. This is important for real-world data in terms of both acoustic conditions and speaking style. We discuss applications in tone and intonation modeling, and demonstrate the efficacy of the approach in a Mandarin Chinese tone-classification experiment. Results suggest that SACD could replace conventional pitchbased methods for modeling gestures in selected spokenlanguage processing tasks."
116547,21258,9804,Syllable Structure in Spoken Arabic: a comparative investigation,2005,"The aim of this study is to demonstrate that rhythm variation across Arabic dialects is to a great extent correlated with the different types of syllabic structure observed in these dialects, especially with regard to the relative complexity of onsets and codas. The main focus is on the relationship between syllabic structures on the one hand, and rhythm classes based on segmental duration on the other. Rhythmic variations in Arabic dialects based on proportions of vocalic and consonantal intervals were described in [1] following experimental procedures put forth by [2]. The present investigation consists in computing the frequency of occurrence of the different types of syllables in Moroccan, Tunisian and Lebanese Arabic representing different areas along the dialect continuum. The experimental data is based on the production of 10 minutes of spontaneous speech by Moroccan, Tunisian and Lebanese subjects. The results show that the syllabic patterns observed could be useful in characterizing the different Arabic dialects and may also constitute a basis for discriminating between them. The occurrence of the various types of syllables is significantly different from one dialect to another; the percentage of simple syllables and long vowels (CV, CVC, CVV, CVVC…) is higher in Lebanese Arabic. Moroccan Arabic, on the other hand, shows a tendency towards a high percentage of complex syllables (CCVC, CCVCC,..etc) and short vowels."
692226,21258,9804,Maximum a Posteriori Adaptation of Network Parameters in Deep Models,2015,"We present a Bayesian approach to adapting parameters of a well-trained context-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to improve automatic speech recognition performance. Given an abundance of DNN parameters but with only a limited amount of data, the effectiveness of the adapted DNN model can often be compromised. We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an augmented linear hidden networks connected to the output tied states, or senones, and compare it to feature space MAP linear regression previously proposed. Experimental evidences on the 20,000-word open vocabulary Wall Street Journal task demonstrate the feasibility of the proposed framework. In supervised adaptation, the proposed MAP adaptation approach provides more than 10% relative error reduction and consistently outperforms the conventional transformation based methods. Furthermore, we present an initial attempt to generate hierarchical priors to improve adaptation efficiency and effectiveness with limited adaptation data by exploiting similarities among senones. Index Terms: deep neural networks, hidden Markov model, Bayesian adaptation, automatic speech recognition."
2306002,21258,9804,THE AURORA EXPERIMENTAL FRAMEWORK FOR THE PERFORMANCE EVALUATION OF SPEECH RECOGNITION SYSTEMS UNDER NOISY CONDITIONS,2000,"This paper describes a database designed to evaluate the performance of speech recognition algorithms in noisy conditions. The database may either be used for the evaluation of front-end feature extraction algorithms using a defined HMM recognition back-end or complete recognition systems. The source speech for this database is the TIdigits, consisting of connected digits task spoken by American English talkers (downsampled to 8kHz) . A selection of 8 different real-world noises have been added to the speech over a range of signal to noise ratios and special care has been taken to control the filtering of both the speech and noise. The framework was prepared as a contribution to the ETSI STQ-AURORA DSR Working Group [1]. Aurora is developing standards for Distributed Speech Recognition (DSR) where the speech analysis is done in the telecommunication terminal and the recognition at a central location in the telecom network. The framework is currently being used to evaluate alternative proposals for front-end feature extraction. The database has been made publicly available through ELRA so that other speech researchers can evaluate and compare the performance of noise robust algorithms. Recognition results are presented for the first standard DSR feature extraction scheme that is based on a cepstral analysis."
2194860,21258,535,"It's not you, it's me: Automatically extracting social meaning from speed dates",2009,"Automatically detecting human social intentions from spoken conversation is an important task for social computing and for dialogue systems. We describe a system for detecting elements of interactional style: whether a speaker is awkward, friendly, or flirtatious. We create and use a new spoken corpus of 991 4-minute speed-dates. Participants rated themselves and each other for these elements of style. Using rich dialogue, lexical, and prosodic features, we are able to detect flirtatious, awkward, and friendly styles in noisy natural conversational data with above 70% accuracy, significantly outperforming not only the baseline but also, for flirtation, outperforming the human interlocutors. We find that features like pitch, energy, and the use of emotional vocabulary help detect flirtation, collaborative conversational style (laughter, questions, collaborative completions) help in detecting friendliness, and disfluencies help in detecting awkwardness. In analyzing why our system outperforms humans, we show that humans are very poor perceivers of flirtatiousness in this task, and instead often project their own intended behavior onto their interlocutors. This talk describes joint work with Dan McFarland (School of Education) and Rajesh Ranganath (Computer Science Department)."
2233479,21258,11470,Emotion detection in task-oriented spoken dialogues,2003,"Detecting emotions in the context of automated call center services can be helpful for following the evolution of the human-computer dialogues, enabling dynamic modification of the dialogue strategies and influencing the final outcome. The emotion detection work reported here is a part of larger study aiming to model user behavior in real interactions. We make use of a corpus of real agent-client spoken dialogues in which the manifestation of emotion is quite complex, and it is common to have shaded emotions since the interlocutors attempt to control the expression of their internal attitude. Our aims are to define appropriate emotions for call center services, to annotate the dialogues and to validate the presence of emotions via perceptual tests and to find robust cues for emotion detection. In contrast to research carried out with artificial data with simulated emotions, for real-life corpora the set of appropriate emotion labels must be determined. Two studies are reported: the first investigates automatic emotion detection using linguistic information, whereas the second concerns perceptual tests for identifying emotions as well as the prosodic and textual cues which signal them. About 11% of the utterances are annotated with non-neutral emotion labels. Preliminary experiments using lexical cues detect about 70% of these labels."
472499,21258,8494,Two-sensor noise robust ASR with missing frames for Aurora2 task,2004,"In a recently proposed system, we have used the missing frames idea for noise robust automatic speech recognition (ASR). The key point behind the missing frames idea is that frames with energies below a certain threshold are considered unreliable frames. We set these frames to a silence floor and treat them as silence frames even if they contain speech signal. Although this causes loss of valuable information such as transitional cues for consonants, we showed that for a small vocabulary task the system substantially decreases the Word Error Rate (WER) at low SNRs. We have also observed that the algorithm decreases the overall computational complexity as opposed to other proposed noise robust systems that typically require considerable computational power. The main drawback of the missing frames system is the difficulty in detecting high energy portions accurately at high noise environments. In this work we propose using a glottal sensor to detect the high energy portions of the acoustic signal. We show that the glottal sensor can detect the high energy speech portions very accurately without adding significant computational complexity. The second contribution of this paper is that we concatenate a speech enhancement algorithm to the front end of the speech recognizer. We show that the enhancement algorithm does not improve the performance of the baseline system much while it decreases the WER substantially for our proposed system."
2829013,21258,9804,Frame-Level Vocal Effort Likelihood Space Modeling for Improved Whisper-Island Detection,2011,"In this study, a frame-based vocal effort likelihood space modeling framework for improved whisper-island detection within normally phonated audio streams is proposed. The proposed method is based on first training a traditional Gaussian mixture model for whisper and neutral speech, which is then employed to extract a newly proposed discriminative feature set entitled Vocal Effort Likelihood (VEL), for whisper-island detection. The VEL feature set is integrated within a BIC/T 2 -BIC segmentation scheme for vocal effort change point(VECP) detection. With the dimension-reduced VEL 2-D feature set, the proposed framework has reduced computational costs versus prior method [1]. Experimental results using the UT-VocalEffort II corpus for whisper-island detection using the proposed framework are presented and compared with a previous algorithm introduced in [1]. The proposed algorithm is shown to improve performance in VECP detection with the lowest MultiError Score(MES) of 6.33. Furthermore, very accurate whisperisland detection was obtained using proposed algorithm, which is useful for sustained performance in speech systems (ASR, Speaker-ID, etc.)which might experience whisper speech. Finally, experimental performance achieves a 100% detection rate for the proposed algorithm, which represents the best whisperisland detection performance with lowest computational costs available in the literature to date. Index Terms: Vocal Effort Likelihood, Vocal Effort, WhisperIsland Detection, GMM Classifier"
1000325,21258,23735,Multi-talker speech recognition under ego-motion noise using Missing Feature Theory,2010,"This paper presents a system that gives a mobile robot the ability to recognize target speaker's speech, even if the robot performs an action and there are multiple speakers talking in the room. Associated problems to this system are twofold: (1) While the robot is moving, the joints inevitably generate ego-motion noise due to its motors. (2) Recognizing target speech against other interfering speech signals is a difficult task. Since typical solutions to (1) and (2), motor noise suppression and sound source separation, both introduce distortion to the processed signals, the performance of automatic speech recognition (ASR) deteriorates. Instead of removing the ego-motion noise with conventional noise suppression methods, in this work, we investigate methods to eliminate the unreliable parts of the audio features that are contaminated by the ego-motion noise. For this purpose, we model masks that filter unreliable speech features based on the ratio of speech and motor noise energies. We analyze the performance of the proposed technique under various test conditions by comparing it to the performance of existing Missing Feature Theory-based ASR implementations. Finally, we propose an integration framework for two different masks that are designed to eliminate ego noise and to filter the leakage energy of interfering sound sources. We demonstrate that the proposed methods achieve a high ASR accuracy."
54356,21258,9804,"Detection, Diarization, and Transcription of Far-Field Lecture Speech",2007,"Speech processing of lectures recorded inside smart rooms has recently attracted much interest. In particular, the topic has been central to the Rich Transcription (RT) Meeting Recognition Evaluation campaign series, sponsored by NIST, with emphasis placed on benchmarking speech activity detection (SAD), speaker diarization (SPKR), speech-to-text (STT), and speakerattributed STT (SASTT) technologies. In this paper, we present the IBM systems developed to address these tasks in preparation for the RT 2007 evaluation, focusing on the far-field condition of lecture data collected as part of European project CHIL. For their development, the systems are benchmarked on a subset of the RT Spring 2006 (RT06s) evaluation test set, where they yield significant improvements for all SAD, SPKR, and STT tasks over RT06s results; for example, a 16% relative reduction in word error rate is reported in STT, attributed to a number of system advances discussed here. Initial results are also presented on SASTT, a task newly introduced in 2007 in place of the discontinued SAD. Index Terms: speech processing, speech recognition, speaker diarization, speech activity detection, lectures, smart rooms."
3032636,21258,11166,Fraud Detection in Voice-Based Identity Authentication Applications and Services,2016,"Keeping track of the multiple passwords, PINs, memorable dates and other authentication details needed to gainremote access to accounts is one of modern life's less appealingchallenges. The employment of a voice-based verification as abiometric technology for both children and adults could be agood replacement to the old fashioned memory dependentprocedure. Using voice for authentication could be beneficial inseveral application areas, including, security, protection, education, call-based and web-based services. Voice-basedbiometric applications are subject to different types of spoofingattacks. The most accessible and affordable type of spoofing for avoice-based biometrics system is a replay attack. Replay, which isto playback a pre-recorded speech sample, presents a genuinerisk to automatic speaker verification technology. This workpresents two architectures for detecting frauds caused by replayattacks in a voice-based biometrics authentication systems. Experimental results confirmed that obtained performancesfrom both methods could further improve by applying a machinelearning algorithm for performing fusion at the score level. Theperformance of both methods further improved by fusion usingindependent sources of scores in different architectures."
21606,21258,9804,A Deterministic plus Stochastic Model of the Residual Signal for Improved Parametric Speech Synthesis,2009,"Speech generated by parametric synthesizers generally suffers from a typical buzziness, similar to what was encountered in old LPC-like vocoders. In order to alleviate this problem, a more suited modeling of the excitation should be adopted. For this, we hereby propose an adaptation of the Deterministic plus Stochastic Model (DSM) for the residual. In this model, the excitation is divided into two distinct spectral bands delimited by the maximum voiced frequency. The deterministic part concerns the low-frequency contents and consists of a decomposition of pitch-synchronous residual frames on an orthonormal basis obtained by Principal Component Analysis. The stochastic component is a high-pass filtered noise whose time structure is modulated by an energy-envelope, similarly to what is done in the Harmonic plus Noise Model (HNM). The proposed residual model is integrated within a HMM-based speech synthesizer and is compared to the traditional excitation through a subjective test. Results show a significative improvement for both male and female voices. In addition the proposed model requires few computational load and memory, which is essential for its integration in commercial applications. Index Terms: HMM-based speech synthesis, residual modeling, Deterministic plus Stochastic model"
1861406,21258,535,A multiplatform speech recognition decoder based on weighted finite-state transducers,2009,"Speech recognition decoders based on static graphs have recently proven to significantly outperform the traditional approach of prefix tree expansion in terms of decoding speed [1], [2]. The reduced search effort makes static graph decoders an attractive alternative for tasks concerned with limited processing power or memory footprint on devices such as PDAs, internet tablets, and smart phones. In this paper we explore the benefits of decoding with an optimized speech recognition network over the fully task-optimized prefix-tree based decoder IBIS [3]. We designed and implemented a new decoder called SWIFT (Speedy WeIgthed Finite-state Transducer) based on WFSTs with its application to embedded platforms in mind. After describing the design, the network construction and storage process, we present evaluation results on a small task suitable for embedded applications, and on a large task, namely the European Parliament Plenary Sessions (EPPS) task from the TC-STAR project [20]. The SWIFT Decoder is up to 50% faster than IBIS on both tasks. In addition, SWIFT achieves significant memory consumption reductions obtained by our innovative network specific storage layout optimization."
1986151,21258,9804,Auto-Checking Speech Transcriptions by Multiple Template Constrained Posterior,2009,"Checking transcription errors in speech database is an important but tedious task that traditionally requires intensive manual labor. In [9], Template Constrained Posterior (TCP) was proposed to automate the checking process by screening potential erroneous sentences with a single context template. However, single template-based method is not robust and requires parameter optimization that still involves some manual work. In this work, we propose to use multiple templates which is more robust and requires no development data for parameter optimization. By using its multiple hypothesis sifting capabilities -- from well-defined, full context to loosely defined context like wild card, the confidence for a focus unit can be measured at different expected accuracy. The joint verification by multiple TCP improves measured confidence of each unit in the transcription and is robust across different speech databases. Experimental results show that the checking process automatically separates erroneous sentences from correct ones: the sentence error hit rate decrease rapidly in the sorted TCP values, from 59% to 7% for the Mexican Spanish database and from 63% to 11% for the American English database, among the top 10% sentences in the rank lists. Index Terms: template constrained posterior, database checking"
36056,21258,9804,Multimodal HMM-Based NAM-to-Speech Conversion,2009,"Abstract Although the segmental intelligibility of converted speech from silent speech using direct signal-to-signal ma pping proposed by Toda et al. [1] is quite acceptable, listeners have sometimes difficulty in chunking the speech continu um into meaningful words due to incomplete phonetic cues pr ovided by output signals. This paper studies another appro ach consisting in combining HMM-based statistical speec h recognition and synthesis techniques, as well as tr aining on aligned corpora, to convert silent speech to audibl e voice. By introducing phonological constraints, such systems are expected to improve the phonetic consistency of out put signals. Facial movements are used in order to impr ove the performance of both recognition and synthesis proce dures. The results show that including these movements imp roves the recognition rate by 6.2% and a final improvemen t of the spectral distortion by 2.7% is observed. The compar ison between direct signal-to-signal and phonetic-based mappings is finally commented in this paper. Index Terms : audiovisual voice conversion, non-audible murmur, whispered speech, silent speech interface, HMM-based conversion."
2195781,21258,535,Discriminative language model adaptation for Mandarin broadcast speech transcription and translation,2007,"This paper investigates unsupervised test-time adaptation of language models (LM) using discriminative methods for a Mandarin broadcast speech transcription and translation task. A standard approach to adapt interpolated language models to is to optimize the component weights by minimizing the perplexity on supervision data. This is a widely made approximation for language modeling in automatic speech recognition (ASR) systems. For speech translation tasks, it is unclear whether a strong correlation still exists between perplexity and various forms of error cost functions in recognition and translation stages. The proposed minimum Bayes risk (MBR) based approach provides a flexible framework for unsupervised LM adaptation. It generalizes to a variety of forms of recognition and translation error metrics. LM adaptation is performed at the audio document level using either the character error rate (CER), or translation edit rate (TER) as the cost function. An efficient parameter estimation scheme using the extended Baum-Welch (EBW) algorithm is proposed. Experimental results on a state-of-the-art speech recognition and translation system are presented. The MBR adapted language models gave the best recognition and translation performance and reduced the TER score by up to 0.54% absolute."
2755608,21258,9804,Robust and accurate features for detecting and diagnosing autism spectrum disorders.,2013,"In this paper, we report experiments on the Interspeech 2013 Autism Challenge, which comprises of two subtasks ‐ detecting children with ASD and classifying them into four subtypes. We apply our recently developed algorithm to extract speech features that overcomes certain weaknesses of other currently available algorithms [1, 2]. From the input speech signal, we estimate the parameters of a harmonic model of the voiced speech for each frame including the fundamental frequency (f0). From the fundamental frequencies and the reconstructed noise-free signal, we compute other derived features such as Harmonicto-Noise Ratio (HNR), shimmer, and jitter. In previous work, we found that these features detect voiced segments and speech more accurately than other algorithms and that they are useful in rating the severity of a subject’s Parkinson’s disease [3]. Here, we employ these features, along with standard features such as energy, cepstral, and spectral features. With these features, we detect ASD using a regression and identify the sub-type using a classifier. We find that our features improve the performance, measured in terms of unweighted average recall (UAR), of detecting autism spectrum disorder by 2.3% and classifying the disorder into four categories by 2.8% over the baseline results. Index Terms: speech analysis, autism spectrum disorder"
2574920,21258,9804,Filtering and Subspace Selection for Spectral Features in Detecting Speech Under Physical Stress,2014,"This paper investigates approaches to modeling the time evolution of short-time spectral features in paralinguistic s peech type classification, where we focus on detection of speech in fluenced by physical exertion. The time series model consist s of autoregressive processes of multiple time scales and orders and is trained to describe the long-term dynamics of a given target speech class. The model is applied in two ways in improving long-term modeling in the detection task: 1) to perform predictive filtering of the features and 2) to automatically select instantaneous classification subspaces. The spectrum analysis me thod underlying the short-time features is also varied between t he standard discrete Fourier transform and a time-weighted linear predictive method which yields smooth all-pole spectrum envelope models. Configurations of the proposed methods are eval uated in the Physical Load task of the Interspeech 2014 Computational Paralinguistics Challenge and show improvement over the baseline timbral classifier and the challenge baseline. Also the interrelationships among the methods are discussed. Index Terms: computational paralinguistics, physical load, modulation filtering, spectrum analysis"
61361,21258,9804,On Lexicon Creation for Turkish LVCSR,2003,"In this paper, we address the lexicon design problem in Turkish large vocabulary speech recognition. Although we focus only on Turkish, the methods described here are general enough that they can be considered for other agglutinative languages like Finnish, Korean etc. In an agglutinative language, several words can be created from a single root word using a rich collection of morphological rules. So, a virtually infinite size lexicon is required to cover the language if words are used as the basic units. The standard approach to this problem is to discover a number of primitive units so that a large set of words can be created by compounding those units. Two broad classes of methods are available for splitting words into their sub-units; morphology-based and data-driven methods. Although the word splitting significantly reduces the out of vocabulary rate, it shrinks the context and increases acoustic confusibility. We have used two methods to address the latter. In one method, we use word counts to avoid splitting of high frequency lexical units, and in the other method, we recompound splits according to a probabilistic measure. We present experimental results that show the methods are very effective to lower the word error rate at the expense of lexicon size."
2790625,21258,9804,Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR,2015,"In this paper we continue to investigate how the deep neural network (DNN) based acoustic models for automatic speech recognition can be trained without hand-crafted feature extraction. Previously, we have shown that a simple fully connected feedforward DNN performs surprisingly well when trained directly on the raw time signal. The analysis of the weights revealed that the DNN has learned a kind of short-time time-frequency decomposition of the speech signal. In conventional feature extraction pipelines this is done manually by means of a filter bank that is shared between the neighboring analysis windows. Following this idea, we show that the performance gap between DNNs trained on spliced hand-crafted features and DNNs trained on raw time signal can be strongly reduced by introducing 1D-convolutional layers. Thus, the DNN is forced to learn a short-time filter bank shared over a longer time span. This also allows us to interpret the weights of the second convolutional layer in the same way as 2D patches learned on critical band energies by typical convolutional neural networks. The evaluation is performed on an English LVCSR task. Trained on the raw time signal, the convolutional layers allow to reduce the WER on the test set from 25.5% to 23.4%, compared to an MFCC based result of 22.1% using fully connected layers. Index Terms: acoustic modeling, raw time signal, convolutional neural networks"
2677099,21258,9804,Recurrent Neural Network Based Language Model Personalization by Social Network Crowdsourcing,2013,"Speech recognition has become an important feature in smartphones in recent years. Different from traditional automatic speech recognition, the speech recognition on smartphones can take advantage of personalized language models to model the linguistic patterns and wording habits of a particular smartphone owner better. Owing to the popularity of social networks in recent years, personal texts and messages are no longer inaccessible. However, data sparseness is still an unsolved problem. In this paper, we propose a three-step adaptation approach to personalize recurrent neural network language models (RNNLMs). We believe that its capability to model word histories as distributed representations of arbitrary length can help mitigate the data sparseness problem. Furthermore, we also propose additional user-oriented features to empower the RNNLMs with stronger capabilities for personalization. The experiments on a Facebook dataset showed that the proposed method not only drastically reduced the model perplexity in preliminary experiments, but also moderately reduced the word error rate in n-best rescoring tests. Index Terms: Recurrent Neural Network, Personalized Language Modeling, Social Network, LM adaptation"
93728,21258,9804,Automatic segmentation of film dialogues into phonemes and graphemes,2003,"In film post-production, efficient methods for re-recording a dialogue or dubbing in a new language require a precisely time-aligned text, with individual letters time-coded to video frame resolution. Currently, this time alignment is performed by experts in a painstaking and slow process. To automate this process, we used CRIM’s largevocabulary HMM speech recognizer as a phoneme segmenter and measured its accuracy on typical film extracts in French and English. Our results reveal several characteristics of film dialogues, in addition to noise, that affect segmentation accuracy, such as speaking style or reverberant recordings. Despite these difficulties, an HMM-based segmenter trained on clean speech can still provide more than 89% acceptable phoneme boundaries on typical film extracts. We also propose a method which provides the correspondence between aligned phonemes and graphemes of the text. The method does not use explicit rules, but rather computes an optimal string alignment according to an edit-distance metric. Together, HMM phoneme segmentation and phonemegrapheme correspondence meet the needs of film postproduction for a time-aligned text, and make it possible to automate a large part of the current post-synch process."
2824901,21258,9804,Use of Generalised Nonlinearity in Vector Taylor Series Noise Compensation for Robust Speech Recognition.,2016,"Designing good normalisation to counter the effect of environmental#R##N#distortions is one of the major challenges for automatic speech#R##N#recognition (ASR). The Vector Taylor series (VTS) method is a powerful#R##N#and mathematically well principled technique that can be applied#R##N#to both the feature and model domains to compensate for both#R##N#additive and convolutional noises. One of the limitations of this#R##N#approach, however, is that it is tied to MFCC (and log-filterbank)#R##N#features and does not extend to other representations such as PLP,#R##N#PNCC and phase-based front-ends that use power transformation#R##N#rather than log compression. This paper aims at broadening the#R##N#scope of the VTS method by deriving a new formulation that assumes#R##N#a power transformation is used as the non-linearity during#R##N#feature extraction. It is shown that the conventional VTS, in the log#R##N#domain, is a special case of the new extended framework. In addition,#R##N#the new formulation introduces one more degree of freedom#R##N#which makes it possible to tune the algorithm to better fit the data#R##N#to the statistical requirements of the ASR back-end. Compared with#R##N#MFCC and conventional VTS, the proposed approach provides upto#R##N#12.2% and 2.0% absolute performance improvements on average, in#R##N#Aurora-4 tasks, respectively"
137668,21258,9804,Using Neutral Speech Models for Emotional Speech Analysis,2007,"Abstract Since emotional speech can be regarded as a variation onneutral (non-emotional) speech, it is expected that a robust neu-tral speech model can be useful in contrasting different emo-tions expressed in speech. This study explores this idea by cre-ating acoustic models trained with spectral features, using theemotionally-neutral TIMIT corpus. The performance is testedwith two emotional speech databases: one recorded with a mi-crophone (acted), and another recorded from a telephone ap-plication (spontaneous). It is found that accuracy up to 78%and 65% can be achieved in the binary and category emotiondiscriminations, respectively. Raw Mel Filter Bank (MFB) out-put was found to perform better than conventional MFCC, withboth broad-band and telephone-band speech. These results sug-gest that well-trained neutral acoustic models can be effectivelyused as a front-end for emotion recognition, and once trainedwith MFB, it may reasonably work well regardless of the chan-nel characteristics.Index Terms: Emotion recognition, Neutral speech, HMMs,Mel ﬁlter bank (MFB), TIMIT"
2075829,21258,9804,Classical and Novel Discriminant Features for Affect Recognition from Speech,2005,"This paper investigates the performance and relevance of a set of acoustic features for the task of automatic recognition of affect from speech using machine learning techniques. Eighty seven novel and classical features related to loudness, intonation, and voice quality, are examined. Using feature selection, the results yield a performance level of 49.4% recognition rate (compared to a human performance rate of 60.4% and a chance level of 20%), while the relevance results show that the more exploratory and novel subset of these features outrank the more classical features in the recognition task. In the active research area of recognition of affect from speech it is of particular interest to obtain acoustic features that provide results closer to those of human recognition abilities. While many now “classic” features have been proposed in the literature, their performance has still fallen short of human recognition, suggesting the need to continue a search for novel features and methods. This paper briefly highlights results from an extensive investigation developing new features, and comparing them side-by-side with classical ones using machine learning techniques. (See [1] for many details omitted in this paper.) Algorithms and features associated with modeling loudness, intonation, and voice quality are highlighted in § 2, 3 and 4 respectively, and results of the experiments in §5 with some concluding remarks in § 6."
1282712,21258,9099,"Sheldon speaking, Bonjour!: Leveraging Multilingual Tracks for (Weakly) Supervised Speaker Identification",2014,"We address the problem of speaker identification in multimedia data, and TV series in particular. While speaker identification is traditionally a supervised machine-learning task, our first contribution is to significantly reduce the need for costly preliminary manual annotations through the use of automatically aligned (and potentially noisy) fan-generated transcripts and subtitles.   We show that both speech activity detection and speech turn identification modules trained in this weakly supervised manner achieve similar performance as their fully supervised counterparts (i.e. relying on fine manual speech/non-speech/speaker annotation).   Our second contribution relates to the use of multilingual audio tracks usually available with this kind of content to significantly improve the overall speaker identification performance. Reproducible experiments (including dataset, manual annotations and source code) performed on the first six episodes of The Big Bang Theory TV series show that combining the French audio track (containing dubbed actor voices) with the English one (with the original actor voices) improves the overall English speaker identification performance by 5% absolute and up to 70% relative on the five main characters."
2768082,21258,9804,Subspace Gaussian Mixture Models for Dialogues Classification,2014,"The main objective of this paper is to identify themes from dialogues of telephone conversations in a real-life customer care service. In order to capture significant semantic content in spite of high expression variability, features are extracted in a large number of hidden spaces constructed with a Latent Dirichlet Allocation (LDA) approach. Multiple views of a spoke document can then be represented with several hidden topic models. Nonetheless, the model diversity due to the multi-model approach introduces a new type of variability. An approach is proposed based on features extracted in a common homogenous subspace with the purpose of reducing the multi-span representation variability. A Gaussian Mixture Model subspace model, inspired by previous work on speaker identification, is proposed for theme identification. This representation, novel for theme classification, is compared with the direct application of multiple topic-model representations. Experiments are reported using a corpus collected in the call center of the Paris Transportation Service. Results show the effectiveness of the proposed representation paradigm with a theme identification accuracy of 78.8%, showing a significant improvement with respect to previous results on the same corpus."
2752293,21258,9804,Pitch pattern variations in three regional varieties of American English,2013,"This acoustic study explored dialect effects on realization of nuclear pitch accents in three regional varieties of American English spoken in central Ohio, southeastern Wisconsin and western North Carolina. Fundamental frequency (f0) change from vowel onset to offset in the most prominent syllable in a sentence was examined along four parameters: maximum f0 change, relative location of f0 maximum, f0 offset and f0 fall from maximum to offset. A robust finding was that the f0 contours in the Southern (North Carolina) variants were significantly distinct from the two Midwestern varieties whose contours did not differ significantly from one another. The Southern vowels had an earlier f0 rise, a greater f0 fall and a lower f0 offset than either Ohio or Wisconsin vowels. There was a sharper f0 drop preceding a voiceless than a voiced syllable coda. No significant dialect-related differences were found for flat f0 contours in unstressed vowels, which were also examined in the study. This study contributes the finding that dynamic variations in pitch are greater for vowels which also exhibit a greater amount of spectral dynamics. The interaction of these two sets of cues contributes to the melodic component associated with a specific regional accent."
301491,21258,9804,The INTERSPEECH 2009 Emotion Challenge,2009,"The last decade has seen a substantial body of literature on the recognition of emotion from speech. However, in comparison to related speech processing tasks such as Automatic Speech and Speaker Recognition, practically no standardised corpora and test-conditions exist to compare performances under exactly the same conditions. Instead a multiplicity of evaluation strategies employed – such as cross-validation or percentage splits without proper instance definition – prevents exact reproducibility. Further, in order to face more realistic scenarios, the community is in desperate need of more spontaneous and less prototypical data. This INTERSPEECH 2009 Emotion Challenge aims at bridging such gaps between excellent research on human emotion recognition from speech and low compatibility of results. The FAU Aibo Emotion Corpus [1] serves as basis with clearly defined test and training partitions incorporating speaker independence and different room acoustics as needed in most reallife settings. This paper introduces the challenge, the corpus, the features, and benchmark results of two popular approaches towards emotion recognition from speech. Index Terms: emotion, challenge, feature types, classification"
219650,21258,9804,VAD-measure-embedded Decoder with Online Model Adaptation,2010,"We previously proposed a decoding method for automatic speech recognition utilizing hypothesis scores weighted by voice activity detection (VAD)-measures. This method uses two Gaussian mixture models (GMMs) to obtain confidence measures: one for speech, the other for non-speech. To achieve good search performance, we need to adapt the GMMs properly for input utterances and environmental noise. We describe a new unsupervised on-line GMM adaptation method based on MAP estimation. The robustness of our method is further improved by weighting updating parameters of GMMs according to the confidence measure for the adaptation data. We also describe an approach to accelerate the adaptation by caching statistical values to adapt GMMs. Experimental results on Drivers’ Japanese Speech Corpus in a Car Environment (DJSC) show that the adaptation with decoding method significantly improves the word accuracy from 54.8% to 59.6%. Moreover, the weighting method improves the robustness of the unsupervised adaptation, and the cache method greatly accelerates the decoding process. Consequently, our adaptive decoding method significantly improves the word accuracy in a noisy environment with only a minor increase in the computational cost. Index Terms: speech recognition, voice activity detection, Gaussian mixture model adaptation"
2632316,21258,256,Learning Representations of Affect from Speech,2015,"There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes i.e. categorical and dimensional affective traits from speech. We show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative of activation intensity and at separating out negative valence (sadness and anger) from positive valence (happiness). We experiment with different input speech features (such as FFT and log-mel spectrograms with temporal context windows), and different autoencoder architectures (such as stacked and deep autoencoders). We also learn utterance specific representations by a combination of denoising autoencoders and BLSTM based recurrent autoencoders. Emotion classification is performed with the learnt temporal/dynamic representations to evaluate the quality of the representations. Experiments on a well-established real-life speech dataset (IEMOCAP) show that the learnt representations are comparable to state of the art feature extractors (such as voice quality features and MFCCs) and are competitive with state-of-the-art approaches at emotion and dimensional affect recognition."
109181,21258,9804,Modeling Vowels for Arabic BN Transcription,2005,"This paper describes the LIMSI Arabic Broadcast News system which produces a vowelized word transcription. The under 10x system, evaluated in the NIST RT-04F evaluation, uses a 3 pass decoding strategy with gender- and bandwidth-specific acoustic models, a vowelized 65k word class pronunciation lexicon and a word-class 4-gram language model. In order to explicitly represent the vowelized word forms, each nonvowelized word entry is considered as a word class regrouping all of its associated vowelized forms. Since Arabic texts are almost exclusively written without vowels, an important challenge is to be able to use these efficiently in a system producing a vowelized output. Since a portion of the acoustic training data was manually transcribed with short vowels, enabling an initial set of acoustic models to be estimated in a supervised manner. The remaining audio data, for which vowels are not annotated, were trained in an implicit manner using the recognizer to choose the preferred form. The system was trained on a total of about 150 hours of audio data and almost 600 million words of Arabic texts, and achieved word error rates of 16.0% and 18.5% on the dev04 and eval04 data, respectively."
271679,21258,9804,A quantitative comparison of glottal closure instant estimation algorithms on a large variety of singing sounds.,2013,"Glottal closure instant (GCI) estimation is a well-studied topic that plays a critical role in several speech processing applications. Many GCI estimation algorithms have been proposed in the literature and shown to provide excellent results on the speech signal. Nonetheless the efficiency of these algorithms for the analysis of the singing voice is still unknown. The goal of this paper is to assess the performance of existing GCI estimation methods on the singing voice with a quantitative comparison. A second goal is to provide a starting point for the adaptation of these algorithms to the singing voice by identifying weaknesses and strengths under different conditions. This study is carried out on a large database of singing sounds with synchronous electroglottography (EGG) recordings, containing a variety of singer categories and singing techniques. The evaluated algorithms are Dynamic Programming Phase Slope Algorithm (DYPSA), Hilbert Envelope-based detection (HE), Speech Event Detection using the Residual Excitation And a Mean-based Signal (SEDREAMS), Yet Another GCI Algorithm (YAGA) and Zero Frequency Resonator-based method (ZFR). The algorithms are evaluated in terms of both reliability and accuracy, over different singing categories, laryngeal mechanisms, and voice qualities. Additionally, the robustness of the algorithms to reverberation is analyzed."
1069685,21258,535,Context-dependent modelling of deep neural network using logistic regression,2013,"The data sparsity problem of context-dependent acoustic modelling in automatic speech recognition is addressed by using the decision tree state clusters as the training targets in the standard context-dependent (CD) deep neural network (DNN) systems. As a result, the CD states within a cluster cannot be distinguished during decoding. This problem, referred to as the clustering problem, is not explicitly addressed in the current literature. In this paper, we formulate the CD DNN as an instance of the canonical state modelling technique based on a set of broad phone classes to address both the data sparsity and the clustering problems. The triphone is clustered into multiple sets of shorter biphones using broad phone contexts to address the data sparsity issue. A DNN is trained to discriminate the biphones within each set. The canonical states are represented by the concatenated log posteriors of all the broad phone DNNs. Logistic regression is used to transform the canonical states into the triphone state output probability. Clustering of the regression parameters is used to reduce model complexity while still achieving unique acoustic scores for all possible triphones. The experimental results on a broadcast news transcription task reveal that the proposed regression-based CD DNN significantly outperforms the standard CD DNN. The best system provides a 2.7% absolute WER reduction compared to the best standard CD DNN system."
2725009,21258,9804,Factor Analysis based Semantic Variability Compensation for Automatic Conversation Representation,2014,"The main objective of this paper is to identify themes from dialogues of telephone conversations in a real-life customer care service. In this task, the word semantic variability contained in these conversations may impact the classification performance by retaining the noise in their vectorial representation. In this article, we propose an original method to compensate this semantic variability using the Factor Analysis (FA) paradigm, initially designed for speech processing tasks to compensate the acoustic variability, mainly in Speaker Verification (SV) and Automatic Speech Recognition (ASR). In our proposal, we used the FA paradigm to estimate the semantic variability as an additive component located in a subspace of low dimension (with respect to the super-vector space). This additive semantic variability is estimated in Factor Analysis model space. From this estimation, a specific vector transformation is obtained and is applied to vectors of dialogue representation. Experiments are reported using a corpus collected in the call center of the Paris Transportation Service. Results show the effectiveness of the proposed representation paradigm with a theme identification accuracy of 80.0%, showing a significant improvement with respect to previous results on the same corpus. Index Terms: Human/Human conversation representation, Semantic variability, Factor analysis, Variability compensation, Automatic classification, Latent Dirichlet Allocation."
73874,21258,9804,Spoken Language and e-inclusion.,2003,"Speech technology can help people with disabilities. Blind and non-speaking people were amongst the first to be provided with commercially available speech synthesis systems, and, to this day, represent a much higher percentage of users of this technology than their numbers would predict. Speech synthesis technology has, for example, transformed the lives of many blind people, but the success of speech output to allow blind people to word processes, browse the web, and use domestic appliances should not to lull us into a false sense of security. In the main, these users were young, aware of their limitations, and of the substantial potential impact of such technology on their life styles, and were generally highly motivated to make a success of their use of the technology. The speech community needs to be aware of the major differences between the young disabled people who have found speech technology so useful, and the other groups of people who are excluded from “e-society”. An example is older people. These have a much greater range of characteristics than younger people and these characteristics change more rapidly with time. Very importantly for speech technologists, most older people possess multiple minor disabilities, which can seriously interact, particularly in the context of a human machine communication. In addition, a relatively high proportion of older people also have a major disability."
2104613,21258,9804,Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks,2013,"In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic speech recognition (ASR) system, the phoneme class conditional probabilities are estimated by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then modeling the acoustic features with an ANN. Recent advances in machine learning techniques, more specifically in the field of image processing and text processing, have shown that such divide and conquer strategy (i.e., separating feature extraction and modeling steps) may not be necessary. Motivated from these studies, in the framework of convolutional neural networks (CNNs), this paper investigates a novel approach, where the input to the ANN is raw speech signal and the output is phoneme class conditional probability estimates. On TIMIT phoneme recognition task, we study different ANN architectures to show the benefit of CNNs and compare the proposed approach against conventional approach where, spectral-based feature MFCC is extracted and modeled by a multilayer perceptron. Our studies show that the proposed approach can yield comparable or better phoneme recognition performance when compared to the conventional approach. It indicates that CNNs can learn features relevant for phoneme classification automatically from the raw speech signal."
2718791,21258,20358,Smartphone App Categorization for Interest Targeting in Advertising Marketplace,2016,"Last decade has witnessed a tremendous expansion of mobile devices, which brought an unprecedented opportunity to reach a large number of mobile users at any point in time. This resulted in a surge of interest of mobile operators and ad publishers to understand usage patterns of mobile apps and allow more relevant content recommendations. Due to a large input space, a critical step in understanding app usage patterns is reducing sparseness by classifying apps into predefined interest taxonomies. However, besides short name and noisy description majority of apps have very limited information available, which makes classification a challenging task. We address this issue and present a novel method to classify apps into interest categories by: 1) embedding apps into low-dimensional space using a neural language model applied on smartphone logs; and 2) applying k-nearest-neighbors classification in the embedding space. To validate the method we run experiments on more than one billion device logs covering hundreds of thousands of apps. To the best of our knowledge this is the first app categorization study at this scale. Empirical results show that the proposed method outperforms the current state-of-the-art."
232354,21258,9804,A New Kernel for SVM MLLR based Speaker Recognition,2007,"Speaker recognition using support vector machines (SVMs) with features derived from generative models has been shown to perform well. Typically, a universal background model (UBM) is adapted to each utterance yielding a set of features that are used in an SVM. We consider the case where the UBM is a Gaussian mixture model (GMM), and maximum likelihood linear regression (MLLR) adaptation is used to adapt the means of the UBM. We examine two possible SVM feature expansions that arise in this context: the first, a GMM supervector is constructed by stacking the means of the adapted GMM, and the second consists of the elements of the MLLR transform. We examine several kernels associated with these expansions. We show that both expansions are equivalent given an appropriate choice of kernels. Experiments performed on the NIST SRE 2006 corpus clearly highlight that our choice of kernels, which are motivated by distance metrics between GMMs, outperform ad-hoc ones. We also apply SVM nuisance attribute projection (NAP) to the kernels as a form of channel compensation and show that, with a proper choice of kernel, we achieve results comparable to existing SVM based recognizers. Index Terms: speaker recognition, MLLR, SVM, supervector"
2630593,21258,9804,Looking for lexical feedback effects in /tl/→/kl/ repairs,2013,"French (or English) native listeners hear /kl/ when presented with the illegal consonant sequence */tl/. This robust case of perceptual repair is usually viewed as operating at a prelexical level of speech processing but the evidence against lexical feedback is somewhat weak. In this study, we report new data supporting the prelexical hypothesis, obtained with a paradigm that avoids most of the possible confounds in previous studies. In a cross-modal auditory–visual priming paradigm, lexical decisions to the same visual target “clavier” are facilitated by the auditory prime *tlavier, not by *dlavier. Likewise, the recognition of “glacier” is facilitated by *dlacier, not by *tlacier. To summarize, velar stop + /l/ words are exclusively facilitated by the dental-initial derived forms with the same voicing. Derived forms with the opposite voicing tend to induce inhibition rather than facilitation. Hence, the observed facilitation effects are not graded from */tl/ to */dl/ or vice versa. We argue that these rather surprising all-or-none priming effects exclude the possibility that the */tl/→/kl/ and */dl/→/gl/ repairs are due, even partly, to lexical feedback. Index Terms: phonotactic, perceptual repair, lexical feedback"
2672526,21258,535,Speaker adaptive joint training of Gaussian mixture models and bottleneck features,2015,"In the tandem approach, the output of a neural network (NN) serves as input features to a Gaussian mixture model (GMM) aiming to improve the emission probability estimates. As has been shown in our previous work, GMM with pooled covariance matrix can be integrated into a neural network framework as a softmax layer with hidden variables, which allows for joint estimation of both neural network and Gaussian mixture parameters. Here, this approach is extended to include speaker adaptive training (SAT) by introducing a speaker dependent neural network layer. Error backpropagation beyond this speaker dependent layer realizes the adaptive training of the Gaussian parameters as well as the optimization of the bottleneck (BN) tandem features of the underlying acoustic model, simultaneously. In this study, after the initialization by constrained maximum likelihood linear regression (CMLLR) the speaker dependent layer itself is kept constant during the joint training. Experiments show that the deeper backpropagation through the speaker dependent layer is necessary for improved recognition performance. The speaker adaptively and jointly trained BN-GMM results in 5% relative improvement over very strong speaker-independent hybrid baseline on the Quaero English broadcast news and conversations task, and on the 300-hour Switchboard task."
2597825,21258,9804,Ranking severity of speech errors by their phonological impact in context,2014,"Children with speech disorders often present with systematic speech error patterns. In clinical assessments of speech disorders, evaluating the severity of the disorder is central. Current measures of severity have limited sensitivity to factors like the frequency of the target sounds in the child’s language and the degree of phonological diversity, which are factors that can be assumed to affect intelligibility. By constructing phonological filters to simulate eight speech error patterns often observed in children, and applying these filters to a phonologically transcribed corpus of 350K words, this study explores three quantitative measures of phonological impact: Percentage of Consonants Correct (PCC), edit distance, and degree of homonymy. These metrics were related to estimated ratings of severity collected from 34 practicing clinicians. The results show an expected high correlation between the PCC and edit distance metrics, but that none of the three metrics align with clinicians’ ratings. Although these results do not generate definite answers to what phonological factors contribute the most to (un)intelligibility, this study demonstrates a methodology that allows for large-scale investigations of the interplay between phonological errors and their impact on speech in context, within and across languages."
210871,21258,9804,Tone Recognition of Continuous Speech of Standard Chinese Using Neural Network and Tone Nucleus Model,2006,"A method is developed for recognizing lexical tone types of Standard Chinese syllables in continuous speech. Neural network (four-layered perceptron) is adopted as classifier. The method includes two steps; first recognizing tone types using prosodic features of voiced part, and then re-recognizing by viewing only on tone nucleus, which is a portion of the syllable showing rather stable fundamental frequency (F0) contour regardless of tone types of the preceding and following syllables. The voiced part (or tone nucleus) is divided into 20 segments, and F0, delta-F0, F0 slope and short-term energy of each segment are served as inputs to the neural network. In order to cope with tone coarticulation, prosodic feature parameters for the last 5 segments of the preceding syllable and the initial 5 segments of the following syllable are included in the neural network inputs. Information on syllable length is also added to the inputs. Tone recognition experiment was conducted for a female speaker's utterances included in HKU96 corpus. The average recognition rate was 86.5 % including neutral tone syllables, when the tone nucleus model was not used. It increased to 86.9 %, when the model was used. The obtained rate is higher by more than 3 points as compared to that obtained by the hidden-Markov-model-based tone recognizer developed by the authors formerly. Index Terms: tone recognition, tone nucleus model, neural network, Standard Chinese"
1971213,21258,23735,Auditory fovea based speech separation and its application to dialog system,2002,"This paper presents an active direction-pass filter (ADPF) that separates sounds originating from the specified direction by using a pair of microphones. Its application to front-end processing for speech recognition is also reported. Since the performance of sound source separation by the ADPF depends on the accuracy of sound source localization (direction), various localization modules including the interaural phase difference, interaural intensity difference for each sub-band, and other visual and auditory processing are integrated hierarchically. The resulting performance of auditory localization varies according to the relative position of the sound source. The resolution of the center of the robot is much higher than that of peripherals, indicating similar property of visual fovea. To make the best use of this property, the ADPF controls the direction of a head by motor movement. In order to recognize sound streams separated by the ADPF, a hidden Markov model based automatic speech recognition is built with multiple acoustic models trained by the output of the ADPF under different conditions. A preliminary dialog system is thus implemented on an upper-torso humanoid. The experimental results prove that it works well even when two speakers speak simultaneously."
2711867,21258,535,The DIRHA-ENGLISH corpus and related tasks for distant-speech recognition in domestic environments,2015,"This paper introduces the contents and the possible usage of the DIRHA-ENGLISH multi-microphone corpus, recently realized under the EC DIRHA project. The reference scenario is a domestic environment equipped with a large number of microphones and microphone arrays distributed in space. The corpus is composed of both real and simulated material, and it includes 12 US and 12 UK English native speakers. Each speaker uttered different sets of phonetically-rich sentences, newspaper articles, conversational speech, keywords, and commands. From this material, a large set of 1-minute sequences was generated, which also includes typical domestic background noise as well as inter/intra-room reverberation effects. Dev and test sets were derived, which represent a very precious material for different studies on multi-microphone speech processing and distant-speech recognition. Various tasks and corresponding Kaldi recipes have already been developed. The paper reports a first set of baseline results obtained using different techniques, including Deep Neural Networks (DNN), aligned with the state-of-the-art at international level."
2658277,21258,9804,Advanced Speech Communication System for Deaf People,2010,"This paper describes the development of an Advanced Speech Communication System for Deaf People and its field evaluation in a real application domain: the renewal of Driver’s License. The system is composed of two modules. The first one is a Spanish into Spanish Sign Language (LSE: Lengua de Signos Espanola) translation module made up of a speech recognizer, a natural language translator (for converting a word sequence into a sequence of signs), and a 3D avatar animation module (for playing back the signs). The second module is a Spoken Spanish generator from sign. writing composed of a visual interface (for specifying a sequence of signs), a language translator (for generating the sequence of words in Spanish), and finally, a text to speech converter. For language translation, the system integrates three technologies: an example.based strategy, a rule.bas ed translation method and a statistical translator. This paper also includes a detailed description of the evaluation carried out in the Local Traffic Office in the city of Toledo (Spain) involving real government employees and deaf people. This evaluation includes objective measurements from the system and subjective information from questionnaires. Index Terms: Deaf people, Spanish Sign Language (LSE), Spoken Language Translation, Driver’s License renewal, e. Inclusion."
1816363,21258,9804,Quantitative Analysis and Synthesis of Syllabic Tones in Vietnamese,2003,"The current paper presents a preliminary study on the production and perception of syllabic tones of Vietnamese. A speech corpus consisting of fifty-two six-syllable sequences with various combinations of tones was uttered by two speakers of Standard Vietnamese, one male and one female. The corpus was labeled on the syllabic level and analyzed using the Fujisaki model. Results show that the six tone types basically fall into two categories: Level, rising, curve and falling tone can be accurately modeled by using tone commands of positive or negative polarity. The so-called drop and broken tones, however, obviously require a special control causing creaky voice and in cases a very fast drop in F0 leading to temporary F0 halving or even quartering. In contrast to the drop tone, the broken tone exhibits an F0 rise and hence a positive tone command right after the creak occurs. Further observations suggest that drop and broken tone do not only differ from the other four tones with respect to their F0 characteristics, but also as to their much tenser articulation. A perception experiment performed with natural and resynthesized stimuli shows, inter alia, that tone 4 is most prone to confusion and that tone 6 obviously requires tense articulation as well as vocal fry to be identified reliably."
2872920,21258,9804,Speakers In The Wild (SITW): The QUT Speaker Recognition System,2016,"This paper presents the QUT speaker recognition system, as a competing system in the Speakers In The Wild (SITW) speaker recognition challenge. Our proposed system achieved an overall ranking of second place, in the main core-core condition evaluations of the SITW challenge. This system uses an ivector/ PLDA approach, with domain adaptation and a deep neural network (DNN) trained to provide feature statistics. The statistics are accumulated by using class posteriors from the DNN, in place of GMM component posteriors in a typical GMM UBM i-vector/PLDA system. Once the statistics have been collected, the i-vector computation is carried out as in a GMM-UBM based system. We apply domain adaptation to the extracted i-vectors to ensure robustness against dataset variability, PLDA modelling is used to capture speaker and session variability in the i-vector space, and the processed i-vectors are compared using the batch likelihood ratio. The final scores are calibrated to obtain the calibrated likelihood scores, which are then used to carry out speaker recognition and evaluate the performance of the system. Finally, we explore the practical application of our system to the core-multi condition recordings of the SITW data and propose a technique for speaker recognition in recordings with multiple speakers."
2400737,21258,535,Towards utterance-based neural network adaptation in acoustic modeling,2015,"Despite the superior classification ability of deep neural networks (DNN), the performance of DNN suffers when there is a mismatch between training and testing conditions. Many speaker adaptation techniques have been proposed for DNN acoustic modeling but in case of environmental robustness the progress is still limited. It is also possible to use techniques developed for adapting speakers to handle the impact of environments at the same time, or to combine both approaches. Directly adapting the large number of DNN parameters is challenging when the adaptation set is small. The learning hidden unit contributions (LHUC) technique for unsupervised speaker adaptation of DNN introduces speaker dependent parameters to the existing speaker independent network to increase the automatic speech recognition (ASR) performance of the target speaker using small amounts of adaptation data. This paper investigates the LHUC to adapt the speech recognizer to target speakers and environments where the impacts of speakers and noise differences are quantified separately. Our finding shows that the LHUC is capable of adapting to both speaker and noise conditions at the same time. Compared to the speaker independent model, about 9% to 13% relative word error rate (WER) improvement are observed for all test conditions using AMI meeting corpus."
201348,21258,9804,On invariant structural representation for speech recognition: theoretical validation and experimental improvement,2009,"One of the most challenging problems in speech recognition is to deal with inevitable acoustic variations caused by nonlinguistic factors. Recently, an invariant structural representation of speech was proposed [1], where the non-linguistic variations are effectively removed though modeling the dynamic and contrastive aspects of speech signals. This paper describes our recent progresses on this problem. Theoretically, we prove that the maximum likelihood based decomposition can lead to the same structural representations for a sequence and its transformed version. Practically, we introduce a method of discriminant analysis of eigen-structure to deal with two limitations of structural representations, namely, high dimensionality and too strong invariance. In the 1st experiment, we evaluate the proposed method through recognizing connected Japanese vowels. The proposed method achieves a recognition rate 99.0%, which is higher than those of the previous structure based recognition methods [2, 3, 4] and word HMM. In the 2nd experiment, we examine the recognition performance of structural representations to vocal tract length (VTL) differences. The experimental results indicate that structural representations have much more robustness to VTL changes than HMM. w Moreover, the proposed method is about 60 times faster than the previous ones. Index Terms: Speech recognition, invariant structure, PCA, discriminative analysis"
2835527,21258,9804,Direct expressive voice training based on semantic selection,2016,"This work aims at creating expressive voices from audiobooks using semantic selection. First, for each#R##N#utterance of the audiobook an acoustic feature vector is extracted, including iVectors built on MFCC and on F0 basis.#R##N#Then, the transcription is projected into a semantic vector space. A seed utterance is projected to the semantic vector space and the N nearest neighbors are selected. The selection is then filtered by selecting only acoustically similar#R##N#data. The proposed technique can be used to train emotional voices by using emotional keywords or phrases as#R##N#seeds, obtaining training data semantically similar to the seed. It can also be used to read larger texts in an expressive#R##N#manner, creating specific voices for each sentence. That later application is compared to a DNN predictor, which#R##N#predicts acoustic features from semantic features. The selected data is used to adapt statistical speech synthesis#R##N#models. The performance of the technique is analyzed objectively and in a perceptive experiment. In the first part of#R##N#the experiment, subjects clearly show preference for particular expressive voices to synthesize semantically expressive#R##N#utterances. In the second part, the proposed method is shown to achieve similar or better performance than the DNN#R##N#based prediction. Copyright © 2016 ISCA."
499879,21258,9804,On the Evaluation of Inversion Mapping Performance in the Acoustic Domain,2013,"The two measures typically used to assess the performance of an inversion mapping method, where the aim is to estimate what articulator movements gave rise to a given acoustic signal, are root mean squared (RMS) error and correlation. In this paper, we investigate whether “task-based” evaluation using an articulatory-controllable HMM-based speech synthesis system can give useful additional information to complement these measures. To assess the usefulness of this evaluation approach, we use articulator trajectories estimated by a range of different inversion mapping methods as input to the synthesiser, and measure their performance in the acoustic domain in terms of RMS error of the generated acoustic parameters and with a listening test involving 30 participants. We then compare these results with the standard RMS error and correlation measures calculated in the articulatory domain. Interestingly, in the acoustic evaluation we observe one method performs with no statistically significant difference from measured articulatory data, and cases where statistically significant differences between methods exist which are not reflected in the results of the two standard measures. From our results, we conclude such task-based evaluation can indeed provide interesting extra information, and gives a useful way to compare inversion methods. Index Terms: Inversion mapping, evaluation, HMM synthesis"
2711394,21258,9804,Robust Voice Activity Detector for Real World Applications Using Harmonicity and Modulation frequency,2011,"The task of robustly detecting distant speech in low SNR environments for automatic speech recognition is examined using a two-stage approach based on two distinguishing features of speech, namely harmonicity and modulation frequency (MF). A modified metric for harmonicity is used as a gating function to a set of parallel classifiers that incorporate MFs computed on different frequency bands. Performance is evaluated on both the frame-level discriminative power and also the system level ASR results on a real-world robotic forklift task. Compared to other previously proposed features such as relative spectral entropy, and classification strategies involving MFs, the combined approach shows good generalization across different kinds of dynamic noise conditions, and obtains a significant improvement on the false alarm rate at low speech miss rate settings. The overall ASR results also improved significantly compared to the ESTI AMR-VAD2, while reducing the number of false alarms by a factor of two. Index Terms: voice activity detection, modulation frequency, harmonicity, human-robot interaction."
3178089,21258,9616,An effective voiceprint based identity authentication system for Mandarin smartphone users,2016,"Voiceprint based identity authentication system (IAS) for smartphone users is highly demanded in mobile internet times. There are some successful application cases for English smartphone users. However, to our knowledge, the research outcomes are few for Mandarin smartphone users. Analysis shows that there remain some issues need to be carefully considered: (1) security issue: vulnerable to replay attacks; (2) user experience issue: zero-tolerance of misreading; (3) channel mismatch issue: perform poorly when user change his smartphone. Taking above issues into account, this study strives to develop an effective voiceprint based IAS (termed as DR-EiSV-IAS) for Mandarin smartphone users. Specifically, a content disorder degree (CDD) module implemented with DNN based digit recognition is introduced to resist replay attacks and enhance the fault-tolerance of misreading. Besides, the speaker verification is carefully designed using enhanced ivector technique where ivector framework is incorporated with WCCN to compensate for channel variability. To facilitate this study, we have built up a Mandarin corpus MTDSR2015, which is the first public and free Mandarin database recorded by smartphones for text-dependent speaker recognition research. Extensive experiments have been conducted on both MTDSR2015 and RSR2015 to validate the effectiveness of our proposed DR-EiSV-IAS."
4788,21258,9804,Maximum-likelihood training of a bipartite acoustic model for speech recognition.,2001,"In a recent paper, we described a compact, context-dependent acoustic model incorporating strong a priori knowledge and designed to support extremely rapid speaker adaptation [9]. The two parts of this “bipartite” model are: 1. A speakerdependent, context-independent (SDCI) part with a small number of parameters called the “eigencentroid”. 2. A speaker-independent, context-dependent (SICD) part with a large number of parameters called the “delta trees”. For the first time, we describe in the current paper the iterative maximum-likelihood procedure employed to train both parts of the model. This paper also gives the first unsupervised adaptation and self-adaptation results for the new model, showing that it outperforms standard techniques when small amounts of adaptation data (10 sec. or less of sp eech) are available. Relative error rate reduction (ERR) is 12.1% for supervised adaptation and 11.2% for unsupervised adaptation on three TIMIT sentences; it is 10.4% for self-adaptation on a single TIMIT sentence. Finally, the paper analyzes the correlation between sex and the SDCI part of the model, and shows how modeling of acoustic variability is affected by the explicit separation into SD and CD components."
2231238,21258,9804,Impact of Audio Segmentation and Segment Clustering on Automated Transcription Accuracy of Large Spoken Archives,2003,"This paper addresses the influence of audio segmentation and segment clustering on automatic transcription accuracy for large spoken archives. The work formspart of the ongoing MALACH project, which is developing advanced techniques for supporting access to the world’s largest digital archive of video oral histories collected in many languages from over 52000 survivors and witnesses of the Holocaust. We present several audio-only and audio-visual segmentation schemes, including two novel schemes: the first is iterative and audio-only, the second uses audio-visual synchrony. Unlike most previous work, we evaluate these schemes in terms of their impact upon recognition accuracy. Results on English interviews show the automatic segmentation schemes give performance comparable to (exhorbitantly expensive and impractically lengthy) manual segmentation when using a single pass decoding strategy based on speaker-independent models. However, when using a multiple pass decoding strategy with adaptation, results are sensitive to both initial audio segmentation and the scheme for clustering segments prior to adaptation: the combination of our best automatic segmentation and clustering scheme has an error rate 8% worse (relative) to manual audio segmentation and clustering due to the occurrence of “speaker-impure” segments."
1814,21258,9804,An Interactive English Pronunciation Dictionary for Korean Learners,2004,"We present research towards developing a pronunciation dictionary that features sensitivity to learners’ native phonology, specifically designed for Korean learners of English-as-a-Foreign-Language (EFL). We envision a future system that can record and process learners’ imitationof thedictionary pronunciation and instantly provide segmental and prosodic feedback on accent. Towards this goal, we have designed and collected a speech corpus to address the phonological and prosodic issues of Korean EFL learners. We leverage the SUMMIT speech recognizer’s ability to model phonological rules to automatically identify non-native phonological phenomena. These phonological rules were carefully constructed to account for the influence of learners’ native language (Korean) on the target language (English). Feedback messages are provided to the learner to point out the non-native phonological variations detected by the speech recognizer in order to help them improve segmental pronunciation. Instructions are also given to the user on the prosodic aspects of the pronunciation, which are based on detected duration and cues. We evaluated the effectiveness of the feedback mechanism by rating 222 English utterances from six native Korean subjects, before and after receiving native-language dependent feedback messages. Human raters judged 61% of the utterances as improved after feedback."
550412,21258,535,EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,2015,"The performance of automatic speech recognition (ASR) has improved tremendously due to the application of deep neural networks (DNNs). Despite this progress, building a new ASR system remains a challenging task, requiring various resources, multiple training stages and significant expertise. This paper presents our Eesen framework which drastically simplifies the existing pipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen involves learning a single recurrent neural network (RNN) predicting context-independent targets (phonemes or characters). To remove the need for pre-generated frame labels, we adopt the connectionist temporal classification (CTC) objective function to infer the alignments between speech and label sequences. A distinctive feature of Eesen is a generalized decoding approach based on weighted finite-state transducers (WFSTs), which enables the efficient incorporation of lexicons and language models into CTC decoding. Experiments show that compared with the standard hybrid DNN systems, Eesen achieves comparable word error rates (WERs), while at the same time speeding up decoding significantly."
90238,21258,9804,Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification,2009,"This paper presents a new speaker verification system architecture based on Joint Factor Analysis (JFA) as feature extractor. In this modeling, the JFA is used to define a new low-dimensional space named the total variability factor space, instead of both channel and speaker variability spaces for the classical JFA. The main contribution in this approach, is the use of the cosine kernel in the new total factor space to design two different systems: the first system is Support Vector Machines based, and the second one uses directly this kernel as a decision score. This last scoring method makes the process faster and less computation complex compared to others classical methods. We tested several intersession compensation methods in total factors, and we found that the combination of Linear Discriminate Analysis and Within Class Covariance Normalization achieved the best performance. We achieved a remarkable results using fast scoring method based only on cosine kernel especially for male trials, we yield an EER of 1.12% and MinDCF of 0.0094 on the English trials of the NIST 2008 SRE dataset. Index Terms: Total variability space, cosine kernel, fast scoring, support vector machines."
2662158,21258,9804,Speech-Based Location Estimation of First Responders in a Simulated Search and Rescue Scenario,2015,"In our research, we explore possible solutions for extracting valuable information about first responders’ (FR) location from speech communication channels during crisis response. Fine-grained identification of fundamental units of meaning (e. g. sentences, named entities and dialogue acts) is sensitive to high error rate in automatic transcriptions of noisy speech. However, looking from a topic-based perspective and utilizing text vectorization techniques such as Latent Dirichlet Allocation (LDA) make this more robust to such errors. In this paper, the location estimation problem is framed as a topic segmentation task on FRs’ spoken reports about their observations and actions. Identifying the changes in the content of a report over time is an indication that the speaker has moved from one particular location to another. This provides an estimation about the location of the speaker. A goal-oriented human/human conversational speech corpus was collected based on an abstract communication model between FR and task leader during a search process in a simulation environment. Results show the effectiveness of a topic-based approach and especially low sensitivity of the LDA-based method to the highly imperfect automatic transcriptions."
2639182,21258,9804,Children's timing and repair strategies for communication in adverse listening conditions,2013,"This study investigated the development of clear speech strategies in children. Groups of 20 talkers aged 9-10 and 13- 14 years were recorded in pairs while they carried out ‘spot the difference’ picture tasks, either while hearing each other normally (‘no barrier’) or when one talker heard the other via a noise vocoder (VOC), which led their interlocutor (‘talker A’) to clarify their speech to maintain effective communication. Data were compared to those previously collected for 20 adults. Mean word duration, number and duration of pauses were calculated for talker A. Strategies used in response to direct requests for clarification were also classified. Children spoke at a slower rate than teens and adults, who did not differ. Relative to ‘no barrier’, all groups significantly reduced their speech rate in VOC and used longer pauses, but the relative change in pause rate across conditions was greater for adults than children or teens. In response to a direct clarification request, children and teens used a higher rate of repetitions than adults who used more varied strategies. These results suggest that although children use some strategies to clarify their speech in difficult conditions, other strategies continue to develop until late adolescence."
223135,21258,9804,Adaptive Multimodal Fusion by Uncertainty Compensation,2006,"While the accuracy of feature measurements heavily depends on changing environmental conditions, studying the consequences of this fact in pattern recognition tasks has received relatively little attention to date. In this work we explicitly take into account feature measurement uncertainty and we show how classification rules should be adjusted to compensate for its effects. Our approach is particularly fruitful in multimodal fusion scenarios, such as audiovisual speech recognition, where multiple streams of complementary time-evolving features are integrated. For such applications, provided that the measurement noise uncertainty for each feature stream can be estimated, the proposed framework leads to highly adaptive multimodal fusion rules which are widely applicable and easy to implement. We further show that previous multimodal fusion methods relying on stream weights fall under our scheme under certain assumptions; this provides novel insights into their applicability for various tasks and suggests new practical ways for estimating the stream weights adaptively. The potential of our approach is demonstrated in audio-visual speech recognition using either synchronous or asynchronous models. Index Terms: multimodal fusion, audiovisual speech recognition, uncertainty compensation, Active Appearance Models, product HMMs, stream weights"
2800358,21258,9804,Automatic Genre and Show Identification of Broadcast Media,2016,"Huge amounts of digital videos are being produced and broadcast#R##N#every day, leading to giant media archives. Effective techniques#R##N#are needed to make such data accessible further. Automatic#R##N#meta-data labelling of broadcast media is an essential task#R##N#for multimedia indexing, where it is standard to use multi-modal#R##N#input for such purposes. This paper describes a novel method#R##N#for automatic detection of media genre and show identities using#R##N#acoustic features, textual features or a combination thereof.#R##N#Furthermore the inclusion of available meta-data, such as time#R##N#of broadcast, is shown to lead to very high performance. Latent#R##N#Dirichlet Allocation is used to model both acoustics and text,#R##N#yielding fixed dimensional representations of media recordings#R##N#that can then be used in Support Vector Machines based classi-#R##N#fication. Experiments are conducted on more than 1200 hours#R##N#of TV broadcasts from the British Broadcasting Corporation#R##N#(BBC), where the task is to categorise the broadcasts into 8 genres#R##N#or 133 show identities. On a 200-hour test set, accuracies of#R##N#98.6% and 85.7% were achieved for genre and show identifi-#R##N#cation respectively, using a combination of acoustic and textual#R##N#features with meta-data."
2867129,21258,9804,Multi-Language Neural Network Language Models.,2016,"Copyright © 2016 ISCA.In recent years there has been considerable interest in neural network based language models. These models typically consist of vocabulary dependent input and output layers and one, or more, hidden layers. A standard problem with these networks is that large quantities of training data are needed to robustly estimate the model parameters. This poses a challenge when only limited data is available for the target language. One way to address this issue is to make use of overlapping vocabularies between related languages. However this is only applicable to a small set of languages, and the impact is expected to be limited for more general applications. This paper describes a general solution that allows data from any language to be used. Here, only the input and output layers are vocabulary dependent whilst hidden layers are shared, language independent. This multi-task training set-up allows the quantity of data available to train the hidden layers to be increased. This multi-language network can be used in a range of configurations, including as initialisation for previously unseen languages. As a proof of concept this paper examines multilingual recurrent neural network language models. Experiments are conducted using language packs released within the IARPA Babel program."
766168,21258,11470,Memory efficient subsequence DTW for Query-by-Example Spoken Term Detection,2013,"In this paper we propose a fast and memory efficient Dynamic Time Warping (MES-DTW) algorithm for the task of Query-by-Example Spoken Term Detection (QbE-STD). The proposed algorithm is based on the subsequence-DTW (S-DTW) algorithm, which allows the search for small spoken queries within a much bigger search collection of spoken documents by considering fixed start-end points in the query and discovering optimal matching subsequences along the search collection. The proposed algorithm applies some modifications to S-DTW that make it better suited for the QbE-STD task, including a way to perform the matching with virtually no system memory, optimal when querying large scale databases. We also describe the system used to perform QbE-STD, including an energy-based quantification for speech/non-speech detection and an overlap detector for putative matches. We test the system proposed using the Mediaeval 2012 spoken-web-search dataset and show that, in addition to the memory savings, the proposed algorithm brings an advantage in terms of matching accuracy (up to 0.235 absolute MTWV increase) and speed (around 25% faster) in comparison to the original S-DTW."
2594587,21258,9804,Tongue Tracking in Ultrasound Images using EigenTongue Decomposition and Artificial Neural Networks,2015,"This paper describes a machine learning approach for extracting automatically the tongue contour in ultrasound images. This method is developed in the context of visual articulatory biofeedback for speech therapy. The goal is to provide a speaker with an intuitive visualization of his/her tongue movement, in real-time, and with minimum human intervention. Contrary to most widely used techniques based on active contours, the proposed method aims at exploiting the information of all image pixels to infer the tongue contour. For that purpose, a compact representation of each image is extracted using a PCA-based decomposition technique (named EigenTongue). Artificial neural networks are then used to convert the extracted visual features into control parameters of a PCA-based tongue contour model. The proposed method is evaluated on 9 speakers, using data recorded with the ultrasound probe hold manually (as in the targeted application). Speaker-dependent experiments demonstrated the effectiveness of the proposed method (with an average error of ~1.3 mm when training from 80 manually annotated images), even when the tongue contour is poorly imaged. The performance was significantly lower in speaker-independent experiments (i.e. when estimating contours on an unknown speaker), likely due to anatomical differences across speakers."
2596456,21258,9804,Multiple-order non-negative matrix factorization for speech enhancement,2014,"Amongst the speech enhancement techniques, statistical models based on Non-negative Matrix Factorization (NMF) have received great attention. In a single channel configuration, NMF is used to describe the spectral content of both the speech and noise sources. As the number of components can have a crucial influence on separation quality, we here propose to investigate model order selection based on the variational Bayesian approximation to the marginal likelihood of models of different orders. To go further, we propose to use model averaging to combine several single-order NMFs and we show that a straightforward application of model averaging principles is inefficient as it turned out to be equivalent to model selection. We thus introduce a parameter to control the entropy of the model order distribution which makes the averaging effective. We also show that our probabilistic model nicely extends to a multiple-order NMF model where several NMFs are jointly estimated and averaged. Experiments are conducted on real data from the CHiME challenge and give an interesting insight on the entropic parameter and model order priors. Separation results are also promising as model averaging outperforms single-order model selection. Finally, our multiple-order NMF shows an interesting gain in computation time."
2777085,21258,9804,Automatic age detection in normal and pathological voice,2015,"Systems that automatically detect voice pathologies are usually trained with recordings belonging to population of all ages.#R##N#However such an approach might be inadequate because of the acoustic variations in the voice caused by the natural aging process. In top of that, elder voices present some perturbations in quality similar to those related to voice disorders, which make the detection of pathologies more troublesome.  With this in mind, the study of methodologies which automatically incorporate information about speakers’ age, aiming at a simplification in the detection of voice disorders is of interest.  In this respect, the present paper introduces an age detector trained with normal and pathological voice, constituting a first step towards the study of age-dependent pathology detectors. The proposed system employs sustained vowels of the Saarbrucken database from which two age groups are examinated: adults and elders. Mel frequency cepstral coefficients for characterization, and Gaussian mixture models for classification are utilized. In addition, fusion of vowels at score level is considered to improve detection performance.  Results suggest that age might be effectively recognized using normal and pathological voices when using sustained vowels as acoustical material, opening up possibilities for the design of automatic age-dependent voice pathology detection systems."
914075,21258,20515,Formant trajectories in linguistic units for text-independent speaker recognition,2013,"Inspired by successful work in forensic speaker identification, this work presents a higher level system for text-independent speaker recognition by means of the temporal trajectories of formant frequencies in linguistic units. Feature extraction from unit-dependent trajectories provides a very flexible system able to be applied in different scenarios. At a fine-grained level, it is possible to provide a calibrated likelihood ratio per linguistic unit under analysis (extremely useful in applications such as forensics), and at a coarse-grained level, the individual contributions of different units can be combined to obtain a more discriminative single system with high potential for combination with short term spectral systems. With development data being extracted from NIST SRE 2004 and 2005 datasets, this approach has been tested on NIST SRE 2006 1side-1side task, English-only male trials, consisting of 9,720 trials from 219 speakers. Remarkable results have been obtained for some single units from extremely short segments of speech, and the combination of several units leads to a relative improvement of 17.2% on EER when fusing with an i-vector system."
2670904,21258,535,"JHU ASpIRE system: Robust LVCSR with TDNNS, iVector adaptation and RNN-LMS",2015,"Multi-style training, using data which emulates a variety of possible test scenarios, is a popular approach towards robust acoustic modeling. However acoustic models capable of exploiting large amounts of training data in a comparatively short amount of training time are essential. In this paper we tackle the problem of reverberant speech recognition using 5500 hours of simulated reverberant data. We use time-delay neural network (TDNN) architecture, which is capable of tackling long-term interactions between speech and corrupting sources in reverberant environments. By sub-sampling the outputs at TDNN layers across time steps, training time is substantially reduced. Combining this with distributed-optimization we show that the TDNN can be trained in 3 days using up to 32 GPUs. Further, iVectors are used as an input to the neural network to perform instantaneous speaker and environment adaptation. Finally, recurrent neural network language models are applied to the lattices to further improve the performance. Our system is shown to provide state-of-the-art results in the IARPA ASpIRE challenge, with 26.5% WER on the dev Jest set."
2710793,21258,9804,Language independent and unsupervised acoustic models for speech recognition and keyword spotting.,2014,"Copyright © 2014 ISCA.Developing high-performance speech processing systems for low-resource languages is very challenging. One approach to address the lack of resources is to make use of data from multiple languages. A popular direction in recent years is to train a multi-language bottleneck DNN. Language dependent and/or multi-language (all training languages) Tandem acoustic models (AM) are then trained. This work considers a particular scenario where the target language is unseen in multi-language training and has limited language model training data, a limited lexicon, and acoustic training data without transcriptions. A zero acoustic resources case is first described where a multilanguage AM is directly applied, as a language independent AM (LIAM), to an unseen language. Secondly, in an unsupervised approach a LIAM is used to obtain hypotheses for the target language acoustic data transcriptions which are then used in training a language dependent AM. 3 languages from the IARPA Babel project are used for assessment: Vietnamese, Haitian Creole and Bengali. Performance of the zero acoustic resources system is found to be poor, with keyword spotting at best 60% of language dependent performance. Unsupervised language dependent training yields performance gains. For one language (Haitian Creole) the Babel target is achieved on the in-vocabulary data."
2641955,21258,535,Personalizing universal recurrent neural network language model with user characteristic features by social network crowdsourcing,2015,"With the popularity of mobile devices, personalized speech recognizer becomes more realizable today and highly attractive. Each mobile device is primarily used by a single user, so it's possible to have a personalized recognizer well matching to the characteristics of individual user. Although acoustic model personalization has been investigated for decades, much less work have been reported on personalizing language model, probably because of the difficulties in collecting enough personalized corpora. Previous work used the corpora collected from social networks to solve the problem, but constructing a personalized model for each user is troublesome. In this paper, we propose a universal recurrent neural network language model with user characteristic features, so all users share the same model, except each with different user characteristic features. These user characteristic features can be obtained by crowdsouring over social networks, which include huge quantity of texts posted by users with known friend relationships, who may share some subject topics and wording patterns. The preliminary experiments on Facebook corpus showed that this proposed approach not only drastically reduced the model perplexity, but offered very good improvement in recognition accuracy in n-best rescoring tests. This approach also mitigated the data sparseness problem for personalized language models."
2788694,21258,9804,Non-native perception of regionally accented speech in a multitalker context,2014,"Noisy listening conditions are challenging to non-native listeners who typically perform poorly while attending to several competing talkers. This study examined whether nonnative listeners are able to utilize dialect-related cues in the target and in the masking speech, even if they do not reach the proficiency level of the native listeners. 35 Indonesian-English bilinguals residing in the United States were presented with speech stimuli from two American English dialects, General American English and Southern American English, which were systematically varied both in the target sentences and in 2-talker masking babble at three sound-to-noise ratios (SNR). We found that the non-native listeners were (1) sensitive to dialect-specific phonetic details in speech of competing talkers and (2) performed in a manner similar to native listeners despite their apparent deficit. However, their performance differed significantly when the speech levels of the competing talkers were equal (0 dB SNR). The differential sensitivity of non-native listeners may reflect their inability to separate utterances of competing talkers when there is not enough contrast in their voice levels. In turn, the lack of sufficient contrast may reduce their ability to benefit from the phonetic-acoustic details necessary to encode the signal and comprehend a message."
2648708,21258,9804,Toward a Continuous Modeling of French Prosodic Structure: Using Acoustic Features to Predict Prominence Location and Prominence Degree.,2011,"The aim of this paper is to present a tool developed in order to generate French rhythmical structure semi-automatically, without taking grammatical cues into account. On the basis of a phonemic alignment, the software first locates prominent syllables by considering basic acoustic features such as F0, duration and silent pause. It then assigns a degree of prominence to each syllable identified. The estimation of this degree results from a computation of the values of silent pause, relative duration and height averages used for prominence detection in the first step. The second part of the article presents an experiment conducted in order to validate the algorithm’s performances, by comparing the predictions of the software with a continuous manual coding carried out by four annotators on a 4-minute stretch of corpus (788 syllables) involving read aloud speech, map task and spontaneous dialogue. The performance of the algorithm is encouraging: a Fleiss’ kappa calculation estimates the rate at 0.8, and a correlation agreement calculation at 91%, in the best cases. Index Terms: prominence, automatic detection, degree of prominence, prosodic structure, French."
2672314,21258,9804,Pitch accent distribution in German infant-directed speech,2015,"Infant-directed speech exhibits slower speech rate, higher pitch and larger f0 excursions than adult-directed speech. Apart from these phonetic properties established in many languages, little is known on the intonational phonological structure in individual languages, i.e. pitch accents and boundary tones and their frequency distribution. Here, we investigated the intonation of infant-directed speech in German. We extracted all turns from the CHILDES database directed towards infants younger than one year (n=585). Two annotators labeled pitch accents and boundary tones according to the autosegmental-metrical intonation system GToBI. Additionally, the tonal movement surrounding the accentual syllable was analyzed. Main results showed a) that 45% of the words carried a pitch accent, b) that phrases ending in a low tone were most frequent, c) that H* accents were generally more frequent than L* accents, d) that H*, L+H* and L* are the most frequent pitch accent types in IDS, and e) that a pattern consisting of an accentual low-pitched syllable preceded by a low tone and followed by a rise or a high tone constitutes the most frequent single pattern. The analyses reveal that the IDS intonational properties lead to a speech style with many tonal alternations, particularly in the vicinity of accented syllables."
2827504,21258,9804,Noise-robust TTS speaker adaptation with statistics smoothing,2014,"Copyright © 2014 ISCA.In practical scenarios for speaker adaptation of speech synthesis systems, the quality of adaptation audio data may be poor. In these situations, it is necessary to make use of the available audio to capture the speaker attributes, whilst aiming to obtain a synthesis voice which does not have any of the lowquality attributes of the audio. One approach to achieving this is to define a sub-space of parametric synthesis parameters in which the adapted system must lie. Though this yields reasonable synthesis quality, target speaker similarity degrades. Quality is also affected in severe noise conditions. This paper describes a smoothing approach that addresses this problem. For a noisy target speaker, first a 'similar speaker' is selected from a database of speakers. Statistics from this speaker are then smoothed with those obtained from the target speaker. By appropriately combining the two sources of information, it is possible to balance similarity and quality. Results indicate that both the quality and similarity can be improved by smoothing, especially for severe noise conditions. The similarity performance, however, varies from speaker to speaker, indicating the importance of a reasonable automatic speaker selection method and the coverage of the candidate speaker pool."
926221,21258,11470,Using emotional noise to uncloud audio-visual emotion perceptual evaluation,2013,"Emotion perception underlies communication and social interaction, shaping how we interpret our world. However, there are many aspects of this process that we still do not fully understand. Notably, we have not yet identified how audio and video information are integrated during the perception of emotion. In this work we present an approach to enhance our understanding of this process using the McGurk effect paradigm, a framework in which stimuli composed of mismatched audio and video cues are presented to human evaluators. Our stimuli set contain sentence-level emotional stimuli with either the same emotion on each channel (“matched”) or different emotions on each channel (“mismatched”, for example, an angry face with a happy voice). We obtain dimensional evaluations (valence and activation) of these emotionally consistent and noisy stimuli using crowd sourcing via Amazon Mechanical Turk. We use these data to investigate the audio-visual feature bias that underlies the evaluation process. We demonstrate that both audio and video information individually contribute to the perception of these dimensional properties. We further demonstrate that the change in perception from the emotionally matched to emotionally mismatched stimuli can be modeled using only unimodal feature variation. These results provide insight into the nature of audio-visual feature integration in emotion perception."
2652619,21258,535,The 2015 sheffield system for longitudinal diarisation of broadcast media,2015,"Speaker diarisation is the task of answering who spoke when within a multi-speaker audio recording. Diarisation of broadcast media typically operates on individual television shows, and is a particularly difficult task, due to a high number of speakers and challenging background conditions. Using prior knowledge, such as that from previous shows in a series, can improve performance. Longitudinal diarisation allows to use knowledge from previous audio files to improve performance, but requires finding matching speakers across consecutive files. This paper describes the University of Sheffield system for participation in the 2015 Multi-Genre Broadcast (MGB) challenge. The challenge required longitudinal diarisation of data from BBC archives, under very constrained resource settings. Our system consists of three main stages: speech activity detection using DNNs with novel adaptation and decoding methods; speaker segmentation and clustering, with adaptation of the DNN-based clustering models; and finally speaker linking to match speakers across shows. The final result on the development set of 19 shows from five different television series provided a Diarisation Error Rate of 50.77% in the diarisation and linking task."
2685051,21258,9804,TTS Synthesis with Bidirectional LSTM based Recurrent Neural Networks,2014,"Feed-forward, Deep neural networks (DNN)-based text-tospeech (TTS) systems have been recently shown to outperform decision-tree clustered context-dependent HMM TTS systems [1, 4]. However, the long time span contextual effect in a speech utterance is still not easy to accommodate, due to the intrinsic, feed-forward nature in DNN-based modeling. Also, to synthesize a smooth speech trajectory, the dynamic features are commonly used to constrain speech parameter trajectory generation in HMM-based TTS [2]. In this paper, Recurrent Neural Networks (RNNs) with Bidirectional Long Short Term Memory (BLSTM) cells are adopted to capture the correlation or co-occurrence information between any two instants in a speech utterance for parametric TTS synthesis. Experimental results show that a hybrid system of DNN and BLSTM-RNN, i.e., lower hidden layers with a feed-forward structure which is cascaded with upper hidden layers with a bidirectional RNN structure of LSTM, can outperform either the conventional, decision tree-based HMM, or a DNN TTS system, both objectively and subjectively. The speech trajectory generated by the BLSTM-RNN TTS is fairly smooth and no dynamic constraints are needed."
1888259,21258,9804,Analysis of Emotional Effect on Speech-Body Gesture Interplay,2014,"In interpersonal interactions, speech and body gesture channels are internally coordinated towards conveying communicative intentions. The speech-gesture relationship is influenced by the internal emotion state underlying the communication. In this paper, we focus on uncovering the emotional effect on the interrelation between speech and body gestures. We investigate acoustic features describing speech prosody (pitch and energy) and vocal tract configuration (MFCCs), as well as three types of body gestures, viz., head motion, lower and upper body motions. We employ mutual information to measure the coordination between the two communicative channels, and analyze the quantified speech-gesture link with respect to distinct levels of emotion attributes, i.e., activation and valence. The results reveal that the speech-gesture coupling is generally tighter for low-level activation and high-level valence, compared to high-level activation and low-level valence. We further propose a framework for modeling the dynamics of speech-gesture interaction. Experimental studies suggest that such quantified coupling representations can well discriminate different levels of activation and valence, reinforcing that emotions are encoded in the dynamics of the multimodal link. We also verify that the structures of the coupling representations are emotiondependent using subspace-based analysis. Index Terms: emotion attributes, body gesture, speech prosody, speech-gesture interplay, mutual information"
2317026,21258,9804,Correlating Student Acoustic-Prosodic Profiles with Student Learning in Spoken Tutoring Dialogues,2005,"We examine correlations between student learning and student acoustic-prosodic profiles, which prior research has shown to be predictive of emotional states. We compare these correlations in two corpora of spoken tutoring dialogues: a human-human corpus and a human-computer corpus. Our results suggest that rather than relying on emotion prediction models developed via the more labor-intensive method of manually labeling emotions, adaptive strategies for our spoken dialogue tutoring system can be developed based on observed acoustic-prosodic profiles that we hypothesize to be reflective of emotion. adaptive techniques can be developed to try to perpetuate those states that positively correlate with learning, and try to alter these states that negatively correlate with learning. In this paper, we investigate whether the approach of de- tecting speech peculiarities is useful for identifying student emotional states that correlate with student learning. We fo- cus on acoustic-prosodic peculiarities that are automatically extractable from the speech signal. We first extract a set of acoustic-prosodic features from each student turn in our two corpora of spoken tutoring dialogues: a human-computer cor- pus and a human-human corpus. We then use these extracted features to build an acoustic-prosodic profile for each student in our two corpora. Finally, we examine correlations between stu- dent learning and our student acoustic-prosodic profiles in each corpus, and we compare these correlations across our two cor- pora. Our results show some significant correlations between these profiles and learning, which suggests that adaptive strate- gies for our spoken dialogue tutoring system can be developed based in part on observed student acoustic-prosodic profiles that we hypothesize to be reflective of emotion, rather than relying on emotion prediction models developed via the labor-intensive method of manually labeling emotions."
2846340,21258,9804,The interplay of intonation and complex lexical tones: how speaker attitudes affect the realization of glottalization on vietnamese sentence-final particles.,2013,"A salient aspect of the tone system of Hanoi Vietnamese is its use of phonation-type characteristics. This pilot study investigates intonational variation in the realization of two tones: tone 3 (nga), a rising tone with a strong glottalization in its first half, and tone 6 (nặng), which starts on a middle pitch and usually falls dramatically because of a strong glottalization in its second half. This study focuses on how speaker attitude affects the realization of glottalization on two sentence-final particles (SFPs) carrying tones 3 and 6: đa [IPA: ɗa3], conveying tense-aspect-modality information, and ạ [IPA: a6], conveying politeness. Audio and electroglottographic recordings from 4 male speakers suggest that glottalization is phased earlier for surprise than for declaration. Irritation also tends to be reflected in earlier glottalization, but with an added glottal constriction at the very end. A methodological challenge is that phonetic realizations of tones 3 and 6 span a wide range of states of the glottis. A procedure is proposed for detecting the complex-repetitive patterns found in cases of lapse into creaky phonation (vocal fry). This helps quantify glottalization phenomena, with a view to arriving at a model that can be used in speech processing."
2183311,21258,11470,Improved Histogram Equalzaiton (HEQ) for Robust Speech Recogntion,2007,"With the rapid development of intelligent transportation systems (ITS), how to provide users with a natural and efficient human-machine interface is now becoming a crucial issue for driver safety. It is no doubt that speech will be one of the best mediators of human-machine interaction; however, the performance of automatic speech recognition (ASR) always radically degrades when the input speech is corrupted by varying noises. In this paper, we consider the use of histogram equalization (HEQ) for robust ASR. A novel data fitting scheme was presented to efficiently approximate the inverse of the cumulative density function of training speech for HEQ, which has the merits of lower storage and time consumption compared to the conventional table-lookup or quantile based HEQ approaches. Moreover, a more elaborate attempt of using multiple inverse functions for different noise conditions was investigated as well. All experiments were carried out on the Aurora-2 standard database and task. Very encouraging results were obtained. The proposed robustness technique has also been properly integrated into our prototype system for in-vehicle traffic information retrieval using spoken queries."
1925746,21258,8960,Speaker Comparison with Inner Product Discriminant Functions,2009,"Speaker comparison, the process of finding the speaker similarity between two speech signals, occupies a central role in a variety of applications—speaker verification, clustering, and identification. Speaker comparison can be placed in a geometric framework by casting the problem as a model comparison process. For a given speech signal, feature vectors are produced and used to adapt a Gaussian mixture model (GMM). Speaker comparison can then be viewed as the process of compensating and finding metrics on the space of adapted models. We propose a framework, inner product discriminant functions (IPDFs), which extends many common techniques for speaker comparison—support vector machines, joint factor analysis, and linear scoring. The framework uses inner products between the parameter vectors of GMM models motivated by several statistical methods. Compensation of nuisances is performed via linear transforms on GMM parameter vectors. Using the IPDF framework, we show that many current techniques are simple variations of each other. We demonstrate, on a 2006 NIST speaker recognition evaluation task, new scoring methods using IPDFs which produce excellent error rates and require significantly less computation than current techniques."
2722395,21258,535,Natural language understanding for partial queries,2015,"Typical natural language understanding systems are built based on the assumption that they have access to the fully formed complete queries. Today's natural user interfaces, however, enable users to interact with various services and agents (e.g. search engines, personal digital assistants) running on desktop computers and laptops. The system is expected to understand the user's intent while the user is typing the query with the goal of increasing system response rate and ultimately improving the user's productivity. Language understanding models built on fully formed queries perform poorly when tested on partial or incomplete queries. In this study, we consider the problem of domain detection for typed partial natural language queries. We design two sets of features in addition to lexical features to train a multi-valued domain classification model. The first feature set consists of character n-gram features, and the second is the class-based features extracted from clustering of word embed-dings. Our experiments show that the two feature sets improve the model's performance by up to 52.8% in comparison to the lexical n-gram baselines."
2591774,21258,9804,Development and implementation of fiducial markers for vocal tract MRI imaging and speech articulatory modelling.,2013,"MRI allows characterizing the shape and position of speech articulators, but not tracking flesh points, since there are no physiological landmarks reliably associated with the highly deformable tissues of these articulators. This information is, however, interesting for studying the biomechanical properties of these organs as well as for modelling the relations between measurement modalities such as MRI and electromagnetic articulography. We have therefore attached fiducial markers made of MRI-visible polymers to a speaker's articulators, and recorded a corpus of MRI midsagittal images. We then determined and analyzed the articulators' contours and the markers' centre coordinates. We found that the apparent sliding of markers with respect to lips and tongue contours ranges between 0.6 and 1.5 cm. Specifically, the curvilinear distances between two tongue flesh points relative to the total length varied up to about ±20%, confirming a non longitudinal elasticity of the contours. However, we have also found that the markers and jaw coordinates can predict the articulators' contours with a variance explanation of about 85 %, and an RMS reconstruction error between 0.08 and 0.15 cm, compared with 74 to 95 % of variance and 0.07 to 0.14 cm of RMS error for the original articulatory models."
2712296,21258,535,The NAIST ASR system for the 2015 Multi-Genre Broadcast challenge: On combination of deep learning systems using a rank-score function,2015,"The Multi-Genre Broadcast challenge is an official challenge of the IEEE Automatic Speech Recognition and Understanding Workshop. This paper presents NAISTs contribution to the premiere of this challenge. The presented speech-to-text system for English makes use of various front-ends (e.g., MFCC, i-vector and FBANK), DNN acoustic models and several language models for decoding and rescoring (N-gram, RNNLM). Subsets of the training data with varying sizes were evaluated with respect to the overall training quality. Two speech segmentation systems were developed for the challenge, based on DNNs and GMM-HMMs. Recognition was performed in three stages: Decoding, lattice rescoring and system combination. This paper focuses on the system combination experiments and presents a rank-score based system weighting approach, which gave better performance compared to a normal system combination strategy. The DNN based ASR system trained on MFCC + i-vector features with the sMBR training criterion gives the best performance of 27.8% WER, and thus significantly outperforms the baseline DNN-HMM sMBR yielding 33.7% WER."
2695572,21258,9804,Gesture Design of Hand-to-Speech Converter Derived from Speech-to-Hand Converter Based on Probabilistic Integration Model,2011,"When dysarthrics, individuals with speaking disabilities, try to communicate using speech, they often have no choice but to use speech synthesizers which require them to type word symbols or sound symbols. Input by this method often makes realtime communication troublesome and dysarthric users struggle to have smooth flowing conversations. In this study, we are developing a novel speech synthesizer where speech is generated through hand motions rather than symbol input. In recent years, statistical voice conversion techniques have been proposed based on space mapping between given parallel utterances. By applying these methods, a hand space was mapped to a vowel space and a converter from hand motions to vowel transitions was developed. It reported that the proposed method is effective enough to generate the five Japanese vowels. In this paper, we discuss the expansion of this system to consonant generation. In order to create the gestures for consonants, a Speech-to-Hand conversion system is firstly developed using parallel data for vowels, in which consonants are not included. Then, we are able to automatically search for candidates for consonant gestures for a Hand-to-Speech system. Index Terms: Dysarthria, speech production, hand motions, media conversion, arrangement of gestures and vowels"
2598307,21258,9804,Feature Space Maximum A Posteriori Linear Regression for Adaptation of Deep Neural Networks,2014,"We propose a feature space maximum a posteriori (MAP) linear regression framework to adapt parameters for context dependent deep neural network hidden Markov models (CD-DNNHMMs). Due to the huge amount of parameters used in DNN acoustic models in large vocabulary continuous speech recognition, the problem of over-fitting can be severe in DNN adaptation, thus often impair the robustness of the adapted DNN model. Linear input network (LIN) as a straight-forward feature space adaptation method for DNN, similar to feature space maximum likelihood linear regression (fMLLR), can potentially suffer from the same robustness situation. The proposed adaptation framework is built based on MAP estimation of the LIN parameters by incorporating prior knowledge into the adaptation process. Experimental results on the Switchboard task show that against the speaker independent CD-DNN-HMM systems, LIN provides 4.28% relative word error rate reduction (WERR) and the proposed fMAPLIN method is able to provide further 1.15% (totally 5.43%) WERR on top of LIN. Index Terms: deep neural network, speaker adaptation, maximum a posteriori estimation"
16707,21258,9804,Speech pauses and gestural holds in parkinson²s disease.,2002,"Parkinson’s disease (PD) belongs to a class of neurodegenerative diseases that affect the patient’s speech, motor, and cognitive capabilities. All three deÞcits affect the multimodal communication channels of speech and gesture. We presenta study on the changes in speech pause patterns and gesture holds before and after treatment. We present the results of a pilot study of two Idiopathic PD patients who have undergone Lee Silverman Voice Treatment (LSVT). We show that therewas a consistentchangein the location of pauses with respect to semantic sentential utterance units. After treatment, the number and duration of pauses within sentential units decreased while the inter sentential pauses increased. This indicates reduction in hesitation and increase in speech phrasing. We also found a decrease in the number of sentence repairs and the time spent in repairs. For gesture, we found that non-rest holds intersecting with within-sentence pauses appears to decline after treatment, as does the ratio of rest holds during speechagainst rest holds between sentences. While the work is preliminary, these patterns suggest that multimodal discourse characteristics might provide accessto the underlying cognitive state under the load of narrative discourse."
2667602,21258,9804,The Voice Prominence Hypothesis: the Interplay of F0 and Voice Source Features in Accentuation,2013,"This paper explores the interplay of source correlates of accentuation, examining a hypothesis (the Voice Prominence Hypothesis) that different source parameters are involved and may serve as equivalent. It predicts that where accentuation is not marked by pitch salience there will be more extensive changes in other source parameters. This follows our assumption that prosodic entities such as accentuation, focus, declination, etc. involve adjustments to the entire voice source and not simply to F0. Twelve 3-accent sentences of Connemara Irish (declaratives, WH questions and Yes/No questions) were analysed. These are typically produced and transcribed as H* H* H*L. Of particular interest were the second accents: although they are heard as accented, there are no particular pitch excursions that would account for their salience. Inverse filtering and subsequent source parameterisation was carried out to yield measures for a range of source parameters. Results support the voice prominence hypothesis: as predicted, the most striking source adjustments were found in the second accent. Even where there is substantial pitch movement (final accent), parameters other than F0 appear to be contributing to the salience of the accented syllable. The precise source changes associated with accentuation varied across sentence types and within the prosodic phrase. Index Terms: voice source, accentuation, prominence."
211358,21258,9804,Tracking and Beamforming for Multiple Simultaneous Speakers with Probabilistic Data Association Filters,2006,"In prior work, we developed a speaker tracking system based on an extended Kalman filter using time delays of arrival (TDOAs) as acoustic features. While this system functioned well, its utility was limited to scenarios in which a single speaker was to be tracked. In this work, we remove this restriction by generalizing the IEKF, first to a probabilistic data association filter, which incorporates a clutter model for rejection of spurious acoustic events, and then to a joint probabilistic data association filter (JPDAF), which maintains a separate state vector for each active speaker. In a set of experiments conducted on seminar and meeting data, the JPDAF speaker tracking system reduced the multiple object tracking errror from 20.7% to 14.3% with respect to the IEKF system. In a set of automatic speech recognition experiments conducted on the output of a 64 channel microphone array which was beamformed using automatic speaker position estimates, applying the JPDAF tracking system reduced word error rate from 67.3% to 66.0%. Moreover, the word error rate on the beamformed output was 13.0% absolute lower than on a single channel of the array. Index Terms: acoustic source localization, Kalman filter, person tracking, far-field speech recognition, microphone arrays"
1914398,21258,9804,Basic speech recognition for spoken dialogues,2009,"Spoken dialogue systems (SDSs) have great potential for information access in the developing world. However, the realisation of that potential requires the solution of several challenging problems, including the development of sufficiently accurate speech recognisers for a diverse multitude of languages. We investigate the feasibility of developing smallvocabulary speaker-independent ASR systems designed for use in a telephone-based information system, using ten resourcescarce languages spoken in South Africa as a case study. We contrast a cross-language transfer approach (using a well-trained system from a different language) with the development of new language-specific corpora and systems, and evaluate the effectiveness of both approaches. We find that limited speech corpora (3to 8 hours of data from around 200 speakers) are sufficient for the development of reasonably accurate recognisers: Error rates are in the range 2% to 12% for a tenword task, where vocabulary words are excluded from training to simulate vocabulary-independent performance. This approach is substantially more accurate than cross-language transfer, and sufficient for the development of basic spoken dialogue systems. Index Terms: speech recognition, limited vocabularies, technology for the developing world"
2703852,21258,535,The 2015 sheffield system for transcription of Multi-Genre Broadcast media,2015,"We describe the University of Sheffield system for participation in the 2015 Multi-Genre Broadcast (MGB) challenge task of transcribing multi-genre broadcast shows. Transcription was one of four tasks proposed in the MGB challenge, with the aim of advancing the state of the art of automatic speech recognition, speaker diarisation and automatic alignment of subtitles for broadcast media. Four topics are investigated in this work: Data selection techniques for training with unreliable data, automatic speech segmentation of broadcast media shows, acoustic modelling and adaptation in highly variable environments, and language modelling of multi-genre shows. The final system operates in multiple passes, using an initial unadapted decoding stage to refine segmentation, followed by three adapted passes: a hybrid DNN pass with input features normalised by speaker-based cepstral normalisation, another hybrid stage with input features normalised by speaker feature-MLLR transformations, and finally a bottleneck-based tandem stage with noise and speaker factorisation. The combination of these three system outputs provides a final error rate of 27.5% on the official development set, consisting of 47 multi-genre shows."
2684177,21258,9804,DIAPIX-FL: A symmetric corpus of problem-solving dialogues in first and second languages,2014,"This paper describes a corpus of conversations recorded using an extension of the DiapixUK task: the Diapix Foreign Language corpus (DIAPIX-FL) . English and Spanish native talkers were recorded speaking both English and Spanish. The bidirectionality of the corpus makes it possible to separate language (English or Spanish) from speaking in a first language (L1) or second language (L2). An acoustic analysis was carried out to analyse changes in F0, voicing, intensity, spectral tilt and formants that might result from speaking in an L2. The effect of L1 and nativeness on turn types was also studied. Factors that were investigated were pausing, elongations, and incomplete words. Speakers displayed certain patterns that suggest an on-going process of L2 phonological acquisition, such as the overall percentage of voicing in their speech. Results also show an increase in hesitation phenomena (pauses, elongations, incomplete turns), a decrease in produced speech and speech rate, a reduction of F0 range, raising of minimum F0 when speaking in the non-native language which are consistent with more tentative speech and may be used as indicators of non-nativeness. Index Terms: L1-L2, DIAPIX"
1934672,21258,21089,Statistical Modeling for Unit Selection in Speech Synthesis,2004,"Traditional concatenative speech synthesis systems use a number of heuristics to define the target and concatenation costs, essential for the design of the unit selection component. In contrast to these approaches, we introduce a general statistical modeling framework for unit selection inspired by automatic speech recognition. Given appropriate data, techniques based on that framework can result in a more accurate unit selection, thereby improving the general quality of a speech synthesizer. They can also lead to a more modular and a substantially more efficient system.We present a new unit selection system based on statistical modeling. To overcome the original absence of data, we use an existing high-quality unit selection system to generate a corpus of unit sequences. We show that the concatenation cost can be accurately estimated from this corpus using a statistical n-gram language model over units. We used weighted automata and transducers for the representation of the components of the system and designed a new and more efficient composition algorithm making use of string potentials for their combination. The resulting statistical unit selection is shown to be about 2.6 times faster than the last release of the AT&T Natural Voices Product while preserving the same quality, and offers much flexibility for the use and integration of new and more complex components."
2684586,21258,9804,Multi-Accent Deep Neural Network Acoustic Model with Accent-Specific Top Layer Using the KLD-Regularized Model Adaptation,2014,"We propose a multi-accent deep neural network acoustic model with an accent-specific top layer and shared bottom hidden layers. The accent-specific top layer is used to model the distinct accent specific patterns. The shared bottom hidden layers allow maximum knowledge sharing between the native and the accent models. This design is particularly attractive when considering deploying such a system to a live speech service due to its computational efficiency. We applied the KL-divergence (KLD) regularized model adaptation to train the accent-specific top layer. On the mobile short message dictation task (SMD), with 1K, 10K, and 100K British or Indian accent adaptation utterances, the proposed approach achieves 18.1%, 26.0%, and 28.5% or 16.1%, 25.4%, and 30.6% word error rate reduction (WERR) for the British and the Indian accent respectively against a baseline cross entropy (CE) model trained from 400 hour data. On the 100K utterance accent adaptation setup, comparable performance gain can be obtained against a baseline CE model trained with 2000 hour data. We observe smaller yet significant WER reduction on a baseline model trained using the MMI sequence-level criterion. Index Terms: Accent speech recognition, model adaptation, KL-divergence regularization"
2717868,21258,9804,Reverberant Speech Recognition Based on Denoising Autoencoder,2013,"Denoising autoencoder is applied to reverberant speech recognition as a noise robust front-end to reconstruct clean speech spectrum from noisy input. In order to capture context effects of speech sounds, a window of multiple short-windowed spectral frames are concatenated to form a single input vector. Additionally, a combination of short and long-term spectra is investigated to properly handle long impulse response of reverberation while keeping necessary time resolution for speech recognition. Experiments are performed using the CENSREC-4dataset that is designed as an evaluation framework for distant-talking speech recognition. Experimental results show that the proposed denoising autoencoder based front-end using the shortwindowed spectra gives better results than conventional methods. By combining the long-term spectra, further improvement is obtained. The recognition accuracy by the proposed method using the short and long-term spectra is 97.0% for the open condition test set of the dataset, whereas it is 87.8% when a multicondition training based baseline is used. As a supplemental experiment, large vocabulary speech recognition is also performed and the effectiveness of the proposed method has been confirmed. Index Terms: Denoising autoencoder, reverberant speech recognition, restricted Boltzmann machine, distant-talking speech recognition, CENSREC-4"
2085203,21258,9804,Effects of frequency shifts on perceived naturalness and gender information in speech,2006,"In natural speech, there is a moderate correlation between the fundamental frequency and formant frequencies across talkers. The present study used a high-quality vocoder to manipulate these properties and determine their contribution to perceived naturalness and voice gender. The stimuli were re-synthesized sentences spoken by two adult males and two adult females. Scale factors were chosen for each sentence and for each talker to produce frequency-shifted versions with a specified mean fundamental frequency (F0) ranging from 60 Hz to 450 Hz in 10 steps, paired with 10 steps in geometric mean formant frequencies ranging from 850 Hz to 2500 Hz. Listeners judged frequency-shifted sentences as more natural when F0 and formant frequencies followed the co-variation of F0 and formant frequencies in natural voices. Sentences with low F0s and low formant frequencies were perceived as masculine, while sentences with high F0 and high formant frequencies were assigned high ratings of femininity. Sentences with “mismatched” F0 and formant frequencies were assigned ratings near the midpoint of the range, indicating gender ambiguity. Frequency-shifted sentences derived from male talkers received consistently higher ratings of masculinity than those derived from females, while sentences from female talkers received higher ratings of femininity, even when assigned scale factors appropriate for the opposite gender, indicating that factors other than F0 and mean formant frequencies contribute to perceived gender."
1216807,21258,23735,Making a robot recognize three simultaneous sentences in real-time,2005,"A humanoid robot under real-world environments usually hears mixtures of sounds, and thus three capabilities are essential for robot audition; sound source localization, separation, and recognition of separated sounds. We have adopted the missing feature theory (MFT) for automatic recognition of separated speech, and developed the robot audition system. A microphone array is used along with a real-time dedicated implementation of geometric source separation (GSS) and a multi-channel post-filter that gives us a further reduction of interferences from other sources. The automatic speech recognition based on MFT recognizes separated sounds by generating missing feature masks automatically from the post-filtering step. The main advantage of this approach for humanoid robots resides in the fact that the ASR with a clean acoustic model can adapt the distortion of separated sound by consulting the post-filter feature masks. In this paper, we used the improved Julius as an MFT-based automatic speech recognizer (ASR). The Julius is a real-time large vocabulary continuous speech recognition (LVCSR) system. We performed the experiment to evaluate our robot audition system. In this experiment, the system recognizes a sentence, not an isolated word. We showed the improvement in the system performance through three simultaneous speech recognition on the humanoid SIG2."
2667449,21258,9804,Task-aware Deep Bottleneck Features for Spoken Language Identification,2014,"Recently, deep bottleneck features (DBF) extracted from a deep neural network (DNN) containing a narrow bottleneck layer, have been applied for language identification (LID), and yield significant performance improvement over state-of-the-art methods on NIST LRE 2009. However, the DNN is trained using a large corpus of specific language which is not directly related to the LID task. More recently, lattice based discriminative training methods for extracting more targeted DBF were proposed for ASR. Inspired by this, this paper proposes to tune the post-trained DNN parameters using an LID-specific training corpus, which may make the resulting DBF, termed a Discriminative DBF (D2BF), more discriminative and task-aware. Specifically, the maximum mutual information (MMI) criterion, with gradient descent, is applied to update the DNN parameters of the bottleneck layer in an iterative fashion. We evaluate the performance of the proposed D2BF using different back-end models, including GMM-MMI and ivector, over the most confused 6-languages selected from NIST LRE 2009. The results show that the proposed D2BF is more appropriate and effective than the original DBF. Index Terms: language identification, deep bottleneck feature, deep neural network, discriminative training, Gaussian mixture model, maximum mutual information"
1476484,21258,535,Automatic pronunciation clustering using a World English archive and pronunciation structure analysis,2013,"English is the only language available for global communication. Due to the influence of speakers' mother tongue, however, those from different regions inevitably have different accents in their pronunciation of English. The ultimate goal of our project is creating a global pronunciation map of World Englishes on an individual basis, for speakers to use to locate similar English pronunciations. If the speaker is a learner, he can also know how his pronunciation compares to other varieties. Creating the map mathematically requires a matrix of pronunciation distances among all the speakers considered. This paper investigates invariant pronunciation structure analysis and Support Vector Regression (SVR) to predict the inter-speaker pronunciation distances. In experiments, the Speech Accent Archive (SAA), which contains speech data of worldwide accented English, is used as training and testing samples. IPA narrow transcriptions in the archive are used to prepare reference pronunciation distances, which are then predicted based on structural analysis and SVR, not with IPA transcriptions. Correlation between the reference distances and the predicted distances is calculated. Experimental results show very promising results and our proposed method outperforms by far a baseline system developed using an HMM-based phoneme recognizer."
2686577,21258,9804,Mitigating the Effects of Non-Stationary Unseen Noises on Language Recognition Performance,2015,"We introduce a new dataset for the study of the effect of highly non-stationary noises on language recognition (LR) performance. The dataset is based on the data from the 2009 Language Recognition Evaluation organized by the National Institute of Standards and Technology (NIST). Randomly selected noises are added to these signals to achieve a chosen signal-tonoise ratio and percentage of corruption. We study the effect of these noises on LR performance as a function of these parameters and present some initial methods to mitigate the degradation, focusing on the speech activity detection (SAD) step. These methods include discarding the C0 coefficient from the features used for SAD, using a more stringent threshold on the SAD scores, thresholding the speech likelihoods returned by the model as an additional way of detecting noise, and a final model adaptation step. We show that a system optimized for clean speech is clearly suboptimal on this new dataset since the proposed methods lead to gains of up to 35% on the corrupted data, without knowledge of the test noises and with very little effect on clean data performance. Index Terms: spoken language recognition, non-stationary noise, speech activity detection"
2838025,21258,9804,Bag-of-Words Input for Long History Representation in Neural Network-based Language Models for Speech Recognition,2015,"In most of previous works on neural network based language models (NNLMs), the words are represented as 1-of-N encoded feature vectors. In this paper we investigate an alternative encoding of the word history, known as bag-of-words (BOW) representation of a word sequence, and use it as an additional input feature to the NNLM. Both the feedforward neural network (FFNN) and the long short-term memory recurrent neural network (LSTM-RNN) language models (LMs) with additional BOW input are evaluated on an English large vocabulary automatic speech recognition (ASR) task. We show that the BOW features significantly improve both the perplexity (PP) and the word error rate (WER) of a standard FFNN LM. In contrast, the LSTM-RNN LM does not benefit from such an explicit long context feature. Therefore the performance gap between feedforward and recurrent architectures for language modeling is reduced. In addition, we revisit the cache based LM, a seeming analog of the BOW for the count based LM, which was unsuccessful for ASR in the past. Although the cache is able to improve the perplexity, we only observe a very small reduction in WER. Index Terms: language modeling, speech recognition, bag-ofwords, feedforward neural networks, recurrent neural networks, long short-term memory, cache language model"
2906787,21258,9804,Relationships Between Functional Load and Auditory Confusability Under Different Speech Environments.,2016,"Functional load (FL) is an information-theoretic measure that#R##N#captures a phoneme’s contribution to successful word identifi-#R##N#cation.  Experimental findings have shown that it can help ex-#R##N#plain  patterns  in  perceptual  accuracy.   Here,  we  ask  whether#R##N#the relationship between FL and perception has larger conse-#R##N#quences  for  the  structure  of  a  language’s  lexicon.   Since  re-#R##N#ducing FL minimizes the risk of misidentifying a word in the#R##N#case where a listener inaccurately perceives the initial phoneme,#R##N#we predicted that in spoken language, where perceptual accu-#R##N#racy  is  important  for  successful  communication,  the  lexicon#R##N#will be structured to reduce FL in auditorily confusable initial#R##N#phonemes more than in written language.  To test this predic-#R##N#tion,  we  compared  FL  of  all  initial  phonemes  in  spoken  and#R##N#academic written genres of the COCA corpus.  We found that#R##N#FL  in  phoneme  pairs  in  the  spoken  corpus  is  overall  higher#R##N#and more variable than in the academic corpus, a natural conse-#R##N#quence of the smaller lexical inventory characteristic of spoken#R##N#language.  In auditorily confusable pairs, however, this differ-#R##N#ence is relatively reduced, such that spoken FL decreases rel-#R##N#ative to academic FL. We argue that this reflects a pressure in#R##N#spoken language to use words for which inaccurate perception#R##N#does minimal damage to word identification."
2838632,21258,9804,GMM-free flat start sequence-discriminative DNN training,2016,"Recently, attempts have been made to remove Gaussian mixture models (GMM) from the training process of deep neural network-based hidden Markov models (HMM/DNN). For the GMM-free training of a HMM/DNN hybrid we have to solve two problems, namely the initial alignment of the frame-level state labels and the creation of context-dependent states. Although flat-start training via iteratively realigning and retraining the DNN using a frame-level error function is viable, it is quite cumbersome. Here, we propose to use a sequence-discriminative training criterion for flat start. While sequence-discriminative training is routinely applied only in the final phase of model training, we show that with proper caution it is also suitable for getting an alignment of context-independent DNN models. For the construction of tied states we apply a recently proposed KL-divergence-based state clustering method, hence our whole training process is GMM-free. In the experimental evaluation we found that the sequence-discriminative flat start training method is not only significantly faster than the straightforward approach of iterative retraining and realignment, but the word error rates attained are slightly better as well."
2157718,21258,9463,Unsupervised and Semi-supervised Learning of Tone and Pitch Accent,2006,"Recognition of tone and intonation is essential for speech recognition and language understanding. However, most approaches to this recognition task have relied upon extensive collections of manually tagged data obtained at substantial time and financial cost. In this paper, we explore two approaches to tone learning with substantially reductions in training data. We employ both unsupervised clustering and semi-supervised learning to recognize pitch accent in English and tones in Mandarin Chinese. In unsupervised Mandarin tone clustering experiments, we achieve 57-87% accuracy on materials ranging from broadcast news to clean lab speech. For English pitch accent in broadcast news materials, results reach 78%. In the semi-supervised framework, we achieve Mandarin tone recognition accuracies ranging from 70% for broadcast news speech to 94% for read speech, outperforming both Support Vector Machines (SVMs) trained on only the labeled data and the 25% most common class assignment level. These results indicate that the intrinsic structure of tone and pitch accent acoustics can be exploited to reduce the need for costly labeled training data for tone learning and recognition."
2765700,21258,9804,Factor analysis for speaker segmentation and improved speaker diarization.,2015,"Speaker diarization includes two steps: speaker segmentation and speaker clustering. Speaker segmentation searches for speaker boundaries, whereas speaker clustering aims at grouping speech segments of the same speaker. In this work, the segmentation is improved by replacing the Bayesian Information Criterion (BIC) with a new iVector-based approach. Unlike BIC-based methods which trigger on any acoustic dissimilarities, the proposed method suppresses phonetic variations and accentuates speaker differences. More specifically our method generates boundaries based on the distance between two speaker factor vectors that are extracted on a frame-by frame basis. The extraction relies on an eigenvoice matrix so that large differences between speaker factor vectors indicate a different speaker. A Mahalanobis-based distance measure, in which the covariance matrix compensates for the remaining and detrimental phonetic variability, is shown to generate accurate boundaries. The detected segments are clustered by a state-of-the-art iVector Probabilistic Linear Discriminant Analysis system. Experiments on the COST278 multilingual broadcast news database show relative reductions of 50% in boundary detection errors. The speaker error rate is reduced by 8% relative."
956137,21258,535,"An investigation of heuristic, manual and statistical pronunciation derivation for Pashto",2011,"In this paper, we study the issue of generating pronunciations for training and decoding with an ASR system for Pashto in the context of a Speech to Speech Translation system developed for TRANSTAC. As with other low resourced languages, a limited amount of acoustic training data was available with a corresponding set of manually produced vowelized pronunciations. We augment this data with other sources, but lack pronunciations for unseen words in the new audio and associated text. Four methods are investigated for generating these pronunciations, or baseforms: an heuristic grapheme to phoneme map, manual annotation, and two methods based on statistical models. The first of these uses a joint Maximum Entropy N-gram model while the other is based on a log-linear Statistical Machine Translation model. We report results on a state of the art, discriminatively trained, ASR system and show that the manual and statistical methods provide an improvement over the grapheme to phoneme map. Moreover, we demonstrate that the automatic statistical methods can perform as well or better than manual generation by native speakers, even in the case where we have a significant number of high quality, manually generated pronunciations beyond those provided by the TRANSTAC program."
90590,21258,9804,CU-Move : Analysis & Corpus Development for Interactive In-Vehicle Speech Systems †,2001,"In this paper, we present our recent work in the analysis and formulation of a new acoustic speech corpus for developing in-vehicle interactive systems for route planning and navigation. The CU-Move Corpus development is partitioned into two phases: [I] acoustic noise collection and analysis across vehicles, and [II] data collection consisting of +1000 speakers from across the United States. We present results from Phase I acoustic noise data analysis across vehicles to determine guidelines for Phase II large-scale data collection using a single vehicle type. A total of 14 noise conditions are identified for analysis across 6 vehicles. We also present our plan for Phase II collection including speakers, dialect regions, data collection hardware, prompts and dialog domains. Since previous studies in speech recognition have shown significant loses in performance when speakers are under task stress, it is important to develop conversational systems that minimize operator stress for the driver. This will be the first U.S. based corpus of its kind consisting of multichannel data, intended for use in developing mixed-initiative dialog speech systems; the initial application being route planning and navigation through a wireless information retrieval sub-system connected to the WWW."
2772655,21258,9804,Building A Naturalistic Emotional Speech Corpus by Retrieving Expressive Behaviors From Existing Speech Corpora,2014,"A key element in affective computing is to have large corpora of genuine emotional samples collected during natural conversations. Recording natural interactions through telephone is an appealing approach to build emotional databases. However, collecting real conversational data with expressive reactions is a challenging task, especially if the recordings are to be shared with the community (e.g., privacy concerns). This study explores a novel approach consisting in retrieving emotional reactions from existing spontaneous speech databases collected for general speech processing problems. Although most of the recordings in these databases are expected to have non-emotional expressions, given the naturalness of the interactions, the flow of the conversation can lead to emotional responses from conversation partners which we aim to retrieve. We use the IEMOCAP and SEMAINE databases to build emotion detector systems. We use these classifiers to identify emotional behaviors from the FISHER database, which is a large conversational speech corpus recorded over the phone. Subjective evaluations over the retrieved samples demonstrate the potential of the proposed scheme to build naturalistic emotional speech database. Index Terms: emotion recognition, expressive speech, information retrieval, emotional databases"
48765,21258,9804,Stochastic Finite State Automata Language Model triggered by Dialogue States,2001,"Within the framework of Natural Spoken Dialogue systems, this paper describes a method for dynamically adapting a Language Model (LM) to the dialogue states detected. This LM combines a standard n-gram model with Stochastic Finite State Automata (SFSAs). During the training process, the sentence corpus used to train the LM is split into several hierarchical clusters in a 2-step process which involves both explicit knowledge and statistical criteria. All the clusters are stored in a binary tree where the whole corpus is attached to the root node. Each level of the tree corresponds to a higher specialization of the sub-corpora attached to the nodes and each node corresponds to a different dialogue state. From the same sentence corpus, SFSAs are extracted in order to model longer contexts than the ones used in the standard n-gram model. A set of SFSAs is attached to each node of the tree as well as a sub-LM which combines a bigram trained on the sub-corpus of the node and the SFSAs selected. A first decoding process calculates a word-graph as well as a first sentence hypothesis. This first hypothesis will be used to find the optimal node in the LM tree. Then, a rescoring process of the word graph using the LM attached to the node selected is performed. By adapting the LM to the dialogue state detected, we show a statistically significant gain in WER on a dialogue corpus collected by France Telecom R&D 1 ."
2696534,21258,9804,Speaker verification based on fusion of acoustic and articulatory information,2013,"We propose a practical, feature-level fusion approach for combining acoustic and articulatory information in speaker verification task. We find that concatenating articulation features obtained from the measured speech production data with conventional Mel-frequency cepstral coefficients (MFCCs) improves the overall speaker verification performance. However, since access to the measured articulatory data is impractical for real world speaker verification applications, we also experiment with estimated articulatory features obtained using acoustic-to-articulatory inversion technique. Specifically, we show that augmenting MFCCs with articulatory features obtained from subject-independent acoustic-to-articulatory inversion technique also significantly enhances the speaker verification performance. This performance boost could be due to the information about inter-speaker variation present in the estimated articulatory features, especially at the mean and variance level. Experimental results on the Wisconsin X-Ray Microbeam database show that the proposed acoustic-estimatedarticulatory fusion approach significantly outperforms the traditional acoustic-only baseline, providing up to 10% relative reduction in Equal Error Rate (EER). We further show that we can achieve an additional 5% relative reduction in EER after score-level fusion. Index Terms: speech production, speaker verification, articulation features, acoustic-to-articulatory inversion, biometrics"
83799,21258,9804,Computer Aided Pronunciation Learning System Using Speech Recognition Techniques,2006,"This paper describes a speech-enabled Computer Aided Pronunciation Learning (CAPL) system HAFSS © . This system was developed for teaching Arabic pronunciations to non-native speakers. A challenging application of HAFSS © is teaching the correct recitation of the holy Qur'an. HAFSS © uses a state of the art speech recognizer to detect errors in user recitation. To increase accuracy of the speech recognizer, only probable pronunciation variants, that cover all common types of recitation errors, are examined by the speech decoder. A module for the automatic generation of pronunciation hypotheses is built as a component of the system. A phoneme duration classification algorithm is implemented to detect recitation errors related to phoneme durations. The decision reached by the recognizer is accompanied by a confidence score to reduce effect of misleading system feedbacks to unpredictable speech inputs. Performance evaluation using a data set that includes 6.6% wrong speech segments showed that the system correctly identified the error in 62.4% of pronunciation errors, reported Repeat Request for 22.4% of the errors and made false acceptance of 14.9% of total errors. Index Terms: Pronunciation error detection, Qur’an recitation"
2649026,21258,9804,All for One: Feature Combination for Highly Channel-Degraded Speech Activity Detection,2013,"Speech activity detection (SAD) on channel transmissions is a critical preprocessing task for speech, speaker and language recognition or for further human analysis. This paper presents a feature combination approach to improve SAD on highly channel degraded speech as part of the Defense Advanced Research Projects Agency’s (DARPA) Robust Automatic Transcription of Speech (RATS) program. The key contribution is the feature combination exploration of different novel SAD features based on pitch and spectro-temporal processing and the standard Mel Frequency Cepstral Coefficients (MFCC) acoustic feature. The SAD features are: (1) a GABOR feature representation, followed by a multilayer perceptron (MLP); (2) a feature that combines multiple voicing features and spectral flux measures (Combo); (3) a feature based on subband autocorrelation (SAcC) and MLP postprocessing and (4) a multiband comb-filter F0 (MBCombF0) voicing measure. We present single, pairwise and all feature combinations, show high error reductions from pairwise feature level combination over the MFCC baseline and show that the best performance is achieved by the combination of all features. Index Terms: speech detection, channel-degraded speech, robust voicing features"
136577,21258,9804,A simulation based parameter optimization for a coarticulation model,2006,"A coarticulation model, namely ‘carrier model’, has been proposed previously by Dang et al. to improve the performance of a physiological articulatory model based speech synthesizer. The carrier model offers a good framework to account for coarticulation in the planning stage, while its parameters need to be refined for improving the performance of the model. This study is to refine the parameters of the carrier model and estimate typical phonetic targets by minimizing the differences between model simulations and observations. A simulation based optimization framework is proposed for this purpose. The framework consists of two layers: obtaining planned targets in a low layer; estimating phonetic targets and optimizing the parameters in a high layer. A direct search method was applied to the low layer due to the non-analytic nature of the articulation model, while the high layer adopts bilevel optimization strategy to decompose the complicated problem into a set of subproblems. A general evaluation was conducted by combining the refined carrier model and the learned phonetic targets together using the physiological articulatory model and the average error between observations and simulations was 0.15 cm over 103 VCV combinations on the jaw, tongue tip and tongue dorsum. Index Terms: speech production, coarticulation, optimization"
2818388,21258,9804,A Metric for Evaluating Speech Recognizer Output based on Human-Perception Model,2015,"Word error rate or character error rate are usually used as the metrics for evaluating the accuracy of speech recognition. These are naturally-defined objective metrics and are helpful for comparing recognition methods fairly. However the overall performance of the recognition systems and the usefulness of the results are not necessarily considered. To address this problem, we study and propose a metric which replicates human-annotated scores using their perception to the recognition results. The features that we use are the numbers of insertion errors, deletion errors, and substitution errors in the characters and the syllables. In addition we studied the numbers of consecutive errors, the misrecognized keywords, and the locations of errors. We created models using linear regression and random forest, predicted human-perceived scores, and compared them with the actual scores using Spearman’s rank-based correlation. According to our experiments the correlation of human perceived scores with character error rates is 0.456, while those with the predicted scores by using a random forest of 10 features is 0.715. The latter is close to the averaged correlation between the scores of the human subjects, 0.765, which suggests that we can predict the human-perceived scores using those features and that we can leverage human perception model for evaluating speech recognition performance. The important factors (features) for the prediction are the numbers of substitution errors and consecutive errors."
2531000,21258,535,Development and portability of ASR and Q&A modules for real-environment speech-oriented guidance systems,2007,"In this paper, we investigate development and portability of ASR and Q&A modules of speech-oriented guidance systems for two different real environments. An initial prototype system has been constructed for a local community center using two years of human-labeled data collected by the system. Collection of real user data is required because ASR task and Q&A domain of a guidance system are defined by the target environment and potential users. However, since human preparation of data is always costly, most often only a relatively small amount real data will be available for system adaptation in practice. Therefore, the portability of the initial prototype system is investigated for a different environment, a local subway station. The purpose is to identify reusable system parts. The ASR module is found to be highly portable across the two environments. However, the portability of the Q&A module was only medium. From an objective analysis it became clear that this is mainly due to the environment-dependent domain differences between the two systems. This implicates that it will always be important to take the behavior of actual users under real conditions into account to build a system with high user satisfaction."
2183925,21258,9463,Automatic acquisition of names using speak and spell mode in spoken dialogue systems,2003,"This paper describes a novel multi-stage recognition procedure for deducing the spelling and pronunciation of an open set of names. The overall goal is the automatic acquisition of unknown words in a human computer conversational system. The names are spoken and spelled in a single utterance, achieving a concise and natural dialogue flow. The first recognition pass extracts letter hypotheses from the spelled part of the waveform and maps them to phonemic hypotheses via a hierarchical sublexical model capable of generating graphemephoneme mappings. A second recognition pass determines the name by combining information from the spoken and spelled part of the waveform, augmented with language model constraints. The procedure is integrated into a spoken dialogue system where users are asked to enroll their names for the first time. The acquisition process is implemented in multiple parallel threads for real-time operation. Subsequent to inducing the spelling and pronunciation of a new name, a series of operations automatically updates the recognition and natural language systems to immediately accommodate the new word. Experiments show promising results for letter and phoneme accuracies on a preliminary dataset."
2733823,21258,9804,Discriminative training of acoustic models for system combination,2013,"In discriminative training methods, the objective function is designed to improve the performance of automatic speech recognition with reference to correct labels using a single system. On the other hand, system combination methods, which output refined hypotheses by a majority voting scheme, need to build multiple systems that generate complementary hypotheses. This paper aims to unify the both requirements within a discriminative training framework based on the mutual information criterion. That is, we construct complementary models by optimizing the proposed objective function, which yields to minimize the mutual information with base systems’ hypotheses, while maximize that with correct labels, at the same time. We also analyze that this scheme corresponds to weight the training data of a complementary system by considering correct and error tendencies in the base systems, which has close relationship with boosting methods. In addition, the proposed method can practically construct complementary systems by simply extending a lattice-based parameter update algorithm in discriminative training, and can adjust the degree of how much the complementary system outputs are different from base system ones. The experiments on highly noisy speech recognition (‘The 2 nd CHiME challenge’) show the effectiveness of the proposed method, compared with a conventional system combina"
329890,21258,9804,Speech/Non-Speech discrimination combining advanced feature extraction and SVM learning,2006,"This paper shows an effective speech/non-speech discrimination method for improving the performance of speech processing systems working in noisy environment. The proposed method uses a trained support vector machine (SVM) that defines an optimized non-linear decision rule over different sets of speech features. Two alternative feature extraction processes based on: i) subband SNR estimation after denoising, and ii) long-term SNR estimation were compared. Both methods show the ability of the SVM-based classifier to learn how the signal is masked by the acoustic noise and to define an effective non-linear decision rule. However, it is shown that a feature vector incorporating contextual information yielded better speech/non-speech discrimination even when no denoising is applied. The experimental analysis carried out on the Spanish SpeechDat-Car database shows clear improvements over standard VADs including ITU G.729, ETSI AMR and ETSI AFE for distributed speech recognition (DSR), and other recently reported VADs. Index Terms: voice activity detection, support vector machine learning, speech enhancement."
214033,21258,9804,CLUSTERGEN: a statistical parametric synthesizer using trajectory modeling.,2006,"Unit selection synthesis has shown itself to be capable of producing high quality natural sounding synthetic speech when constructed from large databases of well-recorded, well-labeled speech. However, the cost in time and expertise of building such voices is still too expensive and specialized to be able to build individual voices for everyone. The quality in unit selection synthesis is directly related to the quality and size of the database used. As we require our speech synthesizers to have more variation, style and emotion, for unit selection synthesis, much larger databases will be required. As an alternative, more recently we have started looking for parametric models for speech synthesis, that are still trained from databases of natural speech but are more robust to errors and allow for better modeling of variation. This paper presents the CLUSTERGEN synthesizer which is implemented within the Festival/FestVox voice building environment. As well as the basic technique, three methods of modeling dynamics in the signal are presented and compared: a simple point model, a basic trajectory model and a trajectory model with overlap and add. Index Terms: speech synthesis, statistical parametric synthesis, trajectory HMMs."
2745029,21258,9804,Adaptation of deep neural network acoustic models using factorised i-vectors.,2014,"Copyright © 2014 ISCA.The use of deep neural networks (DNNs) in a hybrid configuration is becoming increasingly popular and successful for speech recognition. One issue with these systems is how to efficiently adapt them to reflect an individual speaker or noise condition. Recently speaker i-vectors have been successfully used as an additional input feature for unsupervised speaker adaptation. In this work the use of i-vectors for adaptation is extended to incorporate acoustic factorisation. In particular, separate i-vectors are computed to represent speaker and acoustic environment. By ensuring orthogonality between the individual factor representations it is possible to represent a wide range of speaker and environment pairs by simply combining i-vectors from a particular speaker and a particular environment. In this paper the i-vectors are viewed as the weights of a cluster adaptive training (CAT) system, where the underlying models are GMMs rather than HMMs. This allows the factorisation approaches developed for CAT to be directly applied. Initial experiments were conducted on a noise distorted version of the WSJ corpus. Compared to standard speaker-based i-vector adaptation, factorised i-vectors showed performance gains."
552324,21258,8840,Generative Goal-Driven User Simulation for Dialog Management,2012,"User simulation is frequently used to train statistical dialog managers for task-oriented domains. At present, goal-driven simulators (those that have a persistent notion of what they wish to achieve in the dialog) require some task-specific engineering, making them impossible to evaluate intrinsically. Instead, they have been evaluated extrinsically by means of the dialog managers they are intended to train, leading to circularity of argument. In this paper, we propose the first fully generative goal-driven simulator that is fully induced from data, without hand-crafting or goal annotation. Our goals are latent, and take the form of topics in a topic model, clustering together semantically equivalent and phonetically confusable strings, implicitly modelling synonymy and speech recognition noise. We evaluate on two standard dialog resources, the Communicator and Let's Go datasets, and demonstrate that our model has substantially better fit to held out data than competing approaches. We also show that features derived from our model allow significantly greater improvement over a baseline at distinguishing real from randomly permuted dialogs."
2701603,21258,9804,Parameterization of articulatory pattern in speakers with ALS,2014,"A combination of parallel factor analysis (PARAFAC) and principal component analysis (PCA) was used to parameterize the articulatory pattern of tongue, jaw and lip movements in 8 English vowels produced by 7 subjects with amyotrophic lateral sclerosis (ALS). A two-factor PARAFAC model derived an overall articulatory pattern represented by two basic modes dominated by tongue raising and advancement, respectively. The relation between the two articulatory modes and the acoustic formants (F1, F2) followed a simple one-to-one linear mapping. The PCA on the residuals of the PARAFAC model showed various individualized articulatory features superimposed on the overall pattern. These articulatory features contributed in a systematic way to the acoustic deviation across different subjects. The parameterization approach (1) provided a simple and generalizable way to explore the underlying articulatory mechanism of speech decline in ALS and (2) accounted for the articulatory features across affected individuals. With further development of the approach and a comparison with the articulatory pattern for healthy subjects, it is possible to derive a set of quantitative articulatory indicators of speech impairment in ALS. Index Terms: parameterization, articulatory-acoustic mapping, ALS"
1904499,21258,535,DNN acoustic modeling with modular multi-lingual feature extraction networks,2013,"In this work, we propose several deep neural network architectures that are able to leverage data from multiple languages. Modularity is achieved by training networks for extracting high-level features and for estimating phoneme state posteriors separately, and then combining them for decoding in a hybrid DNN/HMM setup. This approach has been shown to achieve superior performance for single-language systems, and here we demonstrate that feature extractors benefit significantly from being trained as multi-lingual networks with shared hidden representations. We also show that existing mono-lingual networks can be re-used in a modular fashion to achieve a similar level of performance without having to train new networks on multi-lingual data. Furthermore, we investigate in extending these architectures to make use of language-specific acoustic features. Evaluations are performed on a low-resource conversational telephone speech transcription task in Vietnamese, while additional data for acoustic model training is provided in Pashto, Tagalog, Turkish, and Cantonese. Improvements of up to 17.4% and 13.8% over mono-lingual GMMs and DNNs, respectively, are obtained."
47269,21258,9804,GESTURAL TRAJECTORY SYMMETRIES AND DISCOURSE SEGMENTATION,2002,"Our approachis motivatedbytheconvictionthat gestureandspeech are coexpressiveof the underlying dynamic ideation that drives human communication. As such, transitions and cohesions is gestural behavior would inform us as to the discourse conceptualization. In this paper, we examine the role of motion symmetries of twohanded gestures in the structuring of speech. We employ a set of hand motion traces extracted from video and compute the correlation of thesetraces. Thesignsandmagnitudesof the correlation coefÞcients computed in the cardinal directions of the subject’s torso (lateral and vertical in this work) characterize the symmetries. We employ a windowed computation approach that permits a balance between temporal resolution and robustnessto noise. The resulting correlation proÞles are merged according to a temporal proximity rule. We apply this analysis to two conversationalvideo sequences. A detailed analysis of the Þrst sequence reveals the persistence of gestural imagery between semantically-similar discourse pieces. A symmetry transition analysis is applied to the second dataset and compared against a manually generated discourse segmentation to demonstrate the potential of cross-modal discourse segmentation."
2278665,21258,9804,A comparative study of glottal open quotient estimation techniques,2013,"The robust and efficient extraction of features related to the glottal excitation source has become increasingly important for speech technology. The glottal open quotient (OQ) is one relevant measurement which is known to significantly vary with changes in voice quality on a breathy to tense continuum. The extraction of OQ, however, is hampered in the time-domain by the difficulty in consistently locating the point of glottal opening as well the computational load of its measurement. Determining OQ correlates in the frequency domain is an attractive alternative, however the lower frequencies of glottal source spectrum are also affected by other aspects of the glottal pulse shape thereby precluding closed-form solutions and straightforward mappings. The present study provides a comparison of three OQ estimation methods and shows a new method based on spectral features and artificial neural networks to outperform existing methods in terms of discrimination of voice quality, lower error values on a large volume of speech data and dramatically reduced computation time. Index Terms: Open Quotient, Glottal source, Voice quality, Artificial Neural Networks"
1445790,21258,535,Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings,2013,"Measures of acoustic similarity between words or other units are critical for segmental exemplar-based acoustic models, spoken term discovery, and query-by-example search. Dynamic time warping (DTW) alignment cost has been the most commonly used measure, but it has well-known inadequacies. Some recently proposed alternatives require large amounts of training data. In the interest of finding more efficient, accurate, and low-resource alternatives, we consider the problem of embedding speech segments of arbitrary length into fixed-dimensional spaces in which simple distances (such as cosine or Euclidean) serve as a proxy for linguistically meaningful (phonetic, lexical, etc.) dissimilarities. Such embeddings would enable efficient audio indexing and permit application of standard distance learning techniques to segmental acoustic modeling. In this paper, we explore several supervised and unsupervised approaches to this problem and evaluate them on an acoustic word discrimination task. We identify several embedding algorithms that match or improve upon the DTW baseline in low-resource settings."
2835680,21258,9804,Bayesian Modeling in Speech Motor Control: A Principled Structure for the Integration of Various Constraints,2016,"The remarkable capacity of the speech motor system to adapt to various speech conditions is due to an excess of degrees of freedom, which enables producing similar acoustical properties with different sets of control strategies. To explain how the Central Nervous System (CNS) selects one of the possible strategies , a common approach in the literature, in line with optimal control theory, is to model speech motor planning as the solution of an optimality problem based on cost functions. Despite the success of this approach, one of its drawbacks is the intrinsic contradiction between the concept of optimality and the observed experimental intra-speaker token-to-token variability. The present paper adopts an alternative approach that formulates feedforward optimal control in a probabilistic Bayesian modeling framework. The framework is shown to be well adapted to the combination of constraints of different physical origins. The integration of force constraints adapted to the control of articulation clarity is taken as an illustration of this feature. Pertinence of the results are illustrated by controlling a biomechanical model of the vocal tract for speech production."
2683153,21258,9804,Imitation interacts with one's second-language phonology but it does not operate cross-linguistically,2013,"This study explored effects of simultaneous use of late bilinguals’ languages on their second-language (L2) pronunciation. We tested (1) if bilinguals effectively inhibit the first language (L1) when simultaneously processing L1 and L2, (2) if bilinguals, like natives, imitate subphonemic variation, (3) if bilinguals’ imitation operates crosslinguistically, and (4) if imitation interacts with phonological structure. Sixteen L1-Czech L2-English speakers heard stimuli with two factors manipulated: language (Czech, English) and Voice Onset Time (VOT) in /p, t, k/ (short, long). They subsequently pronounced English /t/- and /d/-initial words. Speakers’ VOTs in the Czech-Short-VOT, Czech-ExtendedVOT, and English-Reduced-VOT conditions were comparable, but VOTs were more English-like after exposure to English-Long-VOT, which applied to both /t/ and /d/. The conclusions are as follows. (1) Bilinguals’ potentially ineffective L1 inhibition did not affect their L2 production, since exposure to Czech did not lead to VOT reduction. (2) Imitation is not limited to native speech, since bilinguals increased their VOTs following exposure to English-LongVOT. (3) Imitation did not operate cross-linguistically, since bilinguals’ English productions following Czech-Short-VOT and Czech-Extended-VOT did not differ. Finally, (4) imitation does interact with phonology, since exposure to English longVOT /t/ resulted in a reduction in prevoicing of its voiced counterpart, /d/. Index Terms: phonetic imitation, L1 inhibition, bilinguals"
1113279,21258,535,Effective pseudo-relevance feedback for language modeling in speech recognition,2013,"A part and parcel of any automatic speech recognition (ASR) system is language modeling (LM), which helps to constrain the acoustic analysis, guide the search through multiple candidate word strings, and quantify the acceptability of the final output hypothesis given an input utterance. Despite the fact that the n-gram model remains the predominant one, a number of novel and ingenious LM methods have been developed to complement or be used in place of the n-gram model. A more recent line of research is to leverage information cues gleaned from pseudo-relevance feedback (PRF) to derive an utterance-regularized language model for complementing the n-gram model. This paper presents a continuation of this general line of research and its main contribution is two-fold. First, we explore an alternative and more efficient formulation to construct such an utterance-regularized language model for ASR. Second, the utilities of various utterance-regularized language models are analyzed and compared extensively. Empirical experiments on a large vocabulary continuous speech recognition (LVCSR) task demonstrate that our proposed language models can offer substantial improvements over the baseline n-gram system, and achieve performance competitive to, or better than, some state-of-the-art language models."
2513806,21258,9804,Real-Time Integration of Dynamic Context Information for Improving Automatic Speech Recognition,2015,"The use of prior situational/contextual knowledge about a given#R##N#task can significantly improve automatic speech recognition#R##N#(ASR) performance. This is typically done through adaptation#R##N#of acoustic or language models if data is available or using#R##N#knowledge-based rescoring. The main adaptation techniques,#R##N#however, are either domain-specific, which makes them inadequate#R##N#for other tasks, or static and offline, and therefore cannot#R##N#deal with dynamic knowledge. To circumvent this problem,#R##N#we propose a real-time system which dynamically integrates#R##N#situational context into ASR. The context integration is done#R##N#either post-recognition, in which case a weighted Levenshtein#R##N#distance between the ASR hypotheses and the context information#R##N#based on the ASR confidence scores is proposed to extract#R##N#the most likely sequence of spoken words, or pre-recognition,#R##N#where the search space is adjusted to the new situational knowledge#R##N#through adaptation of the finite state machine modeling#R##N#the spoken language. Experiments conducted on 3 hours of#R##N#Air Traffic Control (ATC) data achieved a 51% reduction of#R##N#the Command Error Rate (CmdER) which is used as evaluation#R##N#metric in the ATC domain."
337843,21258,235,Predicting word pronunciation in Japanese,2011,"This paper addresses the problem of predicting the pronunciation of Japanese words, especially those that are newly created and therefore not in the dictionary. This is an important task for many applications including text-to-speech and text input method, and is also challenging, because Japanese kanji (ideographic) characters typically have multiple possible pronunciations. We approach this problem by considering it as a simplified machine translation/transliteration task, and propose a solution that takes advantage of the recent technologies developed for machine translation and transliteration research. More specifically, we divide the problem into two subtasks: (1) Discovering the pronunciation of new words or those words that are difficult to pronounce by mining unannotated text, much like the creation of a bilingual dictionary using the web; (2) Building a decoder for the task of pronunciation prediction, for which we apply the state-of-the-art discriminative substring-based approach. Our experimental results show that our classifier for validating the word-pronunciation pairs harvested from unannotated text achieves over 98% precision and recall. On the pronunciation prediction task of unseen words, our decoder achieves over 70% accuracy, which significantly improves over the previously proposed models."
1835471,21258,9804,Cluster-based polynomial-fit histogram equalization (CPHEQ) for robust speech recognition,2007,"Noise robustness is one of the primary challenges facing most automatic speech recognition (ASR) systems. A vast amount of research efforts on preventing the degradation of ASR performance under various noisy environments have been made during the past several years. In this paper, we consider the use of histogram equalization (HEQ) for robust ASR. In contrast to conventional methods, a novel data fitting method based on polynomial regression was presented to efficiently approximate the inverse of the cumulative density functions of speech feature vectors for HEQ. Moreover, a more elaborate attempt of using such polynomial regression models to directly characterizing the relationship between the speech feature vectors and their corresponding probability distributions, under various noise conditions, was proposed as well. All experiments were carried out on the Aurora-2 database and task. The performance of the presented methods were extensively tested and verified by comparison with the other methods. Experimental results shown that for cleancondition training, our method achieved a considerable word error rate reduction over the baseline system, and also significantly outperformed the other methods. Index Terms: noise robustness, speech recognition, histogram equalization, polynomial regression model"
210867,21258,9804,ACCURATE VOCAL EVENT DETECTION METHOD BASED ON A FIXED-POINT ANALYSIS OF MAPPING FROM TIME TO WEIGHTED AVERAGE GROUP DELAY,2000,A new procedure for event detection and characterization is proposed based on group delay and fixed point analysis. This method enables the detection of precise timing and spread of speech events such as a vocal fold closure. A mapping from the center of a Gaussian time window to the mean time provides event locations as its fixed points. Refining these initial estimates using minimum phase group delay functions derived from the amplitude spectra provides accurate estimates of event locations and durations of excitations of each event. The proposed algorithm was tested using synthetic speech samples and natural speech database of simultaneously recorded sound waveforms and EGG signals. These tests revealed that the proposed method provides estimates of vocal fold closure instants with timing accuracy within 60 µ st o 210µs standard deviations. This algorithm is implemented to be suitable for real-time operation by making extensive use of FFTs without introducing any iterative procedures. It is potentially a very powerful tool for speech diagnosis and construction of very high quality speech manipulation systems.
2652285,21258,9804,Articulatory settings facilitate mechanically advantageous motor control of vocal tract articulators,2013,"It was recently shown that vocal tract postures assumed during pauses in read speech are significantly different from those assumed at absolute rest. This paper examines whether the former category of “articulatory settings” are more mechanically advantageous than absolute rest postures with respect to speech articulation. Appropriate task and articulator variables are extracted from real-time Magnetic Resonance Imaging (rtMRI) data of five speakers reading aloud. Locally-weighted regression is then used to calculate Jacobian matrices representing the transformation between articulatory task velocities and postural velocities. A measure of mechanical advantage is proposed based on the obtained Jacobian. Speech-ready postures and postures during inter-speech pauses are observed to be significantly more mechanically advantageous as compared to rest postures. Furthermore, other postures, such as those that occur during the production of different vowels and consonants, are shown to have mechanical advantages that lie in between this continuum. These results could provide insights into understanding postural motor control and other linguistic phenomena, such as sonority hierarchies, in speech production. Index Terms: speech production, real-time MRI, articulatory setting, postural motor control, task dynamics, forward kinematics, vocal tract shaping."
2505192,21258,9804,Generative and Discriminative Algorithms for Spoken Language Understanding,2007,"Spoken Language Understanding (SLU) for conversational systems (SDS) aims at extracting concept and their relations from spontaneous speech. Previous approaches to SLU have modeled concept relations as stochastic semantic networks ranging from generative approach to discriminative. As spoken dialog systems complexity increases, SLU needs to perform understanding based on a richer set of features ranging from a-priori knowledge, long dependency, dialog history, system belief, etc. This paper studies generative and discriminative approaches to modeling the sentence segmentation and concept labeling. We evaluate algorithms based on Finite State Transducers (FST) as well as discriminative algorithms based on Support Vector Machine sequence classifier based and Conditional Random Fields (CRF). We compare them in terms of concept accuracy, generalization and robustness to annotation ambiguities. We also show how non-local non-lexical features (e.g. a-priori knowledge) can be modeled with CRF which is the best performing algorithm across tasks. The evaluation is carried out on two SLU tasks of different complexity, namely ATIS and MEDIA corpora."
2410278,21258,9804,Complementary approaches for voice disorder assessment,2007,"This paper describes two comparative studies of voice quality assessment based on complementary approaches. The first study was undertaken on 449 speakers (including 391 dysphonic patients) whose voice quality was evaluated in parallel by a perceptual judgment and objective measurements on acoustic and aerodynamic data. Results showed that a nonlinear combination of 7 parameters allowed the classification of 82% voice samples in the same grade as the jury. The second study relates to the adaptation of Automatic Speaker Recognition (ASR) techniques to pathological voice assessment. The system designed for this particular task relies on a GMM based approach, which is the state-of-the-art for ASR. Experiments conducted on 80 female voices provide promising results, underlining the interest of such an approach. We benefit from the multiplicity of theses techniques to evaluate the methodological situation which points fundamental differences between these complementary approaches (bottom-up vs. top-down, global vs. analytic). We also discuss some theoretical aspects about relationship between acoustic measurement and perceptual mechanisms which are often forgotten in the performance race."
1919235,21258,535,Automatic speech recognition based on weighted minimum classification error (W-MCE) training method,2007,"The Bayes decision theory is the foundation of the classical statistical pattern recognition approach. For most of pattern recognition problems, the Bayes decision theory is employed assuming that the system performance metric is defined as the simple error counting, which assigns identical cost to each recognition error. However, this prevalent performance metric is not desirable in many practical applications. For example, the cost of recognition error is required to be differentiated in keyword spotting systems. In this paper, we propose an extended framework for the speech recognition problem with non-uniform classification/recognition error cost. As the system performance metric, the recognition error is weighted based on the task objective. The Bayes decision theory is employed according to this performance metric and the decision rule with a non-uniform error cost function is derived. We argue that the minimum classification error (MCE) method, after appropriate generalization, is the most suitable training algorithm for the optimal classifier design to minimize the weighted error rate. We formulate the weighted MCE (W-MCE) algorithm based on the conventional MCE infrastructure by integrating the error cost and the recognition error count into one objective function. In the context of automatic speech recognition (ASR), we present a variety of training scenarios and weighting strategies under this extended framework. The experimental demonstration for large vocabulary continuous speech recognition is provided to support the effectiveness of our approach."
2275031,21258,9804,An Effective and Efficient Utterance Verification Technology Using Word N-gram Filler Models,2006,"In this paper we propose a novel, effective, and efficient utterance verification (UV) technology for access control in the interactive voice response (IVR) systems. The key of our approach is to construct a context-free grammar by using the secret answer to a question and a word N-gram based filler model. The N-gram filler provides rich alternatives to the secret answer and can potentially improve the accuracy of the UV task. It can also absorb carrier words used by callers and thus can improve the robustness. We also propose using a predictor based on the best alternative to calculate the confidence. We show detailed experimental results on a tough UV test set that contains 930 positive and 930 negative cases and discuss types of questions that are suitable for the UV task. We demonstrate that our approach can achieve a 2.14% equal error rate (EER) on average and 0.8% false accept rate if the false reject rate is 2.6% and above. This is a 49% EER reduction compared w ith the approaches using acoustic fillers, and a 72% EER reduction compared with the posterior probability based confidence measurement. Index Terms: utterance verification, filler model, w ord spotting, confidence measure"
2715147,21258,9804,A digital signal processor implementation of silent/electrolaryngeal speech enhancement based on real-time statistical voice conversion.,2013,"In this paper, we present a digital signal processor (DSP) implementation of real-time statistical voice conversion (VC) for silent speech enhancement and electrolaryngeal speech enhancement. As a silent speech interface, we focus on nonaudible murmur (NAM), which can be used in situations where audible speech is not acceptable. Electrolaryngeal speech is one of the typical types of alaryngeal speech produced by an alternative speaking method for laryngectomees. However, the sound quality of NAM and electrolaryngeal speech suffers from lack of naturalness. VC has proven to be one of the promising approaches to address this problem, and it has been successfully implemented on devices with sufficient computational resources. An implementation on devices that are highly portable but have limited computational resources would greatly contribute to its practical use. In this paper we further implement real-time VC on a DSP. To implement the two speech enhancement systems based on real-time VC, one from NAM to a whispered voice and the other from electrolaryngeal speech to a natural voice, we propose several methods for reducing computational cost while preserving conversion accuracy. We conduct experimental evaluations and show that real-time VC is capable of running on a DSP with little degradation. Index Terms: statistical voice conversion, real-time processing, reduction of computational cost, DSP, non-audible murmur, electrolaryngeal speech"
367447,21258,9804,Developing and Enhancing Posterior Based Speech Recognition Systems,2005,"Local state or phone posterior probabilities are often investigated as local scores (e.g., hybrid HMM/ANN systems) or as transformed acoustic features (e.g., ``Tandem'') to improve speech recogni tion systems. In this paper, we present initial results towards boosting these approaches by improving posterior estimat es, using acoustic context (e.g., as available in the whole utterance), as well as possible prior information (such as topological constraints). In the present work, the enhanced posterior distribution is associated with the ``gamma'' distribution typically used in standard HMMs training, and estimated from local likelihoods (GMM) or local posteriors (ANN). This approach results in a family of new HMM based systems, where only posterior probabilities are used, while also providing a new, principled, approach towards a hierarchical use/integration of these posteriors, from the frame level up to the phone and word levels, and integrating the appropriate context and prior knowledge in each level. In the present work, we used the resulting posteriors as local scores in a Viter bi decoder. On the OGI Numbers'95 database, this resulted in improved recognition performance, compared to a state-of-the-art hybrid HMM/ANN system."
1890843,21258,535,Topic-based speaker recognition for German parliamentary speeches,2009,"In the last decade, high-level features for speaker recognition have become a research focus, as they are believed to alleviate the weak point of the classical spectral/cepstral-feature-based approaches: mismatch in acoustic conditions or channel between training and test data. Identification cues such as prosody, pronunciation, and idiolect have been successfully investigated. Semantic speaker recognition, such as identifying people by the topics they frequently talk about, has not found an equal amount of attention. However, it is a promising approach, especially for broadcast data and multimedia archives, where prominent speakers can be expected to often talk about their specific subjects. This paper reports on our experiments with topic-based speaker recognition on German parliamentary speeches. Text transcripts of speeches of federal ministers were used to train speaker models based on word frequencies. For recognition, these models were applied to automatic speech recognition transcripts of parliamentary speeches and could identify the correct speaker surprisingly well, with an EER of 13.8%. Fusing this approach with a classical GMM-UBM system (with EER 14.3%) yields an improved EER of 8.6%."
2113507,21258,9804,Sequence-discriminative training of deep neural networks,2013,"Sequence-discriminative training of deep neural networks (DNNs) is investigated on a standard 300 hour American En- glish conversational telephone speech task. Different sequence- discriminative criteria — maximum mutual information (MMI), minimum phone error (MPE), state-level minimum Bayes risk (sMBR), and boosted MMI — are compared. Two different heuristics are investigated to improve the performance of the DNNs trained using sequence-based criteria — lattices are re- generated after the first iteration of training; and, for MMI and BMMI, the frames where the numerator and denominator hy- potheses are disjoint are removed from the gradient compu- tation. Starting from a competitive DNN baseline trained us- ing cross-entropy, different sequence-discriminative criteria are shown to lower word error rates by 7-9% relative, on aver- age. Little difference is noticed between the different sequence- based criteria that are investigated. The experiments are done using the open-source Kaldi toolkit, which makes it possible for the wider community to reproduce these results. Index Terms: speech recognition, deep learning, sequence- criterion training, neural networks, reproducible research"
2696255,21258,9804,Linking Loudness Increases in Normal and Lombard Speech to Decreasing Vowel Formant Separation,2013,"The increased vocal effort associated with the Lombard reflex produces speech that is perceived as louder and judged to be more intelligible in noise than normal speech. Previous work illustrates that, on average, Lombard increases in loudness result from boosting spectral energy in a frequency band spanning the range of formants F1-F3, particularly for voiced speech. Observing additionally that increases inloudness across spoken sentences are spectro-temporally localized, the goal of this work is to further isolate these regions of maximal loudness by linking them to specific formant trends, explicitly considering here the vowel formant separation. For both normal and Lombard speech, this work illustrates that, as loudness increases in frequency bands containing formants (e.g. F1-F2 or F2-F3), the observed separation between formant frequencies decreases. From a production standpoint, these results seem to highlight a physiological trait associated with how humans increase the loudness of their speech, namely moving vocal tract resonances closer together. Particularly, for Lombard speech, this phenomena is exaggerated: that is, the Lombard speech is louder and formants in corresponding spectro-temporal regions are even closer together. Index Terms: Lombard Effect, Loudness, Vowel Formant Separation"
2814100,21258,9804,Text-informed speech enhancement with deep neural networks,2015,"A speech signal captured by a distant microphone is generally contaminated by background noise, which severely degrades the audible quality and intelligibility of the observed signal. To resolve this issue, speech enhancement has been intensively studied. In this paper, we consider a text-informed speech enhancement, where the enhancement process is guided by the corresponding text information, i.e., a correct transcription of the target utterance. The proposed deep neural network (DNN)based framework is motivated by the recent success in the textto-speech (TTS) research in employing DNN as well as high audible-quality output signal of the corpus-based speech enhancement which borrows knowledge from the TTS research field. Taking advantage of the nature of DNN that allows us to utilize disparate features in an inference stage, the proposed method infers the clean speech features by jointly using the observed signal and widely-used TTS features derived from the corresponding text. In this paper, we first introduce the background and the details of the proposed method. Then, we show how the text information can be naturally integrated into speech enhancement by utilizing DNN and improve the enhancement performance. Index Terms: speech enhancement, text-to-speech, deep neural network"
1047196,21258,535,Gain estimation approaches in catalog-based single-channel speech-music separation,2011,"In this study, we analyze the gain estimation problem of the catalog-based single-channel speech-music separation method, which we proposed previously. In the proposed method, assuming that we know a catalog of the background music, we developed a generative model for the superposed speech and music spectrograms. We represent the speech spectrogram by a Non-Negative Matrix Factorization (NMF) model and the music spectrogram by a conditional Poisson Mixture Model (PMM). In this model, we assume that the background music is generated by repeating and changing the gain of the jingle in the music catalog. Although the separation performance of the proposed method is satisfactory with known gain values, the performance decreases when the gain value of the jingle is unknown and has to be estimated. In this paper, we address the gain estimation problem of the catalog-based method and propose three different approaches to overcome this problem. One of these approaches is to use Gamma Markov Chain (GMC) probabilistic structure to impose the correlation between the gain parameters across the time frames. By using GMC, the gain parameter is estimated more accurately. The other approaches are maximum a posteriori (MAP) and piece-wise constant estimation (PCE) of the gain values. Although all three methods improve the separation performance as compared to the original method itself, GMC approach achieved the best performance."
2888193,21258,9804,Combining feature and model-based adaptation of RNNLMs for multi-genre broadcast speech recognition,2016,"Recurrent neural network language models (RNNLMs) have consistently outperformed n-gram language models when used in automatic speech recognition (ASR). This is because RNNLMs provide robust parameter estimation through the use of a continuous-space representation of words, and can generally model longer context dependencies than n-grams. The adaptation of RNNLMs to new domains remains an active research area and the two main approaches are: feature-based adaptation, where the input to the RNNLM is augmented with auxiliary features; and model-based adaptation, which includes model fine-tuning and introduction of adaptation layer(s) in the network. This paper explores the properties of both types of adaptation on multi-genre broadcast speech recognition. Two hybrid adaptation techniques are proposed, namely the finetuning of feature-based RNNLMs and the use of a feature-based adaptation layer. A method for the semi-supervised adaptation of RNNLMs, using topic model-based genre classification, is also presented and investigated. The gains obtained with RNNLM adaptation on a system trained on 700h. of speech are consistent using both RNNLMs trained on a small (10Mwords) and large set (660M words), with 10% perplexity and 2% word error rate improvements on a 28:3h. test set."
55922,21258,9804,Building An Integrated Prosodic Model of German,2001,"The intellegibility and naturalness of synthetic speech strongly depends on its prosodic quality. Departing from works by Mixdorff on a linguistically motivated model of German intonation based on the Fujisaki model, the current paper presents statistical results concerning the relationship between linguistic and phonetic information underlying an utterance and its prosodic features. Statistical analysis yields, inter alia, the following pairs of strongest single factor → prosodic feature: boundary depth (right) → syllable duration; boundary depth (left) → phrase command magnitude Ap; accent type (intoneme) → accent command amplitude Aa. These results were employed for training an FFNN-based integrated prosodic model predicting syllable durations along with syllable-aligned Fujisaki control parameters. Correlations between trained and predicted parameters suggest synergy effects, as they are higher for some parameters than correlations yielded when predicting parameters individually from the same set of input features using a regression model. Informal listening tests with first resynthesis examples showed encouraging results."
3008431,21258,23735,One DoF robotic hand that makes human laugh by tickling through rubbing underarm,2016,"This paper describes the development of one DoF robotic hand that makes human laugh by tickling through rubbing underarm. Laughter is attracting research attention because it enhances health by treating or preventing mental diseases. However, laughter has not been used effectively in healthcare because the mechanism of laughter is complicated and is yet to be fully understood. The development of a robot capable of making humans laugh is useful for clarifying the mechanism of laughter because the stimuli by the robot is quantitative and reproductive. Especially, tickling matches to this purpose because the relationship between stimuli and reaction is simpler compared to other techniques. Therefore, this research aimed to develop a robotic hand that can output quantitative and reproductive tickling stimuli for clarifying the mechanism of laughter. Rubbing underarm is selected as a target motion of robot because previous research suggested that this is the best way for making humans feel ticklish. In order to achieve the tickling motion by robots as humans, the required specifications were determined through experimental method. In order to develop a robot that achieves the required fingertip trajectory by simple mechanisms as much as possible, mechanism with crank and link driven by single motor was developed. The result of experimental evaluation shows that the developed robot could make humans laugh by its rubbing motion. In addition, the quantitative tickling motion by developed robotic hand was suggested to be effective for clarifying the mechanism of laughter."
2400001,21258,9804,Statistical multi-stream modeling of real-time MRI articulatory speech data.,2010,"This paper investigates different statistical modeling frameworks for articulatory speech data obtained using real-time (RT) magnetic resonance imaging (MRI). To quantitatively capture the spatio-temporal shaping process of the human vocal tract during speech production a multi-dimensional stream of direct image features is extracted automatically from the MRI recordings. The features are closely related, though not identical, to the tract variables commonly defined in the articulatory phonology theory. The modeling of the shaping process aims at decomposing the articulatory data streams into primitives by segmentation. A variety of approaches are investigated for carrying out the segmentation task including vector quantizers, Gaussian Mixture Models, Hidden Markov Models, and a coupled Hidden Markov Model. We evaluate the performance of the different segmentation schemes qualitatively with the help of a well understood data set which was used in an earlier study of inter-articulatory timing phenomena of American English nasal sounds. Index Terms: speech production, articulatory modeling, realtime magnetic resonance imaging"
2716461,21258,9463,Towards Empathetic Human-Robot Interactions,2016,"Since the late 1990s when speech companies began providing their customer-service software in the market, people have gotten used to speaking to machines. As people interact more often with voice and gesture controlled machines, they expect the machines to recognize different emotions, and understand other high level communication features such as humor, sarcasm and intention. In order to make such communication possible, the machines need an empathy module in them which can extract emotions from human speech and behavior and can decide the correct response of the robot. Although research on empathetic robots is still in the early stage, we described our approach using signal processing techniques, sentiment analysis and machine learning algorithms to make robots that can understand human emotion. We propose Zara the Supergirl as a prototype system of empathetic robots. It is a software based virtual android, with an animated cartoon character to present itself on the screen. She will get smarter and more empathetic through its deep learning algorithms, and by gathering more data and learning from it. In this paper, we present our work so far in the areas of deep learning of emotion and sentiment recognition, as well as humor recognition. We hope to explore the future direction of android development and how it can help improve people's lives."
2611578,21258,344,Designing Language Technology Applications: A Wizard of Oz Driven Prototyping Framework,2014,"Wizard of Oz (WOZ) prototyping employs a human wizard to simulate anticipated functions of a future system. In Natural Language Processing this method is usually used to obtain early feedback on dialogue designs, to collect language corpora, or to explore interaction strategies. Yet, existing tools often require complex client-server configurations and setup routines, or suffer from compatibility problems with different platforms. Integrated solutions, which may also be used by designers and researchers without technical background, are missing. In this paper we present a framework for multi-lingual dialog research, which combines speech recognition and synthesis with WOZ. All components are open source and adaptable to different application scenarios."
2704060,21258,535,Uncertainty estimation of DNN classifiers,2015,"New efficient measures for estimating uncertainty of deep neural network (DNN) classifiers are proposed and successfully applied to multistream-based unsupervised adaptation of ASR systems to address uncertainty derived from noise. The proposed measure is the error from associative memory models trained on outputs of a DNN. In the present study, an attempt is made to use autoencoders for remembering the property of data. Another measure proposed is an extension of the M-measure, which computes the divergences of probability estimates spaced at specific time intervals. The extended measure results in an improved reliability by considering the latent information of phoneme duration. Experimental comparisons carried out in a multistream-based ASR paradigm demonstrates that the proposed measures yielded improvements over the multistyle trained system and system selected based on existing measures. Fusion of the proposed measures achieved almost the same performance as the oracle system selection."
3245356,21258,9616,A new efficient measure for accuracy prediction and its application to multistream-based unsupervised adaptation,2016,"A new efficient measure for predicting estimation accuracy is proposed and successfully applied to multistream-based unsupervised adaptation of ASR systems to address data uncertainty when the ground-truth is unknown. The proposed measure is an extension of the M-measure, which predicts confidence in the output of a probability estimator by measuring the divergences of probability estimates spaced at specific time intervals. In this study, the M-measure was extended by considering the latent phoneme information, resulting in an improved reliability. Experimental comparisons carried out in a multistream-based ASR paradigm demonstrated that the extended M-measure yields a significant improvement over the original M-measure, especially under narrow-band noise conditions."
2619406,21258,235,Towards multimodal modeling of physicians' diagnostic confidence and self-awareness using medical narratives,2014,"Misdiagnosis is a problem in the medical field, often related to physicians’ cognitive errors. Overconfidence is considered a major cause of such errors. Intelligent diagnostic support systems could benefit from understanding how aware physicians are of their performance when they estimate their confidence in a diagnosis (i.e. a physician’s diagnostic self-awareness). Shedding light on the cognitive processes related to such awareness could also help improve medical education. We use a multimodal dataset of medical narratives to computationally model diagnostic confidence and self-awareness based on physicians’ linguistic and eye movement behaviors. Dermatologists viewed images of cutaneous conditions, providing a description, diagnosis, and certainty level for each image case, while their speech and eye movements were recorded. We define both a generalized and a personalized approach to binning confidence levels, used in classification experiments. We also introduce truly multimodal features, which focus on combining linguistic and eye movement data into multimodal attributes. Results indicate that combinations of multiple modalities can outperform their constituent modalities in isolation for these problems."
2167701,21258,535,Three-layer optimizations for fast GMM computations on GPU-like parallel processors,2009,"In this paper we focus on optimizing compute and memory-bandwidth-intensive GMM computations for low-end, small-form-factor devices running on GPU-like parallel processors. With special emphasis on tackling the memory bandwidth issue that is exacerbated by a lack of CPU-like caches providing temporal locality on GPU-like parallel processors, we propose modifications to three well-known GMM computation reduction techniques. We find considerable locality at the frame, CI-GMM, and mixture layers of GMM compute, and show how it can be extracted by following a chunk-based technique of processing multiple frames for every load of a GMM. On a 1,000- word, command-and-control, continuous-speech task, we are able to achieve compute and memory bandwidth savings of over 60% and 90% respectively, with some degradation in accuracy, when compared to existing GPU-based fast GMM computation techniques."
2704061,21258,535,Sparse non-negative matrix language modeling for geo-annotated query session data,2015,"The paper investigates the impact on query language modeling when using skip-grams within query as well as across queries in a given search session, in conjunction with the geo-annotation available for the query stream data. As modeling tool we use the recently proposed sparse non-negative matrix estimation technique, since it offers the same expressive power as the well-established maximum entropy approach in combining arbitrary context features. Experiments on the google.com query stream show that using session-level and geo-location context we can expect reductions in perplexity of 34% relative over the Kneser-Ney N-gram baseline; when evaluating on the 'local subset of the query stream, the relative reduction in PPL is 51%— more than a bit. Both sources of context information (geo-location, and previous queries in session) are about equally valuable in building a language model for the query stream."
1746659,21258,535,Efficient nearly error-less LVCSR decoding based on incremental forward and backward passes,2013,"We show that most search errors can be identified by aligning the results of a symmetric forward and backward decoding pass. Based on this knowledge, we introduce an efficient high-level decoding architecture which yields virtually no search errors, and requires virtually no manual tuning. We perform an initial forward- and backward decoding with tight initial beams, then we identify search errors, and then we recursively increment the beam sizes and perform new forward and backward decodings for erroneous intervals until no more search errors are detected. Consequently, each utterance and even each single word is decoded with the smallest beam size required to decode it correctly. On all tested systems we achieve an error rate equal or very close to classical decoding with ideally tuned beam size, but unsupervisedly without specific tuning, and at around 2 times faster runtime. An additional speedup by factor 2 can be achieved by decoding the forward and backward pass in separate threads."
1812039,21258,535,Efficient use of overlap information in speaker diarization,2007,"Speaker overlap in meetings is thought to be a significant contributor to error in speaker diarization, but it is not clear if overlaps are problematic for speaker clustering and/or if errors could be addressed by assigning multiple labels in overlap regions. In this paper, we look at these issues experimentally, assuming perfect detection of overlaps, to assess the relative importance of these problems and the potential impact of overlap detection. With our best features, we find that detecting overlaps could potentially improve diarization accuracy by 15% relative, using a simple strategy of assigning speaker labels in overlap regions according to the labels of the neighboring segments. In addition, the use of cross-correlation features with MFCC's reduces the performance gap due to overlaps, so that there is little gain from removing overlapped regions before clustering."
2357730,21258,535,Non-native speech databases,2007,"This paper presents a review of already collected non-native speech databases. Although the number of non-native speech databases is significantly less than the one of common speech databases, there were already a lot of data collection efforts taken at different institutes and companies. Because of the comparably small size of the databases, many of them are not available through the common distributors of speech corpora like ELDA or LDC. This leads to the fact that it is hard to keep an overview of what kind of databases have already been collected, and for what purposes there are still no collections. With this paper we hope to provide a useful resource regarding this issue."
1886291,21258,535,Robust speech recognition using a Small Power Boosting algorithm,2009,"In this paper, we present a noise robustness algorithm called Small Power Boosting (SPB). We observe that in the spectral domain, time-frequency bins with smaller power are more affected by additive noise. The conventional way of handling this problem is estimating the noise from the test utterance and doing normalization or subtraction. In our work, in contrast, we intentionally boost the power of time-frequency bins with small energy for both the training and testing datasets. Since time-frequency bins with small power no longer exist after this power boosting, the spectral distortion between the clean and corrupt test sets becomes reduced. This type of small power boosting is also highly related to physiological nonlinearity. We observe that when small power boosting is done, suitable weighting smoothing becomes highly important. Our experimental results indicate that this simple idea is very helpful for very difficult noisy environments such as corruption by background music."
2722413,21258,535,A study of social-affective communication: Automatic prediction of emotion triggers and responses in television talk shows,2015,"Advancements in spoken language technologies have allowed users to interact with computers in an increasingly natural manner. However, most conversational agents or dialogue systems are yet to consider emotional awareness in interaction. To consider emotion in these situations, social-affective knowledge in conversational agents is essential. In this paper, we present a study of the social-affective process in natural conversation from television talk shows. We analyze occurrences of emotion (emotional responses), and the events that elicit them (emotional triggers). We then utilize our analysis for prediction to model the ability of a dialogue system to decide an action and response in an affective interaction. This knowledge has great potential to incorporate emotion into human-computer interaction. Experiments in two languages, English and Indonesian, show that automatic prediction performance surpasses random guessing accuracy."
2856972,21258,9804,Improved Neural Bag-of-Words Model to Retrieve Out-of-Vocabulary Words in Speech Recognition,2016,"Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speech recognition systems used to process di-achronic audio data. To enable recovery of the PNs missed by the system, relevant OOV PNs can be retrieved by exploiting the semantic context of the spoken content. In this paper, we explore the Neural Bag-of-Words (NBOW) model, proposed previously for text classification, to retrieve relevant OOV PNs. We propose a Neural Bag-of-Weighted-Words (NBOW2) model in which the input embedding layer is augmented with a context anchor layer. This layer learns to assign importance to input words and has the ability to capture (task specific) keywords in a NBOW model. With experiments on French broadcast news videos we show that the NBOW and NBOW2 models outper-form earlier methods based on raw embeddings from LDA and Skip-gram. Combining NBOW with NBOW2 gives faster convergence during training."
2423957,21258,8840,A Cost-Benefit Analysis of Hybrid Phone-Manner Representations for ASR,2005,"In the past decade, several researchers have started reinvestigating the use of sub-phonetic models for lexical representations within automatic speech recognition systems. Lest history repeat itself, it may be instructive to mine the further past for models of lexical representations in the lexical access literature. In this work, we re-evaluate the model of Briscoe (1989), in which a hybrid strategy of lexical representation between phones and manner classes is promoted. While many of Briscoe's assumptions do not match up with current ASR processing models, we show that his conclusions are essentially correct, and that reconsidering this structure for ASR lexica is an appropriate avenue for future ASR research."
2494337,21258,535,Lattice-based Viterbi decoding techniques for speech translation,2007,"We describe a cardinal-synchronous Viterbi decoder for statistical phrase-based machine translation which can operate on general ASR lattices (as opposed to confusion networks). The decoder implements constrained source reordering on the input lattice and makes use of an outbound distortion model to score the possible reorderings. The phrase table, representing the decoding search space, is encoded as a weighted finite state acceptor which is determined and minimized. At a high level, the search proceeds by performing simultaneous transitions in two pairs of automata: (input lattice, phrase table FSM) and (phrase table FSM, target language model). An alternative decoding strategy that we explore is to break the search into two independent subproblems: first, we perform monotone lattice decoding and find the best foreign path through the ASR lattice and then, we decode this path with reordering using standard sentence-based SMT. We report experimental results on several testsets of a large scale Arabic-to-English speech translation task in the context of the global autonomous language exploitation (or GALE) DARPA project. The results indicate that, for monotone search, lattice-based decoding outperforms 1-best decoding whereas for search with reordering, only the second decoding strategy was found to be superior to 1-best decoding. In both cases, the improvements hold only for shallow lattices."
1286721,21258,535,Barge-in effects in Bayesian dialogue act recognition and simulation,2013,"Dialogue act recognition and simulation are traditionally considered separate processes. Here, we argue that both can be fruitfully treated as interleaved processes within the same probabilistic model, leading to a synchronous improvement of performance in both. To demonstrate this, we train multiple Bayes Nets that predict the timing and content of the next user utterance. A specific focus is on providing support for barge-ins. We describe experiments using the Let's Go data that show an improvement in classification accuracy (+5%) in Bayesian dialogue act recognition involving barge-ins using partial context compared to using full context. Our results also indicate that simulated dialogues with user barge-in are more realistic than simulations without barge-in events."
2906891,21258,9078,Machine listening techniques as a complement to video image analysis in forensics,2016,"Video is now one of the major sources of information for forensics. However, video documents can be originating from various recording devices (CCTV, mobile devices, etc.) with inconsistent quality and can sometimes be recorded in challenging light or motion conditions. Therefore, the amount of information that can be extracted relying solely on video image can vary to a great extent. Most of the videos however generally include audio recording as well. Machine listening can then become a valuable complement to video image analysis in challenging scenarios. In this paper, the authors present a brief overview of some machine listening techniques and their application to the analysis of video documents for forensics. The applicability of these techniques to forensics problems is then discussed in the light of machine listening system performances."
1151681,21258,8228,Same but different? — Using speech signal features for comparing conversational VoIP quality studies,2012,"In this paper we demonstrate how speech signal features can be used to detect and explain differences in human to human conversation tests. To this end, we compare the results of two conversational VoIP quality experiments designed to quantify the impact of network delay on perceived speech quality. Both studies followed the same procedures and used the same scenarios, but were conducted in two different labs. Our comparison shows that the two studies, despite having been executed correctly using the same test design, still can produce surprisingly different results regarding the users quality perception on a MOS scale. In this respect, speech signal features extracted from conversation recordings help identifying divergent participant behavior as plausible cause for such differences. Our in-depth analysis reveals how novel parameters developed by us like Intended and Unintended Interruption Rate (IIR, UIR) and the corrected Speaker Alternation Rate SARcorr can be used to successfully determine the extent to which the results of different conversational speech quality studies are directly comparable and thus eligible for pooling, or not."
1979387,21258,535,Dynamic network decoding revisited,2009,"We present a dynamic network decoder capable of using large cross-word context models and large n-gram histories. Our method for constructing the search network is designed to process large cross-word context models very efficiently and we address the optimization of the search network to minimize any overhead during run-time for the dynamic network decoder. The search procedure uses the full LM history for lookahead, and path recombination is done as early as possible. In our systematic comparison to a static FSM based decoder, we find the dynamic decoder can run at comparable speed as the static decoder when large language models are used, while the static decoder performs best for small language models. We discuss the use of very large vocabularies of up to 2.5 million words for both decoding approaches and analyze the effect of weak acoustic models for pruning."
943129,21258,235,Example-based speech intention understanding and its application to in-car spoken dialogue system,2002,"This paper proposes a method of speech intention understanding based on dialogue examples. The method uses a spoken dialogue corpus with intention tags to regard the intention of each input utterance as that of the sentence to which it is the most similar in the corpus. The degree of similarity is calculated according to the degree of correspondence in morphemes and dependencies between sentences, and it is weighted by the dialogue context information. An experiment on inference of utterance intentions using a large-scale in-car spoken dialogue corpus of CIAIR has shown 68.9% accuracy. Furthermore, we have developed a prototype system of in-car spoken dialogue processing for a restaurant retrieval task based on our method, and confirmed the feasiblity of the system."
